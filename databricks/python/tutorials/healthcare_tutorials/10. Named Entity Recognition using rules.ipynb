{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eaf2778-6876-42ff-8055-522af9e56462",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d41360ba-b76c-4e58-a405-e5afc2e4d05b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Named Entity Recognition using rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80da0783-173d-4c3a-a460-2346e8319dfa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP Version : 5.1.0\nSpark NLP_JSL Version : 5.1.0\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=7956323724731612#setting/sparkui/0725-092232-stftp6wi/driver-1959237241390579249\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.139.64.13:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2b1ec25ff0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import sparknlp\n",
    "import sparknlp_jsl\n",
    "from sparknlp.base import *\n",
    "from sparknlp.util import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp.pretrained import ResourceDownloader\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "pd.set_option('display.max_columns', 100)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "\n",
    "print(\"Spark NLP Version :\", sparknlp.version())\n",
    "print(\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8842e508-2214-4d7b-9bdc-e9b6a7c86b8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# How the ContextualParser Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a91df146-4daa-4d8a-b38a-feb242ffe514",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Spark NLP's `ContextualParser` is a licensed annotator that allows users to extract entities from a document based on pattern matching. It provides more functionality than its open-source counterpart `EntityRuler` by allowing users to customize specific characteristics for pattern matching. You're able to find entities using regex rules for full and partial matches, a dictionary with normalizing options and context parameters to take into account things such as token distances. \n",
    "\n",
    "There are 3 components necessary to understand when using the `ContextualParser` annotator:\n",
    "\n",
    "1. `ContextualParser` annotator's parameters\n",
    "2. JSON configuration file\n",
    "3. Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd887907-9dfa-4abb-8e3e-c82983fd3229",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1. ContextualParser Annotator Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd24df8e-f1c3-4c4e-8c9d-6c884d1f5fce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here are all the parameters available to use with the `ContextualParserApproach`:\n",
    "  \n",
    "```\n",
    "contextualParser = ContextualParserApproach() \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"entity\") \\\n",
    "    .setCaseSensitive(True) \\\n",
    "    .setJsonPath(\"context_config.json\") \\\n",
    "    .setPrefixAndSuffixMatch(True) \\\n",
    "    .setCompleteContextMatch(True) \\\n",
    "    .setDictionary(\"dictionary.tsv\", options={\"orientation\":\"vertical\"})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c718f47a-57ab-4114-86e2-0576393c88d3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will dive deeper into the details of each parameter, but here's a quick overview:\n",
    "\n",
    "- `setCaseSensitive`: do you want the matching to be case sensitive (applies to all JSON properties apart from the regex property)\n",
    "- `setJsonPath`: the path to your JSON configuration file\n",
    "- `setPrefixAndSuffixMatch`: do you want to match using both the prefix AND suffix properties from the JSON configuration file\n",
    "- `setCompleteContextMatch`: do you want an exact match of prefix and suffix.\n",
    "- `setDictionary`: the path to your dictionary, used for normalizing entities\n",
    "\n",
    "Let's start by looking at the JSON configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09f273d0-0ce1-46bd-9fac-25521a624608",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2. JSON Configuration File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cacfb1a8-bd20-4282-9ecb-714a56d6337f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here is a fully utilized JSON configuration file.\n",
    "```\n",
    "{\n",
    "  \"entity\": \"Gender\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"regex\": \"girl|boy\",\n",
    "  \"completeMatchRegex\": \"true\",\n",
    "  \"matchScope\": \"token\",\n",
    "  \"prefix\": [\"birth\", \"growing\", \"assessment\"],\n",
    "  \"suffix\": [\"faster\", \"velocities\"],\n",
    "  \"contextLength\": 100,\n",
    "  \"contextException\": [\"slightly\"],\n",
    "  \"exceptionDistance\": 40\n",
    " }\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "588753ee-f3ee-4c13-87fa-2dbb5a2fec83",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.1. Basic Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b756ac85-45fa-44bc-9a92-3206ec7163f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are 5 basic properties you can set in your JSON configuration file:\n",
    "\n",
    "- `entity`\n",
    "- `ruleScope`\n",
    "- `regex`\n",
    "- `completeMatchRegex`\n",
    "- `matchScope`\n",
    "\n",
    "Let's first look at the 3 most essential properties to set:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"entity\": \"Digit\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"regex\": \"\\\\d+\" # Note here: backslashes are escape characters in JSON, so for regex pattern \"\\d+\" we need to write it out as \"\\\\d+\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ba1a560-451d-48e3-971f-d03cff1db265",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here, we're looking for tokens in our text that match the regex: \"`\\d+`\" and assign the \"`Digit`\" entity to those tokens. When `ruleScope` is set to \"`sentence`\", we're looking for a match on each *token* of a **sentence**. You can change it to \"`document`\" to look for a match on each *sentence* of a **document**. The latter is particularly useful when working with multi-word matches, but we'll explore this at a later stage.\n",
    "\n",
    "The next properties to look at are `completeMatchRegex` and `matchScope`. To understand their use case, let's take a look at an example where we're trying to match all digits in our text. \n",
    "\n",
    "Let's say we come across the following string: ***XYZ987***\n",
    "\n",
    "Depending on how we set the `completeMatchRegex` and `matchScope` properties, we'll get the following results:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"entity\": \"Digit\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"regex\": \"\\\\d+\",\n",
    "  \"completeMatchRegex\": \"false\",\n",
    "  \"matchScope\": \"token\"\n",
    "}\n",
    "```\n",
    "\n",
    "`OUTPUT: [XYZ987]`\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "  \"entity\": \"Digit\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"regex\": \"\\\\d+\",  \n",
    "  \"completeMatchRegex\": \"false\",\n",
    "  \"matchScope\": \"sub-token\"\n",
    "}\n",
    "```\n",
    "`OUTPUT: [987]`\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "  \"entity\": \"Digit\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"regex\": \"\\\\d+\",\n",
    "  \"completeMatchRegex\": \"true\"\n",
    "  # matchScope is ignored here\n",
    "}\n",
    "```\n",
    "\n",
    "`OUTPUT: []`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abb8944f-a27e-45b1-bd34-9fabfc15fdac",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "`\"completeMatchRegex\": \"true\"` will only return an output if our string was modified in the following way (to get a complete, exact match): **XYZ 987**\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "  \"entity\": \"Digit\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"regex\": \"\\\\d+\",  \n",
    "  \"completeMatchRegex\": \"true\",\n",
    "  \"matchScope\": \"token\" # Note here: sub-token would return the same output\n",
    "}\n",
    "```\n",
    "`OUTPUT: [987]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30bfe82d-b67b-45ac-9694-d545af7fdf67",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2.2. Context Awareness Properties\n",
    "\n",
    "There are 5 properties related to context awareness:\n",
    "\n",
    "- `contextLength`\n",
    "- `prefix`\n",
    "- `suffix`\n",
    "- `contextException`\n",
    "- `exceptionDistance`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4965211-9748-4609-a862-2d93c0222efd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's look at a similar example. Say we have the following text: ***At birth, the typical boy is growing slightly faster than the typical girl, but growth rates become equal at about seven months.***\n",
    "\n",
    "If we want to match the gender that grows faster at birth, we can start by defining our regex: \"`girl|boy`\"\n",
    "\n",
    "Next, we add a prefix (\"`birth`\") and suffix (\"`faster`\") to ask the parser to match the regex only if the word \"`birth`\" comes before and only if the word \"`faster`\" comes after. Finally, we will need to set the `contextLength` - this is the maximum number of tokens after the prefix and before the suffix that will be searched to find a regex match.\n",
    "\n",
    "Here's what the JSON configuration file would look like:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"entity\": \"Gender\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"regex\": \"girl|boy\",\n",
    "  \"contextLength\": 50,\n",
    "  \"prefix\": [\"birth\"],\n",
    "  \"suffix\": [\"faster\"]\n",
    "}\n",
    "```\n",
    "\n",
    "`OUTPUT: [boy]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05482763-0502-4c79-82ef-eaabee51e4e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If you remember, the annotator has a `setPrefixAndSuffixMatch()` parameter. If you set it to `True`, the previous output would remain as is. However, if you had set it to `False` and used the following JSON configuration:\n",
    "  \n",
    "  ```\n",
    "{\n",
    "  \"entity\": \"Gender\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"regex\": \"girl|boy\",\n",
    "  \"contextLength\": 50,\n",
    "  \"prefix\": [\"birth\"],\n",
    "  \"suffix\": [\"faster\", \"rates\"]\n",
    "}\n",
    "```\n",
    "\n",
    "`OUTPUT: [boy, girl]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2ded002-1d98-4afb-9a09-d0f853a08176",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The parser now takes into account either the prefix OR suffix, only one of the condition has to be fulfilled for a match to count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c309391-6523-422d-98bb-8f6887178f2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If you remember, the annotator has a `setCompleteContextMatch()` parameter. If you set it to `True`, and used the following JSON configuration :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24287499-4d58-428c-bdd7-1b34406c37e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "```\n",
    "{\n",
    "  \"entity\": \"Gender\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"regex\": \"girl|boy\",\n",
    "  \"contextLength\": 50,\n",
    "  \"prefix\": [\"birth\"],\n",
    "  \"suffix\": [\"fast\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b72ad8-1084-4024-b89a-393f715d4ad2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "`OUTPUT: []`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3c4c908-1fdd-41e3-aaa4-de88b9191af2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "However if we set `setCompleteContextMatch()` as `False`, and use the same JSON configuration as above, we get the following output :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5a069f7-cabb-44d7-95cb-302ca6974203",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "`OUTPUT: [boy]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05d2a445-a573-4cc8-aee0-88c719e5f6b5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here's the sentence again: ***At birth, the typical boy is growing slightly faster than the typical girl, but growth rates become equal at about seven months.***\n",
    "\n",
    "The last 2 properties related to context awareness are `contextException` and `exceptionDistance`. This rules out matches based on a given exception:\n",
    "  \n",
    "  ```\n",
    "{\n",
    "  \"entity\": \"Gender\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"regex\": \"girl|boy\",\n",
    "  \"contextLength\": 50,\n",
    "  \"prefix\": [\"birth\"],\n",
    "  \"suffix\": [\"faster\", \"rates\"],\n",
    "  \"contextException\": [\"At\"],\n",
    "  \"exceptionDistance\": 5\n",
    "}\n",
    "```\n",
    "`OUTPUT: [girl]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b9484d-97a0-4bd9-8afd-dd8ed315afe8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here we've asked the parser to ignore a match if the token \"`At`\" is within 5 tokens of the matched regex. This caused the token \"`boy`\" to be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a92b0897-1fef-451c-b01a-e8ae8411db03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3. Dictionary\n",
    "\n",
    "Another key feature of the `ContextualParser` annotator is the use of dictionaries. You can specify a path to a dictionary in `tsv` or `csv` format using the `setDictionary()` parameter. Using a dictionary is a useful when you have a list of exact words that you want the parser to pick up when processing some text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b0586fb-76b8-4529-bb0d-8ce4fd6f3461",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3.1. Orientation\n",
    "\n",
    "The first feature to be aware of when it comes to feeding dictionaries is the format of the dictionaries. The `ContextualParser` annotator will accept dictionaries in the horizontal format and in a vertical format. This is how they would look in practice:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d218b148-d6f3-4495-b1d7-83cece1edbdf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Horizontal:\n",
    "\n",
    "| normalize | word1 | word2 | word3     |\n",
    "|-----------|-------|-------|-----------|\n",
    "| female    | woman | girl  | lady      |\n",
    "| male      | man   | boy   | gentleman |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d533b5a-0fa5-4719-8601-31906cb3865e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Vertical:\n",
    "\n",
    "| female    | normalize |\n",
    "|-----------|-----------|\n",
    "| woman     | word1     |\n",
    "| girl      | word2     |\n",
    "| lady      | word3     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12b093d2-493a-4873-932e-fb560166b434",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As you can see, your dictionary needs to have a `normalize` field that lets the annotator know which entity labels to use, and another field that lets the annotator know a list of words it should be looking to match. Here's how to set the format that your dictionary uses:\n",
    "\n",
    "```\n",
    "contextualParser = ContextualParserApproach() \\\n",
    "    .setDictionary(\"dictionary.tsv\", options={\"orientation\":\"vertical\"}) # default is horizontal\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea364934-2333-472e-b1d2-4b29acdcf04a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3.2. Dictionary-related JSON Properties\n",
    "\n",
    "When working with dictionaries, there are 2 properties in the JSON configuration file to be aware of:\n",
    "\n",
    "- `ruleScope`\n",
    "- `matchScope`\n",
    "\n",
    "This is especially true when you have multi-word entities in your dictionary.\n",
    "\n",
    "Let's take an example of a dictionary that contains a list of cities, sometimes made up of multiple words:\n",
    "\n",
    "| normalize | word1 | word2 | word3     |\n",
    "|-----------|-------|-------|-----------|\n",
    "| City      | New York | Salt Lake City  | Washington      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4814ee2a-f2ff-4c89-a09b-cadd601f3162",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's say we're working with the following text: ***I love New York. Salt Lake City is nice too.***\n",
    "\n",
    "With the following JSON properties, here's what you would get:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"entity\": \"City\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"matchScope\": \"sub-token\",\n",
    "}\n",
    "```\n",
    "`OUTPUT: []`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07ec577-6885-47c7-a422-32915d2ea772",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When `ruleScope` is set to `\"sentence\"`, the annotator attempts to find matches at the token level, parsing through each token in the sentence one by one, looking for a match with the dictionary items. Since `\"New York\"` and `\"Salt Lake City\"` are made up of multiple tokens, the annotator would never find a match from the dictionary. Let's change `ruleScope` to `\"document\"`:\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "  \"entity\": \"City\",\n",
    "  \"ruleScope\": \"document\",\n",
    "  \"matchScope\": \"sub-token\",\n",
    "}\n",
    "```\n",
    "`OUTPUT: [New York, Salt Lake City]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd658fdc-0811-4d0a-ab1c-fd8001b65f47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When `ruleScope` is set to `\"document\"`, the annotator attempts to find matches by parsing through each sentence in the document one by one, looking for a match with the dictionary items. Beware of how you set `matchScope`. Taking the previous example, if we were to set `matchScope` to `\"token\"` instead of `\"sub-token\"`, here's what would happen:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"entity\": \"City\",\n",
    "  \"ruleScope\": \"document\",\n",
    "  \"matchScope\": \"token\"\n",
    "}\n",
    "```\n",
    "`OUTPUT: [I love New York., Salt Lake City is nice too.]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b05f7e6-bfc3-4742-83c0-beacef18a3d2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As you can see, when `ruleScope` is at the document level, if you set your `matchScope` to the token level, the annotator will output each sentence containing the matched entities as individual chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0736c3ee-1d88-4258-bc16-a1f6d6545f50",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3.3. Working with Multi-Word Matches\n",
    "\n",
    "Although not directly related to dictionaries, if we build on top of what we've just seen, there is a use-case that is particularly in demand when working with the `ContextualParser` annotator: finding regex matches for chunks of words that span across multiple tokens. \n",
    "\n",
    "Let's re-iterate how the `ruleScope` property works: when `ruleScope` is set to `\"sentence\"`, we're looking for a match on each token of a sentence. When `ruleScope` is set to `\"document\"`, we're looking for a match on each sentence of a document. \n",
    "\n",
    "So now let's imagine you're parsing through medical documents trying to tag the *Family History* headers in those documents.\n",
    "\n",
    "\n",
    "```\n",
    "{\n",
    "  \"entity\": \"Family History Header\",\n",
    "  \"regex\": \"[f|F]amily\\s+[h|H]istory\",  \n",
    "  \"ruleScope\": \"document\",\n",
    "  \"matchScope\": \"sub-token\"\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "`OUTPUT: [Family History, family history, Family history]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "275403f9-bf8c-4c3c-82a3-0257d4b1cca6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If you had set `ruleScope` to  `\"sentence\"`, here's what would have happened:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"entity\": \"Family History Header\",\n",
    "  \"regex\": \"[f|F]amily\\s+[h|H]istory\",  \n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"matchScope\": \"sub-token\"\n",
    "}\n",
    "```\n",
    "\n",
    "`OUTPUT: []`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15bb49dc-d39c-419b-945e-bc9d8e0430f6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Since Family History is divided into two different tokens, the annotator will never find a match since it's now looking for a match on each token of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97af4c21-2d7e-4500-a0d5-7bcbbdbbc372",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Running a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "746746d1-a4bf-4aee-9cca-e3e9f9136844",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Example 1: Detecting Cities\n",
    "\n",
    "Let's try running through some examples to build on top of what you've learned so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c957c29-bc8c-40d2-90a8-2872880f27fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Here's some sample text\n",
    "sample_text = \"\"\"Peter Parker is a nice guy and lives in New York . Bruce Wayne is also a nice guy and lives in San Antonio and Gotham City .\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a811004f-b7ee-47cf-bc15-28aee89ac414",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">res0: Boolean = true\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">res0: Boolean = true\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs mkdirs file:/dbfs/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d5e0fa8-6c1e-4ff4-9161-639c87a6e496",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: cities.tsv: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to detect cities\n",
    "cities = \"\"\"City\\nNew York\\nGotham City\\nSan Antonio\\nSalt Lake City\"\"\"\n",
    "\n",
    "with open('/dbfs/data/cities.tsv', 'w') as f:\n",
    "    f.write(cities)\n",
    "\n",
    "# Check what dictionary looks like\n",
    "!cat cities.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19dc3df7-b405-423b-88f6-b7aea5429a95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create JSON file\n",
    "cities = {\n",
    "  \"entity\": \"City\",\n",
    "  \"ruleScope\": \"document\", \n",
    "  \"matchScope\":\"sub-token\",\n",
    "  \"completeMatchRegex\": \"false\"\n",
    "} \n",
    "\n",
    "import json\n",
    "with open('/dbfs/data/cities.json', 'w') as f:\n",
    "    json.dump(cities, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f34bef2-9e32-46b6-8388-d4c607b662d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "contextual_parser = ContextualParserApproach() \\\n",
    "    .setInputCols([\"sentence\", \"token\"])\\\n",
    "    .setOutputCol(\"entity\")\\\n",
    "    .setJsonPath(\"/dbfs/data/cities.json\")\\\n",
    "    .setCaseSensitive(True)\\\n",
    "    .setDictionary('file:/dbfs/data/cities.tsv', options={\"orientation\":\"vertical\"})\n",
    "\n",
    "chunk_converter = ChunkConverter() \\\n",
    "    .setInputCols([\"entity\"]) \\\n",
    "    .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "parserPipeline = Pipeline(stages=[\n",
    "        document_assembler, \n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        contextual_parser,\n",
    "        chunk_converter,\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "640638b7-9344-4af3-9b5c-a0c1e3fcb4c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a lightpipeline model\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "parserModel = parserPipeline.fit(empty_data)\n",
    "\n",
    "light_model = LightPipeline(parserModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb4b4ff5-1463-43f7-ae57-5dcf18619ad9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Annotate the sample text\n",
    "annotations = light_model.fullAnnotate(sample_text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1922d8eb-9e16-414b-bcd5-9a523b9343a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Annotation(chunk, 40, 47, New York, {'field': 'City', 'tokenIndex': '9', 'ner_source': 'ner_chunk', 'normalized': 'City', 'confidenceValue': '0.50', 'entity': 'City', 'sentence': '0'}, []),\n",
       " Annotation(chunk, 95, 105, San Antonio, {'field': 'City', 'tokenIndex': '10', 'ner_source': 'ner_chunk', 'normalized': 'City', 'confidenceValue': '0.50', 'entity': 'City', 'sentence': '1'}, []),\n",
       " Annotation(chunk, 111, 121, Gotham City, {'field': 'City', 'tokenIndex': '13', 'ner_source': 'ner_chunk', 'normalized': 'City', 'confidenceValue': '0.50', 'entity': 'City', 'sentence': '1'}, [])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check outputs\n",
    "annotations.get('ner_chunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f95f9a93-199b-477f-8f79-89cf122649da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n",
       "    \n",
       "    .spark-nlp-display-scroll-entities {\n",
       "        border: 1px solid #E7EDF0;\n",
       "        border-radius: 3px;\n",
       "        text-align: justify;\n",
       "        \n",
       "    }\n",
       "    .spark-nlp-display-scroll-entities span {  \n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #536B76;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-wrapper{\n",
       "    \n",
       "        display: inline-grid;\n",
       "        text-align: center;\n",
       "        border-radius: 4px;\n",
       "        margin: 0 2px 5px 2px;\n",
       "        padding: 1px\n",
       "    }\n",
       "    .spark-nlp-display-entity-name{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        background: #f1f2f3;\n",
       "        border-width: medium;\n",
       "        text-align: center;\n",
       "        \n",
       "        font-weight: 400;\n",
       "        \n",
       "        border-radius: 5px;\n",
       "        padding: 2px 5px;\n",
       "        display: block;\n",
       "        margin: 3px 2px;\n",
       "    \n",
       "    }\n",
       "    .spark-nlp-display-entity-type{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-resolution{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Vistol Regular', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-others{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        font-weight: 400;\n",
       "    }\n",
       "\n",
       "</style>\n",
       " <span class=\"spark-nlp-display-others\" style=\"background-color: white\">Peter Parker is a nice guy and lives in </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #900BC1\"><span class=\"spark-nlp-display-entity-name\">New York </span><span class=\"spark-nlp-display-entity-type\">City</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> . Bruce Wayne is also a nice guy and lives in </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #900BC1\"><span class=\"spark-nlp-display-entity-name\">San Antonio </span><span class=\"spark-nlp-display-entity-type\">City</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #900BC1\"><span class=\"spark-nlp-display-entity-name\">Gotham City </span><span class=\"spark-nlp-display-entity-type\">City</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> .</span></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize outputs\n",
    "from sparknlp_display import NerVisualizer\n",
    "\n",
    "visualiser = NerVisualizer()\n",
    "\n",
    "ner_vis = visualiser.display(annotations, label_col='ner_chunk', document_col='document', save_path=\"display_result.html\", return_html=True )\n",
    "\n",
    "displayHTML(ner_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "842b1d4c-c1c7-4f83-90b9-30ff270a7ee2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Feel free to experiment with the annotator parameters and JSON properties to see how the output might change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc3edd1a-7fbe-4abc-b5dc-7315d40cc0e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Example 2: Detect Gender and Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a219599-0600-401a-9b49-e4f50901f24a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Here's some sample text\n",
    "sample_text = \"\"\"A 28 year old female with a history of gestational diabetes mellitus diagnosed 8 years ago. \n",
    "                 3 years ago, he reported an episode of HTG-induced pancreatitis . \n",
    "                 5 months old boy with repeated concussions.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14906d9b-0438-42a3-88ce-2a11c5b76cde",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gender = '''male,man,male,boy,gentleman,he,him\n",
    "female,woman,female,girl,lady,old-lady,she,her\n",
    "neutral,they,neutral,it'''\n",
    "\n",
    "with open('/dbfs/data/gender.csv', 'w') as f:\n",
    "    f.write(gender)\n",
    "\n",
    "\n",
    "gender = {\n",
    "  \"entity\": \"Gender\",\n",
    "  \"ruleScope\": \"sentence\", \n",
    "  \"completeMatchRegex\": \"true\",\n",
    "  \"matchScope\":\"token\"\n",
    "}\n",
    "\n",
    "import json\n",
    "\n",
    "with open('/dbfs/data/gender.json', 'w') as f:\n",
    "    json.dump(gender, f)\n",
    "\n",
    "\n",
    "age = {\n",
    "  \"entity\": \"Age\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"matchScope\":\"token\",\n",
    "  \"regex\":\"\\\\d{1,3}\",\n",
    "  \"prefix\":[\"age of\", \"age\"],\n",
    "  \"suffix\": [\"-years-old\", \"years-old\", \"-year-old\",\n",
    "             \"-months-old\", \"-month-old\", \"-months-old\",\n",
    "             \"-day-old\", \"-days-old\", \"month old\",\n",
    "             \"days old\", \"year old\", \"years old\", \n",
    "             \"years\", \"year\", \"months\", \"old\"],\n",
    "  \"contextLength\": 25,\n",
    "  \"contextException\": [\"ago\"],\n",
    "  \"exceptionDistance\": 12\n",
    "}\n",
    "\n",
    "with open('/dbfs/data/age.json', 'w') as f:\n",
    "    json.dump(age, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d651945-8546-4228-9ed4-a9cf1e1e0c82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "gender_contextual_parser = ContextualParserApproach() \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"chunk_gender\") \\\n",
    "    .setJsonPath(\"/dbfs/data/gender.json\") \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setDictionary('dbfs:/data/gender.csv', options={\"delimiter\":\",\"}) \\\n",
    "    .setPrefixAndSuffixMatch(False)      \n",
    "\n",
    "age_contextual_parser = ContextualParserApproach() \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"chunk_age\") \\\n",
    "    .setJsonPath(\"/dbfs/data/age.json\") \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setPrefixAndSuffixMatch(False)\\\n",
    "    .setShortestContextMatch(True)\\\n",
    "    .setOptionalContextRules(False)    \n",
    "\n",
    "chunk_merger = ChunkMergeApproach() \\\n",
    "    .setInputCols([\"chunk_gender\", \"chunk_age\"]) \\\n",
    "    .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "parserPipeline = Pipeline(stages=[\n",
    "        document_assembler, \n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        gender_contextual_parser,\n",
    "        age_contextual_parser,\n",
    "        chunk_merger\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1efbdcae-cff5-4357-902f-d7b2eeb2e2f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "parserModel = parserPipeline.fit(empty_data)\n",
    "\n",
    "light_model = LightPipeline(parserModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f633c51-4e23-46d0-aca7-fe4bdd0eadfd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "annotations = light_model.fullAnnotate(sample_text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c5a9ced-87f2-4425-bf08-763ff5b965e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Annotation(chunk, 2, 3, 28, {'tokenIndex': '1', 'entity': 'Age', 'field': 'Age', 'chunk': '0', 'normalized': '', 'sentence': '0', 'confidenceValue': '0.74'}, []),\n",
       " Annotation(chunk, 14, 19, female, {'tokenIndex': '4', 'entity': 'Gender', 'field': 'Gender', 'chunk': '1', 'normalized': 'female', 'sentence': '0', 'confidenceValue': '0.50'}, []),\n",
       " Annotation(chunk, 123, 124, he, {'tokenIndex': '4', 'entity': 'Gender', 'field': 'Gender', 'chunk': '2', 'normalized': 'male', 'sentence': '1', 'confidenceValue': '0.50'}, []),\n",
       " Annotation(chunk, 194, 194, 5, {'tokenIndex': '0', 'entity': 'Age', 'field': 'Age', 'chunk': '3', 'normalized': '', 'sentence': '2', 'confidenceValue': '0.74'}, []),\n",
       " Annotation(chunk, 207, 209, boy, {'tokenIndex': '3', 'entity': 'Gender', 'field': 'Gender', 'chunk': '4', 'normalized': 'male', 'sentence': '2', 'confidenceValue': '0.50'}, [])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.get('ner_chunk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd0262df-99ad-4619-aaa1-a45f9ec8530f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Highlight the Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6beb0f05-6c01-4546-b5c2-5b1f8e6581db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n",
       "    \n",
       "    .spark-nlp-display-scroll-entities {\n",
       "        border: 1px solid #E7EDF0;\n",
       "        border-radius: 3px;\n",
       "        text-align: justify;\n",
       "        \n",
       "    }\n",
       "    .spark-nlp-display-scroll-entities span {  \n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #536B76;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-wrapper{\n",
       "    \n",
       "        display: inline-grid;\n",
       "        text-align: center;\n",
       "        border-radius: 4px;\n",
       "        margin: 0 2px 5px 2px;\n",
       "        padding: 1px\n",
       "    }\n",
       "    .spark-nlp-display-entity-name{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        background: #f1f2f3;\n",
       "        border-width: medium;\n",
       "        text-align: center;\n",
       "        \n",
       "        font-weight: 400;\n",
       "        \n",
       "        border-radius: 5px;\n",
       "        padding: 2px 5px;\n",
       "        display: block;\n",
       "        margin: 3px 2px;\n",
       "    \n",
       "    }\n",
       "    .spark-nlp-display-entity-type{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-resolution{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Vistol Regular', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-others{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        font-weight: 400;\n",
       "    }\n",
       "\n",
       "</style>\n",
       " <span class=\"spark-nlp-display-others\" style=\"background-color: white\">A </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #ffe0ac\"><span class=\"spark-nlp-display-entity-name\">28 </span><span class=\"spark-nlp-display-entity-type\">Age</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> year old </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #ffacb7\"><span class=\"spark-nlp-display-entity-name\">female </span><span class=\"spark-nlp-display-entity-type\">Gender</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> with a history of gestational diabetes mellitus diagnosed 8 years ago. <br>                 3 years ago, </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #ffacb7\"><span class=\"spark-nlp-display-entity-name\">he </span><span class=\"spark-nlp-display-entity-type\">Gender</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> reported an episode of HTG-induced pancreatitis . <br>                 </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #ffe0ac\"><span class=\"spark-nlp-display-entity-name\">5 </span><span class=\"spark-nlp-display-entity-type\">Age</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> months old </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #ffacb7\"><span class=\"spark-nlp-display-entity-name\">boy </span><span class=\"spark-nlp-display-entity-type\">Gender</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> with repeated concussions.</span></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sparknlp_display import NerVisualizer\n",
    "\n",
    "visualiser = NerVisualizer()\n",
    "\n",
    "ner_vis = visualiser.display(annotations, label_col='ner_chunk', document_col='document', return_html=True)\n",
    "  \n",
    "displayHTML(ner_vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2299aa51-1fcb-42ec-9bbc-05132a22d29b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Feel free to experiment with the annotator parameters and JSON properties to see how the output might change. If you're looking to work on running the pipeline on a full dataset, just make sure to use the `fit()` and `transform()` methods directly on your dataset instead of using the lightpipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a208f02-c743-4d6c-b433-35f2a2111dc3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|                text|            document|            sentence|               token|        chunk_gender|           chunk_age|           ner_chunk|\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n|A 28 year old fem...|[{document, 0, 23...|[{document, 0, 90...|[{token, 0, 0, A,...|[{chunk, 14, 19, ...|[{chunk, 2, 3, 28...|[{chunk, 2, 3, 28...|\n+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create example dataframe with sample text\n",
    "data = spark.createDataFrame([[sample_text]]).toDF(\"text\")\n",
    "\n",
    "# Fit and show\n",
    "results = parserPipeline.fit(data).transform(data)\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5aba8e43-73ae-4de2-b9a3-a4d088339754",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n| result|\n+-------+\n|[28, 5]|\n+-------+\n\n"
     ]
    }
   ],
   "source": [
    "results.select(\"chunk_age.result\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42b99d41-9ed5-4c8b-9b13-7ee85a2dd688",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|           result|\n+-----------------+\n|[female, he, boy]|\n+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "results.select(\"chunk_gender.result\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47ad9130-675b-4ed4-8891-56b4210ed772",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Pretrained Contextual Parser Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cafe52a-2962-4ea5-b6c7-33166c5dc705",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<center><b>Contextual Parser Model List</b>\n",
    "\n",
    "|index|model| description|\n",
    "|-----:|:-----|:-----|\n",
    "| 1| [date_of_birth_parser](https://nlp.johnsnowlabs.com/2023/08/22/date_of_birth_parser_en.html)  | This model can extract date-of-birth (DOB) entities in clinical texts. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85fa914c-198d-4a37-9be5-ddf08f4df28b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## date-of-birth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0c839d2-a238-4c16-9b9b-c98855653137",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_detector_dl_healthcare download started this may take some time.\nApproximate size to download 367.3 KB\n\r[ | ]\r[ / ]\r[ — ]\r[OK!]\ndate_of_birth_parser download started this may take some time.\n\r[ | ]\r[ / ]\r[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = SentenceDetectorDLModel.pretrained(\"sentence_detector_dl_healthcare\",\"en\",\"clinical/models\")\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "dob_contextual_parser = ContextualParserModel.pretrained(\"date_of_birth_parser\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"chunk_dob\")\n",
    "\n",
    "chunk_converter = ChunkConverter() \\\n",
    "    .setInputCols([\"chunk_dob\"]) \\\n",
    "    .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "parserPipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    dob_contextual_parser,\n",
    "    chunk_converter\n",
    "    ])\n",
    "\n",
    "model = parserPipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c027b47-36a2-4519-9479-33df16ce8762",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Record date : 2081-01-04\n",
    "DB : 11.04.1962\n",
    "DT : 12-03-1978\n",
    "DOD : 10.25.23\n",
    "\n",
    "SOCIAL HISTORY:\n",
    "She was born on Nov 04, 1962 in London and got married on 04/05/1979. When she got pregnant on 15 May 1079, the doctor wanted to verify her DOB was November 4, 1962. Her date of birth was confirmed to be 11-04-1962, the patient is 45 years old on 25 Sep 2007.\n",
    "\n",
    "PROCEDURES:\n",
    "Patient was evaluated on 1988-03-15 for allergies. She was seen by the endocrinology service and she was discharged on 9/23/1988.\n",
    "\n",
    "MEDICATIONS\n",
    "1. Coumadin 1 mg daily. Last INR was on August 14, 2007, and her INR was 2.3.\"\"\"\n",
    "\n",
    "result = model.transform(spark.createDataFrame([[text]]).toDF(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edf2ad96-2b22-434b-9be2-2df4ef4cfdd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+---+----------------------------------------------------------------------------------------+\n|sentence_id|chunk           |end|ner_label                                                                               |\n+-----------+----------------+---+----------------------------------------------------------------------------------------+\n|1          |11.04.1962      |40 |{field -> DOB, tokenIndex -> 2, normalized -> , confidenceValue -> 0.72, sentence -> 1} |\n|3          |10.25.23        |71 |{field -> DOB, tokenIndex -> 2, normalized -> , confidenceValue -> 0.53, sentence -> 3} |\n|3          |Nov 04, 1962    |117|{field -> DOB, tokenIndex -> 10, normalized -> , confidenceValue -> 0.70, sentence -> 3}|\n|4          |November 4, 1962|253|{field -> DOB, tokenIndex -> 17, normalized -> , confidenceValue -> 0.70, sentence -> 4}|\n|5          |11-04-1962      |303|{field -> DOB, tokenIndex -> 8, normalized -> , confidenceValue -> 0.54, sentence -> 5} |\n+-----------+----------------+---+----------------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "result.select(F.explode(F.arrays_zip(result.chunk_dob.result,\n",
    "                                     result.chunk_dob.begin,\n",
    "                                     result.chunk_dob.end,\n",
    "                                     result.chunk_dob.metadata)).alias(\"cols\")) \\\n",
    "      .select(F.expr(\"cols['3']['sentence']\").alias(\"sentence_id\"),\n",
    "              F.expr(\"cols['0']\").alias(\"chunk\"),\n",
    "              F.expr(\"cols['2']\").alias(\"end\"),\n",
    "              F.expr(\"cols['3']\").alias(\"ner_label\"))\\\n",
    "      .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eabd32a6-ff8c-423c-93e2-65a927bb4ab9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n",
       "    \n",
       "    .spark-nlp-display-scroll-entities {\n",
       "        border: 1px solid #E7EDF0;\n",
       "        border-radius: 3px;\n",
       "        text-align: justify;\n",
       "        \n",
       "    }\n",
       "    .spark-nlp-display-scroll-entities span {  \n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #536B76;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-wrapper{\n",
       "    \n",
       "        display: inline-grid;\n",
       "        text-align: center;\n",
       "        border-radius: 4px;\n",
       "        margin: 0 2px 5px 2px;\n",
       "        padding: 1px\n",
       "    }\n",
       "    .spark-nlp-display-entity-name{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        background: #f1f2f3;\n",
       "        border-width: medium;\n",
       "        text-align: center;\n",
       "        \n",
       "        font-weight: 400;\n",
       "        \n",
       "        border-radius: 5px;\n",
       "        padding: 2px 5px;\n",
       "        display: block;\n",
       "        margin: 3px 2px;\n",
       "    \n",
       "    }\n",
       "    .spark-nlp-display-entity-type{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-resolution{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Vistol Regular', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-others{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        font-weight: 400;\n",
       "    }\n",
       "\n",
       "</style>\n",
       " <span class=\"spark-nlp-display-others\" style=\"background-color: white\"><br>Record date : 2081-01-04<br>DB : </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #384F6A\"><span class=\"spark-nlp-display-entity-name\">11.04.1962 </span><span class=\"spark-nlp-display-entity-type\">DOB</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"><br>DT : 12-03-1978<br>DOD : </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #384F6A\"><span class=\"spark-nlp-display-entity-name\">10.25.23 </span><span class=\"spark-nlp-display-entity-type\">DOB</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"><br><br>SOCIAL HISTORY:<br>She was born on </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #384F6A\"><span class=\"spark-nlp-display-entity-name\">Nov 04, 1962 </span><span class=\"spark-nlp-display-entity-type\">DOB</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> in London and got married on 04/05/1979. When she got pregnant on 15 May 1079, the doctor wanted to verify her DOB was </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #384F6A\"><span class=\"spark-nlp-display-entity-name\">November 4, 1962 </span><span class=\"spark-nlp-display-entity-type\">DOB</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">. Her date of birth was confirmed to be </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #384F6A\"><span class=\"spark-nlp-display-entity-name\">11-04-1962 </span><span class=\"spark-nlp-display-entity-type\">DOB</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">, the patient is 45 years old on 25 Sep 2007.<br><br>PROCEDURES:<br>Patient was evaluated on 1988-03-15 for allergies. She was seen by the endocrinology service and she was discharged on 9/23/1988.<br><br>MEDICATIONS<br>1. Coumadin 1 mg daily. Last INR was on August 14, 2007, and her INR was 2.3.</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize outputs\n",
    "from sparknlp_display import NerVisualizer\n",
    "\n",
    "light_model = LightPipeline(model)\n",
    "\n",
    "# Annotate the sample text\n",
    "annotations = light_model.fullAnnotate(text)[0]\n",
    "\n",
    "visualiser = NerVisualizer()\n",
    "\n",
    "visualiser.display(annotations, label_col='ner_chunk', document_col='document', save_path=\"display_result_2.html\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2866717378090463,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "10. Named Entity Recognition using rules",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
