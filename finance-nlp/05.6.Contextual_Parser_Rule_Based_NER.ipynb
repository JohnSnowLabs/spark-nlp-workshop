{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8W51t04BN6B"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21lTnEqRBd0s"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/finance-nlp/05.6.Contextual_Parser_Rule_Based_NER.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-vwpSj3BSbj"
      },
      "source": [
        "# ContextualParser (Rule Based NER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "4iIO6G_B3pqq"
      },
      "source": [
        "#ðŸŽ¬ Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPwo4Czy3pqq",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "! pip install -q johnsnowlabs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPsbAnNoPt0Z"
      },
      "source": [
        "##ðŸ”— Automatic Installation\n",
        "Using my.johnsnowlabs.com SSO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L-7mLYp3pqr",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "from johnsnowlabs import nlp, finance\n",
        "\n",
        "# nlp.install(force_browser=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsJvn_WWM2GL"
      },
      "source": [
        "##ðŸ”— Manual downloading\n",
        "If you are not registered in my.johnsnowlabs.com, you received a license via e-email or you are using Safari, you may need to do a manual update of the license.\n",
        "\n",
        "- Go to my.johnsnowlabs.com\n",
        "- Download your license\n",
        "- Upload it using the following command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i57QV3-_P2sQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "print('Please Upload your John Snow Labs License using the button below')\n",
        "license_keys = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGgNdFzZP_hQ"
      },
      "source": [
        "- Install it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfmmPqknP4rR"
      },
      "outputs": [],
      "source": [
        "nlp.install()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCl5ErZkNNLk"
      },
      "source": [
        "#ðŸ“Œ Starting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3jVICoa3pqr"
      },
      "outputs": [],
      "source": [
        "spark = nlp.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZzN6yNBvb2b"
      },
      "source": [
        "#ðŸ”Ž How the ContextualParser Works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSO62FEY0iof"
      },
      "source": [
        "Spark NLP's `ContextualParser` is a licensed annotator that allows users to extract entities from a document based on pattern matching. It provides more functionality than its open-source counterpart `EntityRuler` by allowing users to customize specific characteristics for pattern matching. You're able to find entities using regex rules for full and partial matches, a dictionary with normalizing options and context parameters to take into account things such as token distances. \n",
        "\n",
        "ðŸ“šThere are 3 components necessary to understand when using the `ContextualParser` annotator:\n",
        "\n",
        "1. `ContextualParser` annotator's parameters\n",
        "2. JSON configuration file\n",
        "3. Dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0_3bAzaPbip"
      },
      "source": [
        "##âœ” 1. ContextualParser Annotator Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTSYc7RUQgKh"
      },
      "source": [
        "ðŸ“šHere are all the parameters available to use with the `ContextualParserApproach`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF4_Dm7qVTty"
      },
      "source": [
        "```\n",
        "contextualParser = finance.ContextualParserApproach() \\\n",
        "    .setInputCols([\"sentence\", \"token\"]) \\\n",
        "    .setOutputCol(\"entity\") \\\n",
        "    .setCaseSensitive(True) \\\n",
        "    .setJsonPath(\"context_config.json\") \\\n",
        "    .setPrefixAndSuffixMatch(True) \\\n",
        "    .setCompleteContextMatch(True) \\\n",
        "    .setDictionary(\"dictionary.tsv\", options={\"orientation\":\"vertical\"})\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVmRKjoBZ7HQ"
      },
      "source": [
        "ðŸ“šWe will dive deeper into the details of each parameter, but here's a quick overview:\n",
        "\n",
        "- `setCaseSensitive`: do you want the matching to be case sensitive (applies to all JSON properties apart from the regex property)\n",
        "- `setJsonPath`: the path to your JSON configuration file\n",
        "- `setPrefixAndSuffixMatch`: do you want to match using both the prefix AND suffix properties from the JSON configuration file\n",
        "- `setCompleteContextMatch`: do you want an exact match of prefix and suffix.\n",
        "- `setDictionary`: the path to your dictionary, used for normalizing entities\n",
        "\n",
        "Let's start by looking at the JSON configuration file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVO5m215TjXf"
      },
      "source": [
        "##âœ” 2. JSON Configuration File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNJr5ISlaJsl"
      },
      "source": [
        "ðŸ“šHere is a fully utilized JSON configuration file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q1UuczZVhD_"
      },
      "source": [
        "ðŸ“œ\n",
        "```\n",
        "{\n",
        "  \"entity\": \"FISCAL_YEAR\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"(12|December) (31|Thirty-first), 2020\",\n",
        "  \"completeMatchRegex\": \"true\",\n",
        "  \"matchScope\": \"token\",\n",
        "  \"prefix\": [\"ended\"],\n",
        "  \"suffix\": [\"from\"],\n",
        "  \"contextLength\": 100,\n",
        "  \"contextException\": [\"of\"],\n",
        "  \"exceptionDistance\": 40\n",
        " }\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GChnk1cXaUIZ"
      },
      "source": [
        "###ðŸ“Œ 2.1. Basic Properties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-kJhmUe0f13"
      },
      "source": [
        "ðŸ“šThere are 5 basic properties you can set in your JSON configuration file:\n",
        "\n",
        "- `entity`\n",
        "- `ruleScope`\n",
        "- `regex`\n",
        "- `completeMatchRegex`\n",
        "- `matchScope`\n",
        "\n",
        "Let's first look at the 3 most essential properties to set:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RP8mwtgVkcj"
      },
      "source": [
        "ðŸ“œ\n",
        "```\n",
        "{\n",
        "  \"entity\": \"Digit\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"\\\\d+\" # Note here: backslashes are escape characters in JSON, so for regex pattern \"\\d+\" we need to write it out as \"\\\\d+\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHSsBgoNcJiw"
      },
      "source": [
        "ðŸ“šHere, we're looking for tokens in our text that match the regex: \"`\\d+`\" and assign the \"`Digit`\" entity to those tokens. When `ruleScope` is set to \"`sentence`\", we're looking for a match on each *token* of a **sentence**. You can change it to \"`document`\" to look for a match on each *sentence* of a **document**. The latter is particularly useful when working with multi-word matches, but we'll explore this at a later stage.\n",
        "\n",
        "The next properties to look at are `completeMatchRegex` and `matchScope`. To understand their use case, let's take a look at an example where we're trying to match all digits in our text. \n",
        "\n",
        "Let's say we come across the following string: ***XYZ987***\n",
        "\n",
        "Depending on how we set the `completeMatchRegex` and `matchScope` properties, we'll get the following results:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOxfFn8_VngD"
      },
      "source": [
        "ðŸ“œ\n",
        "```\n",
        "{\n",
        "  \"entity\": \"Digit\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"\\\\d+\",\n",
        "  \"completeMatchRegex\": \"false\",\n",
        "  \"matchScope\": \"token\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZzkfdtDyavl"
      },
      "source": [
        "`OUTPUT: [XYZ987]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K37Ucw75Vrog"
      },
      "source": [
        "ðŸ“œ\n",
        "```\n",
        "{\n",
        "  \"entity\": \"Digit\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"\\\\d+\",  \n",
        "  \"completeMatchRegex\": \"false\",\n",
        "  \"matchScope\": \"sub-token\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkOfYVHGyb20"
      },
      "source": [
        "`OUTPUT: [987]`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jpr2IkFVwKw"
      },
      "source": [
        "ðŸ“œ\n",
        "```\n",
        "{\n",
        "  \"entity\": \"Digit\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"\\\\d+\",\n",
        "  \"completeMatchRegex\": \"true\"\n",
        "  # matchScope is ignored here\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiYD0oF7gJtw"
      },
      "source": [
        "`OUTPUT: []`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFE9Ri2N4xxT"
      },
      "source": [
        "`\"completeMatchRegex\": \"true\"` will only return an output if our string was modified in the following way (to get a complete, exact match): **XYZ 987**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-_sXg5l5NBg"
      },
      "source": [
        "ðŸ“œ\n",
        "```\n",
        "{\n",
        "  \"entity\": \"Digit\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"\\\\d+\",  \n",
        "  \"completeMatchRegex\": \"true\",\n",
        "  \"matchScope\": \"token\" # Note here: sub-token would return the same output\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUeuct_05p3f"
      },
      "source": [
        "`OUTPUT: [987]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlIUAPKazpT9"
      },
      "source": [
        "###ðŸ“Œ 2.2. Context Awareness Properties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D2UfFFBz7fX"
      },
      "source": [
        "ðŸ“œThere are 5 properties related to context awareness:\n",
        "\n",
        "- `contextLength`\n",
        "- `prefix`\n",
        "- `suffix`\n",
        "- `contextException`\n",
        "- `exceptionDistance`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcUlaqmgDL9R"
      },
      "source": [
        "Let's look at a similar example. Say we have the following text: ***At the beginning of the fiscal year, the typical XYZ stock is performing slightly better than the typical ABC stock, but performance levels become equal at about seven months into the fiscal year.***\n",
        "\n",
        "If we want to match the company that grows faster at beginning, we can start by defining our regex: \"`XYZ|ABC`\"\n",
        "\n",
        "Next, we add a prefix (\"`beginning`\") and suffix (\"`better`\") to ask the parser to match the regex only if the word \"`beginning`\" comes before and only if the word \"`better`\" comes after. Finally, we will need to set the `contextLength` - this is the maximum number of tokens after the prefix and before the suffix that will be searched to find a regex match.\n",
        "\n",
        "Here's what the JSON configuration file would look like:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Tq5OJBVzCL"
      },
      "source": [
        "ðŸ“œ\n",
        "```\n",
        "{\n",
        "  \"entity\": \"Company\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"XYZ|ABC\",\n",
        "  \"contextLength\": 50,\n",
        "  \"prefix\": [\"beginning\"],\n",
        "  \"suffix\": [\"better\"],\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5Y5r9pO92B0"
      },
      "source": [
        "`OUTPUT: [XYZ]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXchjZZC_Gm0"
      },
      "source": [
        "If you remember, the annotator has a `setPrefixAndSuffixMatch()` parameter. If you set it to `True`, the previous output would remain as is. However, if you had set it to `False` and used the following JSON configuration:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO64udOnV1GJ"
      },
      "source": [
        "ðŸ“œ\n",
        "```\n",
        "{\n",
        "  \"entity\": \"Company\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"XYZ|ABC\",\n",
        "  \"contextLength\": 50,\n",
        "  \"prefix\": [\"beginning\"],\n",
        "  \"suffix\": [\"levels\", \"better\"],\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm-y_c_RAJpF"
      },
      "source": [
        "`OUTPUT: [XYZ,ABC]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk0nox1sAdw_"
      },
      "source": [
        "The parser now takes into account either the prefix OR suffix, only one of the condition has to be fulfilled for a match to count."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyTdPDlgLmxL"
      },
      "source": [
        "If you remember, the annotator has a `setCompleteContextMatch()` parameter. If you set it to `True`, and used the following JSON configuration :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ycrvCMpMJ_J"
      },
      "source": [
        "ðŸ“œ\n",
        "```\n",
        "{\n",
        "  \"entity\": \"Company\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"XYZ|ABC\",\n",
        "  \"contextLength\": 50,\n",
        "  \"prefix\": [\"beginning\"],\n",
        "  \"suffix\": [\"better\"]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5A67yBZMj2N"
      },
      "source": [
        "`OUTPUT: []`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1duAqATM-ir"
      },
      "source": [
        "However if we set `setCompleteContextMatch()` as `False`, and use the same JSON configuration as above, we get the following output :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0vd_jfsNi7y"
      },
      "source": [
        "`OUTPUT: [XYZ]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdEFJZM1DGQ1"
      },
      "source": [
        "Here's the sentence again: ***At the beginning of the fiscal year, the typical XYZ stock is performing slightly better than the typical ABC stock, but performance levels become equal at about seven months into the fiscal year.***\n",
        "\n",
        "The last 2 properties related to context awareness are `contextException` and `exceptionDistance`. This rules out matches based on a given exception:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JluGupMV5SR"
      },
      "source": [
        "ðŸ“œ\n",
        "```\n",
        "{\n",
        "  \"entity\": \"Company\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"XYZ|ABC\",\n",
        "  \"contextLength\": 50,\n",
        "  \"prefix\": [\"beginning\"],\n",
        "  \"suffix\": [\"levels\", \"better\"],\n",
        "  \"contextException\": [\"At\"],\n",
        "  \"exceptionDistance\": 5\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnFzqpHlC3Qz"
      },
      "source": [
        "`OUTPUT: [ABC]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT09xrE-DCiO"
      },
      "source": [
        "Here we've asked the parser to ignore a match if the token \"`At`\" is within 5 tokens of the matched regex. This caused the token \"`XYZ`\" to be ignored."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ9-Fkr94qAH"
      },
      "source": [
        "If the annotator's `setOptionalContextRules` parameter is set `True`, it allows us to output regex matches regardless of context match (prefix, suffix configuration). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBZ1D92g5aEB"
      },
      "source": [
        "When `shortestContextMatch` parameter is set to `True`, it will stop finding for matches when one of prefix and suffix data is found in the text.\",\n",
        "                                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41wdqkIX5WAy"
      },
      "source": [
        "Confidence Value Scenarios:\n",
        "* When there is regex match only, the confidence value will be 0.5.\n",
        "* When there are regex and prefix matches together, the confidence value will be > 0.5 depending on the distance between target token and the prefix.\n",
        "* When there are regex and suffix matches together, the confidence value will be > 0.5 depending on the distance between target token and the suffix.\n",
        "* When there are regex, prefix, and suffix matches all together, the confidence value will be > than the other scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzFSjw7aVO2b"
      },
      "source": [
        "##âœ” 3. Dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NPiJZx-Va8b"
      },
      "source": [
        "Another key feature of the `ContextualParser` annotator is the use of dictionaries. You can specify a path to a dictionary in `tsv` or `csv` format using the `setDictionary()` parameter. Using a dictionary is a useful when you have a list of exact words that you want the parser to pick up when processing some text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmB9hadyXYeM"
      },
      "source": [
        "###ðŸ“Œ 3.1. Orientation\n",
        "\n",
        "The first feature to be aware of when it comes to feeding dictionaries is the format of the dictionaries. The `ContextualParser` annotator will accept dictionaries in the horizontal format and in a vertical format. This is how they would look in practice:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyDmIqLmWGRy"
      },
      "source": [
        "Horizontal:\n",
        "\n",
        "| normalize | word1 | word2 | word3     |\n",
        "|-----------|-------|-------|-----------|\n",
        "| country    | US | Spain  |  India      |\n",
        "| Company   | Amazon   | Google   | John Snow Labs |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToFbLxsDYInk"
      },
      "source": [
        "\n",
        "Vertical:\n",
        "\n",
        "| country    | company |\n",
        "|-----------|-----------|\n",
        "| US     | Amazon     |\n",
        "| India      | Google     |\n",
        "| Spain      | John Snow Labs     | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCrIw4CRYeBC"
      },
      "source": [
        "As you can see, your dictionary needs to have a `normalize` field that lets the annotator know which entity labels to use, and another field that lets the annotator know a list of words it should be looking to match. Here's how to set the format that your dictionary uses:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_epZRQiV85Y"
      },
      "source": [
        "```\n",
        "contextualParser = finance.ContextualParserApproach() \\\n",
        "    .setDictionary(\"dictionary.tsv\", options={\"orientation\":\"vertical\"}) # default is horizontal\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CvM-FgLZgKk"
      },
      "source": [
        "###ðŸ“Œ 3.2. Dictionary-related JSON Properties\n",
        "\n",
        "When working with dictionaries, there are 2 properties in the JSON configuration file to be aware of:\n",
        "\n",
        "- `ruleScope`\n",
        "- `matchScope`\n",
        "\n",
        "This is especially true when you have multi-word entities in your dictionary.\n",
        "\n",
        "Let's take an example of a dictionary that contains a list of cities, sometimes made up of multiple words:\n",
        "\n",
        "| normalize | word1 | word2 | word3     |\n",
        "|-----------|-------|-------|-----------|\n",
        "| City      | New York | Salt Lake City  | Washington      |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5weyyiIMzNr"
      },
      "source": [
        "Let's say we're working with the following text: ***I love New York. Salt Lake City is nice too.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg8eJwuTJcB6"
      },
      "source": [
        "With the following JSON properties, here's what you would get:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1TNWtywoGWb"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"entity\": \"City\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"matchScope\": \"sub-token\",\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wweg5_C9JuWm"
      },
      "source": [
        "`OUTPUT: []`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNYWPr2eLJ5d"
      },
      "source": [
        "When `ruleScope` is set to `\"sentence\"`, the annotator attempts to find matches at the token level, parsing through each token in the sentence one by one, looking for a match with the dictionary items. Since `\"New York\"` and `\"Salt Lake City\"` are made up of multiple tokens, the annotator would never find a match from the dictionary. Let's change `ruleScope` to `\"document\"`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xib9zHN7oBvV"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"entity\": \"City\",\n",
        "  \"ruleScope\": \"document\",\n",
        "  \"matchScope\": \"sub-token\",\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYmkGdtALXzK"
      },
      "source": [
        "`OUTPUT: [New York, Salt Lake City]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nmc7rvfsdFK9"
      },
      "source": [
        "When `ruleScope` is set to `\"document\"`, the annotator attempts to find matches by parsing through each sentence in the document one by one, looking for a match with the dictionary items. Beware of how you set `matchScope`. Taking the previous example, if we were to set `matchScope` to `\"token\"` instead of `\"sub-token\"`, here's what would happen:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkzDTcgen9aS"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"entity\": \"City\",\n",
        "  \"ruleScope\": \"document\",\n",
        "  \"matchScope\": \"token\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np7rBSQmetA9"
      },
      "source": [
        "`OUTPUT: [I love New York., Salt Lake City is nice too.]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5nNtvLaeOv9"
      },
      "source": [
        "As you can see, when `ruleScope` is at the document level, if you set your `matchScope` to the token level, the annotator will output each sentence containing the matched entities as individual chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zTtAFwqQdNy"
      },
      "source": [
        "###ðŸ“Œ 3.3. Working with Multi-Word Matches\n",
        "\n",
        "Although not directly related to dictionaries, if we build on top of what we've just seen, there is a use-case that is particularly in demand when working with the `ContextualParser` annotator: finding regex matches for chunks of words that span across multiple tokens.Â \n",
        "\n",
        "Let's re-iterate how the `ruleScope` property works: when `ruleScope` is set to `\"sentence\"`, we're looking for a match on each token of a sentence. When `ruleScope` is set to `\"document\"`, we're looking for a match on each sentence of a document.Â \n",
        "\n",
        "So now let's imagine you're parsing through finance documents trying to tag the *John Snow* headers in those documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJWXRrg0Qu0q"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"entity\": \"John Snow\",\n",
        "  \"regex\": \"[j|J]ohn\\s+[s|S]now\",  \n",
        "  \"ruleScope\": \"document\",\n",
        "  \"matchScope\": \"sub-token\"\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP4H1t3CQyg5"
      },
      "source": [
        "`OUTPUT: [John Snow, john snow, John snow]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8PgD_N9RFTP"
      },
      "source": [
        "If you had set `ruleScope` to  `\"sentence\"`, here's what would have happened:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljqiVjWaRJPe"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"entity\": \"John Snow\",\n",
        "  \"regex\": \"[j|J]ohn\\s+[s|S]now\", \n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"matchScope\": \"sub-token\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfcGKW81RMKN"
      },
      "source": [
        "`OUTPUT: []`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMMgQdK5RPYb"
      },
      "source": [
        "Since John Snow is divided into two different tokens, the annotator will never find a match since it's now looking for a match on each token of a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehWiHjziPfGV"
      },
      "source": [
        "#ðŸ”Ž Running a Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thaF2bObwDXd"
      },
      "source": [
        "Let's try running through some examples to build on top of what you've learned so far."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lR6FnTsyBAjn"
      },
      "source": [
        "##âœ” Example: Detect Currency, Amount, Year, Fiscal Year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXK5CLYfDhNB"
      },
      "outputs": [],
      "source": [
        "# Here's some sample text\n",
        "sample_text = \"\"\"Services revenue increased $ 1.1 million, to $ 25.6 million for the year ended December 31, 2020 from $ 24.5 million for the year ended December 31, 2020.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5-tYW1iDhNB"
      },
      "outputs": [],
      "source": [
        "# Create JSON file for gender\n",
        "currency = {\n",
        "  \"entity\": \"currency\",\n",
        "  \"ruleScope\": \"document\", \n",
        "  \"regex\": \"[$]\",\n",
        "  \"matchScope\":\"sub-token\"\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('currency.json', 'w') as f:\n",
        "    json.dump(currency, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bynTPmlQHEPv"
      },
      "outputs": [],
      "source": [
        "# Create JSON file for age\n",
        "amount = {\n",
        "  \"entity\": \"Amount\",\n",
        "  \"ruleScope\": \"document\",\n",
        "  \"matchScope\":\"sub-token\",\n",
        "  \"regex\":\"\\\\d*\\.?\\d million\",\n",
        "  \"contextLength\": 15\n",
        "}\n",
        "\n",
        "with open('amount.json', 'w') as f:\n",
        "    json.dump(amount, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kd_ZcBNdaAGf"
      },
      "outputs": [],
      "source": [
        "# Create JSON file for age\n",
        "date = {\n",
        "  \"entity\": \"fiscal_year\",\n",
        "  \"ruleScope\": \"document\",\n",
        "  \"matchScope\":\"sub-token\",\n",
        "  \"completeMatchRegex\": \"true\",\n",
        "  \"regex\":\"(12|December) (31|Thirty-first), 2020\"\n",
        "}\n",
        "\n",
        "with open('date.json', 'w') as f:\n",
        "    json.dump(date, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJiW9guMcMtp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0GVRWNgcM-t",
        "outputId": "5a05726e-0812-499a-8a74-13b1321631c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "profit_increase\n",
            "Services revenue"
          ]
        }
      ],
      "source": [
        "# Create a dictionary to detect party\n",
        "profit_increase = '''profit_increase\\nServices revenue'''\n",
        "\n",
        "with open('profit_increase.tsv', 'w') as f:\n",
        "    f.write(profit_increase)\n",
        "\n",
        "# Check what dictionary looks like\n",
        "!cat profit_increase.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUg-HGHTcM-u"
      },
      "outputs": [],
      "source": [
        "# Create JSON file\n",
        "profit_increase= {\n",
        "  \"entity\": \"profit_increase\",\n",
        "  \"ruleScope\": \"document\", \n",
        "  \"matchScope\":\"sub-token\",\n",
        "  \"completeMatchRegex\": \"true\"\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('profit_increase.json', 'w') as f:\n",
        "    json.dump(profit_increase, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBQVujx1DhNC"
      },
      "outputs": [],
      "source": [
        "# Build pipeline\n",
        "document_assembler = nlp.DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "text_splitter = finance.TextSplitter() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"sentence\")\n",
        "\n",
        "tokenizer = nlp.Tokenizer() \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "currency_contextual_parser = finance.ContextualParserApproach() \\\n",
        "    .setInputCols([\"sentence\", \"token\"]) \\\n",
        "    .setOutputCol(\"chunk_currency\") \\\n",
        "    .setJsonPath(\"currency.json\") \\\n",
        "    .setCaseSensitive(False) \\\n",
        "    .setPrefixAndSuffixMatch(False)      \n",
        "\n",
        "amount_contextual_parser = finance.ContextualParserApproach() \\\n",
        "    .setInputCols([\"sentence\", \"token\"]) \\\n",
        "    .setOutputCol(\"chunk_amount\") \\\n",
        "    .setJsonPath(\"amount.json\") \\\n",
        "    .setCaseSensitive(False) \\\n",
        "    .setPrefixAndSuffixMatch(False)\\\n",
        "    .setShortestContextMatch(True)\\\n",
        "    .setOptionalContextRules(False) \n",
        "\n",
        "date_parser = finance.ContextualParserApproach() \\\n",
        "    .setInputCols([\"document\", \"token\"]) \\\n",
        "    .setOutputCol(\"chunk_date\")\\\n",
        "    .setJsonPath(\"date.json\") \\\n",
        "    .setCaseSensitive(True) \\\n",
        "    .setPrefixAndSuffixMatch(False)\\\n",
        "    .setOptionalContextRules(False)\n",
        "\n",
        "profit_increase_contextual_parser = finance.ContextualParserApproach() \\\n",
        "    .setInputCols([\"document\", \"token\"])\\\n",
        "    .setOutputCol(\"entity_profit_increase\")\\\n",
        "    .setJsonPath(\"profit_increase.json\")\\\n",
        "    .setDictionary('profit_increase.tsv', options={\"orientation\":\"vertical\"})\\\n",
        "    .setPrefixAndSuffixMatch(False)\\\n",
        "    .setShortestContextMatch(True)\\\n",
        "    .setOptionalContextRules(False)\\\n",
        "    .setCaseSensitive(True)\n",
        "\n",
        "chunk_merger = finance.ChunkMergeApproach() \\\n",
        "    .setInputCols([\"chunk_amount\", \"chunk_currency\", \"chunk_date\",'entity_profit_increase']) \\\n",
        "    .setOutputCol(\"ner_chunk\")\n",
        "\n",
        "parserPipeline = nlp.Pipeline(stages=[\n",
        "        document_assembler, \n",
        "        text_splitter,\n",
        "        tokenizer,\n",
        "        currency_contextual_parser,\n",
        "        amount_contextual_parser,\n",
        "        date_parser,\n",
        "        profit_increase_contextual_parser,\n",
        "        chunk_merger\n",
        "        ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36ZxeslFDhNC"
      },
      "outputs": [],
      "source": [
        "# Create a lightpipeline model\n",
        "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "\n",
        "parserModel = parserPipeline.fit(empty_data)\n",
        "\n",
        "light_model = nlp.LightPipeline(parserModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIhdQ4IjDhNC"
      },
      "outputs": [],
      "source": [
        "# Annotate the sample text\n",
        "annotations = light_model.fullAnnotate(sample_text)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tdgMbaWDhNC",
        "outputId": "bfacef9c-2a97-4c12-b21f-f70edf77b66b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Annotation(chunk, 0, 15, Services revenue, {'tokenIndex': '0', 'entity': 'profit_increase', 'field': 'profit_increase', 'chunk': '0', 'normalized': 'profit_increase', 'sentence': '0', 'confidenceValue': '0.50'}),\n",
              " Annotation(chunk, 27, 27, $, {'tokenIndex': '3', 'entity': 'currency', 'field': 'currency', 'chunk': '1', 'normalized': '', 'sentence': '0', 'confidenceValue': '0.50'}),\n",
              " Annotation(chunk, 29, 39, 1.1 million, {'tokenIndex': '4', 'entity': 'Amount', 'field': 'Amount', 'chunk': '2', 'normalized': '', 'sentence': '0', 'confidenceValue': '0.50'}),\n",
              " Annotation(chunk, 45, 45, $, {'tokenIndex': '8', 'entity': 'currency', 'field': 'currency', 'chunk': '3', 'normalized': '', 'sentence': '0', 'confidenceValue': '0.50'}),\n",
              " Annotation(chunk, 47, 58, 25.6 million, {'tokenIndex': '9', 'entity': 'Amount', 'field': 'Amount', 'chunk': '4', 'normalized': '', 'sentence': '0', 'confidenceValue': '0.50'}),\n",
              " Annotation(chunk, 79, 95, December 31, 2020, {'tokenIndex': '15', 'entity': 'fiscal_year', 'field': 'fiscal_year', 'chunk': '5', 'normalized': '', 'sentence': '0', 'confidenceValue': '0.50'}),\n",
              " Annotation(chunk, 102, 102, $, {'tokenIndex': '20', 'entity': 'currency', 'field': 'currency', 'chunk': '6', 'normalized': '', 'sentence': '0', 'confidenceValue': '0.50'}),\n",
              " Annotation(chunk, 104, 115, 24.5 million, {'tokenIndex': '21', 'entity': 'Amount', 'field': 'Amount', 'chunk': '7', 'normalized': '', 'sentence': '0', 'confidenceValue': '0.50'}),\n",
              " Annotation(chunk, 136, 152, December 31, 2020, {'tokenIndex': '27', 'entity': 'fiscal_year', 'field': 'fiscal_year', 'chunk': '8', 'normalized': '', 'sentence': '0', 'confidenceValue': '0.50'})]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check outputs\n",
        "annotations.get('ner_chunk')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "xw0mQKMxDhND",
        "outputId": "12b75e7e-5b06-4797-896b-bbfc4c1bd67d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n",
              "    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n",
              "    \n",
              "    .spark-nlp-display-scroll-entities {\n",
              "        border: 1px solid #E7EDF0;\n",
              "        border-radius: 3px;\n",
              "        text-align: justify;\n",
              "        \n",
              "    }\n",
              "    .spark-nlp-display-scroll-entities span {  \n",
              "        font-size: 14px;\n",
              "        line-height: 24px;\n",
              "        color: #536B76;\n",
              "        font-family: 'Montserrat', sans-serif !important;\n",
              "    }\n",
              "    \n",
              "    .spark-nlp-display-entity-wrapper{\n",
              "    \n",
              "        display: inline-grid;\n",
              "        text-align: center;\n",
              "        border-radius: 4px;\n",
              "        margin: 0 2px 5px 2px;\n",
              "        padding: 1px\n",
              "    }\n",
              "    .spark-nlp-display-entity-name{\n",
              "        font-size: 14px;\n",
              "        line-height: 24px;\n",
              "        font-family: 'Montserrat', sans-serif !important;\n",
              "        \n",
              "        background: #f1f2f3;\n",
              "        border-width: medium;\n",
              "        text-align: center;\n",
              "        \n",
              "        font-weight: 400;\n",
              "        \n",
              "        border-radius: 5px;\n",
              "        padding: 2px 5px;\n",
              "        display: block;\n",
              "        margin: 3px 2px;\n",
              "    \n",
              "    }\n",
              "    .spark-nlp-display-entity-type{\n",
              "        font-size: 14px;\n",
              "        line-height: 24px;\n",
              "        color: #ffffff;\n",
              "        font-family: 'Montserrat', sans-serif !important;\n",
              "        \n",
              "        text-transform: uppercase;\n",
              "        \n",
              "        font-weight: 500;\n",
              "\n",
              "        display: block;\n",
              "        padding: 3px 5px;\n",
              "    }\n",
              "    \n",
              "    .spark-nlp-display-entity-resolution{\n",
              "        font-size: 14px;\n",
              "        line-height: 24px;\n",
              "        color: #ffffff;\n",
              "        font-family: 'Vistol Regular', sans-serif !important;\n",
              "        \n",
              "        text-transform: uppercase;\n",
              "        \n",
              "        font-weight: 500;\n",
              "\n",
              "        display: block;\n",
              "        padding: 3px 5px;\n",
              "    }\n",
              "    \n",
              "    .spark-nlp-display-others{\n",
              "        font-size: 14px;\n",
              "        line-height: 24px;\n",
              "        font-family: 'Montserrat', sans-serif !important;\n",
              "        \n",
              "        font-weight: 400;\n",
              "    }\n",
              "\n",
              "</style>\n",
              " <span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #400AB2\"><span class=\"spark-nlp-display-entity-name\">Services revenue </span><span class=\"spark-nlp-display-entity-type\">profit_increase</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> increased </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #7C0A08\"><span class=\"spark-nlp-display-entity-name\">$ </span><span class=\"spark-nlp-display-entity-type\">currency</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #0C0532\"><span class=\"spark-nlp-display-entity-name\">1.1 million </span><span class=\"spark-nlp-display-entity-type\">Amount</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">, to </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #7C0A08\"><span class=\"spark-nlp-display-entity-name\">$ </span><span class=\"spark-nlp-display-entity-type\">currency</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #0C0532\"><span class=\"spark-nlp-display-entity-name\">25.6 million </span><span class=\"spark-nlp-display-entity-type\">Amount</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> for the year ended </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #1EB7B0\"><span class=\"spark-nlp-display-entity-name\">December 31, 2020 </span><span class=\"spark-nlp-display-entity-type\">fiscal_year</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> from </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #7C0A08\"><span class=\"spark-nlp-display-entity-name\">$ </span><span class=\"spark-nlp-display-entity-type\">currency</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #0C0532\"><span class=\"spark-nlp-display-entity-name\">24.5 million </span><span class=\"spark-nlp-display-entity-type\">Amount</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> for the year ended </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #1EB7B0\"><span class=\"spark-nlp-display-entity-name\">December 31, 2020 </span><span class=\"spark-nlp-display-entity-type\">fiscal_year</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">.</span></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualize outputs\n",
        "\n",
        "visualiser = nlp.viz.NerVisualizer()\n",
        "\n",
        "visualiser.display(annotations, label_col='ner_chunk', document_col='document', save_path=\"display_result_2.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOtVACnl_t3n"
      },
      "source": [
        "Feel free to experiment with the annotator parameters and JSON properties to see how the output might change. If you're looking to work on running the pipeline on a full dataset, just make sure to use the `fit()` and `transform()` methods directly on your dataset instead of using the lightpipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6nRUUrRRfoB",
        "outputId": "f3329262-8b39-46c8-df03-ad1ede2d5175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+\n",
            "|                text|            document|            sentence|               token|      chunk_currency|        chunk_amount|          chunk_date|entity_profit_increase|           ner_chunk|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+\n",
            "|Services revenue ...|[{document, 0, 15...|[{document, 0, 15...|[{token, 0, 7, Se...|[{chunk, 27, 27, ...|[{chunk, 29, 39, ...|[{chunk, 79, 95, ...|  [{chunk, 0, 15, S...|[{chunk, 0, 15, S...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create example dataframe with sample text\n",
        "data = spark.createDataFrame([[sample_text]]).toDF(\"text\")\n",
        "\n",
        "# Fit and show\n",
        "results = parserPipeline.fit(data).transform(data)\n",
        "results.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTrRlNyfeKwO",
        "outputId": "554b708d-ec5b-484e-b5c1-76b1ed1add30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------+\n",
            "|result                                |\n",
            "+--------------------------------------+\n",
            "|[December 31, 2020, December 31, 2020]|\n",
            "+--------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results.select(\"chunk_date.result\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orB8zsF9G6H4",
        "outputId": "9e33e37b-ee28-4d52-b3eb-4df13e96746b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------+\n",
            "|result                                   |\n",
            "+-----------------------------------------+\n",
            "|[1.1 million, 25.6 million, 24.5 million]|\n",
            "+-----------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results.select(\"chunk_amount.result\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzn7Qv3_HERi",
        "outputId": "57773ea9-bf3b-481d-e19a-79daf4a783a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|result   |\n",
            "+---------+\n",
            "|[$, $, $]|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results.select(\"chunk_currency.result\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtD1HJ7SABU7"
      },
      "source": [
        "Feel free to experiment with the annotator parameters and JSON properties to see how the output might change."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cbB-JNHCu49"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}