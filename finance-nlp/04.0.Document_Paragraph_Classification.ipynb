{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6jlakvDCElY"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ZZiVErWw_MXt"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/finance-nlp/04.0.Document_Paragraph_Classification.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUdOMUOQYJNe"
      },
      "source": [
        "#🔎 Classify Financial Texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "focTiQUhYJNg"
      },
      "source": [
        "In this notebook, you will learn how to use Spark NLP and Finance NLP to perform text classification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNS9C_-2YJNi"
      },
      "source": [
        "###📜 Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3dj43KRYJNi"
      },
      "source": [
        "First, you need to setup the environment to be able to use the licensed package. If you are not running in Google Colab, please check the documentation [here](https://nlp.johnsnowlabs.com/docs/en/licensed_install)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "4iIO6G_B3pqq"
      },
      "source": [
        "#🎬 Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPwo4Czy3pqq",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "! pip install -q johnsnowlabs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPsbAnNoPt0Z"
      },
      "source": [
        "##🔗 Automatic Installation\n",
        "Using [my.johnsnowlabs.com](https://my.johnsnowlabs.com/) SSO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_L-7mLYp3pqr",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "from johnsnowlabs import nlp, finance\n",
        "\n",
        "# nlp.install(force_browser=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsJvn_WWM2GL"
      },
      "source": [
        "##🔗 Manual downloading\n",
        "If you are not registered in my.johnsnowlabs.com, you received a license via e-email or you are using Safari, you may need to do a manual update of the license.\n",
        "\n",
        "- Go to [my.johnsnowlabs.com](https://my.johnsnowlabs.com/)\n",
        "- Download your license\n",
        "- Upload it using the following command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i57QV3-_P2sQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "print('Please Upload your John Snow Labs License using the button below')\n",
        "license_keys = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGgNdFzZP_hQ"
      },
      "source": [
        "- Install it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfmmPqknP4rR"
      },
      "outputs": [],
      "source": [
        "nlp.install()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCl5ErZkNNLk"
      },
      "source": [
        "#📌 Starting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3jVICoa3pqr"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from johnsnowlabs import nlp, finance, viz\n",
        "spark = nlp.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fxm3OQn9YJNh"
      },
      "source": [
        "##🔎 Pretrained models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzDnEztPYJNh"
      },
      "source": [
        "📜For the text classification tasks, we will use two annotators:\n",
        "\n",
        "- `ClassifierDL`: uses the state-of-the-art Universal Sentence Encoder as an input for text classifications. Then, a deep learning model (DNNs) built with TensorFlow that supports `Binary Classification` and `Multiclass Classification` (up to 100 classes).\n",
        "- `MultiClassifierDL`: `Multilabel Classification` (can predict more than one class for each text) using a Bidirectional GRU with Convolution architecture built with TensorFlow that supports up to 100 classes. The inputs are Sentence Embeddings such as state-of-the-art UniversalSentenceEncoder, BertSentenceEmbeddings or SentenceEmbeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6gvJ4RU5hw0"
      },
      "source": [
        "Example Classification models:\n",
        "\n",
        "| title                                                    | language   | predicted_entities                                                                                                      | compatible_editions                |\n",
        "|:---------------------------------------------------------|:-----------|:------------------------------------------------------------------------------------------------------------------------|:-----------------------------------|\n",
        "| Bank Complaints Classification                           | en         | ['Accounts', 'Credit Cards', 'Credit Reporting', 'Debt Collection', 'Loans', 'Money Transfer and Currency', 'Mortgage'] | ['Finance NLP 1.0', 'Finance NLP'] |\n",
        "| Financial Finbert Sentiment Analysis (DistilRoBerta)     | en         | ['positive', 'negative', 'neutral']                                                                                     | ['Finance NLP 1.0', 'Finance NLP'] |\n",
        "| Financial Business Item Binary Classifier                | en         | ['other', 'business']                                                                                                   | ['Finance NLP 1.0', 'Finance NLP'] |\n",
        "| Financial Controls procedures Item Binary Classifier     | en         | ['other', 'controls_procedures']                                                                                        | ['Finance NLP 1.0', 'Finance NLP'] |\n",
        "| Financial Equity Item Binary Classifier                  | en         | ['other', 'equity']                                                                                                     | ['Finance NLP 1.0', 'Finance NLP'] |\n",
        "| Financial Executives compensation Item Binary Classifier | en         | ['other', 'executives_compensation']                                                                                    | ['Finance NLP 1.0', 'Finance NLP'] |\n",
        "| Financial Executives Item Binary Classifier              | en         | ['other', 'executives']                                                                                                 | ['Finance NLP 1.0', 'Finance NLP'] |\n",
        "| Financial Exhibits Item Binary Classifier                | en         | ['other', 'exhibits']                                                                                                   | ['Finance NLP 1.0', 'Finance NLP'] |\n",
        "| Financial Financial conditions Item Binary Classifier    | en         | ['other', 'financial_conditions']                                                                                       | ['Finance NLP 1.0', 'Finance NLP'] |\n",
        "| Financial Financial statements Item Binary Classifier    | en         | ['other', 'financial_statements']                                                                                       | ['Finance NLP 1.0', 'Finance NLP'] |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvvv93e2YJNl"
      },
      "source": [
        "##🔎 Multiclass Classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt-7A1iPj2XS"
      },
      "source": [
        "Multiclass classifiers predicts one class out of a predefined set of possible classes. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBsPTv_6OQdN"
      },
      "source": [
        "####📚 Environmental, Social and Governance (ESG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOZ-CVOLYgm4"
      },
      "source": [
        "📜We will use two classifiers, one with 26 classes:\n",
        "\n",
        "`Business_Ethics`, `Data_Security`, `Access_And_Affordability`, `Business_Model_Resilience`, `Competitive_Behavior`, `Critical_Incident_Risk_Management`, `Customer_Welfare`, `Director_Removal`, `Employee_Engagement_Inclusion_And_Diversity`, `Employee_Health_And_Safety`, `Human_Rights_And_Community_Relations`, `Labor_Practices`, `Management_Of_Legal_And_Regulatory_Framework`, `Physical_Impacts_Of_Climate_Change`, `Product_Quality_And_Safety`, `Product_Design_And_Lifecycle_Management`, `Selling_Practices_And_Product_Labeling`, `Supply_Chain_Management`, `Systemic_Risk_Management`, `Waste_And_Hazardous_Materials_Management`, `Water_And_Wastewater_Management`, `Air_Quality`, `Customer_Privacy`, `Ecological_Impacts`, `Energy_Management`, `GHG_Emissions`\n",
        "\n",
        "\n",
        "and one with only three: `Social`, `Governance`, `Environmental` (or `None`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VCZa88JZYJNm",
        "outputId": "1e1b6b26-209e-4451-96ee-6d8e99753d14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "finclf_augmented_esg download started this may take some time.\n",
            "[OK!]\n",
            "finclf_esg download started this may take some time.\n",
            "[OK!]\n",
            "+--------------------------------------------------------------------------------+------------------------------------------+---------------+\n",
            "|                                                                            text|                                      many|            esg|\n",
            "+--------------------------------------------------------------------------------+------------------------------------------+---------------+\n",
            "|The Canadian Environmental Assessment Agency (CEAA) concluded that in June 20...|[Waste_And_Hazardous_Materials_Management]|[Environmental]|\n",
            "+--------------------------------------------------------------------------------+------------------------------------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "document_assembler = (\n",
        "    nlp.DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        ")\n",
        "\n",
        "tokenizer = nlp.Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
        "\n",
        "many_classes = (\n",
        "    finance.BertForSequenceClassification.pretrained(\n",
        "        \"finclf_augmented_esg\", \"en\", \"finance/models\"\n",
        "    )\n",
        "    .setInputCols([\"document\", \"token\"])\n",
        "    .setOutputCol(\"esg_many\")\n",
        ")\n",
        "\n",
        "three_classes = (\n",
        "    finance.BertForSequenceClassification.pretrained(\n",
        "        \"finclf_esg\", \"en\", \"finance/models\"\n",
        "    )\n",
        "    .setInputCols([\"document\", \"token\"])\n",
        "    .setOutputCol(\"esg\")\n",
        ")\n",
        "\n",
        "pipeline = nlp.Pipeline(\n",
        "    stages=[document_assembler, tokenizer, many_classes, three_classes]\n",
        ")\n",
        "\n",
        "# couple of simple examples\n",
        "example = spark.createDataFrame(\n",
        "    [\n",
        "        [\n",
        "            \"\"\"The Canadian Environmental Assessment Agency (CEAA) concluded that in June 2016 the company had not made an effort\n",
        " to protect public drinking water and was ignoring concerns raised by its own scientists about the potential levels of pollutants in the local water supply.\n",
        "  At the time, there were concerns that the company was not fully testing onsite wells for contaminants and did not use the proper methods for testing because \n",
        "  of its test kits now manufactured in China.A preliminary report by the company in June 2016 was commissioned by the Alberta government to provide recommendations \n",
        "  to Alberta Environment officials\"\"\"\n",
        "        ]\n",
        "    ]\n",
        ").toDF(\"text\")\n",
        "\n",
        "result = pipeline.fit(example).transform(example)\n",
        "\n",
        "# result is a DataFrame\n",
        "result.select(\n",
        "    \"text\", F.expr(\"esg_many.result as many\"), F.expr(\"esg.result as esg\")\n",
        ").show(truncate=80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u4sgWg-YJNn"
      },
      "source": [
        "###📚 Financial News Multilabel Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUl0CVse3Tba"
      },
      "source": [
        "📜This model can identify different topics contained in financial news (trained on news scrapped from the Internet and manual in-house annotations). The available topics are:\n",
        "\n",
        "- `acq`: Acquisition / Purchase operations\n",
        "- `finance`: Generic financial news\n",
        "- `fuel`: News about fuel and energy sources\n",
        "- `jobs`: News about jobs, employment rates, etc.\n",
        "- `livestock`: News about animales and livestock\n",
        "- `mineral`: News about mineral as copper, gold, silver, coal, etc.\n",
        "- `plant`: News about greens, plants, cereals, etc\n",
        "- `trade`: Trading news"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J4r4WRCau5s",
        "outputId": "889e953d-b833-4b7f-eddd-c539d300e305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n",
            "finmulticlf_news download started this may take some time.\n",
            "Approximate size to download 12.3 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "documentAssembler = (\n",
        "    nlp.DocumentAssembler()\n",
        "    .setInputCol(\"text\")\n",
        "    .setOutputCol(\"document\")\n",
        "    .setCleanupMode(\"shrink\")\n",
        ")\n",
        "\n",
        "embeddings = (\n",
        "    nlp.UniversalSentenceEncoder.pretrained()\n",
        "    .setInputCols(\"document\")\n",
        "    .setOutputCol(\"embeddings\")\n",
        ")\n",
        "\n",
        "docClassifier = (\n",
        "    nlp.MultiClassifierDLModel.pretrained(\"finmulticlf_news\", \"en\", \"finance/models\")\n",
        "    .setInputCols(\"embeddings\")\n",
        "    .setOutputCol(\"topics\")\n",
        ")\n",
        "\n",
        "pipeline = nlp.Pipeline().setStages([documentAssembler, embeddings, docClassifier])\n",
        "\n",
        "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "pipelineModel = pipeline.fit(empty_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOa6ba8yN8je",
        "outputId": "8d0ac551-de42-4efe-83ea-80478cf4829d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------+----------------+\n",
            "|                                                        text|          result|\n",
            "+------------------------------------------------------------+----------------+\n",
            "|\n",
            "ECUADOR HAS TRADE SURPLUS IN FIRST FOUR MONTHS Ecuador p...|[finance, trade]|\n",
            "+------------------------------------------------------------+----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "text = [\"\"\"\n",
        "ECUADOR HAS TRADE SURPLUS IN FIRST FOUR MONTHS Ecuador posted a trade surplus of 10.6 mln dlrs in the first four months of 1987 compared with a surplus of 271.7 mln in the same period in 1986, the central bank of Ecuador said in its latest monthly report. Ecuador suspended sales of crude oil, its principal export product, in March after an earthquake destroyed part of its oil-producing infrastructure. Exports in the first four months of 1987 were around 639 mln dlrs and imports 628.3 mln, compared with 771 mln and 500 mln respectively in the same period last year. Exports of crude and products in the first four months were around 256.1 mln dlrs, compared with 403.3 mln in the same period in 1986. The central bank said that between January and May Ecuador sold 16.1 mln barrels of crude and 2.3 mln barrels of products, compared with 32 mln and 2.7 mln respectively in the same period last year. Ecuador's international reserves at the end of May were around 120.9 mln dlrs, compared with 118.6 mln at the end of April and 141.3 mln at the end of May 1986, the central bank said. gold reserves were 165.7 mln dlrs at the end of May compared with 124.3 mln at the end of April.\n",
        "\"\"\"]\n",
        "\n",
        "df = spark.createDataFrame([text]).toDF(\"text\")\n",
        "\n",
        "result = pipelineModel.transform(df)\n",
        "result.select(\"text\", \"topics.result\").show(truncate=60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wznFFR8CSLB5"
      },
      "source": [
        "##🔎 Finding relevant sections of 10-K fillings  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo9iT-7LVKDs"
      },
      "source": [
        "We will use a publicly available information about Cadence in SEC's Edgar database [here](https://www.sec.gov/Archives/edgar/data/813672/000081367222000012/cdns-20220101.htm) and [Wikipedia](https://en.wikipedia.org/wiki/Cadence_Design_Systems) for to illustrate some of our binary classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlEkssWFTTPK"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/finance-nlp/data/cdns-20220101.html.txt -O sample10k.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QaaANmETZ2Z",
        "outputId": "8f5d9160-932b-44a7-8011-8a3ba692a935"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Table of Contents\n",
            "UNITED STATES SECURITIES AND EXCHANGE COMMISSION\n",
            "Washington, D.C. 20549\n",
            "_____________________________________ \n",
            "FORM 10-K \n",
            "_____________________________________  \n",
            "(Mark One)\n",
            "☒\n",
            "ANNUAL \n"
          ]
        }
      ],
      "source": [
        "text = open(\"sample10k.txt\", \"r\").read()\n",
        "print(text[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuJBm8zYVe0O"
      },
      "source": [
        "First, lets split this big text into pages (we identified that every page starts with the string \"Table of Contents\" and use that to split)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQQCVhaqVpRL"
      },
      "outputs": [],
      "source": [
        "document_assembler = (\n",
        "    nlp.DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        ")\n",
        "\n",
        "text_splitter = (\n",
        "    finance.TextSplitter()\n",
        "    .setInputCols([\"document\"])\n",
        "    .setOutputCol(\"pages\")\n",
        "    .setCustomBounds([\"Table of Contents\"])\n",
        "    .setUseCustomBoundsOnly(True)\n",
        ")\n",
        "\n",
        "nlp_pipeline = nlp.Pipeline(stages=[document_assembler, text_splitter])\n",
        "\n",
        "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "text_splitting_pipe = nlp_pipeline.fit(empty_data)\n",
        "text_splitting_lightpipe = nlp.LightPipeline(text_splitting_pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYEwIsqKVzG1",
        "outputId": "a00b870c-021b-43e1-f81f-bc0bd21451fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res = text_splitting_lightpipe.annotate(text)\n",
        "pages = res['pages']\n",
        "pages = [p for p in pages if p.strip() != ''] # We remove empty pages\n",
        "len(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVjLwikeV60w",
        "outputId": "060cd23e-f9c0-462b-b3f2-bb8f86ac727e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UNITED STATES SECURITIES AND EXCHANGE COMMISSION\n",
            "Washington, D.C. 20549\n",
            "_____________________________________ \n",
            "FORM 10-K \n",
            "_____________________________________  \n",
            "(Mark One)\n",
            "☒\n",
            "ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
            "For the fiscal year ended January 1, 2022 \n",
            "OR\n",
            "☐\n",
            "TRANSITION REPORT PURSUANT TO SECTION 13 OR 15(d) OF THE SECURITIES EXCHANGE ACT OF 1934\n",
            "For the transition period from _________ to_________.\n",
            "\n",
            "Commission file number 000-15867 \n",
            "_____________________________________\n",
            " \n",
            "CADENCE DESIGN SYSTEMS, INC. \n",
            "(Exact name of registrant as specified in its charter)\n",
            "____________________________________ \n",
            "Delaware\n",
            " \n",
            "00-0000000\n",
            "(State or Other Jurisdiction ofIncorporation or Organization)\n",
            " \n",
            "(I.R.S. EmployerIdentification No.)\n",
            "2655 Seely Avenue, Building 5,\n",
            "San Jose,\n",
            "California\n",
            " \n",
            "95134\n",
            "(Address of Principal Executive Offices)\n",
            " \n",
            "(Zip Code)\n",
            "(408)\n",
            "-943-1234 \n",
            "(Registrant’s Telephone Number, including Area Code) \n",
            "Securities registered pursuant to Section 12(b) of the Act:\n",
            "Title of Each Class\n",
            "Trading Symbol(s)\n",
            "Names of Each Exchange on which Registered\n",
            "Common Stock, $0.01 par value per share\n",
            "CDNS\n",
            "Nasdaq Global Select Market\n",
            "Securities registered pursuant to Section 12(g) of the Act:\n",
            "None\n",
            "Indicate by check mark if the registrant is a well-known seasoned issuer, as defined in Rule 405 of the Securities Act.  \n",
            " Yes  \n",
            "☒\n",
            "    No  \n",
            "☐\n",
            "Indicate by check mark if the registrant is not required to file reports pursuant to Section 13 or Section 15(d) of the Act.  \n",
            " Yes \n",
            "☐    \n",
            "No  \n",
            "☒\n",
            "Indicate by check mark whether the registrant (1) has filed all reports required to be filed by Section 13 or 15(d) of the Securities Exchange Act of 1934 during the preceding 12 months (or for such shorter period that the registrant was required to file such reports), and (2) has been subject to such filing requirements for the past 90 days.  \n",
            " Yes  \n",
            "☒\n",
            "    No  \n",
            "☐\n",
            "Indicate by check mark whether the registrant has submitted electronically every Interactive Data File required to be submitted pursuant to Rule 405 of Regulation S-T (§ 232.405 of this chapter) during the preceding 12 months (or for such shorter period that the registrant was required to submit such files). \n",
            " Yes  \n",
            "☒\n",
            "    No  \n",
            "☐\n",
            "Indicate by check mark whether the registrant is a large accelerated filer, an accelerated filer, a non-accelerated filer, a smaller reporting company, or an emerging growth company. See the definitions of “large accelerated filer,” “accelerated filer,” “smaller reporting company,” and “emerging growth company” in Rule 12b-2 of the Exchange Act.\n",
            "Large Accelerated Filer\n",
            "☒\n",
            "Accelerated Filer\n",
            "☐\n",
            "Non-accelerated Filer\n",
            "☐\n",
            "Smaller Reporting Company\n",
            "☐\n",
            "Emerging Growth Company\n",
            "☐\n",
            "If an emerging growth company, indicate by check mark if the registrant has elected not to use the extended transition period for complying with any new or revised financial accounting standards provided pursuant to Section 13(a) of the Exchange Act.  \n",
            "☐\n",
            "Indicate by check mark whether the registrant has filed a report on and attestation to its management’s assessment of the effectiveness of its internal control over financial reporting under Section 404(b) of the Sarbanes-Oxley Act (15 U.S.C. 7262(b)) by the registered public accounting firm that prepared or issued its audit report. \n",
            "☒\n",
            "Indicate by check mark whether the registrant is a shell company (as defined in Rule 12b-2 of the Act). \n",
            " Yes \n",
            "☐ \n",
            "No  \n",
            "☒\n",
            "The aggregate market value of the voting and non-voting common equity held by non-affiliates computed by reference to the price at which the common equity was last sold as of the last business day of the registrant’s most recently completed second fiscal quarter ended July 3, 2021 was approximately $38,179,000,000.\n",
            "On February 5, 2022, approximately 277,336,000 shares of the Registrant’s Common Stock, $0.01 par value, were outstanding.\n",
            "DOCUMENTS INCORPORATED BY REFERENCE\n",
            "Portions of the definitive proxy statement for Cadence Design Systems, Inc.’s 2022 Annual Meeting of Stockholders are incorporated by reference into Part III hereof.\n"
          ]
        }
      ],
      "source": [
        "print(pages[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtm5aLGgWDHF"
      },
      "source": [
        "<img src=\"https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings_JSL/Finance/data/10k_image.png?raw=true\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnLUIiIyWoc3"
      },
      "source": [
        "Let's create a funtion that generates pipelines with the desird model, so we can use different binary classifiers with ease."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nD0szSjlWuj9"
      },
      "outputs": [],
      "source": [
        "def get_binary_pipeline(model_name):\n",
        "    documentAssembler = (\n",
        "        nlp.DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        "    )\n",
        "\n",
        "    useEmbeddings = (\n",
        "        nlp.UniversalSentenceEncoder.pretrained()\n",
        "        .setInputCols(\"document\")\n",
        "        .setOutputCol(\"sentence_embeddings\")\n",
        "    )\n",
        "\n",
        "    docClassifier = (\n",
        "        nlp.ClassifierDLModel.pretrained(model_name, \"en\", \"finance/models\")\n",
        "        .setInputCols([\"sentence_embeddings\"])\n",
        "        .setOutputCol(\"category\")\n",
        "    )\n",
        "\n",
        "    nlpPipeline = nlp.Pipeline(stages=[documentAssembler, useEmbeddings, docClassifier])\n",
        "\n",
        "    return nlpPipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm8TJ9wKWH4D"
      },
      "source": [
        "###📚 Finding Summary part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk_qXoRbWKpp"
      },
      "source": [
        "Summary page is usually the first page of the report, but let's suppose we don't know that. This binary classifier will predict `summary` if the page is the summary page or `other` otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNeWAcsFWCnH",
        "outputId": "9b868b32-fe97-4c8a-ce95-d8c97e697b45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n",
            "finclf_form_10k_summary_item download started this may take some time.\n",
            "Approximate size to download 21.2 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "cls_pipeline = get_binary_pipeline(\"finclf_form_10k_summary_item\")\n",
        "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "\n",
        "cls_model = cls_pipeline.fit(empty_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZ25QzEFXPJV",
        "outputId": "7cecb7f5-2abd-4998-ed08-ceb8a8458a5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+\n",
            "|            result|\n",
            "+------------------+\n",
            "|[form_10k_summary]|\n",
            "+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.createDataFrame([[pages[0]]]).toDF(\"text\")\n",
        "result = cls_model.transform(df)\n",
        "result.select('category.result').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoQe_gZom2k8"
      },
      "source": [
        "###📚 Finding Acquisitions and Subsidiaries part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GwLJlYZm-Gp"
      },
      "source": [
        "Let's send some pages and check which one(s) contain that information. In a real case, you could send all the pages to the model, but here for time saving purposes, we will show just a subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKKfzE7qm-xy"
      },
      "outputs": [],
      "source": [
        "candidates = [[pages[0]], [pages[1]], [pages[35]], [pages[50]], [pages[67]]] # Some examples\n",
        "df = spark.createDataFrame(candidates).toDF(\"text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZJMZJ1VnEX2",
        "outputId": "30b4984d-b448-44bc-e2ba-ac1fd2935917"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n",
            "finclf_acquisitions_item download started this may take some time.\n",
            "Approximate size to download 21.3 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "classification_pipeline = get_binary_pipeline('finclf_acquisitions_item')\n",
        "\n",
        "model = classification_pipeline.fit(df)\n",
        "result = model.transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEAus5MXnLdO",
        "outputId": "1b0db3b4-13c4-4fd5-a8b6-a85049682ac7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------+\n",
            "|        result|\n",
            "+--------------+\n",
            "|       [other]|\n",
            "|       [other]|\n",
            "|       [other]|\n",
            "|       [other]|\n",
            "|[acquisitions]|\n",
            "+--------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select('category.result').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9REr6uYnRu8"
      },
      "source": [
        "\n",
        "\n",
        "###📚 Finding About Management and their work experience part\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_r7htornbWy"
      },
      "source": [
        "Let's send some pages and check which one(s) contain that information. In a real case, you could send all the pages to the model, but here for time saving purposes, we will show just a subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16FptfBXnboU",
        "outputId": "4015278e-1ba9-495c-b244-58c75ee03a11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n",
            "finclf_work_experience_item download started this may take some time.\n",
            "Approximate size to download 21.2 MB\n",
            "[OK!]\n",
            "+-----------------+\n",
            "|           result|\n",
            "+-----------------+\n",
            "|          [other]|\n",
            "|          [other]|\n",
            "|          [other]|\n",
            "|[work_experience]|\n",
            "|          [other]|\n",
            "+-----------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "candidates = [[pages[4]], [pages[84]], [pages[85]], [pages[86]], [pages[87]]]\n",
        "df = spark.createDataFrame(candidates).toDF(\"text\")\n",
        "\n",
        "\n",
        "classification_pipeline = get_binary_pipeline('finclf_work_experience_item')\n",
        "model = classification_pipeline.fit(df)\n",
        "result = model.transform(df)\n",
        "result.select('category.result').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPSxpGYCj8cF"
      },
      "source": [
        "###📚 Using LightPipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v2CYCq6j-i0"
      },
      "source": [
        "[LightPipelines](https://nlp.johnsnowlabs.com/docs/en/concepts#using-spark-nlps-lightpipeline) are Spark NLP specific Pipelines, equivalent to Spark ML Pipeline, but meant to deal with smaller amounts of data. They’re useful working with small datasets, debugging results, or when running either training or prediction from an API that serves one-off requests.\n",
        "\n",
        "Spark NLP LightPipelines are Spark ML pipelines converted into a single machine but the multi-threaded task, **becoming more than 10x times faster** for smaller amounts of data (small is relative, but 50k sentences are roughly a good maximum). To use them, we simply plug in a trained (fitted) pipeline and then annotate a plain text. We don't even need to convert the input text to DataFrame in order to feed it into a pipeline that's accepting DataFrame as an input in the first place. This feature would be quite useful when it comes to getting a prediction for a few lines of text from a trained ML model.\n",
        "\n",
        "For more details:\n",
        "[https://medium.com/spark-nlp/spark-nlp-101-lightpipeline-a544e93f20f1](https://medium.com/spark-nlp/spark-nlp-101-lightpipeline-a544e93f20f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juiznw0cdgOb"
      },
      "outputs": [],
      "source": [
        "light_model = nlp.LightPipeline(cls_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kltKN3XEpOHp"
      },
      "source": [
        "You can use strings or list of strings with the method [.annotate()](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/base/light_pipeline/index.html#sparknlp.base.light_pipeline.LightPipeline.annotate) to get the results. To get more metadata in the result, use the method [.fullAnnotate()](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/base/light_pipeline/index.html#sparknlp.base.light_pipeline.LightPipeline.fullAnnotate) instead. The result is a `list` if a `list` is given, or a `dict` if a string was given.\n",
        "\n",
        "To extract the results from the object, you just need to parse the dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9zQvgZftS18",
        "outputId": "08abf068-1648-405a-a8d5-260d17be998e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['document', 'sentence_embeddings', 'category'])"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lp_results = light_model.annotate(pages[0])\n",
        "lp_results.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCCX2g_ctS0D",
        "outputId": "02e39770-66da-4b64-a5f9-4359a9f642ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['form_10k_summary']"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# List with all the chunks\n",
        "lp_results[\"category\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geH7M8XkrBnk"
      },
      "source": [
        "We can see that the `.annotate()` did't return metadata in the `category` item. How can we obtain them? Using the `.fullAnnotate()` instead. This method always returns a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLIGPnJntSx8",
        "outputId": "47767019-72ce-4e75-ba8f-c7c2c1160f01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['document', 'sentence_embeddings', 'category'])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lp_results_full = light_model.fullAnnotate(pages[0])\n",
        "lp_results_full[0].keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDBT6Cb-tSvu",
        "outputId": "8e29dbfd-e227-46a6-e0fc-3ee8577124f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Annotation(category, 0, 4047, form_10k_summary, {'sentence': '0', 'form_10k_summary': '0.99994636', 'other': '5.3589152E-5'})]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lp_results_full[0][\"category\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lROIxxO_r0zR"
      },
      "source": [
        "Now we can see all the metadata in the annotation objects. Let's get the results in a tabular form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "Q0l4JNsPtStH",
        "outputId": "658eaa9c-fe06-4707-bdd1-e8795adf7757"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2e5de77b-47f2-4e64-bea5-a52c46359cc4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>begin</th>\n",
              "      <th>end</th>\n",
              "      <th>category</th>\n",
              "      <th>confidence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>4047</td>\n",
              "      <td>form_10k_summary</td>\n",
              "      <td>0.99994636</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2e5de77b-47f2-4e64-bea5-a52c46359cc4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2e5de77b-47f2-4e64-bea5-a52c46359cc4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2e5de77b-47f2-4e64-bea5-a52c46359cc4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   begin   end          category  confidence\n",
              "0      0  4047  form_10k_summary  0.99994636"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_tabular = []\n",
        "for res in lp_results_full[0][\"category\"]:\n",
        "    results_tabular.append(\n",
        "        (\n",
        "            res.begin,\n",
        "            res.end,\n",
        "            res.result,\n",
        "            res.metadata[\"form_10k_summary\"],\n",
        "        )\n",
        "    )\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(results_tabular, columns=[\"begin\", \"end\", \"category\", \"confidence\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoJspRUoXc8H"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Fxm3OQn9YJNh"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0 (tags/v3.8.0:fa919fd, Oct 14 2019, 19:37:50) [MSC v.1916 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "939480ed579cbcc9bd95c0bb2f0a271d068ec362d36f1415ed941c7dadb52340"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}