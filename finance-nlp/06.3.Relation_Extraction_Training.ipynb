{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8W51t04BN6B"
   },
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro62EycguEpn"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/finance-nlp/06.3.Relation_Extraction_Training.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nT0QsH4lulSx"
   },
   "source": [
    "# Relation Extraction Model Training\n",
    "\n",
    "This is our BertSpan-based Relation Extraction model, based on [this paper](https://arxiv.org/abs/1907.10529), an implemented by John Snow Labs on Tensorflow 1.x\n",
    "\n",
    "Unfortunately, from Nov 2022 Google Colab discontinued the support of TF 1.x. \n",
    "\n",
    "**We are working on the TF 2.x version of it.**\n",
    "\n",
    "In the meantime, please use non-colab environments with jupyter and TF 1.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kVDAlIrvaHc"
   },
   "source": [
    "If you use GPU machine, you can save your training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fXMoxGUuCSj",
    "outputId": "62beb702-a1b9-4b9d-bd85-bcda3b660a2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan  6 20:06:06 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   29C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95lu_FswuCSk"
   },
   "outputs": [],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bV_uPKLEkBOY"
   },
   "source": [
    "# 1.1. Installing Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgui51vikAke"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from os.path import exists\n",
    "\n",
    "with open('your_license_path', 'r') as f:\n",
    "    license_keys = json.load(f)\n",
    "\n",
    "# Defining license key-value pairs as local variables\n",
    "locals().update(license_keys)\n",
    "\n",
    "# Adding license key-value pairs to environment variables\n",
    "os.environ.update(license_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zN6kazlLkP7v"
   },
   "source": [
    "# 1.2. Installing Spark NLP (licensed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fcM-1RQkPhW"
   },
   "outputs": [],
   "source": [
    "# Installing pyspark and spark-nlp\n",
    "! pip install --upgrade pyspark==3.1.2 spark-nlp==$PUBLIC_VERSION\n",
    "\n",
    "# Installing Spark NLP Healthcare\n",
    "! pip install --upgrade spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET\n",
    "\n",
    "# Installing Spark NLP Display Library for visualization\n",
    "! pip install spark-nlp-display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a-_DjLmkYIu"
   },
   "source": [
    "# 1.3. Starting Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "Y8u2LfDqkYcO",
    "outputId": "6bd07e49-4395-4075-97d7-ab2a744b45eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP Version : 4.2.4\n",
      "Spark NLP_JSL Version : 4.2.4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-18-232.us-east-2.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP Licensed</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f901aa8fc18>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import os\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl\n",
    "import sparknlp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Spark NLP Version :\", sparknlp.version())\n",
    "print(\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
    "\n",
    "spark = sparknlp_jsl.start(license_keys['SECRET'])\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3R2CM28aC-wB"
   },
   "source": [
    "# Relation Extraction training using TensorFlow 1.x and BERT\n",
    "\n",
    "Firstly, we install the necessary library and define the necessary functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9a7QwiuC-wC"
   },
   "source": [
    "# 2. Download BERT code implementation and BERT weights\n",
    "In this section we will download official BERT code and the Bert pretrained weights we will use to finetune and create our RE model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASRUv_ZAouZM"
   },
   "source": [
    "## 2.1. Downloading BERT code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lnnr5eENC-wD",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/google-research/bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32eMMkztowrk"
   },
   "source": [
    "## 2.2. Downloading pretrained BERT weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_92PRTXPboJ"
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
    "#!wget https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJq67k3EukMH"
   },
   "outputs": [],
   "source": [
    "!rm -Rf models trained || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcLXdbt2Qyxv"
   },
   "outputs": [],
   "source": [
    "!mkdir models || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8T7HVt4acHKA"
   },
   "outputs": [],
   "source": [
    "!mkdir trained || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_K3_f2DGQwCH"
   },
   "outputs": [],
   "source": [
    "!mv cased_L-12_H-768_A-12.zip ./models\n",
    "#!mv uncased_L-12_H-768_A-12.zip ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mf3iaDXePd35"
   },
   "outputs": [],
   "source": [
    "!cd models && unzip -n cased_L-12_H-768_A-12.zip\n",
    "#!cd models && unzip -n uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ICMfjrjPI3DS"
   },
   "outputs": [],
   "source": [
    "!mv models/cased_L-12_H-768_A-12/* models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plRnHzH7C-wE"
   },
   "source": [
    "## 2.3.Add BERT to System Path\n",
    "Bert code will look for several modules in the system path, so we need to set that they can be also find in `bert` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMzq11giC-wF"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "BERT_CODE = os.getcwd() + \"/bert\"\n",
    "\n",
    "if not BERT_CODE in sys.path:\n",
    "    sys.path += [BERT_CODE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hV32u1bXC-wF"
   },
   "source": [
    "## 3. Library installation\n",
    "Required:\n",
    "```\n",
    "- Java 8\n",
    "- The Data Science classical kit (pandas+numpy+scipy)\n",
    "- Tensorflow 1.x\n",
    "- PySpark+SparkNLP\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvijWW7uC-wG"
   },
   "source": [
    "#### Java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YD3_ZCerC-wG"
   },
   "outputs": [],
   "source": [
    "# Make sure java 8 is installed.\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Zj7BSFxDKFv",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# If not, run:\n",
    "!sudo apt-get update\n",
    "!sudo apt-get purge -y openjdk-11* -qq > /dev/null && sudo apt-get autoremove -y -qq > /dev/null\n",
    "!sudo apt-get install -y openjdk-8-jdk-headless -qq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndZfkT4TDPap"
   },
   "outputs": [],
   "source": [
    "# Make sure java 8 is installed.\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsC41EZ7C-wH"
   },
   "source": [
    "#### Data Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IuziH1twC-wI",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy==1.19.5 scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwbGACJLD6q2",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4v68yr0oC-wJ"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7tCDK5vC-wJ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhTLs1GEDuWc"
   },
   "source": [
    "#### Make sure TF 1.x is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3NBkVhy-rooZ",
    "outputId": "efd0a0cb-b275-475c-f1b7-78584be0151e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae0959zLC-wJ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import modeling\n",
    "import optimization\n",
    "import tokenization\n",
    "import run_classifier\n",
    "import shutil\n",
    "import os\n",
    "import pprint\n",
    "from IPython.display import clear_output\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "from functools import reduce\n",
    "import scipy.stats as stats\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNhPnkptjOSk",
    "outputId": "ef93fe66-6c80-4393-b6b9-5a8619bb33b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device_count {\n",
       "  key: \"GPU\"\n",
       "  value: 1\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = tf.ConfigProto(device_count = {'GPU': 1})\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqdZ1zP6GANF"
   },
   "source": [
    "## Hyperparam configuration\n",
    "There are 2 available: generic BERT and specific BioBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJb7DF_mGhD1"
   },
   "source": [
    "### Base BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OByOhtRZGqBK"
   },
   "outputs": [],
   "source": [
    "class BaseBertI2B2Config:\n",
    "    #maximum sequence length, can be up to 512 for standard Bertmodels\n",
    "    #larger values require more GPU memory\n",
    "    #A GTX1080 Ti with 11 GB of memory can do no more than batch size 16 with max_seq_len 128\n",
    "    MAX_SEQ_LENGTH = 256\n",
    "\n",
    "    #location of pretrained Bert model\n",
    "    BERT_MODEL_PATH = \"./models\"\n",
    "    #location of Bert chekpoint used for initializing the model\n",
    "    BERT_MODEL_CHECKPOINT_PATH = \"{}/bert_model.ckpt\".format(BERT_MODEL_PATH)\n",
    "    #location of Bert configuration file\n",
    "    BERT_MODEL_CONFIG_PATH = \"{}/bert_config.json\".format(BERT_MODEL_PATH)\n",
    "    #location of Bert vocabulary file\n",
    "    BERT_VOCAB_PATH = \"{}/vocab.txt\".format(BERT_MODEL_PATH)\n",
    "\n",
    "    #Location for storing trained models  (in checkpoing format)\n",
    "    CHKPOINT_PATH = \"./trained\"\n",
    "   \n",
    "    #Location to export trained models to (in saved_model format)\n",
    "    EXPORT_PATH = \"./models/basebert_re\"\n",
    "\n",
    "    #Initial LR, real LR depends on warm-up and training progress\n",
    "    LEARNING_RATE = 2e-5\n",
    "    #Number of training epochs (how many time to iterate through the training set)\n",
    "    NUM_TRAIN_EPOCHS = 3\n",
    "    #Proportion of training steps(i.e. number of batches) used for warming up (adaptive LR in the begging)\n",
    "    WARMUP_PROPORTION = 0\n",
    "    #Training batch size\n",
    "    BATCH_SIZE = 16\n",
    "    #Batch size during testing/valdiation\n",
    "    V_BATCH_SIZE = 100\n",
    "\n",
    "    #Sentence column name\n",
    "    SENTENCE_COLUMN = \"text\"\n",
    "    #Relation label column name\n",
    "    REL_LABEL_COLUMN = \"rel\"\n",
    "    #Relation argument binding colum name - used if (some of the) relations are not symmetric\n",
    "    #0 - symmetric relation, argument order doesn't matter\n",
    "    #1 - rel(ARG1, ARG2), where ARGS1 is the entity which first appears in the text\n",
    "    #2 - rel(ARG2, ARG1)\n",
    "    #if None, then ignore argument order(i.e. treat all relations as symmetric)\n",
    "    REL_ARG_BINDING_COLUMN = 'direction'\n",
    "\n",
    "    #Entities positions in the dataset\n",
    "    ENTITY1_BEGIN_COLUMN = \"firstCharEnt1\"\n",
    "    ENTITY1_END_COLUMN = \"lastCharEnt1\"\n",
    "    ENTITY2_BEGIN_COLUMN = \"firstCharEnt2\"\n",
    "    ENTITY2_END_COLUMN = \"lastCharEnt2\"\n",
    "\n",
    "\n",
    "    ENTITY1_START_TAG = \"e1b\"\n",
    "    ENTITY1_END_TAG = \"e1e\"\n",
    "    ENTITY2_START_TAG = \"e2b\"\n",
    "    ENTITY2_END_TAG = \"e2e\"\n",
    "\n",
    "    ENTITY1_START_TAG_ID = 10\n",
    "    ENTITY1_END_TAG_ID = 11\n",
    "    ENTITY2_START_TAG_ID = 12\n",
    "    ENTITY2_END_TAG_ID = 13\n",
    "\n",
    "    \n",
    "    NUM_HIDDEN_UNITS = 0\n",
    "    \n",
    "    DROPOUT_RATE = 0\n",
    "\n",
    "    #stadard padding id value for Bert models\n",
    "    PAD_ID = 0\n",
    "\n",
    "    #proportion of training examples\n",
    "    TRAIN_SET_PROB = 0.8\n",
    "    \n",
    "    #Not used at the moment\n",
    "    REPLACE_ARG_PROB = 0    \n",
    "    \n",
    "    USE_ENTITY_POSITIONS = True\n",
    "    USE_CLS_POSITION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6HGO7u-_-Ei"
   },
   "outputs": [],
   "source": [
    "# By default, we will use BaseBert (see step 1 in Main to change it)\n",
    "BertREConfig = BaseBertI2B2Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEt6gSySHOO0"
   },
   "source": [
    "## Data collection\n",
    "Set of functions to get input data from pandas or Spark dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cda7eoHxHkfG"
   },
   "source": [
    "### Reading RE data from a pandas dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWEMFeYJHh4Z"
   },
   "outputs": [],
   "source": [
    "def collect_data_from_pandas_dataset(dataset):\n",
    "    \n",
    "    rel_labels = sorted(dataset[BertREConfig.REL_LABEL_COLUMN].unique())\n",
    "    \n",
    "    def process_row(row):\n",
    "\n",
    "        row[\"sentence\"] = annotate_sentence(\n",
    "            row[BertREConfig.SENTENCE_COLUMN], \n",
    "            row[BertREConfig.ENTITY1_BEGIN_COLUMN],\n",
    "            row[BertREConfig.ENTITY1_END_COLUMN],\n",
    "            row[BertREConfig.ENTITY2_BEGIN_COLUMN],\n",
    "            row[BertREConfig.ENTITY2_END_COLUMN]\n",
    "        )\n",
    "        row[\"rel_label_id\"] = rel_labels.index(row[BertREConfig.REL_LABEL_COLUMN])\n",
    "        row[\"rel_arg_binding\"] = row[BertREConfig.REL_ARG_BINDING_COLUMN] if BertREConfig.REL_ARG_BINDING_COLUMN else 0\n",
    "\n",
    "        return row\n",
    "    \n",
    "    \n",
    "    dataset = dataset.apply(process_row, axis=1)\n",
    "    \n",
    "    return dataset.sentence, dataset.rel_label_id, dataset.rel_arg_binding, rel_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oa6ZlI3RHoXp"
   },
   "source": [
    "### Reading RE data from a spark dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yC9xTGgjHn3r"
   },
   "outputs": [],
   "source": [
    "def collect_data_from_spark_dataset(dataset):\n",
    "    \n",
    "    rel_labels = sorted([row[0] for row in dataset.select(BertREConfig.REL_LABEL_COLUMN).distinct().collect()])\n",
    "    \n",
    "    def process_row(row):\n",
    "        sentence = annotate_sentence(\n",
    "            row[BertREConfig.SENTENCE_COLUMN], \n",
    "            int(row[BertREConfig.ENTITY1_BEGIN_COLUMN]),\n",
    "            int(row[BertREConfig.ENTITY1_END_COLUMN]),\n",
    "            int(row[BertREConfig.ENTITY2_BEGIN_COLUMN]),\n",
    "            int(row[BertREConfig.ENTITY2_END_COLUMN])\n",
    "        )       \n",
    "        rel_label_id = rel_labels.index(row[BertREConfig.REL_LABEL_COLUMN])\n",
    "        \n",
    "        rel_arg_binding = (\n",
    "            row[BertREConfig.REL_ARG_BINDING_COLUMN] if BertREConfig.REL_ARG_BINDING_COLUMN else 0)\n",
    "        \n",
    "        return (sentence, rel_label_id, rel_arg_binding)\n",
    "    \n",
    "    sentences, rel_label_ids, rel_arg_bindings = tuple(\n",
    "        map(list, zip(*dataset.rdd.map(process_row).collect())))\n",
    "    \n",
    "    return sentences, rel_label_ids, rel_arg_bindings, rel_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hO0lZQ6Mvw_"
   },
   "source": [
    "## Data annotation\n",
    "Set of functions to properly annnotate the sentences using Bert reserved tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "TTvWYiKHC-wL"
   },
   "outputs": [],
   "source": [
    "#Add entity markers to Bert vocabulary\n",
    "def update_vocab():\n",
    "    vocab = []\n",
    "\n",
    "    with open(BertREConfig.BERT_VOCAB_PATH, 'r') as F:\n",
    "        vocab = F.readlines()\n",
    "        vocab[BertREConfig.ENTITY1_START_TAG_ID] = BertREConfig.ENTITY1_START_TAG + \"\\n\"\n",
    "        vocab[BertREConfig.ENTITY1_END_TAG_ID] = BertREConfig.ENTITY1_END_TAG + \"\\n\"\n",
    "        vocab[BertREConfig.ENTITY2_START_TAG_ID] = BertREConfig.ENTITY2_START_TAG + \"\\n\"\n",
    "        vocab[BertREConfig.ENTITY2_END_TAG_ID] = BertREConfig.ENTITY2_END_TAG + \"\\n\"\n",
    "\n",
    "    with open(BertREConfig.BERT_VOCAB_PATH, \"w\") as F:\n",
    "        F.writelines(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQ-m2tWNOcZ8"
   },
   "outputs": [],
   "source": [
    "#Tokenize sentence using Bert tokenizer, adding entity markers\n",
    "def tokenize_sentence(sentence, tokenizer, seq_length=BertREConfig.MAX_SEQ_LENGTH, is_test=False):\n",
    "    \n",
    "    tokens = [\"[CLS]\"]\n",
    "\n",
    "    entity_starts = []\n",
    "    entity_ends = []\n",
    "    \n",
    "    for token in tokenizer.tokenize(sentence)[:seq_length - 2]:\n",
    "        if token in [BertREConfig.ENTITY1_START_TAG, BertREConfig.ENTITY2_START_TAG]:\n",
    "            entity_starts.append(len(tokens))\n",
    "            \n",
    "        elif token in [BertREConfig.ENTITY1_END_TAG, BertREConfig.ENTITY2_END_TAG]:\n",
    "            entity_ends.append(len(tokens))\n",
    "            \n",
    "        tokens.append(token)\n",
    "    \n",
    "    tokens.append(\"[SEP]\")    \n",
    "        \n",
    "    if (len(entity_starts) != 2) or (len(entity_ends) != 2):\n",
    "        return False\n",
    "\n",
    "    if not is_test:\n",
    "        if np.random.rand() < BertREConfig.REPLACE_ARG_PROB:\n",
    "            e1_length_diff = (entity_ends[0] - entity_starts[0]) - 2\n",
    "\n",
    "            tokens = tokens[:entity_starts[0] + 1] + [\"[MASK]\"] + tokens[entity_ends[0]:]\n",
    "\n",
    "            entity_ends[0] = entity_starts[0] + 2        \n",
    "\n",
    "            entity_starts[1] = entity_starts[1] - e1_length_diff\n",
    "            entity_ends[1] = entity_ends[1] - e1_length_diff\n",
    "\n",
    "        if np.random.rand() < BertREConfig.REPLACE_ARG_PROB:\n",
    "            tokens = tokens[:entity_starts[1] + 1] + [\"[MASK]\"] + tokens[entity_ends[1]:]\n",
    "            entity_ends[1] = entity_starts[1] + 2        \n",
    "    \n",
    "    assert(tokens[entity_starts[0]] == BertREConfig.ENTITY1_START_TAG)\n",
    "    assert(tokens[entity_starts[1]] == BertREConfig.ENTITY2_START_TAG)\n",
    "    assert(tokens[entity_ends[0]] == BertREConfig.ENTITY1_END_TAG)\n",
    "    assert(tokens[entity_ends[1]] == BertREConfig.ENTITY2_END_TAG)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "    return (input_ids, entity_starts[0], entity_starts[1], tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     22,
     45
    ],
    "id": "yJ47h7yXC-wL"
   },
   "outputs": [],
   "source": [
    "def annotate_sentence(sentence, e1_begin, e1_end, e2_begin, e2_end):\n",
    "    \n",
    "    a1_start = min(e1_begin - 1, e2_begin)    \n",
    "    a1_end = min(e1_end + 1, e2_end + 1)\n",
    "\n",
    "    a2_start = max(e1_begin - 1, e2_begin)\n",
    "    a2_end = max(e1_end + 1, e2_end + 1)\n",
    "    \n",
    "    new_sentence = \" \".join([\n",
    "        sentence[:a1_start], \n",
    "        BertREConfig.ENTITY1_START_TAG, \n",
    "        sentence[a1_start:a1_end],\n",
    "        BertREConfig.ENTITY1_END_TAG, \n",
    "        sentence[a1_end:a2_start],\n",
    "        BertREConfig.ENTITY2_START_TAG, \n",
    "        sentence[a2_start:a2_end],\n",
    "        BertREConfig.ENTITY2_END_TAG, \n",
    "        sentence[a2_end:]\n",
    "    ])\n",
    "\n",
    "    return new_sentence      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZuklXrSOGjr"
   },
   "source": [
    "## Feature Engineering\n",
    "RE Feature Engineering consists of token ids (input_ids), entities POS and label ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "id": "7UgoCaSEC-wM"
   },
   "outputs": [],
   "source": [
    "#Representation of RE featurues\n",
    "class REFeatures(object):\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"{} ({})\".format(\n",
    "            \", \".join(\n",
    "                map(lambda x: str(x), self.input_ids)), \n",
    "            self.sentence)\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_ids,\n",
    "                 entity1_pos,\n",
    "                 entity2_pos,\n",
    "                 rel_label_id,\n",
    "                 rel_arg_binding,\n",
    "                 sentence=\"\"):\n",
    "\n",
    "        self.input_ids = input_ids\n",
    "        self.entity1_pos = entity1_pos\n",
    "        self.entity2_pos = entity2_pos\n",
    "        self.rel_label_id = rel_label_id\n",
    "        self.rel_arg_binding = rel_arg_binding\n",
    "        self.sentence = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKDfz-YtS1Uy"
   },
   "outputs": [],
   "source": [
    "#Create RE features from a list of sentences and targets\n",
    "def make_features(sentences, targets, tokenizer, is_test=False):    \n",
    "    features = []\n",
    "    for i in range(len(sentences)):\n",
    "        ts = tokenize_sentence(sentences[i], tokenizer, is_test=is_test)\n",
    "        if ts:\n",
    "            features.append(\n",
    "                REFeatures(\n",
    "                    input_ids=ts[0],\n",
    "                    entity1_pos=ts[1],\n",
    "                    entity2_pos=ts[2],\n",
    "                    rel_label_id=targets[i][0],\n",
    "                    rel_arg_binding=targets[i][1],\n",
    "                    sentence=\" \".join(ts[3])\n",
    "                ))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jtoNOaRTlq-"
   },
   "source": [
    "## Batches creation\n",
    "For feeding the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVhHAaF3Thd5"
   },
   "outputs": [],
   "source": [
    "#Make a batch of training /testing examples. \n",
    "#If max_seq_len is None, then use the sequence max length in the batch\n",
    "\n",
    "def make_batch(features, max_seq_len = None):\n",
    "    batch_size = len(features)\n",
    "    use_rel_args = BertREConfig.REL_ARG_BINDING_COLUMN is not None\n",
    "    if max_seq_len is None:\n",
    "        max_seq_len = max([len(f.input_ids) for f in features])\n",
    "    \n",
    "    input_ids = np.ones([batch_size, max_seq_len], dtype=np.int32) * BertREConfig.PAD_ID\n",
    "    input_mask = np.zeros([batch_size, max_seq_len], dtype=np.int32)\n",
    "    segment_ids = np.zeros([batch_size, max_seq_len], dtype=np.int32)\n",
    "    entity1_pos = np.zeros([batch_size], dtype=np.int32)\n",
    "    entity2_pos = np.zeros([batch_size], dtype=np.int32)\n",
    "    rel_label_ids = np.zeros([batch_size], dtype=np.int32)\n",
    "    rel_arg_bindings = np.zeros([batch_size], dtype=np.int32)\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for f in features:\n",
    "        \n",
    "        input_ids[i, :len(f.input_ids)] = np.array(f.input_ids)\n",
    "        input_mask[i, :len(f.input_ids)] = 1\n",
    "        rel_label_ids[i] = f.rel_label_id\n",
    "        rel_arg_bindings[i] = f.rel_arg_binding\n",
    "        entity1_pos[i] = f.entity1_pos\n",
    "        entity2_pos[i] = f.entity2_pos\n",
    "        i += 1\n",
    "    \n",
    "    batch = {\n",
    "        \"input_ids:0\": input_ids,\n",
    "        \"input_mask:0\": input_mask,\n",
    "        \"segment_ids:0\": segment_ids,\n",
    "        \"rel_label_ids:0\": rel_label_ids,\n",
    "        \"entity1_pos:0\": entity1_pos,\n",
    "        \"entity2_pos:0\": entity2_pos,\n",
    "    }\n",
    "    \n",
    "    if use_rel_args:\n",
    "        batch[\"rel_arg_bindings:0\"] = rel_arg_bindings\n",
    "        \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9sho0T5S8EH"
   },
   "source": [
    "## Optimizer creation\n",
    "To carry out gradient descent and weight update with specific warm up, learning rate, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "id": "3OiHJCQTC-wM"
   },
   "outputs": [],
   "source": [
    "#Create Bert RE optimizer graph\n",
    "def create_optimizer(loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu):\n",
    "    \n",
    "    global_step = tf.train.get_or_create_global_step()    \n",
    "    \n",
    "    # Implements linear decay of the learning rate.\n",
    "    learning_rate = tf.train.polynomial_decay(\n",
    "      learning_rate,\n",
    "      global_step,\n",
    "      num_train_steps,\n",
    "      end_learning_rate=0.0,\n",
    "      power=1.0,\n",
    "      cycle=False)\n",
    "\n",
    "    tf.identity(learning_rate, name=\"c_lr\")\n",
    "    \n",
    "    # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n",
    "    # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
    "    \n",
    "    global_steps_int = tf.cast(global_step, tf.int32)\n",
    "    warmup_steps_int = num_warmup_steps\n",
    "\n",
    "    global_steps_float = tf.cast(global_steps_int, tf.float32)\n",
    "    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
    "\n",
    "    warmup_percent_done = global_steps_float / warmup_steps_float\n",
    "    warmup_learning_rate = learning_rate * warmup_percent_done\n",
    "\n",
    "    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
    "    learning_rate = (\n",
    "        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
    "\n",
    "    \n",
    "    tf.identity(learning_rate, name=\"c_lr2\")\n",
    "    \n",
    "    # It is recommended that you use this optimizer for fine tuning, since this\n",
    "    # is how the model was trained (note that the Adam m/v variables are NOT\n",
    "    # loaded from init_checkpoint.)\n",
    "    optimizer = optimization.AdamWeightDecayOptimizer(\n",
    "      learning_rate=learning_rate,\n",
    "      weight_decay_rate=0.01,\n",
    "      beta_1=0.9,\n",
    "      beta_2=0.999,\n",
    "      epsilon=1e-6,\n",
    "      exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads = tf.gradients(loss, tvars)\n",
    "\n",
    "    # This is how the model was pre-trained.\n",
    "    (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
    "\n",
    "    train_op = optimizer.apply_gradients(\n",
    "      zip(grads, tvars), global_step=global_step)\n",
    "\n",
    "    # Normally the global step update is done inside of `apply_gradients`.\n",
    "    # However, `AdamWeightDecayOptimizer` doesn't do this. But if you use\n",
    "    # a different optimizer, you should probably take this line out.\n",
    "    new_global_step = global_step + 1\n",
    "    train_op = tf.group(train_op, [global_step.assign(new_global_step)], name=\"optimizer\")\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mo-ebbmCT1fR"
   },
   "source": [
    "## BERT model creation\n",
    "Creationg of the model using BERT architecture on TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a14QFWt9TFsR"
   },
   "outputs": [],
   "source": [
    "#Create Bert RE model graph, for training (is_trainable = True) and for inference (is_trainable = False)\n",
    "def create_model(\n",
    "    num_relations, \n",
    "    num_arg_bindings=3, \n",
    "    num_hidden_units=BertREConfig.NUM_HIDDEN_UNITS, \n",
    "    chkpoint_path=None, \n",
    "    is_trainable=True,\n",
    "    config=config):\n",
    "    \n",
    "    with tf.Session(config=config) as session:    \n",
    "\n",
    "        num_train_steps = tf.compat.v1.placeholder_with_default(\n",
    "            input=tf.constant(1000, dtype=tf.float32), shape=(), name=\"num_train_steps\")\n",
    "        \n",
    "        num_warm_up_steps = tf.compat.v1.placeholder_with_default(\n",
    "            input=tf.cast(tf.round(0.1 * num_train_steps), tf.int32), shape=(), name=\"num_warm_up_steps\")\n",
    "\n",
    "        input_ids = tf.compat.v1.placeholder(\n",
    "            dtype=tf.compat.v1.int32, shape=(None, None), name=\"input_ids\")\n",
    "        \n",
    "        batch_size = tf.shape(input_ids)[0]\n",
    "        seq_len = tf.shape(input_ids)[1]\n",
    "        \n",
    "        input_mask = tf.compat.v1.placeholder(\n",
    "            dtype=tf.compat.v1.int32, shape=(None, None), name=\"input_mask\")\n",
    "        \n",
    "        segment_ids = tf.compat.v1.placeholder(\n",
    "            dtype=tf.compat.v1.int32, shape=(None, None), name=\"segment_ids\")\n",
    "        \n",
    "        rel_label_ids = tf.compat.v1.placeholder(\n",
    "            dtype=tf.compat.v1.int32, shape=(None), name=\"rel_label_ids\")\n",
    "        \n",
    "        rel_arg_bindings = tf.compat.v1.placeholder_with_default(\n",
    "            input=tf.zeros(shape=(batch_size),dtype=tf.compat.v1.int32), shape=(None), name=\"rel_arg_bindings\")\n",
    "        \n",
    "        entity1_pos = tf.compat.v1.placeholder(\n",
    "            dtype=tf.compat.v1.int32, shape=(None), name=\"entity1_pos\")\n",
    "        \n",
    "        entity2_pos = tf.compat.v1.placeholder(\n",
    "            dtype=tf.compat.v1.int32, shape=(None), name=\"entity2_pos\")\n",
    "        \n",
    "        dropout_rate = tf.compat.v1.placeholder_with_default(\n",
    "            input=tf.constant(BertREConfig.DROPOUT_RATE, dtype=tf.float32), shape=(), name=\"dropout_rate\")\n",
    "        \n",
    "        learning_rate = tf.compat.v1.placeholder_with_default(\n",
    "            input=tf.constant(2e-5, dtype=tf.float32), shape=(), name=\"learning_rate\")\n",
    "        \n",
    "        config = modeling.BertConfig.from_json_file(BertREConfig.BERT_MODEL_CONFIG_PATH)\n",
    "\n",
    "        bert_model = modeling.BertModel(\n",
    "            config=config,\n",
    "            is_training=is_trainable,\n",
    "            input_ids=input_ids,\n",
    "            input_mask=input_mask,\n",
    "            token_type_ids=segment_ids)\n",
    "\n",
    "        if chkpoint_path:\n",
    "            tvars = tf.trainable_variables()\n",
    "            (assignment_map, initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(\n",
    "                tvars, chkpoint_path)\n",
    "            tf.train.init_from_checkpoint(chkpoint_path, assignment_map)    \n",
    "\n",
    "        output_layer = bert_model.get_sequence_output()\n",
    "\n",
    "        if BertREConfig.USE_ENTITY_POSITIONS:\n",
    "            #get entity start marker embeddings\n",
    "\n",
    "            #E1 mask\n",
    "            entity1_mask =  tf.repeat(\n",
    "                tf.one_hot(entity1_pos, seq_len), \n",
    "                config.hidden_size, \n",
    "                axis=1)\n",
    "\n",
    "            #E2 mask\n",
    "            entity2_mask =  tf.repeat(\n",
    "                tf.one_hot(entity2_pos, seq_len), \n",
    "                config.hidden_size, \n",
    "                axis=1)\n",
    "\n",
    "\n",
    "            #Hidden layer representation for E1\n",
    "            entity1_embd = tf.reduce_sum(\n",
    "                tf.reshape(\n",
    "                    (tf.reshape(output_layer, shape=(batch_size, -1)) * entity1_mask), \n",
    "                    shape=[batch_size, seq_len, config.hidden_size]), \n",
    "                axis=1)\n",
    "\n",
    "            #Hidden layer representation for E2\n",
    "            entity2_embd = tf.reduce_sum(\n",
    "                tf.reshape(\n",
    "                    (tf.reshape(output_layer, shape=(batch_size, -1)) * entity2_mask), \n",
    "                    shape=[batch_size, seq_len, config.hidden_size]), \n",
    "                axis=1)\n",
    "\n",
    "            #Concat representions\n",
    "            if BertREConfig.USE_CLS_POSITION:                \n",
    "                classification_layer = tf.concat([entity1_embd, entity2_embd, output_layer[:,0,:]], axis=1)\n",
    "            else:\n",
    "                classification_layer = tf.concat([entity1_embd, entity2_embd], axis=1)\n",
    "        else:\n",
    "            if not BertREConfig.USE_CLS_POSITION:\n",
    "                raise(\"Either USE_ENTITY_POSITIONS or USE_CLS_POSITION should be set to True.\")\n",
    "            else:\n",
    "                classification_layer = output_layer[:,0,:]\n",
    "                \n",
    "        '''Add full connection layer and dropout layer'''\n",
    "        \n",
    "        if num_hidden_units > 0:\n",
    "            fc = tf.layers.dense(\n",
    "                classification_layer, \n",
    "                num_hidden_units, \n",
    "                name='fc1')\n",
    "            fc = tf.nn.relu(fc)\n",
    "        else:\n",
    "            fc = tf.identity(classification_layer, name='fc1')\n",
    "\n",
    "        fc = tf.nn.dropout(fc, rate=dropout_rate)\n",
    "\n",
    "        '''logits'''\n",
    "        rel_label_logits = tf.layers.dense(fc, num_relations, name='rel_label_logits')\n",
    "        rel_label_log_probs = tf.nn.softmax(rel_label_logits, name=\"rel_label_probs\")\n",
    "        rel_label_predictions = tf.argmax(\n",
    "            rel_label_log_probs, axis=-1, output_type=tf.int32, name=\"rel_label_predictions\")\n",
    "\n",
    "        if num_arg_bindings > 1:    \n",
    "            rel_arg_binding_logits = tf.layers.dense(fc, num_arg_bindings, name='rel_arg_binding_logits')\n",
    "            rel_arg_binding_probs = tf.nn.softmax(rel_arg_binding_logits, name=\"rel_arg_binding_probs\")\n",
    "            rel_arg_binding_predictions = tf.argmax(\n",
    "                rel_arg_binding_probs, axis=-1, output_type=tf.int32, name=\"rel_arg_binding_predictions\")\n",
    "        else:\n",
    "            rel_arg_binding_probs = tf.ones_like(rel_arg_bindings, name=\"rel_arg_binding_probs\")\n",
    "            rel_arg_binding_predictions = tf.zeros_like(\n",
    "                rel_arg_bindings, name=\"rel_arg_binding_predictions\")\n",
    "\n",
    "        '''Calculate loss. Convert predicted labels into one hot form. '''            \n",
    "        rel_label_targets = tf.one_hot(rel_label_ids, depth=num_relations)\n",
    "        rel_label_loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                labels=rel_label_targets,\n",
    "                logits=rel_label_logits)\n",
    "\n",
    "        if num_arg_bindings > 1:                \n",
    "            rel_arg_binding_targets = tf.one_hot(rel_arg_bindings, depth=num_arg_bindings)\n",
    "            rel_arg_binding_loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                    labels=rel_arg_binding_targets,\n",
    "                    logits=rel_arg_binding_logits)\n",
    "        else:\n",
    "            rel_arg_binding_loss = 0                    \n",
    "        \n",
    "        rel_label_example_accuracy = tf.cast(\n",
    "            tf.equal(rel_label_predictions, rel_label_ids), tf.float32, name=\"rel_label_acc\")\n",
    "        \n",
    "        rel_label_accuracy = tf.reduce_mean(rel_label_example_accuracy, name=\"rel_label_mean_acc\")        \n",
    "        \n",
    "        rel_arg_binding_example_accuracy = tf.cast(\n",
    "            tf.equal(rel_arg_binding_predictions, rel_arg_bindings), tf.float32, name=\"rel_arg_binding_acc\")\n",
    "        \n",
    "        rel_arg_binding_accuracy = tf.reduce_mean(\n",
    "            rel_arg_binding_example_accuracy, name=\"rel_arg_binding_mean_acc\")\n",
    "        \n",
    "        total_example_accuracy = tf.identity(\n",
    "            rel_label_example_accuracy * rel_arg_binding_example_accuracy, name=\"total_acc\")\n",
    "        \n",
    "        total_accuracy = tf.identity(total_example_accuracy, name=\"total_mean_acc\")\n",
    "        \n",
    "        \n",
    "        loss = tf.reduce_mean(rel_label_loss + rel_arg_binding_loss, name=\"loss\")\n",
    "\n",
    "        if is_trainable:\n",
    "            train_op = create_optimizer(\n",
    "                loss, \n",
    "                learning_rate, \n",
    "                num_train_steps, \n",
    "                num_warm_up_steps,\n",
    "                use_tpu=False)        \n",
    "        else:\n",
    "            train_op = tf.no_op()\n",
    "            \n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        return (\n",
    "                train_op,\n",
    "                loss,       \n",
    "                total_accuracy,\n",
    "                (rel_label_predictions, rel_arg_binding_predictions), \n",
    "                (rel_label_log_probs, rel_arg_binding_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XWjsPywUsNd"
   },
   "source": [
    "## Model saving\n",
    "Function to export the trained BERT model to disk in TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "iK6lj4PiC-wN"
   },
   "outputs": [],
   "source": [
    "def export_model(model_id, is_trainable = True, num_arg_bindings = 3, config=config):\n",
    "    \n",
    "    with tf.Session(config=config) as session:\n",
    "    \n",
    "        model = create_model(\n",
    "            len(rel_labels), \n",
    "            is_trainable=is_trainable,\n",
    "            num_arg_bindings=num_arg_bindings)\n",
    "    \n",
    "        input_tensors = {}\n",
    "        output_tensors = {}\n",
    "\n",
    "        input_tensors_names = {\n",
    "            \"input_ids:0\",\n",
    "            \"input_mask:0\",\n",
    "            \"segment_ids:0\",\n",
    "            \"entity1_pos:0\",\n",
    "            \"entity2_pos:0\",\n",
    "        }\n",
    "\n",
    "        output_tensors_names = [\n",
    "            \"loss:0\",\n",
    "            \"rel_label_acc:0\",\n",
    "            \"rel_arg_binding_acc:0\",\n",
    "            \"total_acc:0\",\n",
    "            \"rel_label_probs:0\",\n",
    "            \"rel_label_predictions:0\",\n",
    "            \"rel_arg_binding_probs:0\",\n",
    "            \"rel_arg_binding_predictions:0\"        \n",
    "        ]\n",
    "\n",
    "\n",
    "        for k in input_tensors_names:\n",
    "            t = session.graph.get_tensor_by_name(k)\n",
    "            input_tensors[t.name] = t\n",
    "\n",
    "        for k in output_tensors_names:\n",
    "            t = session.graph.get_tensor_by_name(k)\n",
    "            output_tensors[k] = t\n",
    "        \n",
    "        print(\"{} trainable variables: \".format(len(tf.trainable_variables())))\n",
    "        size_f = lambda v: reduce(lambda x, y: x*y, v.get_shape().as_list())\n",
    "        n = sum(size_f(v) for v in tf.trainable_variables())\n",
    "        print(\"{} trainbale parameters.\".format(n))    \n",
    "        \n",
    "        tf.train.Saver().restore(session, f\"{BertREConfig.CHKPOINT_PATH}/{model_id}/model\")\n",
    "\n",
    "        shutil.rmtree(BertREConfig.EXPORT_PATH, ignore_errors=True)\n",
    "\n",
    "        #save model\n",
    "        tf.saved_model.simple_save(\n",
    "            session,\n",
    "            BertREConfig.EXPORT_PATH,\n",
    "            inputs=input_tensors,\n",
    "            outputs=output_tensors\n",
    "        )\n",
    "\n",
    "        #copy assets to the destiation folder\n",
    "        shutil.copytree(\n",
    "            f\"{BertREConfig.CHKPOINT_PATH}/{model_id}/assets\", \n",
    "            f\"{BertREConfig.EXPORT_PATH}/assets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcIgaAyfYH_V"
   },
   "source": [
    "## Training\n",
    "Function to train the model for RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kZfx5USVgxQP",
    "outputId": "2cd504ea-89fb-47c3-8f07-0e5a56aa3069"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan  6 20:10:48 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   30C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "id": "PaKoD-QDC-wO"
   },
   "outputs": [],
   "source": [
    "#Train a Bert RE model and save it in the checkpoints folder\n",
    "def train_model(model_id, train_features, test_features, rel_labels, num_arg_bindings = 3, config=config):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    use_rel_args = BertREConfig.REL_ARG_BINDING_COLUMN is not None\n",
    "    \n",
    "    with tf.Session(config=config) as session:\n",
    "\n",
    "        model = create_model(\n",
    "            len(rel_labels), \n",
    "            chkpoint_path=BertREConfig.BERT_MODEL_CHECKPOINT_PATH, \n",
    "            num_arg_bindings=num_arg_bindings, \n",
    "            is_trainable=True,\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        session.run(\"init\")\n",
    "\n",
    "        num_train_steps = (BertREConfig.NUM_TRAIN_EPOCHS * len(train_features)) // BertREConfig.BATCH_SIZE    \n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        print(\"{:^11}{:^11}{:>10}{:>10}{:>10}{:>10}{:>10}{:>10}{:>10}\".format(\n",
    "            \"Epoch\", \"Batch\", \n",
    "            \"Loss\", \n",
    "            \"L_ACC\", \"Arg_ACC\", \"ACC\",\n",
    "            \"vL_ACC\", \"vArg_ACC\", \"vACC\"))\n",
    "\n",
    "        for e in range(BertREConfig.NUM_TRAIN_EPOCHS):\n",
    "\n",
    "            np.random.shuffle(train_features)            \n",
    "\n",
    "            b_loss = []\n",
    "            b_rel_label_acc = []\n",
    "            b_rel_arg_binding_acc = []\n",
    "            b_total_acc = []\n",
    "            for b in range(0, len(train_features) // BertREConfig.BATCH_SIZE):\n",
    "\n",
    "                batch = make_batch(\n",
    "                    train_features[b * BertREConfig.BATCH_SIZE: (b + 1) * BertREConfig.BATCH_SIZE]\n",
    "                )#, max_seq_len=MAX_SEQ_LENGTH)\n",
    "\n",
    "                data = batch\n",
    "\n",
    "                if b == 0:\n",
    "                    data[\"num_train_steps:0\"] = num_train_steps\n",
    "                    data[\"learning_rate:0\"] = BertREConfig.LEARNING_RATE\n",
    "\n",
    "                eval_tensors = [\n",
    "                    \"optimizer\", \n",
    "                    \"loss:0\", \n",
    "                    \"rel_label_mean_acc:0\",\n",
    "                    \"rel_arg_binding_mean_acc:0\",\n",
    "                    \"total_mean_acc:0\"\n",
    "                ]\n",
    "                _, loss, rel_label_acc, rel_arg_bind_acc, total_acc = session.run(\n",
    "                    eval_tensors, feed_dict=data)\n",
    "                b_loss.append(loss)\n",
    "                b_rel_label_acc.append(rel_label_acc)\n",
    "                b_rel_arg_binding_acc.append(rel_arg_bind_acc)\n",
    "                b_total_acc.append(total_acc)\n",
    "\n",
    "                print(\"\\r{:>5}/{:<5}{:>5}/{:<5}{:>10.4f}{:>10.3f}{:>10.3f}{:>10.3f}\".format(\n",
    "                        e+1,\n",
    "                        BertREConfig.NUM_TRAIN_EPOCHS,\n",
    "                        b + 1, \n",
    "                        len(train_features) // BertREConfig.BATCH_SIZE,\n",
    "                        np.mean(b_loss), \n",
    "                        np.mean(b_rel_label_acc), \n",
    "                        np.mean(b_rel_arg_binding_acc), \n",
    "                        np.mean(b_total_acc)  \n",
    "                    ), end=\"\")\n",
    "\n",
    "\n",
    "            v_rel_label_acc = []\n",
    "            v_rel_arg_binding_acc = []\n",
    "            v_total_acc = []\n",
    "\n",
    "            for v_b in range(0, len(test_features) // BertREConfig.V_BATCH_SIZE):\n",
    "                batch = make_batch(\n",
    "                    test_features[v_b * BertREConfig.V_BATCH_SIZE: (v_b + 1) * BertREConfig.V_BATCH_SIZE])\n",
    "\n",
    "                data = batch\n",
    "\n",
    "                eval_tensors = [\n",
    "                    \"rel_label_mean_acc:0\",\n",
    "                    \"rel_arg_binding_mean_acc:0\",\n",
    "                    \"total_mean_acc:0\",\n",
    "                ]\n",
    "\n",
    "                rel_label_acc, rel_arg_bind_acc, total_acc = session.run(eval_tensors, feed_dict=data)\n",
    "                v_rel_label_acc.append(rel_label_acc)\n",
    "                v_rel_arg_binding_acc.append(rel_arg_bind_acc)\n",
    "                v_total_acc.append(total_acc)\n",
    "\n",
    "            print(\"{:>10.3f}{:>10.3f}{:>10.3f}\".format(\n",
    "                np.mean(v_rel_label_acc), \n",
    "                np.mean(v_rel_arg_binding_acc), \n",
    "                np.mean(v_total_acc)))                \n",
    "\n",
    "\n",
    "        \n",
    "        shutil.rmtree(f\"{BertREConfig.CHKPOINT_PATH}/{model_id}\", ignore_errors=True)\n",
    "        os.mkdir(f\"{BertREConfig.CHKPOINT_PATH}/{model_id}\")\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(session, f\"{BertREConfig.CHKPOINT_PATH}/{model_id}/model\")\n",
    "        \n",
    "        os.mkdir(f\"{BertREConfig.CHKPOINT_PATH}/{model_id}/assets/\")\n",
    "        \n",
    "        shutil.copy(\n",
    "            BertREConfig.BERT_VOCAB_PATH, \n",
    "            f\"{BertREConfig.CHKPOINT_PATH}/{model_id}/assets/vocab.txt\")\n",
    "        \n",
    "        with open(f\"{BertREConfig.CHKPOINT_PATH}/{model_id}/assets/categories.txt\", \"wt\") as F:\n",
    "            F.writelines(\"\\n\".join(rel_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sfy0E-sdYJmV"
   },
   "source": [
    "## Evaluation\n",
    "Functions to get the metrics on the RE model and print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     62
    ],
    "id": "wxZCnuUJC-wO"
   },
   "outputs": [],
   "source": [
    "def eval_metrics(model_id, features, rel_labels, num_arg_bindings = 3, exclude_rels=[], config=config):\n",
    "    with tf.Session(config=config) as session:\n",
    "\n",
    "        model = create_model(\n",
    "            len(rel_labels), \n",
    "            num_arg_bindings=num_arg_bindings,\n",
    "            is_trainable=False)\n",
    "        \n",
    "        tf.train.Saver().restore(session, f\"{BertREConfig.CHKPOINT_PATH}/{model_id}/model\")\n",
    "\n",
    "        metrics_data = {}\n",
    "        for rel in rel_labels:\n",
    "            metrics_data[rel] = ([], [], [])\n",
    "            \n",
    "        for v_b in range(0, len(features) // BertREConfig.V_BATCH_SIZE):\n",
    "            batch = make_batch(\n",
    "                features[v_b * BertREConfig.V_BATCH_SIZE: (v_b + 1) * BertREConfig.V_BATCH_SIZE])\n",
    "\n",
    "            data = batch\n",
    "\n",
    "            eval_tensors = [\"total_acc:0\", \"rel_label_ids:0\", \"rel_label_predictions:0\"]\n",
    "\n",
    "            total_acc, rel_label_ids, rel_label_preds = session.run(eval_tensors, feed_dict=data)\n",
    "            \n",
    "            for i in range(len(rel_label_ids)):\n",
    "                acc = total_acc[i]\n",
    "                pred = rel_label_preds[i]\n",
    "                target = rel_label_ids[i]\n",
    "                rel_target = rel_labels[target]\n",
    "                rel_pred = rel_labels[pred]\n",
    "                \n",
    "                metrics_data[rel_target][2].append(1)\n",
    "                \n",
    "                if acc:\n",
    "                    metrics_data[rel_target][0].append(1)\n",
    "                    metrics_data[rel_pred][1].append(1)\n",
    "                else:\n",
    "                    metrics_data[rel_target][0].append(0)\n",
    "                    metrics_data[rel_pred][1].append(0)                \n",
    "\n",
    "        results = {}        \n",
    "        \n",
    "        for rel in [rel for rel in rel_labels if rel not in exclude_rels]:\n",
    "            if len(metrics_data[rel][0]):\n",
    "                recall = np.mean(metrics_data[rel][0])\n",
    "            else:\n",
    "                recall = 0\n",
    "            if len(metrics_data[rel][1]):\n",
    "                precision = np.mean(metrics_data[rel][1])\n",
    "            else:\n",
    "                precision = 0\n",
    "            if (recall + precision):\n",
    "                f1 = 2 * (recall * precision) / (recall + precision)\n",
    "            else:\n",
    "                f1 = np.NaN\n",
    "               \n",
    "            support = np.sum(metrics_data[rel][2])\n",
    "            \n",
    "            results[rel] = (recall, precision, f1, support)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLmYyut-Yait"
   },
   "outputs": [],
   "source": [
    "def print_metrics(results):\n",
    "    print(\"\\n\")\n",
    "    print(\"{:<15}{:>10}{:>10}{:>10}{:>10}\\n\".format(\"Relation\", \"Recall\", \"Precision\", \"F1\", \"Support\"))\n",
    "\n",
    "    for rel in results:\n",
    "\n",
    "        print(f\"{rel:<15}{results[rel][0]:>10.3f}{results[rel][1]:>10.3f}{results[rel][2]:>10.3f}{results[rel][3]:>10}\")\n",
    "\n",
    "    mean_recall = np.mean([results[rel][0] for rel in results])\n",
    "    mean_precision = np.mean([results[rel][1] for rel in results])\n",
    "    mean_f1 = np.mean([results[rel][2] for rel in results])\n",
    "\n",
    "    support_sum = np.sum([results[rel][3] for rel in results])\n",
    "\n",
    "    w_mean_recall = np.sum([results[rel][0] * results[rel][3] for rel in results]) / support_sum\n",
    "    w_mean_precision = np.sum([results[rel][1] * results[rel][3] for rel in results]) / support_sum\n",
    "    w_mean_f1 = np.sum([results[rel][2] * results[rel][3] for rel in results]) / support_sum\n",
    "\n",
    "\n",
    "    metrics_name = \"Avg.\"\n",
    "\n",
    "    print(f\"\\n{metrics_name:<15}{mean_recall:>10.3f}{mean_precision:>10.3f}{mean_f1:>10.3f}\")\n",
    "\n",
    "    metrics_name = \"Weighted Avg.\"\n",
    "\n",
    "    print(f\"\\n{metrics_name:<15}{w_mean_recall:>10.3f}{w_mean_precision:>10.3f}{w_mean_f1:>10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nW1WF2UYhZ6"
   },
   "source": [
    "## MAIN: STEP-BY-STEP RE MODEL TRAINING EXECUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "De4-ElQ5Y16S"
   },
   "source": [
    "### 1. Using BioBERT hyperparams instead of BertBase ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_SBohPwY0xV"
   },
   "outputs": [],
   "source": [
    "# You can change me to BaseBertI2B2Config\n",
    "BertREConfig = BaseBertI2B2Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6l_Z2MvCZHTs"
   },
   "source": [
    "### 2. Update Bert vocabulary with special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45ho-NbBC-wO"
   },
   "outputs": [],
   "source": [
    "#Update Bert vocabylary\n",
    "update_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dM26YYEtaJao"
   },
   "source": [
    "### 3. Creating a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oHNtrmFzaILL"
   },
   "outputs": [],
   "source": [
    "#create tokenizer\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=BertREConfig.BERT_VOCAB_PATH, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWtxzECjbSIb"
   },
   "source": [
    "### 5. Read the input data.\n",
    "It should look like as follows (see output) and have the following columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guu863lUfmT7"
   },
   "source": [
    "| Column      |          Explanation                     |\n",
    "|:-----------:|:----------------------------------------:|\n",
    "|dataset      | train/test                               |\n",
    "|source       | data provider                            |\n",
    "|txt_file     | .txt file                                |\n",
    "|sentence     | tokenized text sentence                  |\n",
    "|sent_id      | sentence id                              |\n",
    "|chunk1       | first entity                             |\n",
    "|begin1       | first token number of the first entity   |\n",
    "|end1         | last token number of the first entity    |\n",
    "|rel          | relation (O for no-relation)             |\n",
    "|chunk2       | second entity                            |\n",
    "|begin2       | first token number of the second entity  |\n",
    "|end2         | last token number of the second entity   |\n",
    "|label1       | label of the first entity                |\n",
    "|label2       | label of the second entity               |\n",
    "|lastCharEnt1 | last char number of the first entity     |\n",
    "|firstCharEnt1| first char number of the first entity    |\n",
    "|lastCharEnt2 | last char number of the second entity    |\n",
    "|firstCharEnt2| first char number of the second entity   |\n",
    "|words_in_ent1| number of words in first entity          |\n",
    "|words_in_ent2| number of words in second entity         |\n",
    "|words_between| word between entities                    |\n",
    "|is_train     | is it used for training?                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EjAJ_5pydV_"
   },
   "source": [
    "Yes, We are ready to train REDL model. Now we will train a REDL model to get relations between **PERSON**, **ORG**, **ROLE** and **DATE** entitties. \n",
    "\n",
    "Let's look at our dataset. \n",
    "\n",
    "Your dataset have to be like this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 654,
     "status": "ok",
     "timestamp": 1673122475295,
     "user": {
      "displayName": "Bünyamin Polat",
      "userId": "03982086590103784785"
     },
     "user_tz": -180
    },
    "id": "3ADL5Gzs3a5z"
   },
   "outputs": [],
   "source": [
    "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings_JSL/Finance/data/relations.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "executionInfo": {
     "elapsed": 2240,
     "status": "ok",
     "timestamp": 1673122483038,
     "user": {
      "displayName": "Bünyamin Polat",
      "userId": "03982086590103784785"
     },
     "user_tz": -180
    },
    "id": "w9fykiyRSYeH",
    "outputId": "aaaf8b6c-b6a6-4f02-ea05-07f108bf79e2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-e5f950c2-03bf-470b-8acb-31ee3ebb2f6e\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>firstCharEnt1</th>\n",
       "      <th>firstCharEnt2</th>\n",
       "      <th>lastCharEnt1</th>\n",
       "      <th>lastCharEnt2</th>\n",
       "      <th>chunk1</th>\n",
       "      <th>chunk2</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>rel</th>\n",
       "      <th>direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>As a result, our business revenues and other r...</td>\n",
       "      <td>233</td>\n",
       "      <td>268</td>\n",
       "      <td>248</td>\n",
       "      <td>291</td>\n",
       "      <td>Matthew Stecker</td>\n",
       "      <td>Chief Executive Officer</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>has_role</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As a result, our business revenues and other r...</td>\n",
       "      <td>296</td>\n",
       "      <td>313</td>\n",
       "      <td>311</td>\n",
       "      <td>338</td>\n",
       "      <td>Mark Szynkowski</td>\n",
       "      <td>Vice President of Finance</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>has_role</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As a result, our business revenues and other r...</td>\n",
       "      <td>296</td>\n",
       "      <td>343</td>\n",
       "      <td>311</td>\n",
       "      <td>371</td>\n",
       "      <td>Mark Szynkowski</td>\n",
       "      <td>Principal Accounting Officer</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>has_role</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>for the year ended December 31, 2020, formatte...</td>\n",
       "      <td>1178</td>\n",
       "      <td>1204</td>\n",
       "      <td>1193</td>\n",
       "      <td>1227</td>\n",
       "      <td>MATTHEW STECKER</td>\n",
       "      <td>Chief Executive Officer</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>has_role</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for the year ended December 31, 2020, formatte...</td>\n",
       "      <td>1204</td>\n",
       "      <td>1271</td>\n",
       "      <td>1227</td>\n",
       "      <td>1286</td>\n",
       "      <td>Chief Executive Officer</td>\n",
       "      <td>Matthew Stecker</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>has_role</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7431</th>\n",
       "      <td>Actual maturities may differ from contractual ...</td>\n",
       "      <td>403</td>\n",
       "      <td>423</td>\n",
       "      <td>421</td>\n",
       "      <td>428</td>\n",
       "      <td>Onkyo Corporation</td>\n",
       "      <td>Onkyo</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7432</th>\n",
       "      <td>\\nF-27\\nEverbridge, Inc.\\nNotes to the Consoli...</td>\n",
       "      <td>285</td>\n",
       "      <td>363</td>\n",
       "      <td>292</td>\n",
       "      <td>373</td>\n",
       "      <td>Company</td>\n",
       "      <td>Techwan SA</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ORG</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7433</th>\n",
       "      <td>Nature of Business and Summary of Significant ...</td>\n",
       "      <td>89</td>\n",
       "      <td>114</td>\n",
       "      <td>111</td>\n",
       "      <td>118</td>\n",
       "      <td>Fair Isaac Corporation</td>\n",
       "      <td>FICO</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7434</th>\n",
       "      <td>These forward-looking statements are not guara...</td>\n",
       "      <td>122</td>\n",
       "      <td>137</td>\n",
       "      <td>133</td>\n",
       "      <td>141</td>\n",
       "      <td>Minute Rice</td>\n",
       "      <td>2006</td>\n",
       "      <td>ORG</td>\n",
       "      <td>DATE</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7435</th>\n",
       "      <td>The Company’s portfolio includes market-leadin...</td>\n",
       "      <td>394</td>\n",
       "      <td>434</td>\n",
       "      <td>417</td>\n",
       "      <td>449</td>\n",
       "      <td>Lumata Deutschland GmbH</td>\n",
       "      <td>Lumata Entities</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7436 rows × 11 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e5f950c2-03bf-470b-8acb-31ee3ebb2f6e')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-e5f950c2-03bf-470b-8acb-31ee3ebb2f6e button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-e5f950c2-03bf-470b-8acb-31ee3ebb2f6e');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                   text  firstCharEnt1  \\\n",
       "0     As a result, our business revenues and other r...            233   \n",
       "1     As a result, our business revenues and other r...            296   \n",
       "2     As a result, our business revenues and other r...            296   \n",
       "3     for the year ended December 31, 2020, formatte...           1178   \n",
       "4     for the year ended December 31, 2020, formatte...           1204   \n",
       "...                                                 ...            ...   \n",
       "7431  Actual maturities may differ from contractual ...            403   \n",
       "7432  \\nF-27\\nEverbridge, Inc.\\nNotes to the Consoli...            285   \n",
       "7433  Nature of Business and Summary of Significant ...             89   \n",
       "7434  These forward-looking statements are not guara...            122   \n",
       "7435  The Company’s portfolio includes market-leadin...            394   \n",
       "\n",
       "      firstCharEnt2  lastCharEnt1  lastCharEnt2                   chunk1  \\\n",
       "0               268           248           291          Matthew Stecker   \n",
       "1               313           311           338          Mark Szynkowski   \n",
       "2               343           311           371          Mark Szynkowski   \n",
       "3              1204          1193          1227          MATTHEW STECKER   \n",
       "4              1271          1227          1286  Chief Executive Officer   \n",
       "...             ...           ...           ...                      ...   \n",
       "7431            423           421           428       Onkyo Corporation    \n",
       "7432            363           292           373                  Company   \n",
       "7433            114           111           118   Fair Isaac Corporation   \n",
       "7434            137           133           141              Minute Rice   \n",
       "7435            434           417           449  Lumata Deutschland GmbH   \n",
       "\n",
       "                            chunk2  label1  label2       rel  direction  \n",
       "0          Chief Executive Officer  PERSON    ROLE  has_role          1  \n",
       "1        Vice President of Finance  PERSON    ROLE  has_role          1  \n",
       "2     Principal Accounting Officer  PERSON    ROLE  has_role          1  \n",
       "3          Chief Executive Officer  PERSON    ROLE  has_role          1  \n",
       "4                  Matthew Stecker    ROLE  PERSON  has_role          2  \n",
       "...                            ...     ...     ...       ...        ...  \n",
       "7431                         Onkyo     ORG   ALIAS     other          0  \n",
       "7432                    Techwan SA     ORG     ORG     other          0  \n",
       "7433                          FICO     ORG   ALIAS     other          0  \n",
       "7434                          2006     ORG    DATE     other          0  \n",
       "7435               Lumata Entities     ORG   ALIAS     other          0  \n",
       "\n",
       "[7436 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('relations.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qoHI5NqjuCSr",
    "outputId": "3fdb4a6a-f065-4230-aa97-c6421b0146d0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label1\n",
       "ORG               2962\n",
       "ROLE              1634\n",
       "PERSON            1108\n",
       "DATE               799\n",
       "PRODUCT            308\n",
       "ALIAS              248\n",
       "PER                181\n",
       "FORMER_NAME        130\n",
       "LOC                 24\n",
       "LIABILITY           15\n",
       "SHAREHOLDER          9\n",
       "AMOUNT               7\n",
       "STOCK_EXCHANGE       5\n",
       "STATE                5\n",
       "PAST                 1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.value_counts('label1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMJUQSXruCSr",
    "outputId": "0bc00ef9-231d-4560-a80d-c88b8f7dd414"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label2\n",
       "ORG               2884\n",
       "ROLE              1585\n",
       "ALIAS             1172\n",
       "DATE               884\n",
       "PERSON             408\n",
       "FORMER_NAME        146\n",
       "PER                135\n",
       "STATE              111\n",
       "SHAREHOLDER         49\n",
       "LOC                 32\n",
       "PAST                10\n",
       "AMOUNT               9\n",
       "PRODUCT              6\n",
       "LIABILITY            4\n",
       "STOCK_EXCHANGE       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.value_counts('label2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6pCobCUvuCSr",
    "outputId": "3505a808-3c3c-4ff8-96c3-3aba2db19903"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rel\n",
       "other                  4071\n",
       "has_role               1713\n",
       "has_role_in_company     650\n",
       "has_role_from           564\n",
       "had_role_until          327\n",
       "works_for               111\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.value_counts('rel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKj-K1dkgukV"
   },
   "outputs": [],
   "source": [
    "#get a list of valid relation names\n",
    "valid_rel_labels = data['rel'].unique()\n",
    "valid_rel_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3aEQUffhJdH"
   },
   "source": [
    "### 6. Create the training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfKsVk08TJQz"
   },
   "outputs": [],
   "source": [
    "df_train = data.sample(frac=0.9, random_state=1)\n",
    "df_test = data.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAhEOmLkhIql"
   },
   "outputs": [],
   "source": [
    "#collect data\n",
    "train_sentences, train_rel_label_ids, train_rel_arg_bindings, rel_labels = (\n",
    "    collect_data_from_pandas_dataset(df_train))\n",
    "\n",
    "test_sentences, test_rel_label_ids, test_rel_arg_bindings, _ = (\n",
    "    collect_data_from_pandas_dataset(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otn-PNtrVFNt"
   },
   "outputs": [],
   "source": [
    "train_sentences = train_sentences.values\n",
    "train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQkdDhf2VG4i"
   },
   "outputs": [],
   "source": [
    "train_rel_label_ids = train_rel_label_ids.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADXviawkVIFq"
   },
   "outputs": [],
   "source": [
    "train_rel_arg_bindings = train_rel_arg_bindings.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGaJETXyVMd7",
    "outputId": "71d0f8c2-b3fe-4290-edec-76d3beadc6ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['had_role_until',\n",
       " 'has_role',\n",
       " 'has_role_from',\n",
       " 'has_role_in_company',\n",
       " 'other',\n",
       " 'works_for']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sWoYdqhWUDD"
   },
   "outputs": [],
   "source": [
    "test_sentences = test_sentences.values\n",
    "test_rel_label_ids = test_rel_label_ids.values\n",
    "test_rel_arg_bindings = test_rel_arg_bindings.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xa_cg2pxhSzL"
   },
   "source": [
    "### 7. Create the features from the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7O2Vr7hhVLY"
   },
   "outputs": [],
   "source": [
    "#create features\n",
    "train_features = make_features(\n",
    "    train_sentences, \n",
    "    list(zip(train_rel_label_ids, train_rel_arg_bindings)),\n",
    "    tokenizer, \n",
    "    is_test=False)\n",
    "\n",
    "if BertREConfig.REPLACE_ARG_PROB and BertREConfig.REPLICATE_DATASET:\n",
    "    train_features += make_features(\n",
    "        train_sentences, \n",
    "        list(zip(train_rel_label_ids, train_rel_arg_bindings)),\n",
    "        tokenizer, \n",
    "        is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "du3lL9UfWbmS"
   },
   "outputs": [],
   "source": [
    "test_features = make_features(\n",
    "    test_sentences, \n",
    "    list(zip(test_rel_label_ids, test_rel_arg_bindings)),\n",
    "    tokenizer, \n",
    "    is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q10cqRFyC-wP",
    "outputId": "41b7045c-c72c-4259-b709-10810e4c9ff2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6193 training examples\n",
      "679 test examples\n"
     ]
    }
   ],
   "source": [
    "print(\"{} training examples\".format(len(train_features)))\n",
    "print(\"{} test examples\".format(len(test_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dceO1UwiO8t"
   },
   "source": [
    "### 8. Training the model\n",
    "Here is where all the fun happens! 🏄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-K7pr_5tin04"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3EscngcHC-wP",
    "outputId": "6221d419-2300-4e29-b815-c0a3348729e2",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch      Batch         Loss     L_ACC   Arg_ACC       ACC    vL_ACC  vArg_ACC      vACC\n",
      "    1/3      387/387      0.6143     0.889     0.920     0.881     0.955     0.965     0.953\n",
      "    2/3      387/387      0.1676     0.972     0.976     0.967     0.963     0.983     0.963\n",
      "    3/3      387/387      0.0620     0.987     0.992     0.984     0.970     0.982     0.970\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    \"EXPERIENCES\", \n",
    "    train_features=train_features, \n",
    "    test_features=test_features, \n",
    "    rel_labels=rel_labels,\n",
    "    num_arg_bindings=3,\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KVfT6p-oVG8"
   },
   "source": [
    "### 9. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DmZ0xIJIC-wP",
    "outputId": "e6a01b3b-5d88-4ef0-9455-75199ecac468"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./trained/EXPERIENCES/model\n",
      "\n",
      "\n",
      "Relation           Recall Precision        F1   Support\n",
      "\n",
      "had_role_until      0.966     0.933     0.949        29\n",
      "has_role            0.964     0.978     0.971       138\n",
      "has_role_from       0.920     0.979     0.948        50\n",
      "has_role_in_company     0.913     0.969     0.940        69\n",
      "other               1.000     0.975     0.987       306\n",
      "works_for           1.000     1.000     1.000         8\n",
      "\n",
      "Avg.                0.960     0.972     0.966\n",
      "\n",
      "Weighted Avg.       0.973     0.973     0.973\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()        \n",
    "metrics = eval_metrics(\n",
    "    \"EXPERIENCES\", test_features, rel_labels, num_arg_bindings=3, exclude_rels=[])        \n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mnx9sUAO7OFr"
   },
   "source": [
    "# Train with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3XfLpm77RzQ",
    "outputId": "19bf8095-7673-445d-d31d-0525743b7d49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch      Batch         Loss     L_ACC   Arg_ACC       ACC    vL_ACC  vArg_ACC      vACC\n",
      "    1/3      429/429      0.5294     0.909     0.929     0.898       nan       nan       nan\n",
      "    2/3      429/429      0.1353     0.976     0.983     0.973       nan       nan       nan\n",
      "    3/3      429/429      0.0529     0.989     0.994     0.987       nan       nan       nan\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()    \n",
    "\n",
    "train_model(\n",
    "    \"EXPERIENCES\", \n",
    "    train_features=train_features+test_features,\n",
    "    test_features=[], \n",
    "    rel_labels=rel_labels,\n",
    "    num_arg_bindings=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7ef7-50obgT"
   },
   "source": [
    "### 10. Finally saving it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "Yvart7zxC-wQ",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "export_model(\"EXPERIENCES\", is_trainable=False, num_arg_bindings=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnbcRRLFuCSs"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get -y install zip unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JteBYaHueUMF"
   },
   "outputs": [],
   "source": [
    "!zip -r redl.zip ./models/basebert_re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l18UPj14uCSs"
   },
   "source": [
    "# We test in SPARK NLP\n",
    "\n",
    "Let's test our model with SparkNLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6imegJc6uCSs"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl\n",
    "import sparknlp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl\n",
    "from sparknlp_jsl.annotator import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sparknlp.training import CoNLL\n",
    "\n",
    "import re\n",
    "import copy\n",
    "\n",
    "import random\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "from termcolor import colored\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ro1T-wcxIlD"
   },
   "source": [
    "Here, we import our model to SparkNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p1ON0kCfuCSs"
   },
   "outputs": [],
   "source": [
    "re = RelationExtractionDLModel().loadSavedModel(f'models/basebert_re', spark)\n",
    "re.write().overwrite().save(f'finre_work_experience')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ycOr1IRFuCSs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_relations_df (results, col='relations'):\n",
    "  \"\"\"Shows a Dataframe with the relations extracted by Spark NLP\"\"\"\n",
    "  rel_pairs=[]\n",
    "  for rel in results[0][col]:\n",
    "      rel_pairs.append((\n",
    "          rel.result, \n",
    "          rel.metadata['entity1'], \n",
    "          rel.metadata['entity1_begin'],\n",
    "          rel.metadata['entity1_end'],\n",
    "          rel.metadata['chunk1'], \n",
    "          rel.metadata['entity2'],\n",
    "          rel.metadata['entity2_begin'],\n",
    "          rel.metadata['entity2_end'],\n",
    "          rel.metadata['chunk2'], \n",
    "          rel.metadata['confidence']\n",
    "      ))\n",
    "\n",
    "  rel_df = pd.DataFrame(rel_pairs, columns=['relation','entity1','entity1_begin','entity1_end','chunk1','entity2','entity2_begin','entity2_end','chunk2', 'confidence'])\n",
    "\n",
    "  return rel_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nLkmHOSxlai"
   },
   "source": [
    "Now before getting relations, we have to extract entities from the given text. For this, we will use `finner_org_per_role_date` NER model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpl3cltZuCSs"
   },
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = SentenceDetectorDLModel.pretrained(\"sentence_detector_dl\",\"en\")\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"sentence\")\\\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols([\"sentence\"])\\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "embeddings = BertEmbeddings.pretrained(\"bert_embeddings_sec_bert_base\",\"en\") \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "ner_model = finance.FinanceNerModel.pretrained('finner_org_per_role_date', 'en', 'finance/models')\\\n",
    "    .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
    "    .setOutputCol(\"ner\")\n",
    "\n",
    "ner_converter = NerConverter()\\\n",
    "    .setInputCols([\"sentence\",\"token\",\"ner\"])\\\n",
    "    .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "pos = PerceptronModel.pretrained()\\\n",
    "    .setInputCols([\"sentence\", \"token\"])\\\n",
    "    .setOutputCol(\"pos\")\n",
    "\n",
    "dependency_parser = DependencyParserModel().pretrained(\"dependency_conllu\", \"en\")\\\n",
    "    .setInputCols([\"sentence\", \"pos\", \"token\"])\\\n",
    "    .setOutputCol(\"dependencies\")\n",
    "\n",
    "re_ner_chunk_filter = RENerChunksFilter()\\\n",
    "    .setInputCols([\"ner_chunk\", \"dependencies\"])\\\n",
    "    .setOutputCol(\"re_ner_chunk\")\\\n",
    "    .setRelationPairs([\"PERSON-ROLE, ORG-ROLE, DATE-ROLE, PERSON-ORG\"])\\\n",
    "    .setMaxSyntacticDistance(5)\n",
    "\n",
    "# We use the load function to run our trained model.\n",
    "re_Model = RelationExtractionDLModel.load('finre_work_experience_md')\\ \n",
    "    .setInputCols([\"re_ner_chunk\", \"sentence\"])\\\n",
    "    .setOutputCol(\"relations\")\\\n",
    "    .setPredictionThreshold(0.5)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "        document_assembler, \n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        embeddings,\n",
    "        ner_model,\n",
    "        ner_converter,\n",
    "        pos,\n",
    "        dependency_parser,\n",
    "        re_ner_chunk_filter,\n",
    "        re_Model\n",
    "])\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "\n",
    "re_model = pipeline.fit(empty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfdSvju-uCSs"
   },
   "outputs": [],
   "source": [
    "light_model = LightPipeline(re_model)\n",
    "\n",
    "text = [\"\"\"In January 2019, Jose Cil was assigned the CEO of Restaurant Brands International, and Daniel Schwartz was assigned the Executive Chairman of the company.\"\"\"]\n",
    "\n",
    "results = light_model.fullAnnotate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tD0sxZs9uCSs",
    "outputId": "bbe56360-8384-40e3-9762-2213f46cbe04"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relation</th>\n",
       "      <th>entity1</th>\n",
       "      <th>entity1_begin</th>\n",
       "      <th>entity1_end</th>\n",
       "      <th>chunk1</th>\n",
       "      <th>entity2</th>\n",
       "      <th>entity2_begin</th>\n",
       "      <th>entity2_end</th>\n",
       "      <th>chunk2</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>has_role_from</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>43</td>\n",
       "      <td>45</td>\n",
       "      <td>CEO</td>\n",
       "      <td>DATE</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>January 2019</td>\n",
       "      <td>0.7172585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>has_role_from</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>120</td>\n",
       "      <td>137</td>\n",
       "      <td>Executive Chairman</td>\n",
       "      <td>DATE</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>January 2019</td>\n",
       "      <td>0.76819783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>has_role</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>Jose Cil</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>43</td>\n",
       "      <td>45</td>\n",
       "      <td>CEO</td>\n",
       "      <td>0.9969518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>has_role</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>Jose Cil</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>120</td>\n",
       "      <td>137</td>\n",
       "      <td>Executive Chairman</td>\n",
       "      <td>0.9954289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>has_role</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>87</td>\n",
       "      <td>101</td>\n",
       "      <td>Daniel Schwartz</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>120</td>\n",
       "      <td>137</td>\n",
       "      <td>Executive Chairman</td>\n",
       "      <td>0.98519737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        relation entity1 entity1_begin entity1_end              chunk1  \\\n",
       "0  has_role_from    ROLE            43          45                 CEO   \n",
       "1  has_role_from    ROLE           120         137  Executive Chairman   \n",
       "2       has_role  PERSON            17          24            Jose Cil   \n",
       "3       has_role  PERSON            17          24            Jose Cil   \n",
       "4       has_role  PERSON            87         101     Daniel Schwartz   \n",
       "\n",
       "  entity2 entity2_begin entity2_end              chunk2  confidence  \n",
       "0    DATE             3          14        January 2019   0.7172585  \n",
       "1    DATE             3          14        January 2019  0.76819783  \n",
       "2    ROLE            43          45                 CEO   0.9969518  \n",
       "3    ROLE           120         137  Executive Chairman   0.9954289  \n",
       "4    ROLE           120         137  Executive Chairman  0.98519737  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_df = get_relations_df (results)\n",
    "rel_df = rel_df[rel_df['relation']!='no_rel']\n",
    "rel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9tB0yVGuCSt"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"We have experienced significant changes in our senior management team over the past several years, including the appointments of Mark Schmitz as our Executive Vice President and Chief Operating Officer in 2019.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8RUzfkiuCSt",
    "outputId": "82863c3e-52a3-409e-fbb2-5e720b8d5cc5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relation</th>\n",
       "      <th>entity1</th>\n",
       "      <th>entity1_begin</th>\n",
       "      <th>entity1_end</th>\n",
       "      <th>chunk1</th>\n",
       "      <th>entity2</th>\n",
       "      <th>entity2_begin</th>\n",
       "      <th>entity2_end</th>\n",
       "      <th>chunk2</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>has_role</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>129</td>\n",
       "      <td>140</td>\n",
       "      <td>Mark Schmitz</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>149</td>\n",
       "      <td>172</td>\n",
       "      <td>Executive Vice President</td>\n",
       "      <td>0.9945273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>has_role</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>129</td>\n",
       "      <td>140</td>\n",
       "      <td>Mark Schmitz</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>178</td>\n",
       "      <td>200</td>\n",
       "      <td>Chief Operating Officer</td>\n",
       "      <td>0.9947194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>has_role_from</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>149</td>\n",
       "      <td>172</td>\n",
       "      <td>Executive Vice President</td>\n",
       "      <td>DATE</td>\n",
       "      <td>205</td>\n",
       "      <td>208</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.9985196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>has_role_from</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>178</td>\n",
       "      <td>200</td>\n",
       "      <td>Chief Operating Officer</td>\n",
       "      <td>DATE</td>\n",
       "      <td>205</td>\n",
       "      <td>208</td>\n",
       "      <td>2019</td>\n",
       "      <td>0.99905354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        relation entity1 entity1_begin entity1_end                    chunk1  \\\n",
       "0       has_role  PERSON           129         140              Mark Schmitz   \n",
       "1       has_role  PERSON           129         140              Mark Schmitz   \n",
       "2  has_role_from    ROLE           149         172  Executive Vice President   \n",
       "3  has_role_from    ROLE           178         200   Chief Operating Officer   \n",
       "\n",
       "  entity2 entity2_begin entity2_end                    chunk2  confidence  \n",
       "0    ROLE           149         172  Executive Vice President   0.9945273  \n",
       "1    ROLE           178         200   Chief Operating Officer   0.9947194  \n",
       "2    DATE           205         208                      2019   0.9985196  \n",
       "3    DATE           205         208                      2019  0.99905354  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = light_model.fullAnnotate(text)\n",
    "rel_df = get_relations_df (results)\n",
    "rel_df = rel_df[rel_df['relation']!='no_rel']\n",
    "rel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ELIMqJyvuCSt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "3f47d918ae832c68584484921185f5c85a1760864bf927a683dc6fb56366cc77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
