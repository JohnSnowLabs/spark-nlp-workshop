{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDqxs6BQJ3uN"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHuWRMttJ6Je"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/open-source-nlp/23.0.AutoGGUF_in_SparkNLP.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okhT7AcXxben"
      },
      "source": [
        "## Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UIOk6snb5tY"
      },
      "outputs": [],
      "source": [
        "!wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash /dev/stdin -p 3.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "WHcuipEociJJ",
        "outputId": "32aaa0a2-c7ee-41ea-f3a9-a46aae918a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark NLP version 5.5.3\n",
            "Apache Spark version: 3.5.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4c558c2c6071:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ce0f00af410>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sparknlp\n",
        "\n",
        "spark = sparknlp.start(gpu=True)\n",
        "\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "\n",
        "print(\"Spark NLP version\", sparknlp.version())\n",
        "print(\"Apache Spark version:\", spark.version)\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmVB_YaYJHf4"
      },
      "source": [
        "# Import llama.cpp ü¶ô models into Spark NLP üöÄ\n",
        "\n",
        "Let's keep in mind a few things before we start üòä\n",
        "\n",
        "- llama.cpp support was introduced in `Spark NLP 5.5.0`, enabling quantized LLM inference on a wide range of devices. Please make sure you have upgraded to the latest Spark NLP release.\n",
        "- You need to use your own `.gguf` model files, which also include the models from the [Hugging Face Models](https://huggingface.co/models?library=gguf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vvjfZSMJRiB"
      },
      "source": [
        "## Download a GGUF Model\n",
        "\n",
        "Lets download a GGUF model to test it out. For this, we will use [bartowski/Phi-3.5-mini-instruct-GGUF](https://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF). It is a 3.8B parameter model which also is available in 4-bit quantization.\n",
        "\n",
        "We can download the model by selecting the q4 GGUF file from the \"Files and versions\" tab.\n",
        "\n",
        "Once downloaded, we can directly import this model into Spark NLP!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYC2tn5CE-mi",
        "outputId": "fca745c1-9a1f-4328-dd4c-9ae794c2e1ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-02 16:44:39--  https://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct-Q4_K_M.gguf?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.59, 3.165.160.11, 3.165.160.12, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.59|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/cd/20/cd204ca23871eaf6393e5d738941cb2934512acd406d6a48c70a1ed50034800c/e4165e3a71af97f1b4820da61079826d8752a2088e313af0c7d346796c38eff5?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Phi-3.5-mini-instruct-Q4_K_M.gguf%3B+filename%3D%22Phi-3.5-mini-instruct-Q4_K_M.gguf%22%3B&Expires=1743615879&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzYxNTg3OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2NkLzIwL2NkMjA0Y2EyMzg3MWVhZjYzOTNlNWQ3Mzg5NDFjYjI5MzQ1MTJhY2Q0MDZkNmE0OGM3MGExZWQ1MDAzNDgwMGMvZTQxNjVlM2E3MWFmOTdmMWI0ODIwZGE2MTA3OTgyNmQ4NzUyYTIwODhlMzEzYWYwYzdkMzQ2Nzk2YzM4ZWZmNT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Ki9yn5YT2ZjzC%7Em4YbVXtW%7Et4hYw1B-BiE9D8zigSVNwDcbz9hqPHVOH05ZRVzIGstnzkvoZErKIwn7%7EwVtmE28A5Q4hwekyYpVCJnuFefeg0C5IjiMXEG9Q0ejp1wM%7EH3Cx7Cm6imt6G6KjemcjTtfkQPznS6FtQnj06Yk3TDOhpE3G9p8YFs7uLC6hkaO9EFmXEXVQg5XjmNtrZbHkud4lx-xZE6JIFmPlIH8yFPbGMoGjcgzfLpu9GibwUThucAwhUxmsNQzZsvxUT-vsj9mgoI3ywOs3XYkg5pmLrap738Ku-7MAtfJzXtsS3SW5Sg%7Ebv7aR3KjbhVTMvscH7w__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-04-02 16:44:39--  https://cdn-lfs-us-1.hf.co/repos/cd/20/cd204ca23871eaf6393e5d738941cb2934512acd406d6a48c70a1ed50034800c/e4165e3a71af97f1b4820da61079826d8752a2088e313af0c7d346796c38eff5?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27Phi-3.5-mini-instruct-Q4_K_M.gguf%3B+filename%3D%22Phi-3.5-mini-instruct-Q4_K_M.gguf%22%3B&Expires=1743615879&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzYxNTg3OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zL2NkLzIwL2NkMjA0Y2EyMzg3MWVhZjYzOTNlNWQ3Mzg5NDFjYjI5MzQ1MTJhY2Q0MDZkNmE0OGM3MGExZWQ1MDAzNDgwMGMvZTQxNjVlM2E3MWFmOTdmMWI0ODIwZGE2MTA3OTgyNmQ4NzUyYTIwODhlMzEzYWYwYzdkMzQ2Nzk2YzM4ZWZmNT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=Ki9yn5YT2ZjzC%7Em4YbVXtW%7Et4hYw1B-BiE9D8zigSVNwDcbz9hqPHVOH05ZRVzIGstnzkvoZErKIwn7%7EwVtmE28A5Q4hwekyYpVCJnuFefeg0C5IjiMXEG9Q0ejp1wM%7EH3Cx7Cm6imt6G6KjemcjTtfkQPznS6FtQnj06Yk3TDOhpE3G9p8YFs7uLC6hkaO9EFmXEXVQg5XjmNtrZbHkud4lx-xZE6JIFmPlIH8yFPbGMoGjcgzfLpu9GibwUThucAwhUxmsNQzZsvxUT-vsj9mgoI3ywOs3XYkg5pmLrap738Ku-7MAtfJzXtsS3SW5Sg%7Ebv7aR3KjbhVTMvscH7w__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.165.160.20, 3.165.160.77, 3.165.160.38, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.165.160.20|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2393232672 (2.2G) [binary/octet-stream]\n",
            "Saving to: ‚ÄòPhi-3.5-mini-instruct-Q4_K_M.gguf‚Äô\n",
            "\n",
            "Phi-3.5-mini-instru 100%[===================>]   2.23G   231MB/s    in 11s     \n",
            "\n",
            "2025-04-02 16:44:50 (201 MB/s) - ‚ÄòPhi-3.5-mini-instruct-Q4_K_M.gguf‚Äô saved [2393232672/2393232672]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "EXPORT_PATH = \"Phi-3.5-mini-instruct-Q4_K_M.gguf\"\n",
        "! wget \"https://huggingface.co/bartowski/Phi-3.5-mini-instruct-GGUF/resolve/main/Phi-3.5-mini-instruct-Q4_K_M.gguf?download=true\" -O  {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdN-6fIG-_GB"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import *\n",
        "\n",
        "# All these params should be identical to the original ONNX model\n",
        "autoGGUFModel = (\n",
        "    AutoGGUFModel.loadSavedModel(EXPORT_PATH, spark)\n",
        "    .setInputCols(\"document\")\n",
        "    .setOutputCol(\"completions\")\n",
        "    .setBatchSize(4)\n",
        "    .setNPredict(20)\n",
        "    .setNGpuLayers(99)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKm32yN7Gyts"
      },
      "source": [
        "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1y-WMPk-_Cp"
      },
      "outputs": [],
      "source": [
        "autoGGUFModel.write().overwrite().save(f\"Phi-3.5-mini-instruct-Q4_K_M_spark_nlp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as9YEZlfG4nG"
      },
      "source": [
        "Let's clean up stuff we don't need anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTHShraQ--_2"
      },
      "outputs": [],
      "source": [
        "!rm -rf {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBk7TOuxG-J0"
      },
      "source": [
        "Awesome  üòé !\n",
        "\n",
        "This is your GGUF model from loaded and saved by Spark NLP üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DHU4GLdHBA3",
        "outputId": "8898eabc-0bd3-40a3-bfec-62cf44046156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 2337152\n",
            "drwxr-xr-x 2 root root       4096 Apr  2 16:45 metadata\n",
            "-rwxr-xr-x 1 root root 2393232672 Apr  2 16:45 Phi-3.5-mini-instruct-Q4_K_M.gguf\n"
          ]
        }
      ],
      "source": [
        "! ls -l Phi-3.5-mini-instruct-Q4_K_M_spark_nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xm5-K7hIHEq5"
      },
      "source": [
        "üî¥ Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny GGUF model üòä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEDV8w7iITbL",
        "outputId": "ecba8d97-2aee-4409-a389-d648303aaef7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|completions                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[{document, 0, 385, Managing hypertension involves a combination of lifestyle modifications and medication. Here are several steps that can be taken to help control high blood pressure:\\n\\n1. Diet: Adopt a heart-healthy diet such as the DASH (Dietary Approaches to Stop Hypertension) diet, which emphasizes fruits, vegetables, whole grains, lean protein, and low-fat dairy products while minimizing sodium, s, {sentence -> 0}, []}]|\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import sparknlp\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "document_assembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "auto_gguf_model = AutoGGUFModel.load(\"Phi-3.5-mini-instruct-Q4_K_M_spark_nlp\")\\\n",
        "    .setInputCols(\"document\")\\\n",
        "    .setOutputCol(\"completions\")\\\n",
        "    .setBatchSize(4)\\\n",
        "    .setNPredict(100)\n",
        "\n",
        "pipeline = Pipeline(stages=[document_assembler, auto_gguf_model])\n",
        "\n",
        "data = spark.createDataFrame([[\"Patient has hypertension \"]]).toDF(\"text\")\n",
        "\n",
        "result = pipeline.fit(data).transform(data)\n",
        "result.select(\"completions\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0DkNgUqugHT"
      },
      "source": [
        "üî¥ Now, we experiment with a question-based prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4euG3-NTiyfk",
        "outputId": "5073cd48-ce98-4771-9378-ed5342e76303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------------------+\n",
            "|                                          text|\n",
            "+----------------------------------------------+\n",
            "|What is the indication for the drug Methadone?|\n",
            "+----------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"What is the indication for the drug Methadone?\"\n",
        "data = spark.createDataFrame([[prompt]]).toDF(\"text\")\n",
        "data.show(truncate=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FldUMQG6tjr7",
        "outputId": "bb57fd1b-7bde-4b50-9f60-b2efc885d3a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|completions                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[{document, 0, 375, Methadone is indicated primarily for the treatment of opioid dependence and for the management of severe chronic pain. Here's a brief explanation of each:\\n\\n1. Opioid Dependence Treatment: Methadone is a synthetic opioid that acts on the same opioid receptors in the brain as heroin, morphine, and other opioids. It helps to prevent withdrawal symptoms and reduces cravings for, {sentence -> 0}, []}]|\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results = pipeline.fit(data).transform(data).cache()\n",
        "results.select(\"completions\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Jr3r9C8twsq",
        "outputId": "c9be30bf-2210-4d1a-98e7-9ea83a116a87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Methadone is indicated primarily for the treatment of opioid dependence and for the management of severe chronic pain. Here's a brief explanation of each:\n",
            "\n",
            "1. Opioid Dependence Treatment: Methadone is a synthetic opioid that acts on the same opioid receptors in the brain as heroin, morphine, and other opioids. It helps to prevent withdrawal symptoms and reduces cravings for\n"
          ]
        }
      ],
      "source": [
        "print(results.select(\"completions\").collect()[0].completions[0].result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htW6-DBDQ4om"
      },
      "source": [
        "üî¥ Now, we test a multiple-choice question prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdQ8h9EEQ5Hc",
        "outputId": "b364c546-2dc5-4e1a-9df7-ae48cf846c16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------------------------------+\n",
            "|                                                                                                text|\n",
            "+----------------------------------------------------------------------------------------------------+\n",
            "|\\nA 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She st...|\n",
            "+----------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"\n",
        "A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7¬∞F (36.5¬∞C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus.\n",
        "Which of the following is the best treatment for this patient?\n",
        "A: Ampicillin\n",
        "B: Ceftriaxone\n",
        "C: Ciprofloxacin\n",
        "D: Doxycycline\n",
        "E: Nitrofurantoin\n",
        "\"\"\"\n",
        "\n",
        "data = spark.createDataFrame([[prompt]]).toDF(\"text\")\n",
        "data.show(truncate=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Uyg14PdQ5iq",
        "outputId": "e9f41a47-3767-4576-dff4-dbb422264ed7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The best treatment for a pregnant woman presenting with a urinary tract infection (UTI), specifically pyelonephritis, would be a safe antibiotic that is effective against common urinary pathogens. In this case, Nitrofurantoin (option E) is considered a first-line treatment for uncomplicated UTIs during pregnancy, provided the infection is in the early stage and there are no other contraindications\n"
          ]
        }
      ],
      "source": [
        "results = pipeline.fit(data).transform(data).cache()\n",
        "\n",
        "print(results.select(\"completions\").collect()[0].completions[0].result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9uWmCvjIt_6"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of GGUF models from HuggingFace ü§ó in Spark NLP üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf3f7IeEn7FU"
      },
      "source": [
        "# llama.cpp ü¶ô embedding models in Spark NLP üöÄ\n",
        "\n",
        "Let's keep in mind a few things before we start üòä\n",
        "\n",
        "- Support for llama.cpp embeddings was introduced in `Spark NLP 5.5.1`, enabling quantized LLM inference on a wide range of devices. Please make sure you have upgraded to the latest Spark NLP release.\n",
        "- You need to use your own `.gguf` model files, which also include the models from the [Hugging Face Models](https://huggingface.co/models?library=gguf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xsSvqqMoy4_"
      },
      "source": [
        "## Download a GGUF Model\n",
        "\n",
        "Lets download a GGUF model to test it out. For this, we will use [nomic-ai/nomic-embed-text-v1.5-GGUF](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF). We can download the model by selecting the Q8_0 GGUF file from the \"Files and versions\" tab.\n",
        "\n",
        "Once downloaded, we can directly import this model into Spark NLP!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e3Sjuyjn9BR",
        "outputId": "44aaa66b-c129-4c5d-c031-6bf53ffbbecb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-04-02 21:04:37--  https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF/resolve/main/nomic-embed-text-v1.5.Q8_0.gguf?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.11, 3.165.160.12, 3.165.160.59, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.11|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/19/39/19396cd98fe8b02e39b1be815db29f6b251fee34fc5d6550db0b478083fdda2f/3e24342164b3d94991ba9692fdc0dd08e3fd7362e0aacc396a9a5c54a544c3b7?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27nomic-embed-text-v1.5.Q8_0.gguf%3B+filename%3D%22nomic-embed-text-v1.5.Q8_0.gguf%22%3B&Expires=1743631478&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzYzMTQ3OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzE5LzM5LzE5Mzk2Y2Q5OGZlOGIwMmUzOWIxYmU4MTVkYjI5ZjZiMjUxZmVlMzRmYzVkNjU1MGRiMGI0NzgwODNmZGRhMmYvM2UyNDM0MjE2NGIzZDk0OTkxYmE5NjkyZmRjMGRkMDhlM2ZkNzM2MmUwYWFjYzM5NmE5YTVjNTRhNTQ0YzNiNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=TwZgfTmbiFn9JTiuO1rICoTmBmmtQqvOA-n8gxd3xZYvAePq4M5EuzHtWiZU8qGZRFgrP6DvoiXasLUVhZ7yFawFTgmZkXC9TDFz1YZ1W7CYbX4AeR6WK8-pwra-72oB17ariQsIRTsUzpHUIJwUHcqEIgVsgyW5%7E4BP302mth2o8B1oqc5GYNV7mlOwNCqkjJN1KwjT5xU4AkuGOIkYa%7EWdkJFCDpM4aQVy5wgw1nKZDZwpv9in0vcwxQuxi6kmHH7Wf%7Ez9rxoJzO5QvnOb5xLxjgd%7E2AKzS8s7r9fv6B2RBiXyDn5J8IVgzLwR0RUKZzBIqvKey9w2eWGsWppHbA__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-04-02 21:04:38--  https://cdn-lfs-us-1.hf.co/repos/19/39/19396cd98fe8b02e39b1be815db29f6b251fee34fc5d6550db0b478083fdda2f/3e24342164b3d94991ba9692fdc0dd08e3fd7362e0aacc396a9a5c54a544c3b7?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27nomic-embed-text-v1.5.Q8_0.gguf%3B+filename%3D%22nomic-embed-text-v1.5.Q8_0.gguf%22%3B&Expires=1743631478&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MzYzMTQ3OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzE5LzM5LzE5Mzk2Y2Q5OGZlOGIwMmUzOWIxYmU4MTVkYjI5ZjZiMjUxZmVlMzRmYzVkNjU1MGRiMGI0NzgwODNmZGRhMmYvM2UyNDM0MjE2NGIzZDk0OTkxYmE5NjkyZmRjMGRkMDhlM2ZkNzM2MmUwYWFjYzM5NmE5YTVjNTRhNTQ0YzNiNz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=TwZgfTmbiFn9JTiuO1rICoTmBmmtQqvOA-n8gxd3xZYvAePq4M5EuzHtWiZU8qGZRFgrP6DvoiXasLUVhZ7yFawFTgmZkXC9TDFz1YZ1W7CYbX4AeR6WK8-pwra-72oB17ariQsIRTsUzpHUIJwUHcqEIgVsgyW5%7E4BP302mth2o8B1oqc5GYNV7mlOwNCqkjJN1KwjT5xU4AkuGOIkYa%7EWdkJFCDpM4aQVy5wgw1nKZDZwpv9in0vcwxQuxi6kmHH7Wf%7Ez9rxoJzO5QvnOb5xLxjgd%7E2AKzS8s7r9fv6B2RBiXyDn5J8IVgzLwR0RUKZzBIqvKey9w2eWGsWppHbA__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.65.229.34, 18.65.229.76, 18.65.229.105, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.65.229.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 146146432 (139M) [application/octet-stream]\n",
            "Saving to: ‚Äònomic-embed-text-v1.5.Q8_0.gguf‚Äô\n",
            "\n",
            "nomic-embed-text-v1 100%[===================>] 139.38M   162MB/s    in 0.9s    \n",
            "\n",
            "2025-04-02 21:04:39 (162 MB/s) - ‚Äònomic-embed-text-v1.5.Q8_0.gguf‚Äô saved [146146432/146146432]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "EXPORT_PATH = \"nomic-embed-text-v1.5.Q8_0.gguf\"\n",
        "! wget \"https://huggingface.co/nomic-ai/nomic-embed-text-v1.5-GGUF/resolve/main/{EXPORT_PATH}?download=true\" -O  {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkHflCVGpGbX"
      },
      "source": [
        "- Let's use the `loadSavedModel` functon in `AutoGGUFModel`\n",
        "- Most params will be set automatically. They can also be set later after loading the model in `AutoGGUFModel` during runtime, so don't worry about setting them now.\n",
        "- `loadSavedModel` accepts two params, first is the path to the exported model. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n",
        "- We can set the model to embedding mode with `setEmbedding`. Afterwards the model will return the embeddings in the Annotations.\n",
        "- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4bzCdRXhpG2w"
      },
      "outputs": [],
      "source": [
        "from sparknlp.annotator import *\n",
        "\n",
        "# All these params should be identical to the original ONNX model\n",
        "autoGGUFEmbeddings = (\n",
        "    AutoGGUFEmbeddings.loadSavedModel(EXPORT_PATH, spark)\n",
        "    .setInputCols(\"document\")\n",
        "    .setOutputCol(\"embeddings\")\n",
        "    .setBatchSize(4)\n",
        "    .setNGpuLayers(99)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yiptcEQrmoS"
      },
      "source": [
        "- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1RO0b6vrnTc"
      },
      "outputs": [],
      "source": [
        "autoGGUFEmbeddings.write().overwrite().save(f\"nomic-embed-text-v1.5.Q8_0.gguf_spark_nlp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tkGr6lDrtBb"
      },
      "source": [
        "Let's clean up stuff we don't need anymore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3z79IpZWrtjv"
      },
      "outputs": [],
      "source": [
        "!rm -rf {EXPORT_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss_bYcxhryvf"
      },
      "source": [
        "Awesome  üòé !\n",
        "\n",
        "This is your GGUF model from loaded and saved by Spark NLP üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SjG6mWdrzZE",
        "outputId": "a174ed9e-ba6c-4ee2-955c-50725b3aceed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 142728\n",
            "drwxr-xr-x 2 root root      4096 Apr  2 21:04 metadata\n",
            "-rwxr-xr-x 1 root root 146146432 Apr  2 21:04 nomic-embed-text-v1.5.Q8_0.gguf\n"
          ]
        }
      ],
      "source": [
        "! ls -l nomic-embed-text-v1.5.Q8_0.gguf_spark_nlp/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxJZV4o1r4WE"
      },
      "source": [
        "Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny GGUF model üòä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxldsUCvr4yU",
        "outputId": "ebd2398c-82a9-404b-f134-bf5436b589b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------------------------+\n",
            "|                                                                      embeddings|\n",
            "+--------------------------------------------------------------------------------+\n",
            "|[[0.0738504, 0.03141154, -0.15088692, 0.015990928, -0.002257554, 0.019066498,...|\n",
            "+--------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "document_assembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "autoGGUFEmbeddings = AutoGGUFEmbeddings.load(\"nomic-embed-text-v1.5.Q8_0.gguf_spark_nlp\")\\\n",
        "    .setInputCols(\"document\")\\\n",
        "    .setOutputCol(\"embeddings\")\n",
        "\n",
        "pipeline = Pipeline(stages=[document_assembler, autoGGUFEmbeddings])\n",
        "\n",
        "data = spark.createDataFrame([[\"Diagnosed with diabetes.\"]]).toDF(\"text\")\n",
        "\n",
        "result = pipeline.fit(data).transform(data)\n",
        "result.select(\"embeddings.embeddings\").show(1, 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGjBpZMmsrrH"
      },
      "source": [
        "That's it! You can now go wild and use hundreds of GGUF models from HuggingFace ü§ó in Spark NLP üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSApVs81s2Gt"
      },
      "source": [
        "# üìú PromptAssembler with AutoGGUFModel\n",
        "\n",
        "Let's keep in mind a few things before we start üòä\n",
        "\n",
        "- llama.cpp support in the form of the `AutoGGUFModel` was introduced in `Spark NLP 5.5.0`, enabling quantized LLM inference on a wide range of devices. Please make sure you have upgraded to the latest Spark NLP release.\n",
        "- The `PromptAssembler` was introduced in `Spark NLP 5.5.1` to enable the construction of message prompts.\n",
        "\n",
        "This notebook will show you how you can construct your own message prompts for the AutoGGUFModel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1McPBERtKKi"
      },
      "source": [
        " üî¥ Let's create a `PromptAssembler` and use it to recreate the following conversation between a chatbot and a user:\n",
        "\n",
        "```\n",
        "SYSTEM: You are a medical assistant.  \n",
        "ASSISTANT: Hello there! How can I assist you today?  \n",
        "USER: I have a headache. What should I do?  \n",
        "```\n",
        "\n",
        "First we need to structure our messages in our Spark DataFrame correctly. For each row, the PromptAssembler expects an array of two-tuples. The first field should be the role and the second field the message. We will call this column `message`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LY4XWvMYssTL",
        "outputId": "6539e52f-f8b0-46b0-dfe3-2c1cf9ad1cf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|messages                                                                                                                                     |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[{system, You are a medical assistant.}, {assistant, Hello there! How can I assist you today?}, {user, I have a headache. What should I do?}]|\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    (\"system\", \"You are a medical assistant.\"),\n",
        "    (\"assistant\", \"Hello there! How can I assist you today?\"),\n",
        "    (\"user\", \"I have a headache. What should I do?\"),\n",
        "]\n",
        "df = spark.createDataFrame([[messages]]).toDF(\"messages\")\n",
        "df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UVSsQFHt1gW"
      },
      "source": [
        "Let's create the PromptAssembler to generate the prompts. We will use the template from [llama3.1 (extracted from the gguf file)](https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF?show_file_info=Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf).\n",
        "\n",
        "By default, the `addAssistant` parameter is set to `True`, so a assistant header will be appended to the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja7v6uF4TE-y"
      },
      "source": [
        "**Extracting chat_template from Hugging Face**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNUdGrNAVz7G"
      },
      "source": [
        "üîπ Hugging Face Authentication for Restricted Models\n",
        "\n",
        "Some models (like Meta's Llama 3.1-8B) require authentication to access files.  \n",
        "Follow these steps to authenticate:\n",
        "\n",
        "1Ô∏è‚É£ Get your Hugging Face token from: https://huggingface.co/settings/tokens  \n",
        "2Ô∏è‚É£ Use the token to log in:\n",
        "\n",
        "   ```python\n",
        "   from huggingface_hub import login  \n",
        "   login(\"your_hf_token_here\")  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pr06E2PHSqTu"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIIkMNB_Stx5",
        "outputId": "84bb8e83-65c9-4ed6-9187-4f059b28d0b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{{- bos_token }}\n",
            "{%- if custom_tools is defined %}\n",
            "    {%- set tools = custom_tools %}\n",
            "{%- endif %}\n",
            "{%- if not tools_in_user_message is defined %}\n",
            "    {%- set tools_in_user_message = true %}\n",
            "{%- endif %}\n",
            "{%- if not date_string is defined %}\n",
            "    {%- set date_string = \"26 Jul 2024\" %}\n",
            "{%- endif %}\n",
            "{%- if not tools is defined %}\n",
            "    {%- set tools = none %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
            "{%- if messages[0]['role'] == 'system' %}\n",
            "    {%- set system_message = messages[0]['content']|trim %}\n",
            "    {%- set messages = messages[1:] %}\n",
            "{%- else %}\n",
            "    {%- set system_message = \"\" %}\n",
            "{%- endif %}\n",
            "\n",
            "{#- System message + builtin tools #}\n",
            "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
            "{%- if builtin_tools is defined or tools is not none %}\n",
            "    {{- \"Environment: ipython\\n\" }}\n",
            "{%- endif %}\n",
            "{%- if builtin_tools is defined %}\n",
            "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
            "{%- endif %}\n",
            "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
            "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
            "{%- if tools is not none and not tools_in_user_message %}\n",
            "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "{%- endif %}\n",
            "{{- system_message }}\n",
            "{{- \"<|eot_id|>\" }}\n",
            "\n",
            "{#- Custom tools are passed in a user message with some extra guidance #}\n",
            "{%- if tools_in_user_message and not tools is none %}\n",
            "    {#- Extract the first user message so we can plug it in here #}\n",
            "    {%- if messages | length != 0 %}\n",
            "        {%- set first_user_message = messages[0]['content']|trim %}\n",
            "        {%- set messages = messages[1:] %}\n",
            "    {%- else %}\n",
            "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
            "{%- endif %}\n",
            "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
            "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
            "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
            "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
            "    {{- \"Do not use variables.\\n\\n\" }}\n",
            "    {%- for t in tools %}\n",
            "        {{- t | tojson(indent=4) }}\n",
            "        {{- \"\\n\\n\" }}\n",
            "    {%- endfor %}\n",
            "    {{- first_user_message + \"<|eot_id|>\"}}\n",
            "{%- endif %}\n",
            "\n",
            "{%- for message in messages %}\n",
            "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
            "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
            "    {%- elif 'tool_calls' in message %}\n",
            "        {%- if not message.tool_calls|length == 1 %}\n",
            "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
            "        {%- endif %}\n",
            "        {%- set tool_call = message.tool_calls[0].function %}\n",
            "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
            "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
            "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
            "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
            "                {%- if not loop.last %}\n",
            "                    {{- \", \" }}\n",
            "                {%- endif %}\n",
            "                {%- endfor %}\n",
            "            {{- \")\" }}\n",
            "        {%- else  %}\n",
            "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
            "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
            "            {{- '\"parameters\": ' }}\n",
            "            {{- tool_call.arguments | tojson }}\n",
            "            {{- \"}\" }}\n",
            "        {%- endif %}\n",
            "        {%- if builtin_tools is defined %}\n",
            "            {#- This means we're in ipython mode #}\n",
            "            {{- \"<|eom_id|>\" }}\n",
            "        {%- else %}\n",
            "            {{- \"<|eot_id|>\" }}\n",
            "        {%- endif %}\n",
            "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
            "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
            "        {%- if message.content is mapping or message.content is iterable %}\n",
            "            {{- message.content | tojson }}\n",
            "        {%- else %}\n",
            "            {{- message.content }}\n",
            "        {%- endif %}\n",
            "        {{- \"<|eot_id|>\" }}\n",
            "    {%- endif %}\n",
            "{%- endfor %}\n",
            "{%- if add_generation_prompt %}\n",
            "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
            "{%- endif %}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import json\n",
        "\n",
        "config_path = hf_hub_download(\"meta-llama/Llama-3.1-8B-Instruct\", \"tokenizer_config.json\")\n",
        "\n",
        "with open(config_path, \"r\") as f:\n",
        "    config = json.load(f)\n",
        "\n",
        "template = config.get(\"chat_template\", \"Not found\")\n",
        "print(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGt-ewRYWYVZ"
      },
      "outputs": [],
      "source": [
        "from sparknlp.base import *\n",
        "\n",
        "promptAssembler = (\n",
        "    PromptAssembler()\n",
        "    .setInputCol(\"messages\")\n",
        "    .setOutputCol(\"prompt\")\n",
        "    .setChatTemplate(template)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pzALyg4t8lK"
      },
      "source": [
        "Let's see how the final prompt looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOca66I5bwXU",
        "outputId": "8eb73066-2004-4553-af82-32aaa4396fc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "You are a medical assistant.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Hello there! How can I assist you today?<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "I have a headache. What should I do?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results = promptAssembler.transform(df).cache()\n",
        "\n",
        "print(results.select(\"prompt\").collect()[0].prompt[0].result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXwTgQVbuCIz"
      },
      "source": [
        "Now you can feed the prompt to a llama3.1 model loaded with AutoGGUFModel. Depending on your messages, you might need to the chat template or system prompt in the AutoGGUFModel. For example:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpaMxcRquOc2"
      },
      "source": [
        "```\n",
        "from sparknlp.annotator import AutoGGUFModel\n",
        "\n",
        "autoGGUFModel = (\n",
        "    AutoGGUFModel.loadSavedModel(\"path/to/llama3.1\", spark)\n",
        "    .setInputCols(\"prompt\")\n",
        "    .setOutputCol(\"completions\")\n",
        "    .setBatchSize(4)\n",
        "    .setNGpuLayers(99)\n",
        "    .setUseChatTemplate(False)  # Don't apply the chat template\n",
        "    .setSystemPrompt(\n",
        "        \"Your system prompt\"\n",
        "    )  # Set custom system prompt if not specified in the messages. Leave empty for default.\n",
        ")\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
