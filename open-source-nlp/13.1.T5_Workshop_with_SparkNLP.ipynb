{"cells":[{"cell_type":"markdown","metadata":{"id":"RiXjNLF2onn2"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"]},{"cell_type":"markdown","metadata":{"id":"kLua-66DlmRP"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/open-source-nlp/13.1.T5_Workshop_with_SparkNLP.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"EwT87Te_Kr3_"},"source":["# **10.1 T5 Workshop with Spark NLP**"]},{"cell_type":"markdown","metadata":{"id":"YcJLn3NGaKIH"},"source":["\n","\n","\n","\n","---\n","\n","\n","\n","# Overview of every task available with T5\n","[The T5 model](https://arxiv.org/pdf/1910.10683.pdf) is trained on various datasets for 17 different tasks which fall into 8 categories.\n","\n","\n","\n","1. Text Summarization\n","2. Question Answering\n","3. Translation\n","4. Sentiment analysis\n","5. Natural Language Inference\n","6. Coreference Resolution\n","7. Sentence Completion\n","8. Word Sense Disambiguation\n","\n","# Every T5 Task with explanation:\n","|Task Name | Explanation | \n","|----------|--------------|\n","|[1.CoLA](https://nyu-mll.github.io/CoLA/)                   | Classify if a sentence is gramaticaly correct|\n","|[2.RTE](https://dl.acm.org/doi/10.1007/11736790_9)                    | Classify whether if a statement can be deducted from a sentence|\n","|[3.MNLI](https://arxiv.org/abs/1704.05426)                   | Classify for a hypothesis and premise whether they contradict or contradict each other or neither of both (3 class).|\n","|[4.MRPC](https://www.aclweb.org/anthology/I05-5002.pdf)                   | Classify whether a pair of sentences is a re-phrasing of each other (semantically equivalent)|\n","|[5.QNLI](https://arxiv.org/pdf/1804.07461.pdf)                   | Classify whether the answer to a question can be deducted from an answer candidate.|\n","|[6.QQP](https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs)                    | Classify whether a pair of questions is a re-phrasing of each other (semantically equivalent)|\n","|[7.SST2](https://www.aclweb.org/anthology/D13-1170.pdf)                   | Classify the sentiment of a sentence as positive or negative|\n","|[8.STSB](https://www.aclweb.org/anthology/S17-2001/)                   | Classify the sentiment of a sentence on a scale from 1 to 5 (21 Sentiment classes)|\n","|[9.CB](https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/601)                     | Classify for a premise and a hypothesis whether they contradict each other or not (binary).|\n","|[10.COPA](https://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2418/0)                   | Classify for a question, premise, and 2 choices which choice the correct choice is (binary).|\n","|[11.MultiRc](https://www.aclweb.org/anthology/N18-1023.pdf)                | Classify for a question, a paragraph of text, and an answer candidate, if the answer is correct (binary),|\n","|[12.WiC](https://arxiv.org/abs/1808.09121)                    | Classify for a pair of sentences and a disambigous word if the word has the same meaning in both sentences.|\n","|[13.WSC/DPR](https://www.aaai.org/ocs/index.php/KR/KR12/paper/view/4492/0)       | Predict for an ambiguous pronoun in a sentence what it is referring to.  |\n","|[14.Summarization](https://arxiv.org/abs/1506.03340)          | Summarize text into a shorter representation.|\n","|[15.SQuAD](https://arxiv.org/abs/1606.05250)                  | Answer a question for a given context.|\n","|[16.WMT1.](https://arxiv.org/abs/1706.03762)                  | Translate English to German|\n","|[17.WMT2.](https://arxiv.org/abs/1706.03762)                   | Translate English to French|\n","|[18.WMT3.](https://arxiv.org/abs/1706.03762)                   | Translate English to Romanian|\n","\n","\n","# Information about pre-procession for T5 tasks\n","\n","## Tasks that require no pre-processing\n","The following tasks work fine without any additional pre-processing, only setting the `task parameter` on the T5 model is required:\n","\n","-  CoLA\n","-  Summarization\n","-  SST2\n","-  WMT1.\n","-  WMT2.\n","-  WMT3.\n","\n","\n","## Tasks that require pre-processing with 1 tag\n","The following tasks require `exactly 1 additional tag` added by manual pre-processing.\n","Set the `task parameter` and then join the sentences on the `tag` for these tasks.\n","\n","- RTE\n","- MNLI\n","- MRPC\n","- QNLI\n","- QQP\n","- SST2\n","- STSB\n","- CB\n","\n","\n","## Tasks that require pre-processing with multiple tags\n","The following tasks require `more than 1 additional tag` added manual by pre-processing.\n","Set the `task parameter` and then prefix sentences with their corresponding tags and join them for these tasks:\n","\n","- COPA\n","- MultiRc\n","- WiC\n","\n","\n","## WSC/DPR is a special case that requires `*` surrounding\n","The task WSC/DPR requires highlighting a pronoun with `*` and configuring a `task parameter`.\n","<br><br><br><br><br>\n","\n","\n","\n","\n","\n","The following sections describe each task in detail, with an example and also a pre-processed example.\n","\n","***NOTE:***  Linebreaks are added to the `pre-processed examples` in the following section. The T5 model also works with linebreaks, but it can hinder the performance and it is not recommended to intentionally add them.\n","\n","\n","\n","# Task 1 [CoLA - Binary Grammatical Sentence acceptability classification](https://nyu-mll.github.io/CoLA/)\n","Judges if a sentence is grammatically acceptable.     \n","This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf).\n","\n","\n","\n","## Example\n","\n","|sentence  | prediction|\n","|------------|------------|\n","| Anna and Mike is going skiing and they is liked is | unacceptable |      \n","| Anna and Mike like to dance | acceptable | \n","\n","\n","## How to configure T5 task for CoLA\n","`.setTask(cola sentence:)` prefix.\n","\n","### Example pre-processed input for T5 CoLA sentence acceptability judgement:\n","```\n","cola \n","sentence: Anna and Mike is going skiing and they is liked is\n","```\n","\n","# Task 2 [RTE - Natural language inference Deduction Classification](https://dl.acm.org/doi/10.1007/11736790_9)\n","The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (entailed) from the other or not.       \n","Classification of sentence pairs as entailed and not_entailed       \n","This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf) and  [SuperGLUE](https://w4ngatang.github.io/static/papers/superglue.pdf).\n","\n","\n","\n","## Example\n","\n","|sentence 1 | sentence 2 | prediction|\n","|------------|------------|----------|\n","Kessler ’s team conducted 60,643 interviews with adults in 14 countries.  |  Kessler ’s team interviewed more than 60,000 adults in 14 countries | entailed\n","Peter loves New York, it is his favorite city| Peter loves new York. | entailed\n","Recent report say Johnny makes he alot of money, he earned 10 million USD each year for the last 5 years.  |Johnny is a millionare | entailment|\n","Recent report say Johnny makes he alot of money, he earned 10 million USD each year for the last 5 years.  |Johnny is a poor man  | not_entailment | \n","| It was raining in England for the last 4 weeks | England was very dry yesterday | not_entailment|\n","\n","## How to configure T5 task for RTE\n","`.setTask('rte sentence1:)` and prefix second sentence with `sentence2:`\n","\n","\n","### Example pre-processed input for T5 RTE - 2 Class Natural language inference\n","```\n","rte \n","sentence1: Recent report say Peter makes he alot of money, he earned 10 million USD each year for the last 5 years. \n","sentence2: Peter is a millionare.\n","```\n","\n","### References\n","- https://arxiv.org/abs/2010.03061\n","\n","\n","# Task 3 [MNLI - 3 Class Natural Language Inference 3-class contradiction classification](https://arxiv.org/abs/1704.05426)\n","Classification of sentence pairs with the labels `entailment`, `contradiction`, and `neutral`.      \n","This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf).\n","\n","\n","This classifier predicts for two sentences :\n","- Whether the first sentence logically and semantically follows from the second sentence as entailment\n","- Whether the first sentence is a contradiction to the second sentence as a contradiction\n","- Whether the first sentence does not entail or contradict the first sentence as neutral\n","\n","| Hypothesis | Premise | prediction|\n","|------------|------------|----------|\n","| Recent report say Johnny makes he alot of money, he earned 10 million USD each year for the last 5 years. |    Johnny is a poor man.  | contradiction|\n","|It rained in England the last 4 weeks.| It was snowing in New York last week| neutral | \n","\n","## How to configure T5 task for MNLI\n","`.setTask('mnli hypothesis:)` and prefix second sentence with `premise:`\n","\n","### Example pre-processed input for T5 MNLI - 3 Class Natural Language Inference\n","\n","```\n","mnli \n","hypothesis: At 8:34, the Boston Center controller received a third, transmission from American 11.    \n","premise: The Boston Center controller got a third transmission from American 11.\n","```\n","\n","\n","# Task 4 [MRPC - Binary Paraphrasing/ sentence similarity classification ](https://www.aclweb.org/anthology/I05-5002.pdf)\n","Detect whether one sentence is a re-phrasing or similar to another sentence      \n","This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf).\n","\n","\n","| Sentence1 | Sentence2 | prediction|\n","|------------|------------|----------|\n","|We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , \" Rumsfeld said .| Rather , the US acted because the administration saw \" existing evidence in a new light , through the prism of our experience on September 11 \" . | equivalent | \n","| I like to eat peanutbutter for breakfast| I like to play football | not_equivalent | \n","\n","\n","## How to configure T5 task for MRPC\n","`.setTask('mrpc sentence1:)` and prefix second sentence with `sentence2:`\n","\n","### Example pre-processed input for T5 MRPC - Binary Paraphrasing/ sentence similarity\n","\n","```\n","mrpc \n","sentence1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , \" Rumsfeld said . \n","sentence2: Rather , the US acted because the administration saw \" existing evidence in a new light , through the prism of our experience on September 11\",\n","```\n","\n","*ISSUE:* Can only get neutral and contradiction as prediction results for tested samples but no entailment predictions.\n","\n","\n","# Task 5 [QNLI - Natural Language Inference question answered classification](https://arxiv.org/pdf/1804.07461.pdf)\n","Classify whether a question is answered by a sentence (`entailed`).       \n","This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf).\n","\n","| Question | Answer | prediction|\n","|------------|------------|----------|\n","|Where did Jebe die?| Ghenkis Khan recalled Subtai back to Mongolia soon afterward, and Jebe died on the road back to Samarkand | entailment|\n","|What does Steve like to eat? | Steve watches TV all day | not_netailment\n","\n","## How to configure T5 task for QNLI - Natural Language Inference question answered classification\n","`.setTask('QNLI sentence1:)` and prefix question with `question:` sentence with `sentence:`:\n","\n","### Example pre-processed input for T5 QNLI - Natural Language Inference question answered classification\n","\n","```\n","qnli\n","question: Where did Jebe die?     \n","sentence: Ghenkis Khan recalled Subtai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand,\n","```\n","\n","\n","# Task 6 [QQP - Binary Question Similarity/Paraphrasing](https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs)\n","Based on a quora dataset, determine whether a pair of questions are semantically equivalent.      \n","This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf).\n","\n","| Question1 | Question2 | prediction|\n","|------------|------------|----------|\n","|What attributes would have made you highly desirable in ancient Rome?        | How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER? | not_duplicate | \n","|What was it like in Ancient rome?  | What was Ancient rome like?| duplicate | \n","\n","\n","## How to configure T5 task for QQP\n",".setTask('qqp question1:) and\n","prefix second sentence with question2:\n","\n","\n","### Example pre-processed input for T5 QQP - Binary Question Similarity/Paraphrasing\n","\n","```\n","qqp \n","question1: What attributes would have made you highly desirable in ancient Rome?        \n","question2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?',\n","```\n","\n","# Task 7 [SST2 - Binary Sentiment Analysis](https://www.aclweb.org/anthology/D13-1170.pdf)\n","Binary sentiment classification.      \n","This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf).\n","\n","| Sentence1 | Prediction  | \n","|-----------|-----------|\n","|it confirms fincher ’s status as a film maker who artfully bends technical know-how to the service of psychological insight |  positive| \n","|I really hated that movie | negative | \n","\n","\n","## How to configure T5 task for  SST2\n","`.setTask('sst2 sentence: ')`\n","\n","### Example pre-processed input for T5 SST2 - Binary Sentiment Analysis\n","\n","```\n","sst2\n","sentence: I hated that movie\n","```\n","\n","\n","\n","# Task8 [STSB - Regressive semantic sentence similarity](https://www.aclweb.org/anthology/S17-2001/)\n","Measures how similar two sentences are on a scale from 0 to 5 with 21 classes representing a regressive label.     \n","This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf).\n","\n","\n","| Question1 | Question2 | prediction|\n","|------------|------------|----------|\n","|What attributes would have made you highly desirable in ancient Rome?        | How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER? | 0 | \n","|What was it like in Ancient rome?  | What was Ancient rome like?| 5.0 | \n","|What was live like as a King in Ancient Rome??       | What is it like to live in Rome? | 3.2 | \n","\n","## How to configure T5 task for STSB\n","`.setTask('stsb sentence1:)` and prefix second sentence with `sentence2:`\n","\n","\n","### Example pre-processed input for T5 STSB - Regressive semantic sentence similarity\n","\n","```\n","stsb\n","sentence1: What attributes would have made you highly desirable in ancient Rome?        \n","sentence2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?',\n","```\n","\n","\n","# Task 9[ CB -  Natural language inference contradiction classification](https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/601)\n","Classify whether a Premise contradicts a Hypothesis.    \n","Predicts entailment, neutral and contradiction     \n","This is a sub-task of [SuperGLUE](https://w4ngatang.github.io/static/papers/superglue.pdf).\n","\n","\n","| Hypothesis | Premise | Prediction | \n","|--------|-------------|----------|\n","|Valence was helping | Valence the void-brain, Valence the virtuous valet. Why couldn’t the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping'| Contradiction|\n","\n","\n","## How to configure T5 task for CB\n","`.setTask('cb hypothesis:)` and prefix premise with `premise:`\n","\n","### Example pre-processed input for T5 CB -  Natural language inference contradiction classification\n","\n","```\n","cb \n","hypothesis: Valence was helping      \n","premise: Valence the void-brain, Valence the virtuous valet. Why couldn’t the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping,\n","```\n","\n","\n","# Task 10 [COPA - Sentence Completion/ Binary choice selection](https://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2418/0)\n","The Choice of Plausible Alternatives (COPA) task by Roemmele et al. (2011) evaluates\n","causal reasoning between events, which requires commonsense knowledge about what usually takes\n","place in the world. Each example provides a premise and either asks for the correct cause or effect\n","from two choices, thus testing either ``backward`` or `forward causal reasoning`. COPA data, which\n","consists of 1,000 examples total, can be downloaded at https://people.ict.usc.e\n","\n","This is a sub-task of [SuperGLUE](https://w4ngatang.github.io/static/papers/superglue.pdf).\n","\n","This classifier selects from a choice of `2 options` which one the correct is based on a `premise`.\n","\n","\n","## forward causal reasoning\n","Premise: The man lost his balance on the ladder.     \n","question: What happened as a result?        \n","Alternative 1: He fell off the ladder.       \n","Alternative 2: He climbed up the ladder.\n","## backwards causal reasoning\n","Premise: The man fell unconscious. What was the cause\n","of this?       \n","Alternative 1: The assailant struck the man in the head.      \n","Alternative 2: The assailant took the man’s wallet.\n","\n","\n","| Question | Premise | Choice 1 | Choice  2 | Prediction | \n","|--------|-------------|----------|---------|-------------|\n","|effect | Politcal Violence broke out in the nation. | many citizens relocated to the capitol. |  Many citizens took refuge in other territories | Choice 1  | \n","|correct| The men fell unconscious | The assailant struckl the man in the head | he assailant s took the man's wallet. | choice1 | \n","\n","\n","## How to configure T5 task for COPA\n","`.setTask('copa choice1:)`, prefix choice2 with `choice2:` , prefix premise with `premise:` and prefix the question with `question`\n","\n","### Example pre-processed input for T5 COPA - Sentence Completion/ Binary choice selection\n","\n","```\n","copa \n","choice1:   He fell off the ladder    \n","choice2:   He climbed up the lader       \n","premise:   The man lost his balance on the ladder \n","question:  effect\n","```\n","\n","\n","\n","\n","# Task 11 [MultiRc - Question Answering](https://www.aclweb.org/anthology/N18-1023.pdf)\n","Evaluates an `answer` for a `question` as `true` or `false` based on an input `paragraph`\n","The T5 model predicts for a `question` and a `paragraph` of `sentences` wether an `answer` is true or not,\n","based on the semantic contents of the paragraph.        \n","This is a sub-task of [SuperGLUE](https://w4ngatang.github.io/static/papers/superglue.pdf).\n","\n","\n","\n","**Exceeds human performance by a large margin**\n","\n","\n","\n","| Question                                                     | Answer                                                              | Prediction | paragraph|\n","|--------------------------------------------------------------|---------------------------------------------------------------------|------------|----------|\n","| Why was Joey surprised the morning he woke up for breakfast? | There was only pie to eat, rather than traditional breakfast foods  |  True   |Once upon a time, there was a squirrel named Joey. Joey loved to go outside and play with his cousin Jimmy. Joey and Jimmy played silly games together, and were always laughing. One day, Joey and Jimmy went swimming together 50 at their Aunt Julie’s pond. Joey woke up early in the morning to eat some food before they left. He couldn’t find anything to eat except for pie! Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast. After he ate, he and Jimmy went to the pond. On their way there they saw their friend Jack Rabbit. They dove into the water and swam for several hours. The sun was out, but the breeze was cold. Joey and Jimmy got out of the water and started walking home. Their fur was wet, and the breeze chilled them. When they got home, they dried off, and Jimmy put on his favorite purple shirt. Joey put on a blue shirt with red and green dots. The two squirrels ate some food that Joey’s mom, Jasmine, made and went off to bed.,          |\n","| Why was Joey surprised the morning he woke up for breakfast? | There was a T-Rex in his garden  |  False   |Once upon a time, there was a squirrel named Joey. Joey loved to go outside and play with his cousin Jimmy. Joey and Jimmy played silly games together, and were always laughing. One day, Joey and Jimmy went swimming together 50 at their Aunt Julie’s pond. Joey woke up early in the morning to eat some food before they left. He couldn’t find anything to eat except for pie! Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast. After he ate, he and Jimmy went to the pond. On their way there they saw their friend Jack Rabbit. They dove into the water and swam for several hours. The sun was out, but the breeze was cold. Joey and Jimmy got out of the water and started walking home. Their fur was wet, and the breeze chilled them. When they got home, they dried off, and Jimmy put on his favorite purple shirt. Joey put on a blue shirt with red and green dots. The two squirrels ate some food that Joey’s mom, Jasmine, made and went off to bed.,          |\n","\n","## How to configure T5 task for MultiRC\n","`.setTask('multirc questions:)`  followed by `answer:` prefix for the answer to evaluate, followed by `paragraph:` and then a series of sentences, where each sentence is prefixed with `Sent n:`prefix second sentence with sentence2:\n","\n","\n","### Example pre-processed input for T5 MultiRc task:\n","```\n","multirc questions:  Why was Joey surprised the morning he woke up for breakfast?      \n","answer:             There was a T-REX in his garden.      \n","paragraph:      \n","Sent 1:             Once upon a time, there was a squirrel named Joey.      \n","Sent 2:             Joey loved to go outside and play with his cousin Jimmy.      \n","Sent 3:             Joey and Jimmy played silly games together, and were always laughing.      \n","Sent 4:             One day, Joey and Jimmy went swimming together 50 at their Aunt Julie’s pond.      \n","Sent 5:             Joey woke up early in the morning to eat some food before they left.      \n","Sent 6:             He couldn’t find anything to eat except for pie!      \n","Sent 7:             Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast.      \n","Sent 8:             After he ate, he and Jimmy went to the pond.      \n","Sent 9:             On their way there they saw their friend Jack Rabbit.      \n","Sent 10:            They dove into the water and swam for several hours.      \n","Sent 11:            The sun was out, but the breeze was cold.      \n","Sent 12:            Joey and Jimmy got out of the water and started walking home.      \n","Sent 13:            Their fur was wet, and the breeze chilled them.      \n","Sent 14:            When they got home, they dried off, and Jimmy put on his favorite purple shirt.      \n","Sent 15:            Joey put on a blue shirt with red and green dots.      \n","Sent 16:            The two squirrels ate some food that Joey’s mom, Jasmine, made and went off to bed.      \n","```\n","\n","\n","# Task 12 [WiC - Word sense disambiguation](https://arxiv.org/abs/1808.09121)\n","Decide for `two sentence`s with a shared `disambigous word` wether they have the target word has the same `semantic meaning` in both sentences.       \n","This is a sub-task of [SuperGLUE](https://w4ngatang.github.io/static/papers/superglue.pdf).\n","\n","\n","|Predicted | disambigous word| Sentence 1 | Sentence 2 | \n","|----------|-----------------|------------|------------|\n","| False    | kill            | He totally killed that rock show! | The airplane crash killed his family | \n","| True     | window          | The expanded window will give us time to catch the thieves.|You have a two-hour window for turning in your homework. |     \n","| False     | window          | He jumped out of the window.|You have a two-hour window for turning in your homework. |     \n","\n","\n","## How to configure T5 task for MultiRC\n","`.setTask('wic pos:)`  followed by `sentence1:` prefix for the first sentence, followed by `sentence2:` prefix for the second sentence.\n","\n","\n","### Example pre-processed input for T5  WiC task:\n","\n","```\n","wic pos:\n","sentence1:    The expanded window will give us time to catch the thieves.\n","sentence2:    You have a two-hour window of turning in your homework.\n","word :        window\n","```\n","\n","\n","\n","# Task 13 [WSC and DPR - Coreference resolution/ Pronoun ambiguity resolver  ](https://www.aaai.org/ocs/index.php/KR/KR12/paper/view/4492/0)\n","Predict for an `ambiguous pronoun` to which `noun` it is referring to.     \n","This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf) and [SuperGLUE](https://w4ngatang.github.io/static/papers/superglue.pdf).\n","\n","|Prediction| Text | \n","|----------|-------|\n","| stable   | The stable was very roomy, with four good stalls; a large swinging window opened into the yard , which made *it* pleasant and airy. | \n","\n","\n","\n","## How to configure T5 task for WSC/DPR\n","`.setTask('wsc:)` and surround pronoun with asteriks symbols..\n","\n","\n","### Example pre-processed input for T5  WSC/DPR  task:\n","The `ambiguous pronous` should be surrounded with `*` symbols.\n","\n","***Note*** Read [Appendix A.](https://arxiv.org/pdf/1910.10683.pdf#page=64&zoom=100,84,360) for more info\n","```\n","wsc: \n","The stable was very roomy, with four good stalls; a large swinging window opened into the yard , which made *it* pleasant and airy.\n","```\n","\n","\n","# Task 14 [Text summarization](https://arxiv.org/abs/1506.03340)\n","`Summarizes` a paragraph into a shorter version with the same semantic meaning.\n","\n","| Predicted summary| Text | \n","|------------------|-------|\n","| manchester united face newcastle in the premier league on wednesday . louis van gaal's side currently sit two points clear of liverpool in fourth . the belgian duo took to the dance floor on monday night with some friends .            | the belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal’s side currently sit two points clear of liverpool in fourth . | \n","\n","\n","## How to configure T5 task for summarization\n","`.setTask('summarize:)`\n","\n","\n","### Example pre-processed input for T5 summarization task:\n","This task requires no pre-processing, setting the task to `summarize` is sufficient.\n","```\n","the belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal’s side currently sit two points clear of liverpool in fourth .\n","```\n","\n","# Task 15 [SQuAD - Context based question answering](https://arxiv.org/abs/1606.05250)\n","Predict an `answer` to a `question` based on input `context`.\n","\n","|Predicted Answer | Question | Context | \n","|-----------------|----------|------|\n","|carbon monoxide| What does increased oxygen concentrations in the patient’s lungs displace? | Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the ’bends’) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment.\n","|pie| What did Joey eat for breakfast?| Once upon a time, there was a squirrel named Joey. Joey loved to go outside and play with his cousin Jimmy. Joey and Jimmy played silly games together, and were always laughing. One day, Joey and Jimmy went swimming together 50 at their Aunt Julie’s pond. Joey woke up early in the morning to eat some food before they left. Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast. After he ate, he and Jimmy went to the pond. On their way there they saw their friend Jack Rabbit. They dove into the water and swam for several hours. The sun was out, but the breeze was cold. Joey and Jimmy got out of the water and started walking home. Their fur was wet, and the breeze chilled them. When they got home, they dried off, and Jimmy put on his favorite purple shirt. Joey put on a blue shirt with red and green dots. The two squirrels ate some food that Joey’s mom, Jasmine, made and went off to bed,'|  \n","\n","## How to configure T5 task parameter for Squad Context based question answering\n","`.setTask('question:)` and prefix the context which can be made up of multiple sentences with `context:`\n","\n","## Example pre-processed input for T5 Squad Context based question answering:\n","```\n","question: What does increased oxygen concentrations in the patient’s lungs displace? \n","context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the ’bends’) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment.\n","```\n","\n","\n","\n","# Task 16 [WMT1 Translate English to German](https://arxiv.org/abs/1706.03762)\n","For translation tasks use the `marian` model\n","## How to configure T5 task parameter for WMT Translate English to German\n","`.setTask('translate English to German:)`\n","\n","# Task 17 [WMT2 Translate English to French](https://arxiv.org/abs/1706.03762)\n","For translation tasks use the `marian` model\n","## How to configure T5 task parameter for WMT Translate English to French\n","`.setTask('translate English to French:)`\n","\n","\n","# 18 [WMT3 - Translate English to Romanian](https://arxiv.org/abs/1706.03762)\n","For translation tasks use the `marian` model\n","## How to configure T5 task parameter for English to Romanian\n","`.setTask('translate English to Romanian:)`\n"]},{"cell_type":"markdown","metadata":{"id":"l73NeLo6r71r"},"source":["# Spark-NLP Example for every Task:\n","\n","\n","# Install Spark NLP"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5PbgLMh5Yag","vscode":{"languageId":"python"}},"outputs":[],"source":["!pip install -q pyspark==3.3.0 spark-nlp==4.3.2"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":254},"executionInfo":{"elapsed":126899,"status":"ok","timestamp":1680627970284,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":240},"id":"LCqxx2hs6oyw","outputId":"b4da1523-2029-49e1-da7c-5509b204b5fb","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Spark NLP version 4.3.2\n","Apache Spark version: 3.3.0\n"]},{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://233304db021a:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.0</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Spark NLP</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f2642ea4580>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import sparknlp\n","\n","spark = sparknlp.start()\n","\n","print(\"Spark NLP version\", sparknlp.version())\n","print(\"Apache Spark version:\", spark.version)\n","\n","spark"]},{"cell_type":"markdown","metadata":{"id":"zRfTmN5Rp59D"},"source":["## Define Document assembler and T5 model for running the tasks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VEIuNVngE77l","vscode":{"languageId":"python"}},"outputs":[],"source":["import pandas as pd\n","pd.set_option('display.width', 100000)\n","pd.set_option('max_colwidth', 8000)\n","pd.set_option('display.max_rows', 500)\n","pd.set_option('display.max_columns', 500)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":139272,"status":"ok","timestamp":1680628110737,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":240},"id":"3fJqtnZtaDpd","outputId":"fd117a0a-e564-4a9e-c06f-8d4b999b366f","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["t5_base download started this may take some time.\n","Approximate size to download 451.8 MB\n","[OK!]\n"]}],"source":["import sparknlp\n","\n","from sparknlp.base import *\n","from sparknlp.common import *\n","from sparknlp.annotator import *\n","from pyspark.ml import Pipeline\n","\n","documentAssembler = DocumentAssembler() \\\n","    .setInputCol(\"text\") \\\n","    .setOutputCol(\"document\") \n","\n","# Can take in document or sentence columns\n","t5 = T5Transformer.pretrained(name='t5_base',lang='en')\\\n","    .setInputCols('document')\\\n","    .setOutputCol(\"T5\")\\\n","    .setMaxOutputLength(400)"]},{"cell_type":"markdown","metadata":{"id":"KBKk3WWFabhn"},"source":["\n","# Task 1 [CoLA - Binary Grammatical Sentence acceptability classification](https://nyu-mll.github.io/CoLA/)\n","Judges if a sentence is grammatically acceptable.     \n","This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf).\n","\n","\n","\n","## Example\n","\n","|sentence  | prediction|\n","|------------|------------|\n","| Anna and Mike is going skiing and they is liked is | unacceptable |      \n","| Anna and Mike like to dance | acceptable | \n","\n","## How to configure T5 task for CoLA\n","`.setTask(cola sentence:)` prefix.\n","\n","### Example pre-processed input for T5 CoLA sentence acceptability judgement:\n","```\n","cola \n","sentence: Anna and Mike is going skiing and they is liked is\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10810,"status":"ok","timestamp":1680628121537,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":240},"id":"roTo1IGqaayd","outputId":"76043f21-ef4a-4291-a441-61679e6f417d","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------------------------------+--------------+\n","|text                                              |result        |\n","+--------------------------------------------------+--------------+\n","|Anna and Mike is going skiing and they is liked is|[unacceptable]|\n","|Anna and Mike like to dance                       |[acceptable]  |\n","+--------------------------------------------------+--------------+\n","\n"]}],"source":["# Set the task on T5\n","t5.setTask('cola sentence:')\n","\n","# Build pipeline with T5\n","pipe_components = [documentAssembler,t5]\n","pipeline = Pipeline().setStages(pipe_components)\n","\n","# define Data\n","sentences = [['Anna and Mike is going skiing and they is liked is'],['Anna and Mike like to dance']]\n","df = spark.createDataFrame(sentences).toDF(\"text\")\n","\n","#Predict on text data with T5\n","model = pipeline.fit(df)\n","annotated_df = model.transform(df)\n","annotated_df.select(['text','t5.result']).show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"FeSXp9bVio-T"},"source":["# Task 2 [RTE - Natural language inference Deduction Classification](https://dl.acm.org/doi/10.1007/11736790_9)\n","The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (entailed) from the other or not.       \n","Classification of sentence pairs as entailment and not_entailment       \n","This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf) and  [SuperGLUE](https://w4ngatang.github.io/static/papers/superglue.pdf).\n","\n","\n","\n","## Example\n","\n","|sentence 1 | sentence 2 | prediction|\n","|------------|------------|----------|\n","Kessler ’s team conducted 60,643 interviews with adults in 14 countries.  |  Kessler ’s team interviewed more than 60,000 adults in 14 countries | entailment\n","Peter loves New York, it is his favorite city| Peter loves new York. | entailment\n","Recent report say Johnny makes he alot of money, he earned 10 million USD each year for the last 5 years.  |Johnny is a millionare | entailment|\n","Recent report say Johnny makes he alot of money, he earned 10 million USD each year for the last 5 years.  |Johnny is a poor man  | not_entailment | \n","| It was raining in England for the last 4 weeks | England was very dry yesterday | not_entailment|\n","\n","## How to configure T5 task for RTE\n","`.setTask('rte sentence1:)` and prefix second sentence with `sentence2:`\n","\n","\n","### Example pre-processed input for T5 RTE - 2 Class Natural language inference\n","```\n","rte \n","sentence1: Recent report say Peter makes he alot of money, he earned 10 million USD each year for the last 5 years. \n","sentence2: Peter is a millionare.\n","```\n","\n","### References\n","- https://arxiv.org/abs/2010.03061\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7243,"status":"ok","timestamp":1680628128776,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":240},"id":"azVTmLRHf_tO","outputId":"caad9385-aef6-4eae-8bb5-667bce56d802","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------------------------------------------------------------------------------------------------------------+----------------+\n","|text                                                                                                                                      |result          |\n","+------------------------------------------------------------------------------------------------------------------------------------------+----------------+\n","|Recent report say Peter makes he alot of money, he earned 10 million USD each year for the last 5 years.  sentence2: Peter is a millionare|[entailment]    |\n","|Recent report say Peter makes he alot of money, he earned 10 million USD each year for the last 5 years.  sentence2: Peter is a poor man  |[not_entailment]|\n","+------------------------------------------------------------------------------------------------------------------------------------------+----------------+\n","\n"]}],"source":["# Set the task on T5\n","t5.setTask('rte sentence1:')\n","\n","# Build pipeline with T5\n","pipe_components = [documentAssembler,t5]\n","pipeline = Pipeline().setStages( pipe_components)\n","\n","# define Data, add additional tags between sentences\n","sentences = [['Recent report say Peter makes he alot of money, he earned 10 million USD each year for the last 5 years.  sentence2: Peter is a millionare'],\n","             ['Recent report say Peter makes he alot of money, he earned 10 million USD each year for the last 5 years.  sentence2: Peter is a poor man']]\n","\n","df = spark.createDataFrame(sentences).toDF(\"text\")\n","\n","#Predict on text data with T5\n","model = pipeline.fit(df)\n","annotated_df = model.transform(df)\n","annotated_df.select(['text','t5.result']).show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"4SnQfLLIjZjG"},"source":["\n","# Task 3 [MNLI - 3 Class Natural Language Inference 3-class contradiction classification](https://arxiv.org/abs/1704.05426)\n","Classification of sentence pairs with the labels `entailment`, `contradiction`, and `neutral`.      \n","This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf).\n","\n","\n","This classifier predicts for two sentences :\n","- Whether the first sentence logically and semantically follows from the second sentence as entailment\n","- Whether the first sentence is a contradiction to the second sentence as a contradiction\n","- Whether the first sentence does not entail or contradict the first sentence as neutral\n","\n","| Hypothesis | Premise | prediction|\n","|------------|------------|----------|\n","| Recent report say Johnny makes he alot of money, he earned 10 million USD each year for the last 5 years. |    Johnny is a poor man.  | contradiction|\n","|It rained in England the last 4 weeks.| It was snowing in New York last week| neutral | \n","\n","## How to configure T5 task for MNLI\n","`.setTask('mnli hypothesis:)` and prefix second sentence with `premise:`\n","\n","### Example pre-processed input for T5 MNLI - 3 Class Natural Language Inference\n","\n","```\n","mnli \n","hypothesis: At 8:34, the Boston Center controller received a third, transmission from American 11.    \n","premise: The Boston Center controller got a third transmission from American 11.\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3628,"status":"ok","timestamp":1680628132376,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":240},"id":"PP_-VKIojMUh","outputId":"685ee640-3584-4fb9-fe36-4152edbe6a06","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+---------------+\n","|                text|         result|\n","+--------------------+---------------+\n","| hypothesis: At 8...|      [neutral]|\n","|hypothesis: Recen...|[contradiction]|\n","+--------------------+---------------+\n","\n"]}],"source":["# Set the task on T5\n","t5.setTask('mnli ')\n","\n","# Build pipeline with T5\n","pipe_components = [documentAssembler,t5]\n","pipeline = Pipeline().setStages( pipe_components)\n","\n","# define Data, add additional tags between sentences\n","sentences = [[''' hypothesis: At 8:34, the Boston Center controller received a third, transmission from American 11.\n","                  premise: The Boston Center controller got a third transmission from American 11.'''],\n","             ['''hypothesis: Recent report say Johnny makes he alot of money, he earned 10 million USD each year for the last 5 years.\n","                 premise: Johnny is a poor man.''']]\n","\n","df = spark.createDataFrame(sentences).toDF(\"text\")\n","\n","#Predict on text data with T5\n","model = pipeline.fit(df)\n","annotated_df = model.transform(df)\n","annotated_df.select(['text','t5.result']).show()#.toPandas().head(5) <-- for better vis of result data frame"]},{"cell_type":"markdown","metadata":{"id":"FDaoLg8Dkj8W"},"source":["\n","# Task 4 [MRPC - Binary Paraphrasing/ sentence similarity classification ](https://www.aclweb.org/anthology/I05-5002.pdf)\n","Detect whether one sentence is a re-phrasing or similar to another sentence      \n","This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf).\n","\n","\n","| Sentence1 | Sentence2 | prediction|\n","|------------|------------|----------|\n","|We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , \" Rumsfeld said .| Rather , the US acted because the administration saw \" existing evidence in a new light , through the prism of our experience on September 11 \" . | equivalent | \n","| I like to eat peanutbutter for breakfast| I like to play football | not_equivalent | \n","\n","\n","## How to configure T5 task for MRPC\n","`.setTask('mrpc sentence1:)` and prefix second sentence with `sentence2:`\n","\n","### Example pre-processed input for T5 MRPC - Binary Paraphrasing/ sentence similarity\n","\n","```\n","mrpc \n","sentence1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , \" Rumsfeld said . \n","sentence2: Rather , the US acted because the administration saw \" existing evidence in a new light , through the prism of our experience on September 11\",\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"elapsed":4666,"status":"ok","timestamp":1680628137007,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":240},"id":"KmRnjdDBkFJf","outputId":"83b5a37b-75ac-4b0c-db8b-4958e8cae638","vscode":{"languageId":"python"}},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-2ead0648-fb2b-4f90-b897-c2b6c8297fa6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>result</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>sentence1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , \" Rumsfeld said .\\n                  sentence2: Rather , the US acted because the administration saw \" existing evidence in a new light , through the prism of our experience on September 11 \"</td>\n","      <td>[equivalent]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>sentence1: I like to eat peanutbutter for breakfast\\n                  sentence2: I like to play football.</td>\n","      <td>[not_equivalent]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2ead0648-fb2b-4f90-b897-c2b6c8297fa6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2ead0648-fb2b-4f90-b897-c2b6c8297fa6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2ead0648-fb2b-4f90-b897-c2b6c8297fa6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                                                                                                                                                                                                                                                                                                text            result\n","0   sentence1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , \" Rumsfeld said .\\n                  sentence2: Rather , the US acted because the administration saw \" existing evidence in a new light , through the prism of our experience on September 11 \"       [equivalent]\n","1                                                                                                                                                                                                                         sentence1: I like to eat peanutbutter for breakfast\\n                  sentence2: I like to play football.  [not_equivalent]"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Set the task on T5\n","t5.setTask('mrpc ')\n","\n","# Build pipeline with T5\n","pipe_components = [documentAssembler,t5]\n","pipeline = Pipeline().setStages( pipe_components)\n","\n","# define Data, add additional tags between sentences\n","sentences = [[''' sentence1: We acted because we saw the existing evidence in a new light , through the prism of our experience on 11 September , \" Rumsfeld said .\n","                  sentence2: Rather , the US acted because the administration saw \" existing evidence in a new light , through the prism of our experience on September 11 \" '''],\n","             [''' sentence1: I like to eat peanutbutter for breakfast\n","                  sentence2: I like to play football.''']]\n","\n","\n","\n","df = spark.createDataFrame(sentences).toDF(\"text\")\n","\n","#Predict on text data with T5\n","model = pipeline.fit(df)\n","annotated_df = model.transform(df)\n","annotated_df.select(['text','t5.result']).toPandas()#show()"]},{"cell_type":"markdown","metadata":{"id":"pHcRyNahk8x-"},"source":["\n","# Task 5 [QNLI - Natural Language Inference question answered classification](https://arxiv.org/pdf/1804.07461.pdf)\n","Classify whether a question is answered by a sentence (`entailed`).       \n","This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf).\n","\n","| Question | Answer | prediction|\n","|------------|------------|----------|\n","|Where did Jebe die?| Ghenkis Khan recalled Subtai back to Mongolia soon afterward, and Jebe died on the road back to Samarkand | entailment|\n","|What does Steve like to eat? | Steve watches TV all day | not_netailment\n","\n","## How to configure T5 task for QNLI - Natural Language Inference question answered classification\n","`.setTask('QNLI sentence1:)` and prefix question with `question:` sentence with `sentence:`:\n","\n","### Example pre-processed input for T5 QNLI - Natural Language Inference question answered classification\n","\n","```\n","qnli\n","question: Where did Jebe die?     \n","sentence: Ghenkis Khan recalled Subtai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand,\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"executionInfo":{"elapsed":5870,"status":"ok","timestamp":1680628142868,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":240},"id":"JGG9z8Vmk4zJ","outputId":"49f0bb1b-b3e8-4bb1-ed42-1667f2a8a3d9","vscode":{"languageId":"python"}},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-d3d6f0ce-3685-41fd-a520-14c9023dadc7\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>result</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>question:  Where did Jebe die?    \\n                  sentence: Ghenkis Khan recalled Subtai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand,</td>\n","      <td>[entailment]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>question: What does Steve like to eat?\\t\\n                 sentence: \\tSteve watches TV all day</td>\n","      <td>[not_entailment]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d3d6f0ce-3685-41fd-a520-14c9023dadc7')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d3d6f0ce-3685-41fd-a520-14c9023dadc7 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d3d6f0ce-3685-41fd-a520-14c9023dadc7');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                                                                                                                                           text            result\n","0   question:  Where did Jebe die?    \\n                  sentence: Ghenkis Khan recalled Subtai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand,      [entailment]\n","1                                                                               question: What does Steve like to eat?\\t\\n                 sentence: \\tSteve watches TV all day  [not_entailment]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Set the task on T5\n","t5.setTask('QNLI ')\n","\n","# Build pipeline with T5\n","pipe_components = [documentAssembler,t5]\n","pipeline = Pipeline().setStages( pipe_components)\n","\n","# define Data, add additional tags between sentences\n","sentences = [[''' question:  Where did Jebe die?    \n","                  sentence: Ghenkis Khan recalled Subtai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand,'''],\n","             ['''question: What does Steve like to eat?\t\n","                 sentence: \tSteve watches TV all day''']]\n","\n","df = spark.createDataFrame(sentences).toDF(\"text\")\n","\n","#Predict on text data with T5\n","model = pipeline.fit(df)\n","annotated_df = model.transform(df)\n","annotated_df.select(['text','t5.result']).toPandas()#.show()"]},{"cell_type":"markdown","metadata":{"id":"lzBXz5calRkP"},"source":["\n","# Task 6 [QQP - Binary Question Similarity/Paraphrasing](https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs)\n","Based on a quora dataset, determine whether a pair of questions are semantically equivalent.      \n","This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf).\n","\n","| Question1 | Question2 | prediction|\n","|------------|------------|----------|\n","|What attributes would have made you highly desirable in ancient Rome?        | How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER? | not_duplicate | \n","|What was it like in Ancient rome?  | What was Ancient rome like?| duplicate | \n","\n","\n","## How to configure T5 task for QQP\n",".setTask('qqp question1:) and\n","prefix second sentence with question2:\n","\n","\n","### Example pre-processed input for T5 QQP - Binary Question Similarity/Paraphrasing\n","\n","```\n","qqp \n","question1: What attributes would have made you highly desirable in ancient Rome?        \n","question2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?',\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"executionInfo":{"elapsed":4215,"status":"ok","timestamp":1680628147079,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":240},"id":"2WWW-0X6lRJT","outputId":"59ebda17-8723-43af-a2bc-1efd23ea861d","vscode":{"languageId":"python"}},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-c63b6f9c-9cb7-44be-8e44-103246472e0d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>result</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>question1:  What attributes would have made you highly desirable in ancient Rome?    \\n                  question2:  How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?'\\n</td>\n","      <td>[not_duplicate]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>question1: What was it like in Ancient rome?\\n                 question2: \\tWhat was Ancient rome like?\\n</td>\n","      <td>[duplicate]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c63b6f9c-9cb7-44be-8e44-103246472e0d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c63b6f9c-9cb7-44be-8e44-103246472e0d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c63b6f9c-9cb7-44be-8e44-103246472e0d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                                                                                                                                                            text           result\n","0   question1:  What attributes would have made you highly desirable in ancient Rome?    \\n                  question2:  How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?'\\n                [not_duplicate]\n","1                                                                        question1: What was it like in Ancient rome?\\n                 question2: \\tWhat was Ancient rome like?\\n                    [duplicate]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# Set the task on T5\n","t5.setTask('qqp ')\n","\n","# Build pipeline with T5\n","pipe_components = [documentAssembler,t5]\n","pipeline = Pipeline().setStages( pipe_components)\n","\n","# define Data, add additional tags between sentences\n","sentences = [[''' question1:  What attributes would have made you highly desirable in ancient Rome?    \n","                  question2:  How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?'\n","              '''],\n","             ['''question1: What was it like in Ancient rome?\n","                 question2: \tWhat was Ancient rome like?\n","              ''']]\n","\n","\n","df = spark.createDataFrame(sentences).toDF(\"text\")\n","\n","#Predict on text data with T5\n","model = pipeline.fit(df)\n","annotated_df = model.transform(df)\n","annotated_df.select(['text','t5.result']).toPandas()#.show()"]},{"cell_type":"markdown","metadata":{"id":"GsvuFTkjlm-N"},"source":["\n","# Task 7 [SST2 - Binary Sentiment Analysis](https://www.aclweb.org/anthology/D13-1170.pdf)\n","Binary sentiment classification.      \n","This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf).\n","\n","| Sentence1 | Prediction  | \n","|-----------|-----------|\n","|it confirms fincher ’s status as a film maker who artfully bends technical know-how to the service of psychological insight |  positive| \n","|I really hated that movie | negative | \n","\n","\n","## How to configure T5 task for  SST2\n","`.setTask('sst2 sentence: ')`\n","\n","### Example pre-processed input for T5 SST2 - Binary Sentiment Analysis\n","\n","```\n","sst2\n","sentence: I hated that movie\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"executionInfo":{"elapsed":3291,"status":"ok","timestamp":1680628150364,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":240},"id":"FPP4pMVQlOtz","outputId":"866fb521-4251-4bb3-ab86-bad17c4a98b6","vscode":{"languageId":"python"}},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-5a7ebd4a-b72d-4c7f-b6aa-5817a5fad322\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>result</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I really hated that movie</td>\n","      <td>[negative]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>it confirms fincher ’s status as a film maker who artfully bends technical know-how to the service of psychological insight</td>\n","      <td>[positive]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5a7ebd4a-b72d-4c7f-b6aa-5817a5fad322')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-5a7ebd4a-b72d-4c7f-b6aa-5817a5fad322 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5a7ebd4a-b72d-4c7f-b6aa-5817a5fad322');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                                                                                            text      result\n","0                                                                                                      I really hated that movie  [negative]\n","1    it confirms fincher ’s status as a film maker who artfully bends technical know-how to the service of psychological insight  [positive]"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Set the task on T5\n","t5.setTask('sst2 sentence: ')\n","\n","# Build pipeline with T5\n","pipe_components = [documentAssembler,t5]\n","pipeline = Pipeline().setStages( pipe_components)\n","\n","# define Data, add additional tags between sentences\n","sentences = [[''' I really hated that movie'''],\n","             ['''  it confirms fincher ’s status as a film maker who artfully bends technical know-how to the service of psychological insight''']]\n","\n","df = spark.createDataFrame(sentences).toDF(\"text\")\n","\n","#Predict on text data with T5\n","model = pipeline.fit(df)\n","annotated_df = model.transform(df)\n","annotated_df.select(['text','t5.result']).toPandas()#show()"]},{"cell_type":"markdown","metadata":{"id":"dpZQ_H8fl4OV"},"source":["\n","# Task8 [STSB - Regressive semantic sentence similarity](https://www.aclweb.org/anthology/S17-2001/)\n","Measures how similar two sentences are on a scale from 0 to 5 with 21 classes representing a regressive label.     \n","This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf).\n","\n","\n","| Question1 | Question2 | prediction|\n","|------------|------------|----------|\n","|What attributes would have made you highly desirable in ancient Rome?        | How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER? | 0 | \n","|What was it like in Ancient rome?  | What was Ancient rome like?| 5.0 | \n","|What was live like as a King in Ancient Rome??       | What is it like to live in Rome? | 3.2 | \n","\n","## How to configure T5 task for STSB\n","`.setTask('stsb sentence1:)` and prefix second sentence with `sentence2:`\n","\n","\n","### Example pre-processed input for T5 STSB - Regressive semantic sentence similarity\n","\n","```\n","stsb\n","sentence1: What attributes would have made you highly desirable in ancient Rome?        \n","sentence2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?',\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"elapsed":4813,"status":"ok","timestamp":1680628155165,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":240},"id":"yPODqDWYl35B","outputId":"e61e9177-1134-47dc-cc32-954da3c6739e","vscode":{"languageId":"python"}},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-db2b7e71-4c03-40b5-8fde-adf5b25010ec\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>result</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>sentence1:  What attributes would have made you highly desirable in ancient Rome?  \\n                  sentence2:  How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?'\\n</td>\n","      <td>[0.0]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>sentence1: What was it like in Ancient rome?\\n                  sentence2: \\tWhat was Ancient rome like?</td>\n","      <td>[5.0]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>sentence1: What was live like as a King in Ancient Rome??\\n                  sentence2: \\tWhat was Ancient rome like?</td>\n","      <td>[3.2]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-db2b7e71-4c03-40b5-8fde-adf5b25010ec')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-db2b7e71-4c03-40b5-8fde-adf5b25010ec button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-db2b7e71-4c03-40b5-8fde-adf5b25010ec');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                                                                                                                                                          text result\n","0   sentence1:  What attributes would have made you highly desirable in ancient Rome?  \\n                  sentence2:  How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?'\\n                [0.0]\n","1                                                                                     sentence1: What was it like in Ancient rome?\\n                  sentence2: \\tWhat was Ancient rome like?  [5.0]\n","2                                                                        sentence1: What was live like as a King in Ancient Rome??\\n                  sentence2: \\tWhat was Ancient rome like?  [3.2]"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# Set the task on T5\n","t5.setTask('stsb ')\n","\n","# Build pipeline with T5\n","pipe_components = [documentAssembler,t5]\n","pipeline = Pipeline().setStages( pipe_components)\n","\n","# define Data, add additional tags between sentences\n","sentences = [[''' sentence1:  What attributes would have made you highly desirable in ancient Rome?  \n","                  sentence2:  How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?'\n","              '''],\n","             [''' sentence1: What was it like in Ancient rome?\n","                  sentence2: \tWhat was Ancient rome like?'''],\n","             [''' sentence1: What was live like as a King in Ancient Rome??\n","                  sentence2: \tWhat was Ancient rome like?''']]\n","\n","df = spark.createDataFrame(sentences).toDF(\"text\")\n","\n","#Predict on text data with T5\n","model = pipeline.fit(df)\n","annotated_df = model.transform(df)\n","annotated_df.select(['text','t5.result']).toPandas()#show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"2pGz7_qsmQUF"},"source":["\n","# Task 9[ CB -  Natural language inference contradiction classification](https://ojs.ub.uni-konstanz.de/sub/index.php/sub/article/view/601)\n","Classify whether a Premise contradicts a Hypothesis.    \n","Predicts entailment, neutral and contradiction     \n","This is a sub-task of [SuperGLUE](https://w4ngatang.github.io/static/papers/superglue.pdf).\n","\n","\n","| Hypothesis | Premise | Prediction | \n","|--------|-------------|----------|\n","|Valence was helping | Valence the void-brain, Valence the virtuous valet. Why couldn’t the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping'| Contradiction|\n","\n","\n","## How to configure T5 task for CB\n","`.setTask('cb hypothesis:)` and prefix premise with `premise:`\n","\n","### Example pre-processed input for T5 CB -  Natural language inference contradiction classification\n","\n","```\n","cb \n","hypothesis: Valence was helping      \n","premise: Valence the void-brain, Valence the virtuous valet. Why couldn’t the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping,\n","```\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81},"executionInfo":{"elapsed":1696,"status":"ok","timestamp":1680628156855,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":240},"id":"XjGzx2v8l2lk","outputId":"4dee2989-fd17-4c03-e03c-6a0d794cda0c","vscode":{"languageId":"python"}},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-fe06451b-6bf1-4561-8e80-68b5ad191910\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>result</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\\n              hypothesis: Recent report say Johnny makes he alot of money, he earned 10 million USD each year for the last 5 years.\\n              premise: Johnny is a poor man.\\n</td>\n","      <td>[contradiction]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fe06451b-6bf1-4561-8e80-68b5ad191910')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-fe06451b-6bf1-4561-8e80-68b5ad191910 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-fe06451b-6bf1-4561-8e80-68b5ad191910');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                                                                                                                                                                                text           result\n","0  \\n              hypothesis: Recent report say Johnny makes he alot of money, he earned 10 million USD each year for the last 5 years.\\n              premise: Johnny is a poor man.\\n                              [contradiction]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Set the task on T5\n","t5.setTask('cb ')\n","\n","# Build pipeline with T5\n","pipe_components = [documentAssembler,t5]\n","pipeline = Pipeline().setStages( pipe_components)\n","\n","# define Data, add additional tags between sentences\n","sentences = [\n","             [\n","              '''\n","              hypothesis: Recent report say Johnny makes he alot of money, he earned 10 million USD each year for the last 5 years.\n","              premise: Johnny is a poor man.\n","                            ''']\n","             ]\n","\n","\n","\n","df = spark.createDataFrame(sentences).toDF(\"text\")\n","\n","#Predict on text data with T5\n","model = pipeline.fit(df)\n","annotated_df = model.transform(df)\n","annotated_df.select(['text','t5.result']).toPandas()#show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"Q8Zg4o7NnDy7"},"source":["\n","# Task 10 [COPA - Sentence Completion/ Binary choice selection](https://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2418/0)\n","The Choice of Plausible Alternatives (COPA) task by Roemmele et al. (2011) evaluates\n","causal reasoning between events, which requires commonsense knowledge about what usually takes\n","place in the world. Each example provides a premise and either asks for the correct cause or effect\n","from two choices, thus testing either ``backward`` or `forward causal reasoning`. COPA data, which\n","consists of 1,000 examples total, can be downloaded at https://people.ict.usc.e\n","\n","This is a sub-task of [SuperGLUE](https://w4ngatang.github.io/static/papers/superglue.pdf).\n","\n","This classifier selects from a choice of `2 options` which one the correct is based on a `premise`.\n","\n","\n","## forward causal reasoning\n","Premise: The man lost his balance on the ladder.     \n","question: What happened as a result?        \n","Alternative 1: He fell off the ladder.       \n","Alternative 2: He climbed up the ladder.\n","## backwards causal reasoning\n","Premise: The man fell unconscious. What was the cause\n","of this?       \n","Alternative 1: The assailant struck the man in the head.      \n","Alternative 2: The assailant took the man’s wallet.\n","\n","\n","| Question | Premise | Choice 1 | Choice  2 | Prediction | \n","|--------|-------------|----------|---------|-------------|\n","|effect | Politcal Violence broke out in the nation. | many citizens relocated to the capitol. |  Many citizens took refuge in other territories | Choice 1  | \n","|correct| The men fell unconscious | The assailant struckl the man in the head | he assailant s took the man's wallet. | choice1 | \n","\n","\n","## How to configure T5 task for COPA\n","`.setTask('copa choice1:)`, prefix choice2 with `choice2:` , prefix premise with `premise:` and prefix the question with `question`\n","\n","### Example pre-processed input for T5 COPA - Sentence Completion/ Binary choice selection\n","\n","```\n","copa \n","choice1:   He fell off the ladder    \n","choice2:   He climbed up the lader       \n","premise:   The man lost his balance on the ladder \n","question:  effect\n","```\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81},"executionInfo":{"elapsed":2001,"status":"ok","timestamp":1680628158844,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":240},"id":"bTt90f0pmtxi","outputId":"87ddc578-50e2-4ef4-dd9a-fe697e20e8e4","vscode":{"languageId":"python"}},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-53fdfef2-213e-4304-b3e4-cc02f2c49618\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>result</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\\n            choice1:   He fell off the ladder    \\n            choice2:   He climbed up the lader       \\n            premise:   The man lost his balance on the ladder \\n            question:  effect</td>\n","      <td>[choice1]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-53fdfef2-213e-4304-b3e4-cc02f2c49618')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-53fdfef2-213e-4304-b3e4-cc02f2c49618 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-53fdfef2-213e-4304-b3e4-cc02f2c49618');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                                                                                                                                                                                        text     result\n","0  \\n            choice1:   He fell off the ladder    \\n            choice2:   He climbed up the lader       \\n            premise:   The man lost his balance on the ladder \\n            question:  effect  [choice1]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# Set the task on T5\n","t5.setTask('copa ')\n","\n","# Build pipeline with T5\n","pipe_components = [documentAssembler,t5]\n","pipeline = Pipeline().setStages( pipe_components)\n","\n","# define Data, add additional tags between sentences\n","sentences = [['''\n","            choice1:   He fell off the ladder    \n","            choice2:   He climbed up the lader       \n","            premise:   The man lost his balance on the ladder \n","            question:  effect''']]\n","\n","\n","df = spark.createDataFrame(sentences).toDF(\"text\")\n","\n","#Predict on text data with T5\n","model = pipeline.fit(df)\n","annotated_df = model.transform(df)\n","annotated_df.select(['text','t5.result']).toPandas()#show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"MtTZAk02nR16"},"source":["\n","# Task 11 [MultiRc - Question Answering](https://www.aclweb.org/anthology/N18-1023.pdf)\n","Evaluates an `answer` for a `question` as `true` or `false` based on an input `paragraph`\n","The T5 model predicts for a `question` and a `paragraph` of `sentences` wether an `answer` is true or not,\n","based on the semantic contents of the paragraph.        \n","This is a sub-task of [SuperGLUE](https://w4ngatang.github.io/static/papers/superglue.pdf).\n","\n","\n","\n","**Exceeds human performance by a large margin**\n","\n","\n","\n","| Question                                                     | Answer                                                              | Prediction | paragraph|\n","|--------------------------------------------------------------|---------------------------------------------------------------------|------------|----------|\n","| Why was Joey surprised the morning he woke up for breakfast? | There was only pie to eat, rather than traditional breakfast foods  |  True   |Once upon a time, there was a squirrel named Joey. Joey loved to go outside and play with his cousin Jimmy. Joey and Jimmy played silly games together, and were always laughing. One day, Joey and Jimmy went swimming together 50 at their Aunt Julie’s pond. Joey woke up early in the morning to eat some food before they left. He couldn’t find anything to eat except for pie! Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast. After he ate, he and Jimmy went to the pond. On their way there they saw their friend Jack Rabbit. They dove into the water and swam for several hours. The sun was out, but the breeze was cold. Joey and Jimmy got out of the water and started walking home. Their fur was wet, and the breeze chilled them. When they got home, they dried off, and Jimmy put on his favorite purple shirt. Joey put on a blue shirt with red and green dots. The two squirrels ate some food that Joey’s mom, Jasmine, made and went off to bed.,          |\n","| Why was Joey surprised the morning he woke up for breakfast? | There was a T-Rex in his garden  |  False   |Once upon a time, there was a squirrel named Joey. Joey loved to go outside and play with his cousin Jimmy. Joey and Jimmy played silly games together, and were always laughing. One day, Joey and Jimmy went swimming together 50 at their Aunt Julie’s pond. Joey woke up early in the morning to eat some food before they left. He couldn’t find anything to eat except for pie! Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast. After he ate, he and Jimmy went to the pond. On their way there they saw their friend Jack Rabbit. They dove into the water and swam for several hours. The sun was out, but the breeze was cold. Joey and Jimmy got out of the water and started walking home. Their fur was wet, and the breeze chilled them. When they got home, they dried off, and Jimmy put on his favorite purple shirt. Joey put on a blue shirt with red and green dots. The two squirrels ate some food that Joey’s mom, Jasmine, made and went off to bed.,          |\n","\n","## How to configure T5 task for MultiRC\n","`.setTask('multirc questions:)`  followed by `answer:` prefix for the answer to evaluate, followed by `paragraph:` and then a series of sentences, where each sentence is prefixed with `Sent n:`prefix second sentence with sentence2:\n","\n","\n","### Example pre-processed input for T5 MultiRc task:\n","```\n","multirc questions:  Why was Joey surprised the morning he woke up for breakfast?      \n","answer:             There was a T-REX in his garden.      \n","paragraph:      \n","Sent 1:             Once upon a time, there was a squirrel named Joey.      \n","Sent 2:             Joey loved to go outside and play with his cousin Jimmy.      \n","Sent 3:             Joey and Jimmy played silly games together, and were always laughing.      \n","Sent 4:             One day, Joey and Jimmy went swimming together 50 at their Aunt Julie’s pond.      \n","Sent 5:             Joey woke up early in the morning to eat some food before they left.      \n","Sent 6:             He couldn’t find anything to eat except for pie!      \n","Sent 7:             Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast.      \n","Sent 8:             After he ate, he and Jimmy went to the pond.      \n","Sent 9:             On their way there they saw their friend Jack Rabbit.      \n","Sent 10:            They dove into the water and swam for several hours.      \n","Sent 11:            The sun was out, but the breeze was cold.      \n","Sent 12:            Joey and Jimmy got out of the water and started walking home.      \n","Sent 13:            Their fur was wet, and the breeze chilled them.      \n","Sent 14:            When they got home, they dried off, and Jimmy put on his favorite purple shirt.      \n","Sent 15:            Joey put on a blue shirt with red and green dots.      \n","Sent 16:            The two squirrels ate some food that Joey’s mom, Jasmine, made and went off to bed.      \n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8220,"status":"ok","timestamp":1680628167053,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":240},"id":"U87YZ46LnNpx","outputId":"f572ed9b-0357-4afa-8d23-6442788d17c1","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n","|text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |result |\n","+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n","|\\nquestions:  Why was Joey surprised the morning he woke up for breakfast?      \\nanswer:             There was a T-REX in his garden.      \\nparagraph:      \\nSent 1:             Once upon a time, there was a squirrel named Joey.      \\nSent 2:             Joey loved to go outside and play with his cousin Jimmy.      \\nSent 3:             Joey and Jimmy played silly games together, and were always laughing.      \\nSent 4:             One day, Joey and Jimmy went swimming together 50 at their Aunt Julie’s pond.      \\nSent 5:             Joey woke up early in the morning to eat some food before they left.      \\nSent 6:             He couldn’t find anything to eat except for pie!      \\nSent 7:             Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast.      \\nSent 8:             After he ate, he and Jimmy went to the pond.      \\nSent 9:             On their way there they saw their friend Jack Rabbit.      \\nSent 10:            They dove into the water and swam for several hours.      \\nSent 11:            The sun was out, but the breeze was cold.      \\nSent 12:            Joey and Jimmy got out of the water and started walking home.      \\nSent 13:            Their fur was wet, and the breeze chilled them.      \\nSent 14:            When they got home, they dried off, and Jimmy put on his favorite purple shirt.      \\nSent 15:            Joey put on a blue shirt with red and green dots.      \\nSent 16:            The two squirrels ate some food that Joey’s mom, Jasmine, made and went off to bed.      \\n             |[False]|\n","|\\nquestions:  Why was Joey surprised the morning he woke up for breakfast?      \\nanswer:             There was only pie for breakfast.      \\nparagraph:      \\nSent 1:             Once upon a time, there was a squirrel named Joey.      \\nSent 2:             Joey loved to go outside and play with his cousin Jimmy.      \\nSent 3:             Joey and Jimmy played silly games together, and were always laughing.      \\nSent 4:             One day, Joey and Jimmy went swimming together 50 at their Aunt Julie’s pond.      \\nSent 5:             Joey woke up early in the morning to eat some food before they left.      \\nSent 6:             He couldn’t find anything to eat except for pie!      \\nSent 7:             Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast.      \\nSent 8:             After he ate, he and Jimmy went to the pond.      \\nSent 9:             On their way there they saw their friend Jack Rabbit.      \\nSent 10:            They dove into the water and swam for several hours.      \\nSent 11:            The sun was out, but the breeze was cold.      \\nSent 12:            Joey and Jimmy got out of the water and started walking home.      \\nSent 13:            Their fur was wet, and the breeze chilled them.      \\nSent 14:            When they got home, they dried off, and Jimmy put on his favorite purple shirt.      \\nSent 15:            Joey put on a blue shirt with red and green dots.      \\nSent 16:            The two squirrels ate some food that Joey’s mom, Jasmine, made and went off to bed.      \\n            |[True] |\n","+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------+\n","\n"]}],"source":["# Set the task on T5\n","t5.setTask('multirc ')\n","\n","# Build pipeline with T5\n","pipe_components = [documentAssembler,t5]\n","pipeline = Pipeline().setStages( pipe_components)\n","\n","# define Data, add additional tags between sentences\n","sentences = [['''\n","questions:  Why was Joey surprised the morning he woke up for breakfast?      \n","answer:             There was a T-REX in his garden.      \n","paragraph:      \n","Sent 1:             Once upon a time, there was a squirrel named Joey.      \n","Sent 2:             Joey loved to go outside and play with his cousin Jimmy.      \n","Sent 3:             Joey and Jimmy played silly games together, and were always laughing.      \n","Sent 4:             One day, Joey and Jimmy went swimming together 50 at their Aunt Julie’s pond.      \n","Sent 5:             Joey woke up early in the morning to eat some food before they left.      \n","Sent 6:             He couldn’t find anything to eat except for pie!      \n","Sent 7:             Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast.      \n","Sent 8:             After he ate, he and Jimmy went to the pond.      \n","Sent 9:             On their way there they saw their friend Jack Rabbit.      \n","Sent 10:            They dove into the water and swam for several hours.      \n","Sent 11:            The sun was out, but the breeze was cold.      \n","Sent 12:            Joey and Jimmy got out of the water and started walking home.      \n","Sent 13:            Their fur was wet, and the breeze chilled them.      \n","Sent 14:            When they got home, they dried off, and Jimmy put on his favorite purple shirt.      \n","Sent 15:            Joey put on a blue shirt with red and green dots.      \n","Sent 16:            The two squirrels ate some food that Joey’s mom, Jasmine, made and went off to bed.      \n","             '''],\n","             \n","             ['''\n","questions:  Why was Joey surprised the morning he woke up for breakfast?      \n","answer:             There was only pie for breakfast.      \n","paragraph:      \n","Sent 1:             Once upon a time, there was a squirrel named Joey.      \n","Sent 2:             Joey loved to go outside and play with his cousin Jimmy.      \n","Sent 3:             Joey and Jimmy played silly games together, and were always laughing.      \n","Sent 4:             One day, Joey and Jimmy went swimming together 50 at their Aunt Julie’s pond.      \n","Sent 5:             Joey woke up early in the morning to eat some food before they left.      \n","Sent 6:             He couldn’t find anything to eat except for pie!      \n","Sent 7:             Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast.      \n","Sent 8:             After he ate, he and Jimmy went to the pond.      \n","Sent 9:             On their way there they saw their friend Jack Rabbit.      \n","Sent 10:            They dove into the water and swam for several hours.      \n","Sent 11:            The sun was out, but the breeze was cold.      \n","Sent 12:            Joey and Jimmy got out of the water and started walking home.      \n","Sent 13:            Their fur was wet, and the breeze chilled them.      \n","Sent 14:            When they got home, they dried off, and Jimmy put on his favorite purple shirt.      \n","Sent 15:            Joey put on a blue shirt with red and green dots.      \n","Sent 16:            The two squirrels ate some food that Joey’s mom, Jasmine, made and went off to bed.      \n","            ''']]\n","\n","df = spark.createDataFrame(sentences).toDF(\"text\")\n","\n","#Predict on text data with T5\n","model = pipeline.fit(df)\n","annotated_df = model.transform(df)\n","annotated_df.select(['text','t5.result']).show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"KevnNo0pnpaA"},"source":["\n","# Task 12 [WiC - Word sense disambiguation](https://arxiv.org/abs/1808.09121)\n","Decide for `two sentence`s with a shared `disambigous word` wether they have the target word has the same `semantic meaning` in both sentences.       \n","This is a sub-task of [SuperGLUE](https://w4ngatang.github.io/static/papers/superglue.pdf).\n","\n","\n","|Predicted | disambigous word| Sentence 1 | Sentence 2 | \n","|----------|-----------------|------------|------------|\n","| False    | kill            | He totally killed that rock show! | The airplane crash killed his family | \n","| True     | window          | The expanded window will give us time to catch the thieves.|You have a two-hour window for turning in your homework. |     \n","| False     | window          | He jumped out of the window.|You have a two-hour window for turning in your homework. |     \n","\n","\n","## How to configure T5 task for MultiRC\n","`.setTask('wic pos:)`  followed by `sentence1:` prefix for the first sentence, followed by `sentence2:` prefix for the second sentence.\n","\n","\n","### Example pre-processed input for T5  WiC task:\n","\n","```\n","wic pos:\n","sentence1:    The expanded window will give us time to catch the thieves.\n","sentence2:    You have a two-hour window of turning in your homework.\n","word :        window\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1005,"status":"ok","timestamp":1680628168025,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":240},"id":"1KoYaT8cnboA","outputId":"4b1048d6-dbaf-4f2d-fee8-98cf2bad1321","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n","|                                                                                                                                                                                text|result|\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n","|\\npos:\\nsentence1:    The expanded window will give us time to catch the thieves.\\nsentence2:    You have a two-hour window of turning in your homework.\\nword :        window\\n\\...|[True]|\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n","\n"]}],"source":["# Set the task on T5\n","t5.setTask('wic ')\n","\n","# Build pipeline with T5\n","pipe_components = [documentAssembler,t5]\n","pipeline = Pipeline().setStages( pipe_components)\n","\n","# define Data, add additional tags between sentences\n","sentences = [['''\n","pos:\n","sentence1:    The expanded window will give us time to catch the thieves.\n","sentence2:    You have a two-hour window of turning in your homework.\n","word :        window\n","\n","            ''']]\n","\n","\n","\n","df = spark.createDataFrame(sentences).toDF(\"text\")\n","\n","#Predict on text data with T5\n","model = pipeline.fit(df)\n","annotated_df = model.transform(df)\n","annotated_df.select(['text','t5.result']).show(truncate=180)"]},{"cell_type":"markdown","metadata":{"id":"YvcjGf82n39w"},"source":["\n","# Task 13 [WSC and DPR - Coreference resolution/ Pronoun ambiguity resolver  ](https://www.aaai.org/ocs/index.php/KR/KR12/paper/view/4492/0)\n","Predict for an `ambiguous pronoun` to which `noun` it is referring to.     \n","This is a sub-task of [GLUE](https://arxiv.org/pdf/1804.07461.pdf) and [SuperGLUE](https://w4ngatang.github.io/static/papers/superglue.pdf).\n","\n","|Prediction| Text | \n","|----------|-------|\n","| stable   | The stable was very roomy, with four good stalls; a large swinging window opened into the yard , which made *it* pleasant and airy. | \n","\n","\n","\n","## How to configure T5 task for WSC/DPR\n","`.setTask('wsc:)` and surround pronoun with asteriks symbols..\n","\n","\n","### Example pre-processed input for T5  WSC/DPR  task:\n","The `ambiguous pronous` should be surrounded with `*` symbols.\n","\n","***Note*** Read [Appendix A.](https://arxiv.org/pdf/1910.10683.pdf#page=64&zoom=100,84,360) for more info\n","```\n","wsc: \n","The stable was very roomy, with four good stalls; a large swinging window opened into the yard , which made *it* pleasant and airy.\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26384,"status":"ok","timestamp":1680628194402,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":240},"id":"g4ZLodm1nyGQ","outputId":"d6364f51-b099-49de-f8ac-7f78d18711b2","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n","|text                                                                                                                               |result                                                                                                                                   |\n","+-----------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n","|The stable was very roomy, with four good stalls; a large swinging window opened into the yard , which made *it* pleasant and airy.|[wsc The stable was very roomy, with four good stalls; a large swinging window opened into the yard , which made *it* pleasant and airy.]|\n","+-----------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+\n","\n"]}],"source":["# Does not work yet 100% correct\n","# Set the task on T5\n","t5.setTask('wsc')\n","\n","# Build pipeline with T5\n","pipe_components = [documentAssembler,t5]\n","pipeline = Pipeline().setStages( pipe_components)\n","\n","# define Data, add additional tags between sentences\n","sentences = [['''The stable was very roomy, with four good stalls; a large swinging window opened into the yard , which made *it* pleasant and airy.''']]\n","\n","\n","\n","df = spark.createDataFrame(sentences).toDF(\"text\")\n","\n","#Predict on text data with T5\n","model = pipeline.fit(df)\n","annotated_df = model.transform(df)\n","annotated_df.select(['text','t5.result']).show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"_MyugUXQoJd8"},"source":["\n","# Task 14 [Text summarization](https://arxiv.org/abs/1506.03340)\n","`Summarizes` a paragraph into a shorter version with the same semantic meaning.\n","\n","| Predicted summary| Text | \n","|------------------|-------|\n","| manchester united face newcastle in the premier league on wednesday . louis van gaal's side currently sit two points clear of liverpool in fourth . the belgian duo took to the dance floor on monday night with some friends .            | the belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal’s side currently sit two points clear of liverpool in fourth . | \n","\n","\n","## How to configure T5 task for summarization\n","`.setTask('summarize:)`\n","\n","\n","### Example pre-processed input for T5 summarization task:\n","This task requires no pre-processing, setting the task to `summarize` is sufficient.\n","```\n","the belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal’s side currently sit two points clear of liverpool in fourth .\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":89582,"status":"ok","timestamp":1680628283954,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":240},"id":"hBM0VycaoAnB","outputId":"0816d171-0ea7-4a17-e4d5-2f04d9afa8b2","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |result                                                                                                                                                                                                                                                                                                                                                  |\n","+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|\\nThe belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal’s side currently sit two points clear of liverpool in fourth .\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |[manchester united face newcastle in the premier league on wednesday . louis van gaal’s side currently sit two points clear of liverpool in fourth .]                                                                                                                                                                                                   |\n","|  Calculus, originally called infinitesimal calculus or \"the calculus of infinitesimals\", is the mathematical study of continuous change, in the same way that geometry is the study of shape and algebra is the study of generalizations of arithmetic operations. It has two major branches, differential calculus and integral calculus; the former concerns instantaneous rates of change, and the slopes of curves, while integral calculus concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus, and they make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit.[1] Infinitesimal calculus was developed independently in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz.[2][3] Today, calculus has widespread uses in science, engineering, and economics.[4] In mathematics education, calculus denotes courses of elementary mathematical analysis, which are mainly devoted to the study of functions and limits. The word calculus (plural calculi) is a Latin word, meaning originally \"small pebble\" (this meaning is kept in medicine – see Calculus (medicine)). Because such pebbles were used for calculation, the meaning of the word has evolved and today usually means a method of computation. It is therefore used for naming specific methods of calculation and related theories, such as propositional calculus, Ricci calculus, calculus of variations, lambda calculus, and process calculus.\\n            |[calculus, originally called infinitesimal calculus, is the mathematical study of continuous change . it has two major branches, differential calculus and integral calculus . the former concerns instantaneous rates of change, and the slopes of curves . integral calculus concerns accumulation of quantities, and areas under or between curves .]|\n","+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n"]}],"source":["# Set the task on T5\n","t5.setTask('summarize ')\n","\n","# Build pipeline with T5\n","pipe_components = [documentAssembler,t5]\n","pipeline = Pipeline().setStages( pipe_components)\n","\n","# define Data, add additional tags between sentences\n","sentences = [['''\n","The belgian duo took to the dance floor on monday night with some friends . manchester united face newcastle in the premier league on wednesday . red devils will be looking for just their second league away win in seven . louis van gaal’s side currently sit two points clear of liverpool in fourth .\n","            '''],\n","            ['''  Calculus, originally called infinitesimal calculus or \"the calculus of infinitesimals\", is the mathematical study of continuous change, in the same way that geometry is the study of shape and algebra is the study of generalizations of arithmetic operations. It has two major branches, differential calculus and integral calculus; the former concerns instantaneous rates of change, and the slopes of curves, while integral calculus concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus, and they make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit.[1] Infinitesimal calculus was developed independently in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz.[2][3] Today, calculus has widespread uses in science, engineering, and economics.[4] In mathematics education, calculus denotes courses of elementary mathematical analysis, which are mainly devoted to the study of functions and limits. The word calculus (plural calculi) is a Latin word, meaning originally \"small pebble\" (this meaning is kept in medicine – see Calculus (medicine)). Because such pebbles were used for calculation, the meaning of the word has evolved and today usually means a method of computation. It is therefore used for naming specific methods of calculation and related theories, such as propositional calculus, Ricci calculus, calculus of variations, lambda calculus, and process calculus.\n","            ''']]\n","\n","\n","\n","df = spark.createDataFrame(sentences).toDF(\"text\")\n","\n","#Predict on text data with T5\n","model = pipeline.fit(df)\n","annotated_df = model.transform(df)\n","annotated_df.select(['text','t5.result']).show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"ZyxqNOO1obBv"},"source":["\n","# Task 15 [SQuAD - Context based question answering](https://arxiv.org/abs/1606.05250)\n","Predict an `answer` to a `question` based on input `context`.\n","\n","|Predicted Answer | Question | Context | \n","|-----------------|----------|------|\n","|carbon monoxide| What does increased oxygen concentrations in the patient’s lungs displace? | Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the ’bends’) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment.\n","|pie| What did Joey eat for breakfast?| Once upon a time, there was a squirrel named Joey. Joey loved to go outside and play with his cousin Jimmy. Joey and Jimmy played silly games together, and were always laughing. One day, Joey and Jimmy went swimming together 50 at their Aunt Julie’s pond. Joey woke up early in the morning to eat some food before they left. Usually, Joey would eat cereal, fruit (a pear), or oatmeal for breakfast. After he ate, he and Jimmy went to the pond. On their way there they saw their friend Jack Rabbit. They dove into the water and swam for several hours. The sun was out, but the breeze was cold. Joey and Jimmy got out of the water and started walking home. Their fur was wet, and the breeze chilled them. When they got home, they dried off, and Jimmy put on his favorite purple shirt. Joey put on a blue shirt with red and green dots. The two squirrels ate some food that Joey’s mom, Jasmine, made and went off to bed,'|  \n","\n","## How to configure T5 task parameter for Squad Context based question answering\n","`.setTask('question:)` and prefix the context which can be made up of multiple sentences with `context:`\n","\n","## Example pre-processed input for T5 Squad Context based question answering:\n","```\n","question: What does increased oxygen concentrations in the patient’s lungs displace? \n","context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the ’bends’) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment.\n","```\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3852,"status":"ok","timestamp":1680628287766,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":240},"id":"OiotszcWoY4r","outputId":"36ba7c80-aa4f-453a-c30f-f6d43465b93a","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+\n","|text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |result           |\n","+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+\n","|What does increased oxygen concentrations in the patient’s lungs displace? \\ncontext: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the ’bends’) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment.\\n|[carbon monoxide]|\n","+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+\n","\n"]}],"source":["# Set the task on T5\n","t5.setTask('question: ')\n","\n","# Build pipeline with T5\n","pipe_components = [documentAssembler,t5]\n","pipeline = Pipeline().setStages( pipe_components)\n","\n","# define Data, add additional tags between sentences\n","sentences ='''What does increased oxygen concentrations in the patient’s lungs displace? \n","context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the ’bends’) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment.\n","'''\n","\n","df = spark.createDataFrame([[sentences]]).toDF(\"text\")\n","\n","#Predict on text data with T5\n","model = pipeline.fit(df)\n","annotated_df = model.transform(df)\n","annotated_df.select(['text','t5.result']).show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"EV4YUQdkoqXw"},"source":["# Task 16 [WMT1 Translate English to German](https://arxiv.org/abs/1706.03762)\n","For translation tasks use the `marian` model\n","## How to configure T5 task parameter for WMT Translate English to German\n","`.setTask('translate English to German:)`\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6314,"status":"ok","timestamp":1680628294038,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":240},"id":"2aEz3xiGombi","outputId":"1d29b8be-c8e0-4a12-85b3-82468eaad654","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------------------------------+-----------------------------------------------------+\n","|text                                              |result                                               |\n","+--------------------------------------------------+-----------------------------------------------------+\n","|I like sausage and Tea for breakfast with potatoes|[Ich mag Wurst und Tee zum Frühstück mit Kartoffeln.]|\n","+--------------------------------------------------+-----------------------------------------------------+\n","\n"]}],"source":["# Set the task on T5\n","t5.setTask('translate English to German: ')\n","\n","# Build pipeline with T5\n","pipe_components = [documentAssembler,t5]\n","pipeline = Pipeline().setStages( pipe_components)\n","\n","# define Data, add additional tags between sentences\n","sentences = [['''I like sausage and Tea for breakfast with potatoes'''],]\n","\n","df = spark.createDataFrame(sentences).toDF(\"text\")\n","\n","#Predict on text data with T5\n","model = pipeline.fit(df)\n","annotated_df = model.transform(df)\n","annotated_df.select(['text','t5.result']).show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"kZQe18dQo3CH"},"source":["# Task 17 [WMT2 Translate English to French](https://arxiv.org/abs/1706.03762)\n","For translation tasks use the `marian` model\n","## How to configure T5 task parameter for WMT Translate English to French\n","`.setTask('translate English to French:)`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12084,"status":"ok","timestamp":1680628306091,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":240},"id":"SaSJkuK8o1sL","outputId":"efe2c93c-2bd4-41c3-87e2-5097e5261565","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------------------------------+----------------------------------------------------------------------------+\n","|text                                              |result                                                                      |\n","+--------------------------------------------------+----------------------------------------------------------------------------+\n","|I like sausage and Tea for breakfast with potatoes|[J'aime les saucisses et le thé au petit déjeuner avec les pommes de terre.]|\n","+--------------------------------------------------+----------------------------------------------------------------------------+\n","\n"]}],"source":["# Set the task on T5\n","t5.setTask('translate English to French: ')\n","\n","# Build pipeline with T5\n","pipe_components = [documentAssembler,t5]\n","pipeline = Pipeline().setStages( pipe_components)\n","\n","# define Data, add additional tags between sentences\n","sentences = [['''I like sausage and Tea for breakfast with potatoes''']]\n","\n","df = spark.createDataFrame(sentences).toDF(\"text\")\n","\n","#Predict on text data with T5\n","model = pipeline.fit(df)\n","annotated_df = model.transform(df)\n","annotated_df.select(['text','t5.result']).show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"_nozgTDFo7cK"},"source":["# 18 [WMT3 - Translate English to Romanian](https://arxiv.org/abs/1706.03762)\n","For translation tasks use the `marian` model\n","## How to configure T5 task parameter for English to Romanian\n","`.setTask('translate English to Romanian:)`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6853,"status":"ok","timestamp":1680628312916,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":240},"id":"_879n2Ljo5tc","outputId":"8af16612-a3b2-4526-bae8-1d64fce9bfe6","vscode":{"languageId":"python"}},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------------------------------+--------------------------------------------------------+\n","|text                                              |result                                                  |\n","+--------------------------------------------------+--------------------------------------------------------+\n","|I like sausage and Tea for breakfast with potatoes|[Mi-ar plăcea cârnaţi şi ceai la micul dejun cu cartofi]|\n","+--------------------------------------------------+--------------------------------------------------------+\n","\n"]}],"source":["# Set the task on T5\n","t5.setTask('translate English to Romanian: ')\n","\n","# Build pipeline with T5\n","pipe_components = [documentAssembler,t5]\n","pipeline = Pipeline().setStages( pipe_components)\n","\n","# define Data, add additional tags between sentences\n","sentences = [['''I like sausage and Tea for breakfast with potatoes''']]\n","\n","df = spark.createDataFrame(sentences).toDF(\"text\")\n","\n","#Predict on text data with T5\n","model = pipeline.fit(df)\n","annotated_df = model.transform(df)\n","annotated_df.select(['text','t5.result']).show(truncate=False)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}