{"cells":[{"cell_type":"markdown","metadata":{"id":"Bc3hWxLTwYNi"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"]},{"cell_type":"markdown","metadata":{"id":"qTZhT1L1waFH"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/open-source-nlp/23.1.Native_GGUF_Models_in_SparkNLP.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"EMymqyPmidpT"},"source":["This notebook demonstrates how to integrate and use **GGUF models** within the Spark NLP ecosystem."]},{"cell_type":"markdown","metadata":{"id":"iXNQya-jjBzR"},"source":["## Colab Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OKPwJq5OjBzR"},"outputs":[],"source":["!wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":260},"executionInfo":{"elapsed":132333,"status":"ok","timestamp":1762341364205,"user":{"displayName":"Muhammad Abdullah","userId":"08204441800691646310"},"user_tz":-300},"id":"tpUzV-YWjBzR","outputId":"2954fdc1-cdd5-4b34-90f8-25a80df3324f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Spark NLP version 6.2.0\n","Apache Spark version: 3.4.4\n"]},{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7a8155a74ad0>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://93c27e53dc40:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.4.4</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Spark NLP</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":3}],"source":["import time\n","import pandas as pd\n","import numpy as np\n","\n","import sparknlp\n","from sparknlp.base import *\n","from sparknlp.annotator import *\n","from pyspark.ml import Pipeline\n","\n","from pyspark.sql import functions as F\n","\n","spark = sparknlp.start(gpu=True)\n","\n","print(\"Spark NLP version\", sparknlp.version())\n","print(\"Apache Spark version:\", spark.version)\n","\n","spark"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64,"status":"ok","timestamp":1762326466678,"user":{"displayName":"Muhammad Abdullah","userId":"08204441800691646310"},"user_tz":-300},"id":"H3EQMBCAWoV3","outputId":"778f14a4-a47c-4e4e-aaea-7bdbe6d0b99f"},"outputs":[{"output_type":"stream","name":"stdout","text":["com.johnsnowlabs.nlp_jsl-llamacpp-gpu-1.0.2-compat-rc1.jar\n","com.johnsnowlabs.nlp_jsl-openvino-cpu_2.12-0.2.0.jar\n","com.johnsnowlabs.nlp_spark-nlp-gpu_2.12-6.2.0.jar\n","com.johnsnowlabs.nlp_tensorflow-gpu_2.12-0.4.4.jar\n"]}],"source":["!ls /root/.ivy2/jars | grep com.johnsnowlabs"]},{"cell_type":"markdown","metadata":{"id":"obAo8NCY9uQj"},"source":["setting `sparknlp.start(gpu=True)` download all the necessary gpu jars\n","\n","now we can levergae **GGUF** models better"]},{"cell_type":"markdown","source":["lets fetch some files for later examples from: https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/open-source-nlp/data"],"metadata":{"id":"O65dLQM-3-wV"}},{"cell_type":"code","source":["%%bash\n","set -e\n","git clone -q --no-checkout https://github.com/JohnSnowLabs/spark-nlp-workshop.git tmp\n","cd tmp\n","git sparse-checkout set reader2doc reader2table reader2image\n","git checkout -q\n","mkdir -p /content/files\n","mv reader2doc reader2table reader2image /content/files/\n","cd ..\n","rm -rf tmp"],"metadata":{"id":"7Z14FVr-379a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dHbFX4iZH4KN"},"source":["## GGUF: A Compact Binary Format for Efficient Model Inference\n","\n","[**GGUF**](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) is a modern binary file format developed within the [**GGML**](https://github.com/ggml-org/ggml) ecosystem, the same low-level runtime that powers tools such as [*llama.cpp*](https://github.com/ggerganov/llama.cpp). It was designed to make **loading and executing large models fast and lightweight**, especially on systems that rely on CPUs instead of GPUs.\n","\n","‎\n","\n","<p align=\"center\">\n","  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/gguf-spec.png\" alt=\"gguf-spec\" width=\"1000\">\n","</p>\n","\n","<p align=\"center\">\n","  <em>As we can see in this graph, unlike tensor-only file formats like safetensors GGUF encodes both the tensors and a standardized set of metadata. Diagram by <a href=\"https://github.com/mishig25\">@mishig25/</a> (GGUF v3)\n","  </em>\n","</p>\n","\n","‎\n","‎\n","\n","Unlike conventional model files (for example, PyTorch `.bin` or TensorFlow `.pb`), which mostly store raw tensors, **GGUF encodes both the tensor data and rich metadata** that describes the model’s structure, tokenizer, architecture, and quantization details.\n","This self-contained structure allows inference engines to start execution immediately without rebuilding computational graphs or importing heavy Python frameworks.\n"]},{"cell_type":"markdown","metadata":{"id":"z9ueKRyBL-lT"},"source":["### How GGUF Makes Models Smaller and Faster\n","\n","**GGUF** works by using a method called **quantization**, which means storing model weights (the numbers inside the model) with fewer bits.  \n","This reduces the model’s **size** and makes it **run faster**, especially on CPUs.\n","\n","For example, a normal 16-bit or 32-bit model might be **tens of gigabytes**, but a quantized GGUF model can be **under 5 GB** with almost the same accuracy.\n","\n","### The Main Types of Quantization in GGUF\n","\n","‎\n","\n","| Type | Bit Precision | What It Means |\n","|------|----------------|---------------|\n","| **F32 / F16 / BF16** | 32 or 16 bits | The original high-precision formats. Best for fine-tuning or maximum accuracy. |\n","| **Q8_K – Q2_K** | 8 to 2 bits | “K-Quant” models. These use different bit levels in different layers to save space while keeping accuracy. |\n","| **IQ4 / IQ3 / IQ2 / IQ1** | 4 to 1 bits | “Importance Quantization” models. These use smarter compression to keep the most important weights more precise. |\n","\n","‎\n","‎\n","\n","The **K-Quant** models (such as `Q4_K` and `Q2_K`) use **mixed precision across layers**, meaning GGUF adjusts how many bits each layer uses depending on its sensitivity to information loss. Important layers retain higher precision while less critical ones use fewer bits. On average, `Q4_K` models use about **4.5 bits per weight** and `Q2_K` around **2.6 bits**, allowing them to stay accurate while using far less memory.\n","\n","Building on this, the **IQ (Importance Quantization)** family identifies which weights most influence the model’s output and assigns them higher precision, while compressing less important ones more aggressively. Using **importance matrices** and **scaling factors**, IQ quantization achieves extreme compression sometimes as low as **1.5 bits per weight** with minimal performance loss. Common IQ variants include:\n","- **IQ4_XS** → ≈4.25 bpw (near full quality)\n","- **IQ3_S** → ≈3.44 bpw (balanced)\n","- **IQ2_S** → ≈2.5 bpw (lightweight)\n","- **IQ1_M** → ≈1.75 bpw (extremely compact but still suitable for embeddings and chat tasks)\n"]},{"cell_type":"markdown","metadata":{"id":"mtjLoY0lfwpF"},"source":["# The GGUF Annotator Ecosystem in SparkNLP"]},{"cell_type":"markdown","metadata":{"id":"FJQQLJ8sfg4W"},"source":["Spark NLP provides **native support for these GGUF models**, which you can leverage through the following annotators:\n","\n","- `AutoGGUFEmbeddings` → Generate dense sentence embeddings for RAG, clustering, or semantic search tasks.\n","- `AutoGGUFModel` → Perform chat, Q&A, and text completion using a compact, high-performance language model.\n","- `AutoGGUFVisionModel` → Extract visual features, perform image-to-text processing, and handle multimodal tasks.\n","- `AutoGGUFReranker` → Score candidate texts or documents for relevance, improving ranking and retrieval workflows.\n","\n","These annotators integrate seamlessly into Spark NLP pipelines, allowing you to combine embeddings, generation, ranking, and vision models efficiently in distributed workflows.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7nxJMgCHnOWc"},"source":["> All models for these annotators are available on our [Model Hub](https://sparknlp.org/models).\n"]},{"cell_type":"markdown","metadata":{"id":"A6rEfl7Oj2Pb"},"source":["## AutoGGUFEmbeddings"]},{"cell_type":"markdown","source":["This is an annotator that generates dense vector embeddings from text using quantized GGUF models. It converts sentences, paragraphs, or documents into numerical representations that capture semantic meaning, making them ideal for similarity search, clustering, or retrieval tasks.\n","\n","You can basically use any of the state-of-the-art embedding models you’ve been working with in PyTorch, ONNX, or OpenVINO. But now in GGUF format, directly inside Spark NLP."],"metadata":{"id":"viVx8t0f7-Ye"}},{"cell_type":"markdown","metadata":{"id":"kojuH-FEsjym"},"source":["Docs: https://sparknlp.org/docs/en/annotators#autoggufembeddings\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sknQC31kjXsR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762325262678,"user_tz":-300,"elapsed":52713,"user":{"displayName":"Muhammad Abdullah","userId":"01603058919468487503"}},"outputId":"b156dd62-432d-4612-a801-3f3aa1478ebc"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------------------------------------------------------------------+\n","|                                                                      embeddings|\n","+--------------------------------------------------------------------------------+\n","|[[-0.0123171685, -0.038418945, -0.005815847, 0.019013459, 0.019893406, -0.019...|\n","+--------------------------------------------------------------------------------+\n","\n"]}],"source":["document = (\n","    DocumentAssembler()\n","    .setInputCol(\"text\")\n","    .setOutputCol(\"document\")\n",")\n","\n","autoGGUFEmbeddings = (\n","    AutoGGUFEmbeddings.pretrained(\"Qwen3_Embedding_0.6B_Q8_0_gguf\")\n","    .setInputCols([\"document\"])\n","    .setOutputCol(\"embeddings\")\n",")\n","\n","pipeline = Pipeline().setStages([document, autoGGUFEmbeddings])\n","\n","data = spark.createDataFrame([[\"The moons of Jupiter are 77 in total, with 79 confirmed natural satellites and 2 man-made ones.\"]]).toDF(\"text\")\n","result = pipeline.fit(data).transform(data)\n","\n","result.select(\"embeddings.embeddings\").show(truncate=80)\n"]},{"cell_type":"markdown","metadata":{"id":"VsU566Akqrwk"},"source":["### **Exploring Key Parameters for `AutoGGUFEmbeddings`**\n","\n","<!-- ‎\n","\n","| Parameter       | Description                                                    | Typical Use / Notes                                                                 |\n","| --------------- | -------------------------------------------------------------- | ----------------------------------------------------------------------------------- |\n","| `inputCols`     | Columns containing input text/annotations                      | Set this to the previous annotation column, usually `\"document\"`      |\n","| `outputCol`     | Column to store embeddings                                     | Default is `\"embeddings\"`. Change if needed to avoid overwriting.                   |\n","| `batchSize`     | Size of each batch during processing                           | Controls memory vs speed tradeoff; larger batches use more RAM/VRAM but are faster. |\n","| `nUbatch`       | Physical batch size for prompt processing                      | Usually >=32 for optimal BLAS usage; often matches `batchSize`.                     |\n","| `poolingType`   | How token embeddings are combined into a single vector         | Options: `\"mean\"`, `\"max\"`, `\"cls\"` (default uses model’s setting).                 |\n","| `nCtx`          | Size of the prompt context                                     | Typically set if you expect long inputs; defaults to model max context.             |\n","| `lazyAnnotator` | Whether embeddings are generated lazily in recursive pipelines | Useful if you don’t want immediate computation in large pipelines.                  |\n","| `useMmap`       | Whether to memory-map model file for faster load               | Can speed up load; may increase page swaps if system memory is limited.             |\n","| `useMlock`      | Keep model fully in RAM                                        | Prevents swapping; useful on servers with enough RAM to avoid I/O bottlenecks.      |\n","\n","\n","‎ -->\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IwA6TFftxSnV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762325262678,"user_tz":-300,"elapsed":11,"user":{"displayName":"Muhammad Abdullah","userId":"01603058919468487503"}},"outputId":"4570f01a-6634-413c-c4e1-b419bfd95ed9"},"outputs":[{"output_type":"stream","name":"stdout","text":["batchSize: Size of every batch (undefined)\n","defragmentationThreshold: Set the KV cache defragmentation threshold (undefined)\n","flashAttention: Whether to enable Flash Attention (undefined)\n","gpuSplitMode: Set how to split the model across GPUs (undefined)\n","grpAttnN: Set the group-attention factor (undefined)\n","grpAttnW: Set the group-attention width (undefined)\n","inputCols: previous annotations columns, if renamed (undefined)\n","lazyAnnotator: Whether this AnnotatorModel acts as lazy in RecursivePipelines (default: False)\n","mainGpu: Set the main GPU that is used for scratch and small tensors. (undefined)\n","nBatch: Set the logical batch size for prompt processing (must be >=32 to use BLAS) (default: 512)\n","nChunks: Set the maximal number of chunks to process (undefined)\n","nCtx: Set the size of the prompt context (default: 4096)\n","nGpuLayers: Set the number of layers to store in VRAM (-1 - use default) (default: 99)\n","nSequences: Set the number of sequences to decode (undefined)\n","nThreads: Set the number of threads to use during generation (undefined)\n","nThreadsBatch: Set the number of threads to use during batch and prompt processing (undefined)\n","nUbatch: Set the physical batch size for prompt processing (must be >=32 to use BLAS) (undefined)\n","noKvOffload: Whether to disable KV offload (undefined)\n","numaStrategy: Set optimization strategies that help on some NUMA systems (if available) (undefined)\n","outputCol: output annotation column. can be left default. (undefined)\n","poolingType: Set the pooling type for embeddings, use model default if unspecified (default: MEAN)\n","ropeFreqBase: Set the RoPE base frequency, used by NTK-aware scaling (undefined)\n","ropeFreqScale: Set the RoPE frequency scaling factor, expands context by a factor of 1/N (undefined)\n","ropeScalingType: Set the RoPE frequency scaling method, defaults to linear unless specified by the model (undefined)\n","tensorSplit: Set how split tensors should be distributed across GPUs (undefined)\n","useMlock: Whether to force the system to keep model in RAM rather than swapping or compressing (undefined)\n","useMmap: Whether to use memory-map model (faster load but may increase pageouts if not using mlock) (undefined)\n","yarnAttnFactor: Set the YaRN scale sqrt(t) or attention magnitude (undefined)\n","yarnBetaFast: Set the YaRN low correction dim or beta (undefined)\n","yarnBetaSlow: Set the YaRN high correction dim or alpha (undefined)\n","yarnExtFactor: Set the YaRN extrapolation mix factor (undefined)\n","yarnOrigCtx: Set the YaRN original context size of model (undefined)\n"]}],"source":["print(AutoGGUFEmbeddings().explainParams())"]},{"cell_type":"markdown","metadata":{"id":"1WD_4XUCzl_r"},"source":["Understanding `poolingType`\n","\n","When a model encodes a sequence of tokens, it produces **one embedding vector per token**.  \n","The `poolingType` parameter controls **how these token embeddings are combined into a single sentence embedding**.\n","\n","‎\n","\n","Main types of Pooling\n","\n","| Pooling Type | How It Works                           | Effect on Resulting Embedding                                                                                       |\n","|--------------|--------------------------------------|--------------------------------------------------------------------------------------------------------------------|\n","| `mean`       | Averages all token embeddings element-wise | Captures the **overall semantic content** of the sentence. Produces smooth and balanced embeddings, ideal for general similarity tasks. Less sensitive to individual token variations. |\n","| `max`        | Takes the maximum value across tokens element-wise | Emphasizes the **strongest features** from the sentence. Highlights key tokens and dominant signals, but may overrepresent rare or extreme token values, possibly exaggerating differences.\n","| `cls`        | Uses the embedding of the `[CLS]` token (or first token) | Represents a **learned summary** of the entire input based on the model’s pretraining. Effective for classification tasks but might lose fine-grained information from other tokens and be less stable in clustering.\n","\n","‎\n","\n","Cause and Effect Examples\n","\n","- **`poolingType = max`**  \n","  Each embedding dimension is determined by the strongest token feature, causing strong words to dominate the vector. This can exaggerate differences in similarity comparisons between otherwise similar sentences.\n","\n","- **`poolingType = cls`**  \n","  Utilizes only the `[CLS]` token embedding as a summary representation, reflecting the model’s learned \"gist\" of the input. Often effective for classification but may lead to coarser embeddings and less stability for semantic clustering.\n","\n","- **`poolingType = mean`**  \n","  Generates embeddings by averaging semantic content across all tokens, resulting in smoother similarity scores and more balanced clustering. This approach tends to reflect the sentence's overall meaning best.\n","\n","‎\n","\n","Why `poolingType` Matters\n","\n","- **Semantic similarity tasks:** `mean` pooling typically produces the most intuitive and robust embeddings.  \n","- **Highlighting key tokens or features:** `max` pooling emphasizes important and dominant tokens.  \n","- **Model-specific summary tasks (e.g., classification):** `cls` pooling works best, especially when aligned with the model’s pretraining objectives.  \n","- **Sequence-focused tasks:** `last` pooling can be useful when sequence-ending tokens carry crucial information.  \n","- **Token-level tasks:** `none` pooling keeps token embeddings separate.\n","\n","‎\n","\n","Choosing the right pooling strategy impacts how sentence embeddings capture meaning and perform in downstream tasks and should be aligned with your specific NLP goals."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2586,"status":"ok","timestamp":1762325265263,"user":{"displayName":"Muhammad Abdullah","userId":"01603058919468487503"},"user_tz":-300},"id":"9dQuXrRVrHkg","outputId":"775f2e69-0032-4255-9a69-0f9da965a923"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------------------------------------------------------------------+\n","|                                                                      embeddings|\n","+--------------------------------------------------------------------------------+\n","|[[-0.012416776, -0.038989745, -0.0057717287, 0.017973736, 0.020065758, -0.019...|\n","+--------------------------------------------------------------------------------+\n","\n"]}],"source":["autoGGUFEmbeddings.setPoolingType(\"MEAN\")\n","\n","result = pipeline.fit(data).transform(data)\n","result.select(\"embeddings.embeddings\").show(truncate=80)\n"]},{"cell_type":"markdown","metadata":{"id":"mo1F4MzKsdHV"},"source":["Understanding `batchSize`\n","\n","- **`batchSize`**  \n","  Determines how many input samples (e.g., sentences or documents) are processed simultaneously in one inference pass. Larger batch sizes improve throughput by leveraging parallelism, making embedding generation faster. However, this requires more RAM or VRAM to hold the data and intermediate computations. If memory is limited, large batch sizes might cause out-of-memory errors or system swapping, which significantly slows processing. Smaller batch sizes use less memory but increase total processing time.\n","\n","> Start with a batch size that fits your memory and experiment to find the best trade-off between speed and stability."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10276,"status":"ok","timestamp":1762331626443,"user":{"displayName":"Muhammad Abdullah","userId":"08204441800691646310"},"user_tz":-300},"id":"3HCxmHn-WFa2","outputId":"7f2f421d-7990-41ae-b39f-93a88a3514a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------------------------------------------------------------------+\n","|                                                                            text|\n","+--------------------------------------------------------------------------------+\n","| Short sellers, Wall Street's dwindling band of ultra cynics, are seeing gree...|\n","| Private investment firm Carlyle Group, which has a reputation for making wel...|\n","| Soaring crude prices plus worries about the economy and the outlook for earn...|\n","| Authorities have halted oil export flows from the main pipeline in southern ...|\n","| Tearaway world oil prices, toppling records and straining wallets, present a...|\n","+--------------------------------------------------------------------------------+\n","only showing top 5 rows\n","\n"]}],"source":["!wget -q -O news_category_train.csv https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/news_Category/news_category_train.csv\n","\n","df = pd.read_csv(\"news_category_train.csv\", on_bad_lines='skip', header=0, usecols=[1], names=[\"text\"]).iloc[:1000]\n","dataframe = spark.createDataFrame(df).cache()\n","dataframe.show(5, truncate=80)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":150116,"status":"ok","timestamp":1762325418763,"user":{"displayName":"Muhammad Abdullah","userId":"01603058919468487503"},"user_tz":-300},"id":"VlPizFTLZ1j7","outputId":"b1a6db1b-8711-4185-c107-3a36b937c21f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Batch Size 1   | Total:  30.86s | Per seq: 0.0309s\n","Batch Size 2   | Total:  21.76s | Per seq: 0.0218s\n","Batch Size 4   | Total:  14.52s | Per seq: 0.0145s\n","Batch Size 8   | Total:  14.56s | Per seq: 0.0146s\n","Batch Size 16  | Total:  15.90s | Per seq: 0.0159s\n","Batch Size 32  | Total:  15.44s | Per seq: 0.0154s\n","Batch Size 64  | Total:  16.01s | Per seq: 0.0160s\n","Batch Size 128 | Total:  17.79s | Per seq: 0.0178s\n"]}],"source":["def bench_embeddings(pipeline, embeddings, df, batch_sizes):\n","    results = []\n","    for b in batch_sizes:\n","        embeddings.setBatchSize(b).setNUbatch(32)\n","        data = df.select(\"text\")\n","        start = time.time()\n","        pipeline.fit(data).transform(data).select(\"embeddings\").collect()\n","        end = time.time()\n","        total = end - start\n","        per_seq = total / data.count()\n","        print(f\"Batch Size {b:<3} | Total: {total:6.2f}s | Per seq: {per_seq:.4f}s\")\n","        results.append((b, total, per_seq))\n","\n","bench_embeddings(pipeline, autoGGUFEmbeddings, dataframe, [1, 2, 4, 8, 16, 32, 64, 128])\n"]},{"cell_type":"markdown","metadata":{"id":"75RTn7fio4Ss"},"source":["Batch Size 4-8 seems to be the sweet spot for our data"]},{"cell_type":"markdown","source":["check out [HasLlamaCppInferenceProperties](https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/main/scala/com/johnsnowlabs/nlp/HasLlamaCppInferenceProperties.scala), [HasLlamaCppModelProperties](https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/main/scala/com/johnsnowlabs/nlp/HasLlamaCppModelProperties.scalar) or refer to the llama.cpp documentation of [server.cpp](https://github.com/ggerganov/llama.cpp/tree/7d5e8777ae1d21af99d4f95be10db4870720da91/examples/server) for more information!"],"metadata":{"id":"ZC-QfoFr-MJr"}},{"cell_type":"markdown","metadata":{"id":"wsc1A76OrY6t"},"source":["## AutoGGUFModel"]},{"cell_type":"markdown","source":["This is an annotator that performs text generation and instruction following using lightweight GGUF-format language models.\n","\n","Docs: https://sparknlp.org/docs/en/annotators#autoggufmodel"],"metadata":{"id":"hFOh8osj8YXP"}},{"cell_type":"markdown","source":["Basic document summarization using `AutoGGUFModel`"],"metadata":{"id":"Gryb_-Sad5-L"}},{"cell_type":"code","source":["from sparknlp.annotator import AutoGGUFModel\n","from sparknlp.reader.reader2doc import Reader2Doc\n","\n","reader2doc = Reader2Doc().setContentPath(\"/content/files/reader2doc\")\n","\n","auto_gguf_model = (\n","    AutoGGUFModel.pretrained(\"phi_4_mini_instruct_bf16_gguf\", \"en\")\n","    .setInputCols([\"document\"])\n","    .setOutputCol(\"completions\")\n","    .setSystemPrompt(\"You are a helpful assistant. Read the text below and write a clear, concise summary capturing the key ideas, facts, and tone.\")\n","    .setCachePrompt(True)\n","    .setNPredict(200)\n",")\n","\n","pipeline = Pipeline().setStages([\n","    reader2doc,\n","    auto_gguf_model\n","])\n","\n","empty_df = spark.createDataFrame([], \"string\").toDF(\"text\")\n","\n","model = pipeline.fit(empty_df)\n","result = model.transform(empty_df)\n"],"metadata":{"id":"GnYWgI9ER4x9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761324741247,"user_tz":-300,"elapsed":657363,"user":{"displayName":"Abdullah Mubeen","userId":"17886490017623663394"}},"outputId":"124702a4-a6c9-45e0-b221-9635a09a2498"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["phi_4_mini_instruct_bf16_gguf download started this may take some time.\n","Approximate size to download 5.7 GB\n","[OK!]\n"]}]},{"cell_type":"code","source":["result.select(\"fileName\", \"completions.result\").show(truncate=False)"],"metadata":{"id":"GOCUaDbXcTKL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761324813345,"user_tz":-300,"elapsed":72099,"user":{"displayName":"Abdullah Mubeen","userId":"17886490017623663394"}},"outputId":"43196c15-2051-47a2-99d9-546e4340710a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|fileName                             |result                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n","+-------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|SparkNLP_New_Notebooks_Proposals.xlsx|[Notebook Topic: Exploring the Impact of Climate Change on Global Biodiversity\\nAnnotators/Transformers: Dr. Jane Goodall, Dr. David Attenborough, Dr. E.O. Wilson\\nRelated Existing Notebook: \"Climate Change and Biodiversity: A Comprehensive Analysis\" by Dr. Sarah Johnson\\nSummary: This notebook focuses on the effects of climate change on global biodiversity, with insights from renowned experts Dr. Jane Goodall, Dr. David Attenborough, and Dr. E.O. Wilson. It builds upon Dr. Sarah Johnson's comprehensive analysis, aiming to deepen understanding and explore potential solutions to mitigate these impacts.]                                                      |\n","|financial_impact_analysis.xlsx       |[The SparkNLPReader package provides native file readers for various file formats including .pdf, .xlsx, .pptx, .xml, and .txt. It allows direct ingestion of these files into Spark pipelines without the need for external tools. The package includes readers such as SparkNLPReader, Partition, PartitionTransformer, PDFReader, ExcelReader, PowerPointReader, TextReader, and XMLReader. A demonstration of this process can be found in the \"Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb\" notebook.]                                                                                                                                                         |\n","|dataset_inventory.xls                |[The dataset has an unspecified number of rows and features, with a single target variable. The comment section is available for additional information or notes.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n","|data_centric_ai_whitepaper.pdf       |[The text discusses the importance of data-centric AI in enhancing real-world reliability. It emphasizes the need for robust, high-quality data to train AI systems effectively. The tone is informative and persuasive, advocating for the adoption of data-centric approaches in AI development.]                                                                                                                                                                                                                                                                                                                                                                                    |\n","|predictive_analytics_future.pptx     |[Predictive analytics is set to revolutionize various industries by harnessing the power of data, machine learning, and artificial intelligence. The technology is poised to provide unprecedented insights, allowing businesses to anticipate trends, improve decision-making, and gain a competitive edge. As the field continues to evolve, it will undoubtedly shape the future of business and technology.]                                                                                                                                                                                                                                                                       |\n","|enterprise_ai_strategy.ppt           |[The text discusses the challenges and strategies involved in scaling artificial intelligence (AI) at an enterprise level. It highlights the need for large datasets, computational power, and skilled personnel. The tone is informative and analytical, providing insights into the complexities of implementing AI solutions in a large-scale business environment.]                                                                                                                                                                                                                                                                                                                |\n","|predictive_maintenance_report.doc    |[A Predictive Maintenance System is an advanced technique that uses data analysis and machine learning to predict when equipment maintenance is required. This approach helps in preventing unexpected equipment failures, reducing downtime, and saving costs. The system analyzes historical and real-time data to identify patterns and anomalies that could indicate potential issues. By predicting maintenance needs, organizations can schedule repairs during non-peak hours, thereby increasing efficiency and productivity. The tone of the summary is informative and straightforward, focusing on the key benefits and functionality of the Predictive Maintenance System.]|\n","|labeling_insights_note.txt           |[The author suggests that predictive errors in models are often due to inconsistent data labeling rather than the model itself. They recommend that before tuning hyperparameters, the definitions used for data labeling should be refined.]                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n","|openai_reply_to_nvidia.msg           |[The sender is seeking a collaboration to develop a next-generation artificial intelligence. The tone is professional and forward-thinking.]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n","|ai_logistics_executive_summary.docx  |[Artificial Intelligence (AI) is revolutionizing the logistics industry by enhancing efficiency, accuracy, and decision-making. Key applications include predictive analytics for demand forecasting, autonomous vehicles for transportation, and intelligent robotics for warehouse management. These innovations result in reduced costs, improved customer satisfaction, and a competitive edge for businesses embracing AI in their logistics operations. The tone of the summary is informative and optimistic about the potential of AI in logistics.]                                                                                                                           |\n","|model_performance_dashboard.html     |[The Data Science Metrics Dashboard is a tool designed to track and visualize key performance indicators (KPIs) in data science projects. It provides real-time insights into various metrics such as model accuracy, data quality, and processing speed, enabling data scientists to monitor their progress and make informed decisions. The dashboard's user-friendly interface allows for easy interpretation of complex data, fostering a more efficient and effective data science workflow. The tone of the text is informative and professional, focusing on the practical benefits of the dashboard.]                                                                          |\n","|nvidia_to_openai_ai_collaboration.eml|[The text refers to the emergence of advanced AI technologies that are designed to work collaboratively with humans. These next-generation AI systems are expected to enhance productivity, foster innovation, and streamline complex tasks. The tone suggests a positive outlook on the potential benefits of human-AI partnerships.]                                                                                                                                                                                                                                                                                                                                                 |\n","+-------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["### **Exploring Key Parameters for `AutoGGUFModel`**\n","\n","| **Parameter**         | **Description**                                             | **Typical Use / Notes**                                              |\n","| --------------------- | ----------------------------------------------------------- | -------------------------------------------------------------------- |\n","| **systemPrompt**      | Sets a global instruction or persona for the model          | Controls tone, reasoning depth, and response style                   |\n","| **cachePrompt**       | Enables caching of previous prompts for conversational flow | Maintains multi-turn chat context across generations                 |\n","| **chatTemplate**      | Defines the message formatting for chat-style interactions  | Applied automatically for most chat-tuned GGUF models                |\n","| **useChatTemplate**   | Toggles use of the chat template                            | Set `True` for multi-turn or role-based conversations                |\n","| **modelAlias**        | Assigns a shorthand name to the model                       | Useful for logging and managing multiple models in pipelines         |\n","| **nCtx**              | Maximum token length per input sequence                     | Increase to handle longer inputs; larger values require more memory  |\n","| **temperature**       | Controls randomness in generation                           | Lower = deterministic; higher (0.7–1.0) = more creative outputs      |\n","| **topP**              | Nucleus sampling probability threshold                      | Typical range: 0.8–0.95; balances diversity and coherence            |\n","| **topK**              | Limits sampling to top-K likely tokens                      | Common range: 40–100; influences variability and determinism         |\n","| **repeatPenalty**     | Penalizes repeated phrases                                  | Values around 1.1–1.2 reduce looping or redundant output             |\n","| **presencePenalty**   | Discourages reuse of tokens                                 | Encourages introducing new concepts or vocabulary                    |\n","| **frequencyPenalty**  | Penalizes frequent token repetition                         | Similar to presence penalty; improves output diversity               |\n","| **stopStrings**       | Defines custom stop tokens or phrases                       | Example: `[\"User:\", \"###\"]` to stop generation at a marker           |\n","| **nPredict**          | Sets the maximum number of tokens to generate               | Controls output length; `-1` allows full auto-completion             |\n","| **grammar**           | Enforces a formal grammar on the output                     | Ideal for structured outputs (e.g., JSON, code, categorical answers) |\n","| **removeThinkingTag** | Removes internal reasoning tags like `<think>`              | Keeps final outputs clean and display-ready                          |\n","| **batchSize**         | Number of sequences processed concurrently                  | Balances throughput and memory usage during inference                |\n","| **flashAttention**    | Enables FlashAttention optimization                         | Improves speed and efficiency on GPU-based inference                 |\n"],"metadata":{"id":"9tLaLAUZQBW9"}},{"cell_type":"markdown","source":["covering some common parameters"],"metadata":{"id":"BtkaKRkiHtJr"}},{"cell_type":"code","source":["sys_prompt = \"\"\"You are Qwen, a helpful, intelligent, and precise AI assistant.\n","You provide responses that are clear, accurate, and well reasoned.\n","\n","Behavior guidelines:\n","- Responses are concise but complete, explaining reasoning when relevant.\n","- Examples or analogies may be used to clarify complex ideas.\n","- Ambiguities are acknowledged with possible interpretations and balanced handling.\n","- Avoid unsupported assumptions or speculation; prioritize factual accuracy.\n","- Responses should be formatted neatly, using bullet points or code blocks when helpful.\n","\"\"\"\n","\n","document_assembler = (\n","    DocumentAssembler()\n","    .setInputCol(\"text\")\n","    .setOutputCol(\"document\")\n",")\n","\n","autoGGUFModel = (\n","    AutoGGUFModel.pretrained(\"qwen3_4b_bf16_gguf\", \"en\")\n","    .setInputCols([\"document\"])\n","    .setOutputCol(\"completions\")\n","    .setSystemPrompt(sys_prompt)\n","    .setCachePrompt(True)\n","    # .setChatTemplate(\"\") # you can get the chat template for this model from: https://huggingface.co/Qwen/Qwen3-4B?chat_template=default it's already applied by deafult\n","    # .setUseChatTemplate(True)\n","    .setNPredict(-1)\n","    .setNCtx(32768) # this model has an context Length (nctx) of 32,768 natively and 131,072 tokens with YaRN.\n","    .setBatchSize(4)\n","    .setNUbatch(32)\n",")\n","\n","pipeline = Pipeline().setStages([document_assembler, autoGGUFModel])\n","\n","# you can disable thinking for qwen models with \"/no_think\" prefixes in your prompts or system prompt on simpler tasks for faster responses\n","data = spark.createDataFrame([[\n","    \"Explain why gradient clipping is used during neural network training, and give a short example.\"\n","]]).toDF(\"text\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ghWtS2-hkZ6w","executionInfo":{"status":"ok","timestamp":1762330606792,"user_tz":-300,"elapsed":3177,"user":{"displayName":"Muhammad Abdullah","userId":"08204441800691646310"}},"outputId":"bc841ac6-1c56-4609-d139-8fb76fa0bf1f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["qwen3_4b_bf16_gguf download started this may take some time.\n","Approximate size to download 6 GB\n","[OK!]\n"]}]},{"cell_type":"code","source":["result = pipeline.fit(data).transform(data)\n","print(result.select(\"completions.result\").first().result[0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aU4vCZID-aTa","executionInfo":{"status":"ok","timestamp":1762329387359,"user_tz":-300,"elapsed":132597,"user":{"displayName":"Muhammad Abdullah","userId":"08204441800691646310"}},"outputId":"d5c72666-f066-4d67-b756-84265ea70535"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<think>\n","Okay, the user is asking why gradient clipping is used in neural network training and wants a short example. Let me start by recalling what gradient clipping is. From what I remember, gradient clipping is a technique used to prevent the gradients from becoming too large during backpropagation. Large gradients can cause problems like vanishing or exploding gradients, which can make training unstable or slow.\n","\n","So the main reason for using gradient clipping is to maintain the stability of the training process. When gradients are too big, they can cause the weights to update by huge amounts, leading to divergence. Clipping limits the gradient's magnitude, so the updates are more controlled.\n","\n","Now, I need to explain why this is important. Maybe mention that without clipping, the model might not converge or might oscillate. Also, it helps in preventing the loss from exploding, which can make the training process difficult.\n","\n","For the example part, let's think of a scenario where gradients become too large. Suppose during backpropagation, the gradient of a certain parameter reaches a value like 1000. Without clipping, this would cause the weight update to be massive, which could push the model's parameters far from their optimal values. By clipping the gradient to a maximum value, say 1.0, the update is scaled down, ensuring more stable learning.\n","\n","Wait, maybe I should use a specific example with numbers. Let's say the gradient is calculated as 1000, and the clipping threshold is 1.0. Then the gradient is divided by 1000, making it 1.0. This prevents the weights from updating too much. The example should show how clipping affects the gradient magnitude and the subsequent weight update.\n","\n","I should also mention that this is particularly useful in recurrent neural networks (RNNs) where gradients can accumulate over time, leading to exploding gradients. But maybe the user doesn't need that level of detail. Keep it general.\n","\n","So the answer should first explain the purpose of gradient clipping, then give a simple example with numbers to illustrate the process. Make sure it's clear and concise, avoiding jargon where possible.\n","</think>\n","\n","Gradient clipping is used during neural network training to prevent the gradients from becoming excessively large (exploding gradients), which can destabilize training or cause the model to diverge. Large gradients lead to large weight updates, which may overshoot optimal values, hinder convergence, or cause numerical instability.\n","\n","### Example:\n","Suppose during backpropagation, the gradient of a parameter is calculated as **1000**. Without clipping, this would cause a massive weight update (e.g., subtracting 1000 from the weight). However, with gradient clipping (e.g., a threshold of **1.0**), the gradient is scaled down to **1.0**, ensuring the weight update remains small and controlled. This prevents the model from diverging and promotes stable training.\n","\n","### Key Benefit:\n","By limiting gradient magnitudes, clipping ensures smoother, more reliable convergence during training.\n"]}]},{"cell_type":"markdown","source":["Understanding the `grammar` parameter\n","\n","lets you constrain model generation using a BNF-like (Backus–Naur Form) syntax definition. This is not for natural language “grammar” like English, it’s for formal grammars that define what output patterns are valid (e.g., valid JSON, SQL, lists, etc.).\n","\n","Say you want the model to only output one of `\"yes\"` or `\"no\"`:"],"metadata":{"id":"z_JaPJVO37VE"}},{"cell_type":"code","source":["grammar = r\"\"\"\n","root ::= \"yes\" | \"no\"\n","\"\"\"\n","\n","autoGGUFModel.setGrammar(grammar)\n","\n","data = spark.createDataFrame([[\"Is the statement 'Water boils at 100 degrees Celsius' scientifically correct?\"]]).toDF(\"text\")\n","\n","pipeline = Pipeline().setStages([document_assembler, autoGGUFModel])\n","result = pipeline.fit(data).transform(data)\n","\n","result.select(\"completions.result\").show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UDMhK8MR7ISg","executionInfo":{"status":"ok","timestamp":1762330807295,"user_tz":-300,"elapsed":2103,"user":{"displayName":"Muhammad Abdullah","userId":"08204441800691646310"}},"outputId":"8d3278cd-c6ea-4169-f228-fa060df99c63"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+\n","|result|\n","+------+\n","| [yes]|\n","+------+\n","\n"]}]},{"cell_type":"markdown","source":["check out [HasLlamaCppInferenceProperties](https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/main/scala/com/johnsnowlabs/nlp/HasLlamaCppInferenceProperties.scala), [HasLlamaCppModelProperties](https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/main/scala/com/johnsnowlabs/nlp/HasLlamaCppModelProperties.scalar) or refer to the llama.cpp documentation of [server.cpp](https://github.com/ggerganov/llama.cpp/tree/7d5e8777ae1d21af99d4f95be10db4870720da91/examples/server) for more information!"],"metadata":{"id":"pb5jrs2Z8z-u"}},{"cell_type":"markdown","source":["## AutoGGUFVisionModel"],"metadata":{"id":"LBhgYqOuwtxv"}},{"cell_type":"markdown","source":["This is an annotator that enables multimodal understanding by combining text prompts with visual input. It can analyze images and respond to natural language instructions, extracting structured information, descriptions, or summaries from visual data."],"metadata":{"id":"rt6o1jH18MAm"}},{"cell_type":"markdown","source":["This section builds a multimodal pipeline that takes a medical document image and a guiding text prompt to extract structured information."],"metadata":{"id":"GGAL8HHQsuds"}},{"cell_type":"code","source":["!wget -q -O prescription.png https://github.com/JohnSnowLabs/spark-nlp-workshop/raw/master/healthcare-nlp/data/ocr/prescription_02.png\n"],"metadata":{"id":"226uKCR7vTBH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"\"\"You are an expert medical document parser. Analyze the given image of a prescription or outpatient summary and extract all relevant structured information clearly.\n","\n","Identify and return the following fields where available:\n","- Hospital/Clinic Name\n","- Department\n","- Patient Information: Name, Age, Sex\n","- Identifiers: Hospital No, Episode No, Episode Date\n","- Doctor Details: Name, Department, Designation (if any)\n","- Consultation Notes or Diagnosis: summary of the patient’s condition, symptoms, and relevant findings\n","- Treatment Plan: list all prescribed medications with full details (drug name, dosage, frequency, and duration)\n","- Follow-up Instructions: review timeline, tests, or other advice\n","\n","Preserve the original units, abbreviations, and formatting of medicines as they appear.\n","\n","Output the extracted data in JSON format, structured like this example:\n","\n","{\n","  \"hospital_name\": \"\",\n","  \"department\": \"\",\n","  \"patient\": {\n","    \"name\": \"\",\n","    \"age\": \"\",\n","    \"sex\": \"\"\n","  },\n","  \"identifiers\": {\n","    \"hospital_no\": \"\",\n","    \"episode_no\": \"\",\n","    \"episode_date\": \"\"\n","  },\n","  \"doctor\": {\n","    \"name\": \"\",\n","    \"department\": \"\"\n","  },\n","  \"consultation_notes\": \"\",\n","  \"treatment\": [\n","    {\n","      \"drug_name\": \"\",\n","      \"dosage\": \"\",\n","    }\n","  ],\n","  \"follow_up\": \"\"\n","}\"\"\"\n"],"metadata":{"id":"G_zXDlc5zzDq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = ImageAssembler.loadImagesAsBytes(spark, \"prescription.png\")\n","data = data.withColumn(\"prompt\", F.lit(prompt))\n","\n","document_assembler = (\n","    DocumentAssembler()\n","    .setInputCol(\"prompt\")\n","    .setOutputCol(\"prompt_document\")\n",")\n","\n","image_assembler = (\n","    ImageAssembler()\n","    .setInputCol(\"image\")\n","    .setOutputCol(\"image_assembler\")\n",")\n","\n","autoGGUFVisionModel = (\n","    AutoGGUFVisionModel.pretrained(\"qwen2_vl_2b_instruct_q4_gguf\")\n","    .setInputCols([\"prompt_document\", \"image_assembler\"])\n","    .setOutputCol(\"completions\")\n","    .setNPredict(-1)\n","    .setBatchSize(1)\n","    .setNCtx(32768)\n","    .setNGpuLayers(-1)\n","    .setFlashAttention(True)\n",")\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    image_assembler,\n","    autoGGUFVisionModel\n","])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SR6E-Fd1CHnr","executionInfo":{"status":"ok","timestamp":1762342788713,"user_tz":-300,"elapsed":4067,"user":{"displayName":"Muhammad Abdullah","userId":"08204441800691646310"}},"outputId":"0abf43ca-0234-40da-ad75-df6e57d6ebd0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["qwen2_vl_2b_instruct_q4_gguf download started this may take some time.\n","Approximate size to download 1.5 GB\n","[OK!]\n"]}]},{"cell_type":"code","source":["result = pipeline.fit(data).transform(data)\n","print(result.select(\"completions.result\").first().result[0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fsJb68Zxx6dZ","executionInfo":{"status":"ok","timestamp":1762342842640,"user_tz":-300,"elapsed":53924,"user":{"displayName":"Muhammad Abdullah","userId":"08204441800691646310"}},"outputId":"4929be8c-e742-4471-b32e-6e86fbfddf12"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["```json\n","[\n","  {\n","    \"hospital_name\": \"Department of Rheumatology\",\n","    \"department\": \"Rheumatology\",\n","    \"patient\": {\n","      \"name\": \"Ms Rukhsana Shaheen\",\n","      \"age\": \"56 yrs/Female\"\n","    },\n","    \"identifiers\": {\n","      \"hospital_no\": \"MH005990453\",\n","      \"episode_no\": \"02/07/2021 08:31AM\",\n","      \"episode_date\": \"02/07/2021\"\n","    },\n","    \"doctor\": {\n","      \"name\": \"DR Darshan Singh Bhakuni\",\n","      \"department\": \"Rheumatology\"\n","    },\n","    \"consultation_notes\": \"Video consultation done. Known case of systemic lupus erythematosus and scleroderma overlap with interstitial lung disease on medication.\",\n","    \"treatment\": [\n","      {\n","        \"drug_name\": \"Tab Sildinafil Citrate\",\n","        \"dosage\": \"0.5 mg twice a day after meals\",\n","        \"frequency\": \"\",\n","        \"duration\": \"\"\n","      }\n","    ],\n","    \"follow_up\": \"Review after 4 weeks.\"\n","]\n","```\n"]}]},{"cell_type":"markdown","source":["## AutoGGUFReranker"],"metadata":{"id":"zW2k-6DI1lnb"}},{"cell_type":"markdown","source":["This is an annotator that reorders documents based on how relevant they are to a given query. It assigns each document a `relevance_score`, allowing the most relevant passages to appear first.\n","\n","This is particularly useful in *RAG systems*, where reranking acts as a second-pass filter that refines search results. After the initial retrieval step (like vector similarity), the reranker reevaluates each document in the context of your query and reorders them by relevance. This adds a bit of latency but delivers far more accurate results. It’s especially useful because feeding irrelevant context to an LLM wastes tokens, increases cost, and can lead to hallucinations or incorrect answers."],"metadata":{"id":"PjMbDOI73q2M"}},{"cell_type":"markdown","metadata":{"id":"jfxg-zr94Hjt"},"source":["Docs: https://sparknlp.org/docs/en/annotators#autoggufreranker\n"]},{"cell_type":"code","source":["document = (\n","    DocumentAssembler()\n","    .setInputCol(\"text\")\n","    .setOutputCol(\"document\")\n",")\n","\n","reranker = (\n","    AutoGGUFReranker.pretrained(\"bge_reranker_v2_m3_Q4_K_M\")\n","    .setInputCols([\"document\"])\n","    .setOutputCol(\"reranked_documents\")\n","    .setQuery(\"A man is eating pasta.\")\n",")\n","\n","finisher = (\n","    GGUFRankingFinisher()\n","    .setInputCols(\"reranked_documents\")\n","    .setOutputCol(\"finished_reranked_documents\")\n","    .setMinRelevanceScore(0.4) # Drops scores below 0.4\n","    .setMinMaxScaling(True)\n",")\n","\n","pipeline = Pipeline().setStages([document, reranker, finisher])\n","\n","data = spark.createDataFrame([\n","    [\"A man is eating food.\"],\n","    [\"A man is eating a piece of bread.\"],\n","    [\"The girl is carrying a baby.\"],\n","    [\"A man is riding a horse.\"]\n","]).toDF(\"text\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P9bvdJ_Q2Ww-","executionInfo":{"status":"ok","timestamp":1762346035119,"user_tz":-300,"elapsed":4136,"user":{"displayName":"Muhammad Abdullah","userId":"08204441800691646310"}},"outputId":"aa60dc45-8a1b-42d6-f408-985e57149a07"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["bge_reranker_v2_m3_Q4_K_M download started this may take some time.\n","Approximate size to download 396.7 MB\n","[OK!]\n"]}]},{"cell_type":"code","source":["result = pipeline.fit(data).transform(data)\n","result.selectExpr(\"explode(finished_reranked_documents) as doc\") \\\n","  .selectExpr(\"doc.metadata['rank'] as rank\", \"doc.result as text\", \"doc.metadata['relevance_score'] as score\") \\\n","  .show(truncate=False)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bF2KjTmhAMpT","executionInfo":{"status":"ok","timestamp":1762346040720,"user_tz":-300,"elapsed":5591,"user":{"displayName":"Muhammad Abdullah","userId":"08204441800691646310"}},"outputId":"e6764527-9e06-496b-c048-f86bb0f4ee63"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+----+---------------------------------+------------------+\n","|rank|text                             |score             |\n","+----+---------------------------------+------------------+\n","|1   |A man is eating food.            |1.0               |\n","|2   |A man is eating a piece of bread.|0.7244979587924696|\n","+----+---------------------------------+------------------+\n","\n"]}]},{"cell_type":"markdown","source":["# Import your own GGUF models into Spark NLP"],"metadata":{"id":"m4tVHyW7-R3B"}},{"cell_type":"markdown","source":["You can also use your own `.gguf` models with any of the annotators by calling `.loadSavedModel()` instead of `.pretrained()`."],"metadata":{"id":"q5FNjk1lGxkP"}},{"cell_type":"code","source":["!wget -c https://huggingface.co/MaziyarPanahi/gemma-2b-it-GGUF/resolve/main/gemma-2b-it.Q8_0.gguf \\\n","     -O gemma-2b-it.Q8_0.gguf\n"],"metadata":{"id":"TaMK9XxBAqiS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["AutoGGUFModel\\\n","  .loadSavedModel(\"gemma-2b-it.Q8_0.gguf\", spark)\\\n","  .setInputCols([\"document\"])\\\n","  .setOutputCol(\"completions\")\\\n","  .write().overwrite().save(\"gemma_2b_it_q8_0_gguf\")\n","  # ^ Optionally save it on disk so it is easier to be moved around and also be used later via .load function\n"],"metadata":{"id":"RpqbuMCnB-0W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls -lh gemma_2b_it_q8_0_gguf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iI4MF2TXDmJi","executionInfo":{"status":"ok","timestamp":1762346836448,"user_tz":-300,"elapsed":129,"user":{"displayName":"Muhammad Abdullah","userId":"08204441800691646310"}},"outputId":"218b2f33-29d3-4fa3-efb7-30f93bfc4334"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 2.5G\n","-rw-r--r-- 1 root root 2.5G Nov  5 12:46 gemma-2b-it.Q8_0.gguf\n","drwxr-xr-x 2 root root 4.0K Nov  5 12:46 metadata\n"]}]},{"cell_type":"markdown","source":["inference"],"metadata":{"id":"HpG-YkqTDTyh"}},{"cell_type":"code","source":["document_assembler = (\n","    DocumentAssembler()\n","    .setInputCol(\"text\")\n","    .setOutputCol(\"document\")\n",")\n","\n","auto_gguf_model_loaded = (\n","    AutoGGUFModel.load(\"gemma_2b_it_q8_0_gguf\")\n","    .setInputCols([\"document\"])\n","    .setOutputCol(\"completions\")\n","    .setSystemPrompt(\"You are a story writing assistant.\")\n","    .setCachePrompt(True)\n","    .setNPredict(-1)\n",")\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    auto_gguf_model_loaded\n","])\n","\n","prompt = spark.createDataFrame([[\"Write a story about llamas\"]]).toDF(\"text\")\n","\n","result = pipeline.fit(data).transform(data)\n","print(result.select(\"completions.result\").first().result[0])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b9l7u66WDVDy","executionInfo":{"status":"ok","timestamp":1762347365776,"user_tz":-300,"elapsed":19895,"user":{"displayName":"Muhammad Abdullah","userId":"08204441800691646310"}},"outputId":"e113972a-f220-49a8-a78e-3f153ace533e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The man, weathered and with eyes weary from a long day, sat in his humble apartment, the aroma of a simple stew filling the air. He took a bite of the stew, the flavors slowly mingling on his tongue.\n","\n","He was a man of few means, but his heart was full. He had worked tirelessly all day, his hands calloused and his muscles aching. He had faced challenges that he had overcome with grit and determination.\n","\n","As he chewed the stew, he reflected on his journey. He had come a long way from where he started. He had faced poverty and loss, but he had never given up on his dreams. He had always held onto the hope that he could build a better future for himself and his family.\n","\n","He smiled as he took another bite of the stew, his eyes twinkling with the satisfaction of a hard-earned meal. He knew that life was never easy, but he was grateful for the simple pleasures in life, the small moments of joy that made his heart soar.\n","\n","As the stew finished cooking, he looked up, a sense of contentment washing over him. He had come a long way, and he was proud of himself for what he had accomplished. He knew that there were still challenges to come, but he was ready to face them with the same determination and resilience that had gotten him this far.\n"]}]},{"cell_type":"markdown","source":["That's it! You can now go wild and use any GGUF model in Spark NLP 🚀"],"metadata":{"id":"GDMhbjxaG6so"}}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}