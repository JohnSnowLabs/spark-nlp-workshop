{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab5d9ccc",
   "metadata": {
    "id": "ab5d9ccc"
   },
   "source": [
    "# ðŸŒ Comparative Analysis: Unstructured vs SparkNLP (Reader2Doc) for HTML Ingestion\n",
    "\n",
    "This notebook demonstrates how to extract structured information from **HTML documents** using two powerful tools:\n",
    "\n",
    "- **Unstructured**: Extracts elements (text, titles, tables, images) preserving document structure.\n",
    "- **SparkNLP (Reader2Doc)**: Reads HTML files into Spark NLP pipelines as text documents for large-scale NLP tasks.\n",
    "\n",
    "We'll analyze their differences in handling structured HTML content containing **headings**, **paragraphs**, **tables**, and **images**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "L6ra6SNdDYrt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "L6ra6SNdDYrt",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5379e7fb-2f03-4c5b-d0d1-b000629f31aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unstructured[local-inference]\n",
      "  Downloading unstructured-0.18.26-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from unstructured[local-inference]) (3.4.4)\n",
      "Collecting filetype (from unstructured[local-inference])\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-magic (from unstructured[local-inference])\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from unstructured[local-inference]) (6.0.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from unstructured[local-inference]) (3.9.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from unstructured[local-inference]) (2.32.4)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from unstructured[local-inference]) (4.13.5)\n",
      "Collecting emoji (from unstructured[local-inference])\n",
      "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting dataclasses-json (from unstructured[local-inference])\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting python-iso639 (from unstructured[local-inference])\n",
      "  Downloading python_iso639-2025.11.16-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting langdetect (from unstructured[local-inference])\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from unstructured[local-inference]) (2.0.2)\n",
      "Collecting rapidfuzz (from unstructured[local-inference])\n",
      "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
      "Collecting backoff (from unstructured[local-inference])\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from unstructured[local-inference]) (4.15.0)\n",
      "Collecting unstructured-client (from unstructured[local-inference])\n",
      "  Downloading unstructured_client-0.42.6-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from unstructured[local-inference]) (2.0.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from unstructured[local-inference]) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from unstructured[local-inference]) (5.9.5)\n",
      "Collecting python-oxmsg (from unstructured[local-inference])\n",
      "  Downloading python_oxmsg-0.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: html5lib in /usr/local/lib/python3.12/dist-packages (from unstructured[local-inference]) (1.1)\n",
      "Collecting pi_heif (from unstructured[local-inference])\n",
      "  Downloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: xlrd in /usr/local/lib/python3.12/dist-packages (from unstructured[local-inference]) (2.0.2)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (from unstructured[local-inference]) (3.1.5)\n",
      "Collecting python-pptx>=1.0.1 (from unstructured[local-inference])\n",
      "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: markdown in /usr/local/lib/python3.12/dist-packages (from unstructured[local-inference]) (3.10)\n",
      "Collecting pdf2image (from unstructured[local-inference])\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting msoffcrypto-tool (from unstructured[local-inference])\n",
      "  Downloading msoffcrypto_tool-5.4.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pypandoc (from unstructured[local-inference])\n",
      "  Downloading pypandoc-1.16.2-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from unstructured[local-inference]) (2.2.2)\n",
      "Collecting python-docx>=1.1.2 (from unstructured[local-inference])\n",
      "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting pikepdf (from unstructured[local-inference])\n",
      "  Downloading pikepdf-10.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.3 kB)\n",
      "Collecting unstructured-inference>=1.1.1 (from unstructured[local-inference])\n",
      "  Downloading unstructured_inference-1.1.4-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[local-inference])\n",
      "  Downloading unstructured.pytesseract-0.3.15-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting effdet (from unstructured[local-inference])\n",
      "  Downloading effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting onnxruntime>=1.19.0 (from unstructured[local-inference])\n",
      "  Downloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Collecting google-cloud-vision (from unstructured[local-inference])\n",
      "  Downloading google_cloud_vision-3.11.0-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting pypdf (from unstructured[local-inference])\n",
      "  Downloading pypdf-6.5.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from unstructured[local-inference]) (3.6.1)\n",
      "Collecting onnx>=1.17.0 (from unstructured[local-inference])\n",
      "  Downloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting pdfminer.six (from unstructured[local-inference])\n",
      "  Downloading pdfminer_six-20260107-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.17.0->unstructured[local-inference]) (5.29.5)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.17.0->unstructured[local-inference]) (0.5.4)\n",
      "Collecting coloredlogs (from onnxruntime>=1.19.0->unstructured[local-inference])\n",
      "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.19.0->unstructured[local-inference]) (25.9.23)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.19.0->unstructured[local-inference]) (25.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.19.0->unstructured[local-inference]) (1.14.0)\n",
      "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.12/dist-packages (from python-pptx>=1.0.1->unstructured[local-inference]) (11.3.0)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx>=1.0.1->unstructured[local-inference])\n",
      "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: python-multipart in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[local-inference]) (0.0.20)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[local-inference]) (0.36.0)\n",
      "Requirement already satisfied: opencv-python>=4.12 in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[local-inference]) (4.12.0.88)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[local-inference]) (3.10.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[local-inference]) (2.9.0+cpu)\n",
      "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[local-inference]) (1.0.22)\n",
      "Requirement already satisfied: transformers>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[local-inference]) (4.57.3)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[local-inference]) (1.12.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from unstructured-inference>=1.1.1->unstructured[local-inference]) (1.16.3)\n",
      "Collecting pypdfium2 (from unstructured-inference>=1.1.1->unstructured[local-inference])\n",
      "  Downloading pypdfium2-5.3.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->unstructured[local-inference]) (2.8)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->unstructured[local-inference])\n",
      "  Downloading marshmallow-3.26.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->unstructured[local-inference])\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from effdet->unstructured[local-inference]) (0.24.0+cpu)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from effdet->unstructured[local-inference]) (2.0.10)\n",
      "Requirement already satisfied: omegaconf>=2.0 in /usr/local/lib/python3.12/dist-packages (from effdet->unstructured[local-inference]) (2.3.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[local-inference]) (2.28.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-vision->unstructured[local-inference]) (2.43.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-vision->unstructured[local-inference]) (1.76.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-vision->unstructured[local-inference]) (1.26.1)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.12/dist-packages (from html5lib->unstructured[local-inference]) (1.17.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from html5lib->unstructured[local-inference]) (0.5.1)\n",
      "Requirement already satisfied: cryptography>=39.0 in /usr/local/lib/python3.12/dist-packages (from msoffcrypto-tool->unstructured[local-inference]) (43.0.3)\n",
      "Collecting olefile>=0.46 (from msoffcrypto-tool->unstructured[local-inference])\n",
      "  Downloading olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->unstructured[local-inference]) (8.3.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->unstructured[local-inference]) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->unstructured[local-inference]) (2025.11.3)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl->unstructured[local-inference]) (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->unstructured[local-inference]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->unstructured[local-inference]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->unstructured[local-inference]) (2025.3)\n",
      "Collecting Deprecated (from pikepdf->unstructured[local-inference])\n",
      "  Downloading deprecated-1.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->unstructured[local-inference]) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->unstructured[local-inference]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->unstructured[local-inference]) (2025.11.12)\n",
      "Requirement already satisfied: aiofiles>=24.1.0 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[local-inference]) (24.1.0)\n",
      "Requirement already satisfied: httpcore>=1.0.9 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[local-inference]) (1.0.9)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[local-inference]) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.11.2 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[local-inference]) (2.12.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from unstructured-client->unstructured[local-inference]) (1.0.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=39.0->msoffcrypto-tool->unstructured[local-inference]) (2.0.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[local-inference]) (1.72.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-cloud-vision->unstructured[local-inference]) (1.71.2)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[local-inference]) (6.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[local-inference]) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[local-inference]) (4.9.1)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore>=1.0.9->unstructured-client->unstructured[local-inference]) (0.16.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->unstructured-client->unstructured[local-inference]) (4.12.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf>=2.0->effdet->unstructured[local-inference]) (4.9.3)\n",
      "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf>=2.0->effdet->unstructured[local-inference]) (6.0.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured[local-inference]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured[local-inference]) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.11.2->unstructured-client->unstructured[local-inference]) (0.4.2)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm->unstructured-inference>=1.1.1->unstructured[local-inference]) (0.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[local-inference]) (3.20.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[local-inference]) (75.2.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[local-inference]) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->unstructured-inference>=1.1.1->unstructured[local-inference]) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.19.0->unstructured[local-inference]) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.25.1->unstructured-inference>=1.1.1->unstructured[local-inference]) (0.22.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->unstructured-inference>=1.1.1->unstructured[local-inference]) (1.2.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[local-inference])\n",
      "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.19.0->unstructured[local-inference])\n",
      "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.1.1->unstructured[local-inference]) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.1.1->unstructured[local-inference]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.1.1->unstructured[local-inference]) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.1.1->unstructured[local-inference]) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->unstructured-inference>=1.1.1->unstructured[local-inference]) (3.2.5)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=39.0->msoffcrypto-tool->unstructured[local-inference]) (2.23)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-cloud-vision->unstructured[local-inference]) (0.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->unstructured-inference>=1.1.1->unstructured[local-inference]) (3.0.3)\n",
      "Downloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (18.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m94.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnxruntime-1.23.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (17.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unstructured_inference-1.1.4-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.9/47.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unstructured.pytesseract-0.3.15-py3-none-any.whl (14 kB)\n",
      "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m112.5/112.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading google_cloud_vision-3.11.0-py3-none-any.whl (529 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m529.1/529.1 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading msoffcrypto_tool-5.4.2-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading pdfminer_six-20260107-py3-none-any.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pi_heif-1.1.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pikepdf-10.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypandoc-1.16.2-py3-none-any.whl (19 kB)\n",
      "Downloading pypdf-6.5.0-py3-none-any.whl (329 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m329.6/329.6 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_iso639-2025.11.16-py3-none-any.whl (167 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.8/167.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Downloading python_oxmsg-0.0.2-py3-none-any.whl (31 kB)\n",
      "Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unstructured-0.18.26-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading unstructured_client-0.42.6-py3-none-any.whl (219 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m219.6/219.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.26.2-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading deprecated-1.3.1-py2.py3-none-any.whl (11 kB)\n",
      "Downloading pypdfium2-5.3.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=9c3062b98623ddf8d5772702ccf48340f80ebe07c01fc9c14e07ff76f4e5c58d\n",
      "  Stored in directory: /root/.cache/pip/wheels/c1/67/88/e844b5b022812e15a52e4eaa38a1e709e99f06f6639d7e3ba7\n",
      "Successfully built langdetect\n",
      "Installing collected packages: filetype, XlsxWriter, unstructured.pytesseract, rapidfuzz, python-magic, python-iso639, python-docx, pypdfium2, pypdf, pypandoc, pi_heif, pdf2image, olefile, mypy-extensions, marshmallow, langdetect, humanfriendly, emoji, Deprecated, backoff, typing-inspect, python-pptx, python-oxmsg, pikepdf, onnx, coloredlogs, unstructured-client, pdfminer.six, onnxruntime, msoffcrypto-tool, dataclasses-json, unstructured, unstructured-inference, google-cloud-vision, effdet\n",
      "Successfully installed Deprecated-1.3.1 XlsxWriter-3.2.9 backoff-2.2.1 coloredlogs-15.0.1 dataclasses-json-0.6.7 effdet-0.4.1 emoji-2.15.0 filetype-1.2.0 google-cloud-vision-3.11.0 humanfriendly-10.0 langdetect-1.0.9 marshmallow-3.26.2 msoffcrypto-tool-5.4.2 mypy-extensions-1.1.0 olefile-0.47 onnx-1.20.0 onnxruntime-1.23.2 pdf2image-1.17.0 pdfminer.six-20260107 pi_heif-1.1.1 pikepdf-10.1.0 pypandoc-1.16.2 pypdf-6.5.0 pypdfium2-5.3.0 python-docx-1.2.0 python-iso639-2025.11.16 python-magic-0.4.27 python-oxmsg-0.0.2 python-pptx-1.0.2 rapidfuzz-3.14.3 typing-inspect-0.9.0 unstructured-0.18.26 unstructured-client-0.42.6 unstructured-inference-1.1.4 unstructured.pytesseract-0.3.15\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "ec6589c5272f48d2aaa945e51444b857",
       "pip_warning": {
        "packages": [
         "google"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install unstructured[local-inference]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U0EBnfS08N-W",
   "metadata": {
    "id": "U0EBnfS08N-W"
   },
   "source": [
    "## Generating Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "atQR9izQ2KCW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "atQR9izQ2KCW",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7132841f-00f4-4421-fb9b-4cb85578ec95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-08 16:51:17--  http://!wget/\n",
      "Resolving !wget (!wget)... failed: Name or service not known.\n",
      "wget: unable to resolve host address â€˜!wgetâ€™\n",
      "--2026-01-08 16:51:17--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/SPARKNLP-1320-Benchmarkunstructured-vs-SparkNLP-in-unstructured-document-processing/open-source-nlp/data/readers/html/EHR-2024-11-456123.html\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20124 (20K) [text/plain]\n",
      "Saving to: â€˜html_docs/EHR-2024-11-456123.htmlâ€™\n",
      "\n",
      "\r",
      "EHR-2024-11-456123.   0%[                    ]       0  --.-KB/s               \r",
      "EHR-2024-11-456123. 100%[===================>]  19.65K  --.-KB/s    in 0s      \n",
      "\n",
      "2026-01-08 16:51:18 (110 MB/s) - â€˜html_docs/EHR-2024-11-456123.htmlâ€™ saved [20124/20124]\n",
      "\n",
      "FINISHED --2026-01-08 16:51:18--\n",
      "Total wall clock time: 0.08s\n",
      "Downloaded: 1 files, 20K in 0s (110 MB/s)\n",
      "--2026-01-08 16:51:18--  http://!wget/\n",
      "Resolving !wget (!wget)... failed: Name or service not known.\n",
      "wget: unable to resolve host address â€˜!wgetâ€™\n",
      "--2026-01-08 16:51:18--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/SPARKNLP-1320-Benchmarkunstructured-vs-SparkNLP-in-unstructured-document-processing/open-source-nlp/data/readers/html/EHR-2024-11-654987.html\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 20687 (20K) [text/plain]\n",
      "Saving to: â€˜html_docs/EHR-2024-11-654987.htmlâ€™\n",
      "\n",
      "EHR-2024-11-654987. 100%[===================>]  20.20K  --.-KB/s    in 0s      \n",
      "\n",
      "2026-01-08 16:51:18 (93.7 MB/s) - â€˜html_docs/EHR-2024-11-654987.htmlâ€™ saved [20687/20687]\n",
      "\n",
      "FINISHED --2026-01-08 16:51:18--\n",
      "Total wall clock time: 0.08s\n",
      "Downloaded: 1 files, 20K in 0s (93.7 MB/s)\n",
      "--2026-01-08 16:51:18--  http://!wget/\n",
      "Resolving !wget (!wget)... failed: Name or service not known.\n",
      "wget: unable to resolve host address â€˜!wgetâ€™\n",
      "--2026-01-08 16:51:18--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/SPARKNLP-1320-Benchmarkunstructured-vs-SparkNLP-in-unstructured-document-processing/open-source-nlp/data/readers/html/EHR-2024-11-789321.html\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 21456 (21K) [text/plain]\n",
      "Saving to: â€˜html_docs/EHR-2024-11-789321.htmlâ€™\n",
      "\n",
      "EHR-2024-11-789321. 100%[===================>]  20.95K  --.-KB/s    in 0s      \n",
      "\n",
      "2026-01-08 16:51:18 (78.5 MB/s) - â€˜html_docs/EHR-2024-11-789321.htmlâ€™ saved [21456/21456]\n",
      "\n",
      "FINISHED --2026-01-08 16:51:18--\n",
      "Total wall clock time: 0.06s\n",
      "Downloaded: 1 files, 21K in 0s (78.5 MB/s)\n",
      "--2026-01-08 16:51:18--  http://!wget/\n",
      "Resolving !wget (!wget)... failed: Name or service not known.\n",
      "wget: unable to resolve host address â€˜!wgetâ€™\n",
      "--2026-01-08 16:51:18--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/SPARKNLP-1320-Benchmarkunstructured-vs-SparkNLP-in-unstructured-document-processing/open-source-nlp/data/readers/html/EHR-2024-11-789351.html\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 23301 (23K) [text/plain]\n",
      "Saving to: â€˜html_docs/EHR-2024-11-789351.htmlâ€™\n",
      "\n",
      "EHR-2024-11-789351. 100%[===================>]  22.75K  --.-KB/s    in 0s      \n",
      "\n",
      "2026-01-08 16:51:18 (122 MB/s) - â€˜html_docs/EHR-2024-11-789351.htmlâ€™ saved [23301/23301]\n",
      "\n",
      "FINISHED --2026-01-08 16:51:18--\n",
      "Total wall clock time: 0.06s\n",
      "Downloaded: 1 files, 23K in 0s (122 MB/s)\n",
      "--2026-01-08 16:51:18--  http://!wget/\n",
      "Resolving !wget (!wget)... failed: Name or service not known.\n",
      "wget: unable to resolve host address â€˜!wgetâ€™\n",
      "--2026-01-08 16:51:18--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/SPARKNLP-1320-Benchmarkunstructured-vs-SparkNLP-in-unstructured-document-processing/open-source-nlp/data/readers/html/EHR-2024-11-789456.html\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18713 (18K) [text/plain]\n",
      "Saving to: â€˜html_docs/EHR-2024-11-789456.htmlâ€™\n",
      "\n",
      "EHR-2024-11-789456. 100%[===================>]  18.27K  --.-KB/s    in 0s      \n",
      "\n",
      "2026-01-08 16:51:18 (116 MB/s) - â€˜html_docs/EHR-2024-11-789456.htmlâ€™ saved [18713/18713]\n",
      "\n",
      "FINISHED --2026-01-08 16:51:18--\n",
      "Total wall clock time: 0.06s\n",
      "Downloaded: 1 files, 18K in 0s (116 MB/s)\n",
      "--2026-01-08 16:51:18--  http://!wget/\n",
      "Resolving !wget (!wget)... failed: Name or service not known.\n",
      "wget: unable to resolve host address â€˜!wgetâ€™\n",
      "--2026-01-08 16:51:18--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/SPARKNLP-1320-Benchmarkunstructured-vs-SparkNLP-in-unstructured-document-processing/open-source-nlp/data/readers/html/EHR-2024-11-112233.html\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2026-01-08 16:51:18 ERROR 404: Not Found.\n",
      "\n",
      "--2026-01-08 16:51:18--  http://!wget/\n",
      "Resolving !wget (!wget)... failed: Name or service not known.\n",
      "wget: unable to resolve host address â€˜!wgetâ€™\n",
      "--2026-01-08 16:51:18--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/SPARKNLP-1320-Benchmarkunstructured-vs-SparkNLP-in-unstructured-document-processing/open-source-nlp/data/readers/html/EHR-2024-11-334455.html\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2026-01-08 16:51:18 ERROR 404: Not Found.\n",
      "\n",
      "--2026-01-08 16:51:18--  http://!wget/\n",
      "Resolving !wget (!wget)... failed: Name or service not known.\n",
      "wget: unable to resolve host address â€˜!wgetâ€™\n",
      "--2026-01-08 16:51:18--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/SPARKNLP-1320-Benchmarkunstructured-vs-SparkNLP-in-unstructured-document-processing/open-source-nlp/data/readers/html/EHR-2024-11-445566.html\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2026-01-08 16:51:18 ERROR 404: Not Found.\n",
      "\n",
      "--2026-01-08 16:51:18--  http://!wget/\n",
      "Resolving !wget (!wget)... failed: Name or service not known.\n",
      "wget: unable to resolve host address â€˜!wgetâ€™\n",
      "--2026-01-08 16:51:18--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/SPARKNLP-1320-Benchmarkunstructured-vs-SparkNLP-in-unstructured-document-processing/open-source-nlp/data/readers/html/EHR-2024-11-556677.html\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2026-01-08 16:51:19 ERROR 404: Not Found.\n",
      "\n",
      "--2026-01-08 16:51:19--  http://!wget/\n",
      "Resolving !wget (!wget)... failed: Name or service not known.\n",
      "wget: unable to resolve host address â€˜!wgetâ€™\n",
      "--2026-01-08 16:51:19--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/SPARKNLP-1320-Benchmarkunstructured-vs-SparkNLP-in-unstructured-document-processing/open-source-nlp/data/readers/html/EHR-2024-11-778899.html\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 404 Not Found\n",
      "2026-01-08 16:51:19 ERROR 404: Not Found.\n",
      "\n",
      "--2026-01-08 16:51:19--  http://!wget/\n",
      "Resolving !wget (!wget)... failed: Name or service not known.\n",
      "wget: unable to resolve host address â€˜!wgetâ€™\n",
      "--2026-01-08 16:51:19--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/SPARKNLP-1320-Benchmarkunstructured-vs-SparkNLP-in-unstructured-document-processing/open-source-nlp/data/readers/html/EHR-2025-12-000001.html\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15992 (16K) [text/plain]\n",
      "Saving to: â€˜html_docs/EHR-2025-12-000001.htmlâ€™\n",
      "\n",
      "EHR-2025-12-000001. 100%[===================>]  15.62K  --.-KB/s    in 0s      \n",
      "\n",
      "2026-01-08 16:51:19 (96.5 MB/s) - â€˜html_docs/EHR-2025-12-000001.htmlâ€™ saved [15992/15992]\n",
      "\n",
      "FINISHED --2026-01-08 16:51:19--\n",
      "Total wall clock time: 0.07s\n",
      "Downloaded: 1 files, 16K in 0s (96.5 MB/s)\n",
      "--2026-01-08 16:51:19--  http://!wget/\n",
      "Resolving !wget (!wget)... failed: Name or service not known.\n",
      "wget: unable to resolve host address â€˜!wgetâ€™\n",
      "--2026-01-08 16:51:19--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/SPARKNLP-1320-Benchmarkunstructured-vs-SparkNLP-in-unstructured-document-processing/open-source-nlp/data/readers/html/EHR-2025-12-000002.html\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16690 (16K) [text/plain]\n",
      "Saving to: â€˜html_docs/EHR-2025-12-000002.htmlâ€™\n",
      "\n",
      "EHR-2025-12-000002. 100%[===================>]  16.30K  --.-KB/s    in 0s      \n",
      "\n",
      "2026-01-08 16:51:19 (101 MB/s) - â€˜html_docs/EHR-2025-12-000002.htmlâ€™ saved [16690/16690]\n",
      "\n",
      "FINISHED --2026-01-08 16:51:19--\n",
      "Total wall clock time: 0.06s\n",
      "Downloaded: 1 files, 16K in 0s (101 MB/s)\n"
     ]
    }
   ],
   "source": [
    "!mkdir html_docs\n",
    "!wget -P html_docs !wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/open-source-nlp/data/readers/html/EHR-2024-11-456123.html\n",
    "!wget -P html_docs !wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/open-source-nlp/data/readers/html/EHR-2024-11-654987.html\n",
    "!wget -P html_docs !wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/open-source-nlp/data/readers/html/EHR-2024-11-789321.html\n",
    "!wget -P html_docs !wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/open-source-nlp/data/readers/html/EHR-2024-11-789351.html\n",
    "!wget -P html_docs !wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/open-source-nlp/data/readers/html/EHR-2024-11-789456.html\n",
    "!wget -P html_docs !wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/open-source-nlp/data/readers/html/EHR-2024-11-112233.html\n",
    "!wget -P html_docs !wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/open-source-nlp/data/readers/html/EHR-2024-11-334455.html\n",
    "!wget -P html_docs !wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/open-source-nlp/data/readers/html/EHR-2024-11-445566.html\n",
    "!wget -P html_docs !wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/open-source-nlp/data/readers/html/EHR-2024-11-556677.html\n",
    "!wget -P html_docs !wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/open-source-nlp/data/readers/html/EHR-2024-11-778899.html\n",
    "!wget -P html_docs !wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/open-source-nlp/data/readers/html/EHR-2025-12-000001.html\n",
    "!wget -P html_docs !wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/open-source-nlp/data/readers/html/EHR-2025-12-000002.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e28347",
   "metadata": {
    "id": "18e28347"
   },
   "source": [
    "## 1ï¸âƒ£ Unstructured: HTML Partition and Cleaning\n",
    "\n",
    "Unstructured can parse HTML files and extract text, links, tables, and images into structured **elements**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "rTlIRYsfH0MH",
   "metadata": {
    "id": "rTlIRYsfH0MH"
   },
   "outputs": [],
   "source": [
    "from unstructured.partition.html import partition_html\n",
    "from unstructured.cleaners.core import (\n",
    "    clean_extra_whitespace,\n",
    "    replace_unicode_quotes,\n",
    "    clean_non_ascii_chars,\n",
    "    clean_bullets\n",
    ")\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "def remove_colons(text: str) -> str:\n",
    "    return re.sub(r\":\", \"\", text)\n",
    "\n",
    "def clean_element_text(text: str) -> str:\n",
    "    \"\"\"Clean up text extracted from HTML using Unstructured.\"\"\"\n",
    "    text = clean_extra_whitespace(text)\n",
    "    text = replace_unicode_quotes(text)\n",
    "    text = clean_non_ascii_chars(text)\n",
    "    text = clean_bullets(text)\n",
    "    text = remove_colons(text)  # only remove \":\" characters\n",
    "    return text.strip()\n",
    "\n",
    "def ingest_and_clean_unstructured_html(html_path: str):\n",
    "    \"\"\"Extract and clean HTML text elements (Title, Table, etc.) returning simplified JSON.\"\"\"\n",
    "    elements = partition_html(filename=html_path)\n",
    "    cleaned_output = []\n",
    "\n",
    "    for el in elements:\n",
    "        if hasattr(el, 'text') and el.text:\n",
    "            cleaned_text = clean_element_text(el.text)\n",
    "            cleaned_output.append({\n",
    "                \"filename\": os.path.basename(html_path),\n",
    "                \"type\": el.category if hasattr(el, 'category') else el.__class__.__name__,\n",
    "                \"text\": cleaned_text\n",
    "            })\n",
    "    return cleaned_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "E5w844Z1zPWT",
   "metadata": {
    "id": "E5w844Z1zPWT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def process_html_directory(directory_path: str, output_json: str = \"clean_output_unstructured.json\"):\n",
    "    \"\"\"Process all HTML files in a directory and combine their extracted data.\"\"\"\n",
    "    all_results = []\n",
    "\n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.lower().endswith(\".html\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            print(f\"ðŸ” Processing: {file_path}\")\n",
    "\n",
    "            try:\n",
    "                output_html = ingest_and_clean_unstructured_html(file_path)\n",
    "                all_results.extend(output_html)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error processing {filename}: {e}\")\n",
    "\n",
    "    # Print the clean JSON output to console\n",
    "    # print(json.dumps(all_results, indent=2, ensure_ascii=False))\n",
    "\n",
    "    # Save all combined results to one JSON file\n",
    "    with open(output_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"âœ… Finished! Output saved to: {output_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "Lib4Dj_0zTfU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Lib4Dj_0zTfU",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "aec9c660-de73-4e66-ccf1-0ca6ed54971b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Processing: html_docs/EHR-2024-11-789456.html\n",
      "ðŸ” Processing: html_docs/EHR-2025-12-000001.html\n",
      "ðŸ” Processing: html_docs/EHR-2025-12-000002.html\n",
      "ðŸ” Processing: html_docs/EHR-2024-11-789351.html\n",
      "ðŸ” Processing: html_docs/EHR-2024-11-456123.html\n",
      "ðŸ” Processing: html_docs/EHR-2024-11-789321.html\n",
      "ðŸ” Processing: html_docs/EHR-2024-11-654987.html\n",
      "âœ… Finished! Output saved to: clean_output_unstructured.json\n"
     ]
    }
   ],
   "source": [
    "directory = \"html_docs\"\n",
    "process_html_directory(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_QJ-u2TSkpv1",
   "metadata": {
    "id": "_QJ-u2TSkpv1"
   },
   "source": [
    "### Unstructured Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zlvjx1Zngtu7",
   "metadata": {
    "id": "zlvjx1Zngtu7"
   },
   "source": [
    "Let's take a look at a sample output of the file `EHR-2024-11-456123.html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vE2oJDtid-fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vE2oJDtid-fe",
    "outputId": "5a3e856d-4ec4-4126-95bc-2851da74dd49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'filename': 'EHR-2024-11-456123.html',\n",
       "  'type': 'Title',\n",
       "  'text': 'PATIENT MEDICAL SUMMARY'},\n",
       " {'filename': 'EHR-2024-11-456123.html',\n",
       "  'type': 'UncategorizedText',\n",
       "  'text': 'Metropolitan Cancer Center | Electronic Health Record'},\n",
       " {'filename': 'EHR-2024-11-456123.html',\n",
       "  'type': 'UncategorizedText',\n",
       "  'text': 'Generated November 15, 2024 | Report ID EHR-2024-11-456123'},\n",
       " {'filename': 'EHR-2024-11-456123.html',\n",
       "  'type': 'Title',\n",
       "  'text': 'Patient Name'},\n",
       " {'filename': 'EHR-2024-11-456123.html',\n",
       "  'type': 'UncategorizedText',\n",
       "  'text': 'DAVIDSON, JAMES'},\n",
       " {'filename': 'EHR-2024-11-456123.html',\n",
       "  'type': 'Title',\n",
       "  'text': 'Medical Record Number'},\n",
       " {'filename': 'EHR-2024-11-456123.html',\n",
       "  'type': 'UncategorizedText',\n",
       "  'text': 'MRN-2024-456123'},\n",
       " {'filename': 'EHR-2024-11-456123.html',\n",
       "  'type': 'Title',\n",
       "  'text': 'Date of Birth'},\n",
       " {'filename': 'EHR-2024-11-456123.html',\n",
       "  'type': 'UncategorizedText',\n",
       "  'text': '04/10/1962 (62 years)'},\n",
       " {'filename': 'EHR-2024-11-456123.html', 'type': 'Title', 'text': 'Sex'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON data from the uploaded file\n",
    "file_path = \"/content/clean_output_unstructured.json\"\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Filter records by filename\n",
    "filtered_records = [record for record in data if record[\"filename\"] == \"EHR-2024-11-456123.html\"]\n",
    "\n",
    "# Print first 10 records\n",
    "filtered_records[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816eef93",
   "metadata": {
    "id": "816eef93"
   },
   "source": [
    "## 2ï¸âƒ£ SparkNLP: Reader2Doc and DocumentNormalizer for HTML\n",
    "\n",
    "SparkNLP can read and normalize HTML text using the **Reader2Doc** component followed by **DocumentNormalizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ao4nrabyMILY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ao4nrabyMILY",
    "outputId": "c0059531-d002-4544-cbbf-7e636a44362d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m317.4/317.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dataproc-spark-connect 1.0.1 requires pyspark[connect]~=4.0.0, but you have pyspark 3.5.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q --force-reinstall pyspark==3.5.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dA5i4A7kXVQ5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dA5i4A7kXVQ5",
    "outputId": "9a912a41-103a-4214-fd07-4763c8078964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spark-nlp\n",
      "  Downloading spark_nlp-6.3.1-py2.py3-none-any.whl.metadata (19 kB)\n",
      "Downloading spark_nlp-6.3.1-py2.py3-none-any.whl (745 kB)\n",
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/745.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m745.1/745.1 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: spark-nlp\n",
      "Successfully installed spark-nlp-6.3.1\n"
     ]
    }
   ],
   "source": [
    " !pip install spark-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1aef43fc",
   "metadata": {
    "id": "1aef43fc"
   },
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.reader.reader2doc import Reader2Doc\n",
    "from sparknlp.annotator import DocumentNormalizer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "Q24VZICGKfCc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q24VZICGKfCc",
    "outputId": "7a008db6-3935-440c-f073-f2e32fef1e5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.7\n",
      "Scala Version: version 2.12.18\n",
      "Java Version: 17.0.17\n"
     ]
    }
   ],
   "source": [
    "print(\"Spark version:\", spark.version)\n",
    "print(\"Scala Version:\", spark.sparkContext._jvm.scala.util.Properties.versionString())\n",
    "print(\"Java Version:\", spark.sparkContext._jvm.java.lang.System.getProperty(\"java.version\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "JKJvJWATUoUr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JKJvJWATUoUr",
    "outputId": "2b5c370e-dfe8-4559-f713-5688b84e0300"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_detector_dl download started this may take some time.\n",
      "Approximate size to download 354.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import SentenceDetectorDLModel\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "empty_df = spark.createDataFrame([], 'string').toDF('text')\n",
    "\n",
    "reader2doc = Reader2Doc() \\\n",
    "    .setContentType('text/html') \\\n",
    "    .setContentPath(directory) \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "normalizer = DocumentNormalizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('normalized') \\\n",
    "    .setAutoMode(\"HTML_CLEAN\") \\\n",
    "    .setPatterns([(\":\")])\n",
    "\n",
    "sentence_detector = SentenceDetectorDLModel() \\\n",
    "    .pretrained() \\\n",
    "    .setInputCols(['normalized']) \\\n",
    "    .setOutputCol('sentences') \\\n",
    "    .setExplodeSentences(True)\n",
    "\n",
    "pipeline = Pipeline(stages=[reader2doc, normalizer, sentence_detector])\n",
    "model = pipeline.fit(empty_df)\n",
    "result_df = model.transform(empty_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bH-XIbjTkzEG",
   "metadata": {
    "id": "bH-XIbjTkzEG"
   },
   "source": [
    "### SparkNLP Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M_bsIMzCe5-w",
   "metadata": {
    "id": "M_bsIMzCe5-w"
   },
   "source": [
    "Let's take a look at a sample output of the for the file `EHR-2024-11-456123.html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "IHefITKiRSAP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IHefITKiRSAP",
    "outputId": "43e183eb-e16f-433e-e6e3-78a6520b8b1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|result                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[PATIENT MEDICAL SUMMARY Metropolitan Cancer Center | Electronic Health Record Generated November 15, 2024 | Report ID EHR-2024-11-456123 Patient Name DAVIDSON, JAMES Medical Record Number MRN-2024-456123 Date of Birth 04/10/1962 (62 years) Sex Male Primary Physician Dr. Michael Chen, MD Last Visit 11/15/2024 ?]                                                                                                                                                                                                                                                                                                                                    |\n",
      "|[Active & Chronic Diagnoses Non-Small Cell Lung Carcinoma (Active) Histology Adenocarcinoma Location Left upper lobe, apicoposterior segment Stage T2bN0M0 (Stage II) Tumor Size 3.1 cm Diagnosed March 2024 Status Treatment planning - Surgery + adjuvant chemotherapy Chronic Obstructive Pulmonary Disease (Chronic) Diagnosed 2018 Status Mild-moderate, managed with bronchodilators FEV1 2.10 L (65% predicted) Former Tobacco Use Disorder (Resolved) Pack-years 15 pack-years Quit Date 2006 (18 years smoke-free) Status Former smoker, excellent cessation Essential Hypertension (Chronic) Diagnosed 2015 Status Well-controlled on medication ?]|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "filtered_df = result_df.filter(col(\"filename\") == \"EHR-2024-11-456123.html\")\n",
    "filtered_df.select(\"sentences.result\").show(truncate=False, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QM21iySgfIeC",
   "metadata": {
    "id": "QM21iySgfIeC"
   },
   "source": [
    "When comparing with the output from Unstructured, we observe that:\n",
    "1. The desired output can be obtained without any additional code, by simply leveraging Spark SQL\n",
    "2. The output provides the raw text directly, without applying any categorization or structural labeling. So, in case we need this kind of data we don't need to create an additional normalization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "WbxQmoPieCJc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "WbxQmoPieCJc",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "620cca0c-0787-40df-f916-a44ef8ae125e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Clean JSON saved to clean_output.json\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode, col\n",
    "import json\n",
    "\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "flat_df = (\n",
    "    result_df\n",
    "    .withColumn(\"sentence\", explode(\"sentences\"))  # explode the correct column\n",
    "    .select(\n",
    "        col(\"filename\"),\n",
    "        col(\"sentence.result\").alias(\"result\")      # extract the text part\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# --- Collect to driver (if small enough) ---\n",
    "data = flat_df.toJSON().map(json.loads).collect()\n",
    "\n",
    "# --- Pretty print or save ---\n",
    "#print(json.dumps(data, indent=2, ensure_ascii=False))\n",
    "\n",
    "# Optional: save as a JSON file\n",
    "with open(\"clean_output_sparknlp.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… Clean JSON saved to clean_output.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ui0_BndtitF0",
   "metadata": {
    "collapsed": true,
    "id": "ui0_BndtitF0",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"clean_output_sparknlp.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    clean_output_sparknlp = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1yZmA2OwFO4u",
   "metadata": {
    "id": "1yZmA2OwFO4u"
   },
   "source": [
    "## Tokens Comparison Unstructured vs SparkNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1jOaFpLEjJoD",
   "metadata": {
    "id": "1jOaFpLEjJoD"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "with open(\"clean_output_unstructured.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    clean_output_unstructured = json.load(f)\n",
    "\n",
    "def flatten_unstructured_output(data):\n",
    "    grouped = defaultdict(list)\n",
    "    for entry in data:\n",
    "        grouped[entry[\"filename\"]].append(entry[\"text\"])\n",
    "\n",
    "    flattened = []\n",
    "    for filename, texts in grouped.items():\n",
    "        joined_text = \" \".join(texts)\n",
    "        flattened.append({\n",
    "            \"filename\": filename,\n",
    "            \"result\": joined_text\n",
    "        })\n",
    "    return flattened\n",
    "\n",
    "flaten_output_unstructured = flatten_unstructured_output(clean_output_unstructured)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "jIVWYwXQjevQ",
   "metadata": {
    "id": "jIVWYwXQjevQ"
   },
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70MvI7SRDuhG",
   "metadata": {
    "id": "70MvI7SRDuhG"
   },
   "outputs": [],
   "source": [
    "# Create dictionaries: filename â†’ tokens\n",
    "tokens_unstructured_dict = {\n",
    "    item[\"filename\"]: tokenize(item[\"result\"])\n",
    "    for item in flaten_output_unstructured\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8YRN6GW793HW",
   "metadata": {
    "id": "8YRN6GW793HW"
   },
   "outputs": [],
   "source": [
    "tokens_sparknlp_dict = {\n",
    "    item[\"filename\"]: tokenize(item[\"result\"])\n",
    "    for item in clean_output_sparknlp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "FOToYAJRtocB",
   "metadata": {
    "id": "FOToYAJRtocB"
   },
   "outputs": [],
   "source": [
    "# --- Loop and append tokens ---\n",
    "for item in clean_output_sparknlp:\n",
    "    filename = item[\"filename\"]\n",
    "    text = item[\"result\"]\n",
    "    tokens = tokenize(text)\n",
    "    tokens_sparknlp_dict[filename].extend(tokens)  # âœ… append instead of overwrite\n",
    "\n",
    "# --- Convert back to a normal dict ---\n",
    "tokens_sparknlp_dict = dict(tokens_sparknlp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "_eMs06QyJGOR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_eMs06QyJGOR",
    "outputId": "a7b78222-14f8-47e3-8e9e-98825b91e641"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-08 16:54:18--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/SPARKNLP-1320-Benchmarkunstructured-vs-SparkNLP-in-unstructured-document-processing/open-source-nlp/data/readers/benchmark/ehr_tokens_ground_truth.json\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 152654 (149K) [text/plain]\n",
      "Saving to: â€˜ehr_tokens_ground_truth.jsonâ€™\n",
      "\n",
      "\r",
      "          ehr_token   0%[                    ]       0  --.-KB/s               \r",
      "ehr_tokens_ground_t 100%[===================>] 149.08K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2026-01-08 16:54:18 (51.5 MB/s) - â€˜ehr_tokens_ground_truth.jsonâ€™ saved [152654/152654]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/open-source-nlp/data/readers/benchmark/ehr_tokens_ground_truth.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "SowK-wP3GuVj",
   "metadata": {
    "id": "SowK-wP3GuVj"
   },
   "outputs": [],
   "source": [
    "with open(\"ehr_tokens_ground_truth.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    tokens_groundtruth_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "rCEw3AYxCV5S",
   "metadata": {
    "id": "rCEw3AYxCV5S"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def normalize_token(t):\n",
    "    \"\"\"Normalize tokens for reliable comparison.\"\"\"\n",
    "    t = unicodedata.normalize(\"NFKC\", t)  # normalize unicode\n",
    "    t = re.sub(r\"\\s+\", \" \", t.strip())    # collapse whitespace\n",
    "    return t.lower()\n",
    "\n",
    "def compare_sequence(predicted_tokens, groundtruth_tokens, output_dir=\".\", prefix=\"comparison\"):\n",
    "    \"\"\"\n",
    "    Compare two token lists ignoring case, order, and Unicode issues.\n",
    "    Properly surfaces missing/extra tokens even when duplicates exist.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Normalize ---\n",
    "    pred_norm = [normalize_token(t) for t in predicted_tokens]\n",
    "    gt_norm = [normalize_token(t) for t in groundtruth_tokens]\n",
    "\n",
    "    # --- Convert to sets for difference detection ---\n",
    "    pred_set, gt_set = set(pred_norm), set(gt_norm)\n",
    "\n",
    "    missing_in_pred = sorted(list(gt_set - pred_set))  # in GT but not in pred\n",
    "    extra_in_pred = sorted(list(pred_set - gt_set))    # in pred but not in GT\n",
    "\n",
    "    # --- Compute metrics ---\n",
    "    intersection = len(pred_set & gt_set)\n",
    "    union = len(pred_set | gt_set)\n",
    "    jaccard = round(intersection / union, 4) if union > 0 else 1.0\n",
    "    match = pred_set == gt_set\n",
    "\n",
    "    # --- Build results ---\n",
    "    result_data = {\n",
    "        \"summary\": {\n",
    "            \"match\": match,\n",
    "            \"pred_count\": len(pred_norm),\n",
    "            \"gt_count\": len(gt_norm),\n",
    "            \"jaccard_similarity\": jaccard,\n",
    "            \"missing_count\": len(missing_in_pred),\n",
    "            \"extra_count\": len(extra_in_pred),\n",
    "        },\n",
    "        \"details\": {\n",
    "            \"missing_in_pred\": missing_in_pred,\n",
    "            \"extra_in_pred\": extra_in_pred,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # --- Save results ---\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, f\"{prefix}_sequence_comparison.json\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(result_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"âœ… Comparison done â€” Match: {match}, Jaccard: {jaccard:.4f}\")\n",
    "    print(f\"ðŸ”¸ Missing tokens: {len(missing_in_pred)}\")\n",
    "    print(f\"ðŸ”¸ Extra tokens:   {len(extra_in_pred)}\")\n",
    "    print(f\"ðŸ“ Saved to: {output_path}\")\n",
    "\n",
    "    return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "iNJXkWLi6gUd",
   "metadata": {
    "id": "iNJXkWLi6gUd"
   },
   "outputs": [],
   "source": [
    "def match_groundtruth(html_filename, groundtruth_dict):\n",
    "    \"\"\"\n",
    "    Find the corresponding ground truth entry for a given HTML filename.\n",
    "    Matches by EHR ID (e.g., EHR-2024-11-654987).\n",
    "    \"\"\"\n",
    "    match = re.search(r\"(EHR-\\d{4}-\\d{2}-\\d+)\", html_filename)\n",
    "    if not match:\n",
    "        return None\n",
    "    ehr_id = match.group(1)\n",
    "    for gt_name in groundtruth_dict.keys():\n",
    "        if ehr_id in gt_name:\n",
    "            return gt_name\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "GM--uRu26oKo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GM--uRu26oKo",
    "outputId": "1770aa64-2460-4471-d3f7-4cee89cedd50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ No ground truth found for EHR-2024-11-789456.html\n",
      "ðŸ” Comparing EHR-2025-12-000001.html â†” EHR-2025-12-000001.txt\n",
      "âœ… Comparison done â€” Match: False, Jaccard: 0.9787\n",
      "ðŸ”¸ Missing tokens: 4\n",
      "ðŸ”¸ Extra tokens:   5\n",
      "ðŸ“ Saved to: outputs/tmp/_sequence_comparison.json\n",
      "âœ… Comparison done â€” Match: False, Jaccard: 0.9645\n",
      "ðŸ”¸ Missing tokens: 10\n",
      "ðŸ”¸ Extra tokens:   5\n",
      "ðŸ“ Saved to: outputs/tmp/_sequence_comparison.json\n",
      "âš ï¸ No ground truth found for EHR-2025-12-000002.html\n",
      "ðŸ” Comparing EHR-2024-11-789351.html â†” EHR-2024-11-789351_text_only.txt\n",
      "âœ… Comparison done â€” Match: False, Jaccard: 0.9766\n",
      "ðŸ”¸ Missing tokens: 5\n",
      "ðŸ”¸ Extra tokens:   8\n",
      "ðŸ“ Saved to: outputs/tmp/_sequence_comparison.json\n",
      "âœ… Comparison done â€” Match: False, Jaccard: 0.9676\n",
      "ðŸ”¸ Missing tokens: 10\n",
      "ðŸ”¸ Extra tokens:   8\n",
      "ðŸ“ Saved to: outputs/tmp/_sequence_comparison.json\n",
      "ðŸ” Comparing EHR-2024-11-456123.html â†” EHR-2024-11-456123_text_only.txt\n",
      "âœ… Comparison done â€” Match: False, Jaccard: 0.9713\n",
      "ðŸ”¸ Missing tokens: 7\n",
      "ðŸ”¸ Extra tokens:   7\n",
      "ðŸ“ Saved to: outputs/tmp/_sequence_comparison.json\n",
      "âœ… Comparison done â€” Match: False, Jaccard: 0.9507\n",
      "ðŸ”¸ Missing tokens: 17\n",
      "ðŸ”¸ Extra tokens:   7\n",
      "ðŸ“ Saved to: outputs/tmp/_sequence_comparison.json\n",
      "ðŸ” Comparing EHR-2024-11-789321.html â†” EHR-2024-11-789321_text_only.txt\n",
      "âœ… Comparison done â€” Match: False, Jaccard: 0.9784\n",
      "ðŸ”¸ Missing tokens: 5\n",
      "ðŸ”¸ Extra tokens:   6\n",
      "ðŸ“ Saved to: outputs/tmp/_sequence_comparison.json\n",
      "âœ… Comparison done â€” Match: False, Jaccard: 0.9686\n",
      "ðŸ”¸ Missing tokens: 10\n",
      "ðŸ”¸ Extra tokens:   6\n",
      "ðŸ“ Saved to: outputs/tmp/_sequence_comparison.json\n",
      "ðŸ” Comparing EHR-2024-11-654987.html â†” EHR-2024-11-654987_text_only.txt\n",
      "âœ… Comparison done â€” Match: False, Jaccard: 0.9711\n",
      "ðŸ”¸ Missing tokens: 4\n",
      "ðŸ”¸ Extra tokens:   9\n",
      "ðŸ“ Saved to: outputs/tmp/_sequence_comparison.json\n",
      "âœ… Comparison done â€” Match: False, Jaccard: 0.9578\n",
      "ðŸ”¸ Missing tokens: 10\n",
      "ðŸ”¸ Extra tokens:   9\n",
      "ðŸ“ Saved to: outputs/tmp/_sequence_comparison.json\n",
      "âœ… All comparison results saved to outputs/all_comparisons.json\n"
     ]
    }
   ],
   "source": [
    "import os, re, json\n",
    "\n",
    "# Master results dictionary\n",
    "all_results = {}\n",
    "\n",
    "# Main loop through all HTMLs\n",
    "for html_name in tokens_unstructured_dict.keys():\n",
    "    gt_name = match_groundtruth(html_name, tokens_groundtruth_dict)\n",
    "\n",
    "    if gt_name is None:\n",
    "        print(f\"âš ï¸ No ground truth found for {html_name}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"ðŸ” Comparing {html_name} â†” {gt_name}\")\n",
    "    all_results[html_name] = {}\n",
    "\n",
    "    # Compare SparkNLP vs Ground Truth\n",
    "    if html_name in tokens_sparknlp_dict:\n",
    "        result_spark = compare_sequence(\n",
    "            tokens_sparknlp_dict[html_name],\n",
    "            tokens_groundtruth_dict[gt_name],\n",
    "            output_dir=\"outputs/tmp\",  # disable per-file output\n",
    "            prefix=\"\"\n",
    "        )\n",
    "        all_results[html_name][\"sparknlp\"] = result_spark\n",
    "\n",
    "    # Compare Unstructured vs Ground Truth\n",
    "    if html_name in tokens_unstructured_dict:\n",
    "        result_unstructured = compare_sequence(\n",
    "            tokens_unstructured_dict[html_name],\n",
    "            tokens_groundtruth_dict[gt_name],\n",
    "            output_dir=\"outputs/tmp\",\n",
    "            prefix=\"\"\n",
    "        )\n",
    "        all_results[html_name][\"unstructured\"] = result_unstructured\n",
    "\n",
    "\n",
    "# Save single consolidated JSON\n",
    "output_path = \"outputs/all_comparisons.json\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"âœ… All comparison results saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
