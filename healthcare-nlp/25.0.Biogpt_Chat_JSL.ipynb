{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n"
   ],
   "metadata": {
    "id": "X64RTULpsvUT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/healthcare-nlp/25.0.Biogpt_Chat_JSL.ipynb)\n",
    "\n"
   ],
   "metadata": {
    "id": "TGSKYVuqsuE5"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BC5b1eU_QDg"
   },
   "source": [
    "# **BioGPT - Chat JSL - Closed Book Question Answering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEjqwB2PHuuS"
   },
   "source": [
    "The objective of this notebook is to explore the Biomedical Generative Pre-trained Transformer (BioGPT) models - `biogpt_chat_jsl` and `biogpt_chat_jsl_conversational_en`, for closed book question answering. These models are pre-trained on large biomedical text data and can generate coherent and relevant responses to biomedical questions.\n",
    "\n",
    "üìñ Learning Objectives:\n",
    "\n",
    "- Learn how to use the BioGPT models in Spark NLP for closed book question answering tasks, including loading pre-trained models and configuring the pipeline.\n",
    "\n",
    "- Understand the parameters and options available for the BioGPT models to customize the text generation process based on specific use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okhT7AcXxben"
   },
   "source": [
    "# ‚öíÔ∏è Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7dOaR_TlgE-"
   },
   "source": [
    "üìåTo run this yourself, you will need to upload your license keys to the notebook. Just Run The Cell Below in order to do that. Also You can open the file explorer on the left side of the screen and upload `license_keys.json` to the folder that opens.\n",
    "Otherwise, you can look at the example outputs at the bottom of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQLe_InJtnzA"
   },
   "outputs": [],
   "source": [
    "# Install the johnsnowlabs library to access Spark-OCR and Spark-NLP for Healthcare, Finance, and Legal.\n",
    "! pip install -q johnsnowlabs"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "print('Please Upload your John Snow Labs License using the button below')\n",
    "license_keys = files.upload()"
   ],
   "metadata": {
    "id": "Jjj9gCdWMXyF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from johnsnowlabs import nlp, medical, visual\n",
    "\n",
    "# After uploading your license run this to install all licensed Python Wheels and pre-download Jars the Spark Session JVM\n",
    "nlp.install()"
   ],
   "metadata": {
    "id": "L1LFkCjFMyxi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from johnsnowlabs import nlp, medical, visual\n",
    "import pandas as pd\n",
    "\n",
    "# Automatically load license data and start a session with all jars user has access to\n",
    "spark = nlp.start()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fCy9pQxhhIkD",
    "outputId": "17535fcf-910b-4fa1-dd1f-6053b6001606"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql as SQL\n",
    "from pyspark import keyword_only\n",
    "import textwrap"
   ],
   "metadata": {
    "id": "gTVeDWGmhKuk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-62Qs6RAIC1V"
   },
   "source": [
    "# \tüìéüè• `biogpt_chat_jsl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twxRx_PGIGm7"
   },
   "source": [
    "This model is based on BioGPT finetuned with medical conversations happening in a clinical settings and can answer clinical questions related to symptoms, drugs, tests, and diseases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCFN2tYF3X-Z",
    "outputId": "af2b35ba-18f1-4fd6-8120-c3dc2a92fb0a"
   },
   "outputs": [],
   "source": [
    "document_assembler = nlp.DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"documents\")\n",
    "    \n",
    "gpt_qa = medical.TextGenerator().pretrained(\"biogpt_chat_jsl\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols(\"documents\")\\\n",
    "    .setOutputCol(\"answer\")\\\n",
    "    .setMaxNewTokens(299)\\\n",
    "    .setStopAtEos(True)\\\n",
    "    .setDoSample(False)\\\n",
    "    .setTopK(3)\\\n",
    "    .setRandomSeed(42)\n",
    "    \n",
    "pipeline = nlp.Pipeline().setStages([document_assembler, gpt_qa])\n",
    "\n",
    "question = \"What medications are commonly used to treat emphysema?\"\n",
    "TEXT = [f\"question: {question} answer:\"]\n",
    "data = spark.createDataFrame([TEXT]).toDF(\"text\")\n",
    "\n",
    "result = pipeline.fit(data).transform(data)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9vKgjtjIjLA",
    "outputId": "e4d3b80e-1294-4cb9-f109-697a4daa7d55"
   },
   "outputs": [],
   "source": [
    "result.select(\"answer.result\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv3a3Mm8aTh6"
   },
   "source": [
    "## **üìç LightPipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIMoP1-fvfF1",
    "outputId": "577cde63-4134-4168-9e42-ac882217ffdf"
   },
   "outputs": [],
   "source": [
    "gpt_qa = medical.TextGenerator().pretrained(\"biogpt_chat_jsl\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols(\"documents\")\\\n",
    "    .setOutputCol(\"answer\")\\\n",
    "    .setMaxNewTokens(299)\\\n",
    "    .setStopAtEos(True)\\\n",
    "    .setDoSample(False)\\\n",
    "    .setTopK(3)\\\n",
    "    .setRandomSeed(42)\n",
    "    \n",
    "pipeline = nlp.Pipeline().setStages([document_assembler, gpt_qa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSfsStyhCUKa"
   },
   "outputs": [],
   "source": [
    "question = \"What are the risk factors for developing heart disease?\"\n",
    "TEXT = [f\"question: {question} answer:\"]\n",
    "\n",
    "model = pipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\"))\n",
    "light_model = nlp.LightPipeline(model)\n",
    "light_result = light_model.annotate(TEXT)\n",
    "answer_text = light_result[0][\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RfIJU_0ACYfU",
    "outputId": "01923c3f-0645-4b3f-af50-10521a011757"
   },
   "outputs": [],
   "source": [
    "# Extract the text after 'answer:'\n",
    "final_answer = answer_text[0][len(TEXT[0]) + 1:].strip()\n",
    "\n",
    "# Format the text into paragraphs\n",
    "wrapped_text = textwrap.fill(final_answer, width=120)\n",
    "\n",
    "print(\"‚û§ Answer: \\n{}\".format(wrapped_text))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ta3HNysnaYy4"
   },
   "source": [
    "## üö© `setMaxNewTokens`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fpp0aiEzbH37"
   },
   "source": [
    "- This parameter sets the maximum number of new tokens that the GPT model will generate for the output, constraining the length of the generated response and managing the computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrdPu1wK8-Pw"
   },
   "source": [
    "Pipeline with `setMaxNewTokens(128)` and `setMaxNewTokens(299)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jbbh6hy_w11",
    "outputId": "7fa0259a-ac5f-4196-9e13-b3d198d05599"
   },
   "outputs": [],
   "source": [
    "# Default parameters\n",
    "gpt_qa = medical.TextGenerator().pretrained(\"biogpt_chat_jsl\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols(\"documents\") \\\n",
    "    .setOutputCol(\"answer\") \\\n",
    "    .setStopAtEos(True)\\\n",
    "    .setDoSample(False)\\\n",
    "    .setTopK(3) \\\n",
    "    .setRandomSeed(42)\n",
    "\n",
    "MaxNewTokens = [128, 299]\n",
    "\n",
    "\n",
    "# Sample question\n",
    "question = \"How can asthma be treated?\"\n",
    "TEXT = [f\"question: {question} answer:\"]\n",
    "\n",
    "for j in MaxNewTokens:\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Parameters:\") \n",
    "    print(f\"\\nsetMaxNewTokens({j}):\")\n",
    "    gpt_qa.setMaxNewTokens(j)\n",
    "    pipeline = nlp.Pipeline().setStages([document_assembler, gpt_qa])\n",
    "\n",
    "    light_model = nlp.LightPipeline(pipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\")))\n",
    "    answer_default = light_model.annotate(TEXT)\n",
    "    \n",
    "    answer_text = answer_default[0][\"answer\"][0][len(TEXT[0]) + 1:].strip()\n",
    "    wrapped_answer_text = textwrap.fill(answer_text, width=150)\n",
    "    token_count = len(answer_text.split())\n",
    "    print(\"‚û§ Answer:\")\n",
    "    print(wrapped_answer_text)\n",
    "    print(f\"Number of tokens used: {token_count}\")\n",
    "    print(\"-\" * 40)  # Separator line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b><h1><font color='darkred'>!!! ATTENTION !!! </font><h1><b>\n",
    "\n",
    "<b>before running the following cells, <font color='darkred'>RESTART the COLAB RUNTIME </font> than start your session and go ahead.<b>"
   ],
   "metadata": {
    "id": "M6sjuM3NW-ZS"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYOrd_2OLSyD"
   },
   "source": [
    "# \tüìéüè• `biogpt_chat_jsl_conversational`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4vCaKWzyBCX"
   },
   "source": [
    "This model is based on BioGPT finetuned with medical conversations happening in a clinical settings and can answer clinical questions related to symptoms, drugs, tests, and diseases. The difference between this model and `biogpt_chat_jsl` is that this model produces more concise/smaller response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uXW20vHBLd3u",
    "outputId": "dd01f049-9c93-4345-a5a4-be0bdb6915c4"
   },
   "outputs": [],
   "source": [
    "document_assembler = nlp.DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"documents\")\n",
    "    \n",
    "gpt_qa = medical.TextGenerator().pretrained(\"biogpt_chat_jsl_conversational\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols(\"documents\")\\\n",
    "    .setOutputCol(\"answer\")\\\n",
    "    .setMaxNewTokens(399)\\\n",
    "    .setStopAtEos(True)\\\n",
    "    .setDoSample(False)\\\n",
    "    .setTopK(1)\\\n",
    "    .setRandomSeed(42)\n",
    "    \n",
    "pipeline = nlp.Pipeline().setStages([document_assembler, gpt_qa])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zy_Jt_MkCfvn"
   },
   "outputs": [],
   "source": [
    "question = \"What is the difference between melanoma and sarcoma?\"\n",
    "TEXT = [f\"question: {question} answer:\"]\n",
    "\n",
    "model = pipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\"))\n",
    "light_model = nlp.LightPipeline(model)\n",
    "light_result = light_model.annotate(TEXT)\n",
    "answer_text = light_result[0][\"answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pqxGMy4Chmz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2d22d05a-4ff7-4ce4-f260-f6fda510ec46"
   },
   "outputs": [],
   "source": [
    "# Extract the text after 'answer:'\n",
    "final_answer = answer_text[0][len(TEXT[0]) + 1:].strip()\n",
    "\n",
    "# Format the text into paragraphs\n",
    "wrapped_text = textwrap.fill(final_answer, width=120)\n",
    "\n",
    "print(\"‚û§ Answer: \\n{}\".format(wrapped_text))\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}