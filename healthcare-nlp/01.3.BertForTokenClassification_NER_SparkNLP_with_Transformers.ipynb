{"cells":[{"cell_type":"markdown","metadata":{"id":"gbt8Q6j3RJxW"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"]},{"cell_type":"markdown","metadata":{"id":"xZOMrR1fRZq3"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/healthcare-nlp/01.3.BertForTokenClassification_NER_SparkNLP_with_Transformers.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"dhiz8WdthoJm"},"source":["# BertForTokenClassification NER Model Training with Transformers\n","\n","In this notebook, you will find how to train BertForTokenClassification NER model with transformers and then import into Spark NLP. (There is no Approach() in this notebook, so you can use only transformers for training.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D5W96WSqSPOc"},"outputs":[],"source":["# %%capture\n","! pip install -q seqeval\n","! pip install -q transformers==4.25.1 \n","! pip install -q tensorflow==2.11.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ELqzaf32MT6E"},"outputs":[],"source":["# Install the johnsnowlabs library to access Spark-OCR and Spark-NLP for Healthcare, Finance, and Legal.\n","! pip install -q johnsnowlabs "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RO2dIA414yL_"},"outputs":[],"source":["from google.colab import files\n","print('Please Upload your John Snow Labs License using the button below')\n","license_keys = files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dmcB5zVBHZO8"},"outputs":[],"source":["from johnsnowlabs import nlp, medical\n","\n","# # After uploading your license run this to install all licensed Python Wheels and pre-download Jars the Spark Session JVM\n","nlp.install()"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7597,"status":"ok","timestamp":1681380572578,"user":{"displayName":"Monster C","userId":"08787989274818793476"},"user_tz":-180},"id":"lQ8-BI-_5QjG","outputId":"a60fbe91-7b11-4f14-c6de-564541d97369"},"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ‘Œ Detected license file /content/4.3.2.spark_nlp_for_healthcare.json\n","ðŸ‘Œ Launched \u001b[92mcpu optimized\u001b[39m session with with: ðŸš€Spark-NLP==4.3.2, ðŸ’ŠSpark-Healthcare==4.3.2, running on âš¡ PySpark==3.1.2\n"]}],"source":["from johnsnowlabs import nlp, medical\n","import pandas as pd\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Automatically load license data and start a session with all jars user has access to\n","spark = nlp.start()"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":7824,"status":"ok","timestamp":1681380580398,"user":{"displayName":"Monster C","userId":"08787989274818793476"},"user_tz":-180},"id":"Bz7nYxDbk_xV","outputId":"6229a6fe-7bef-403f-cb7f-e92942accfe1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Tesla T4'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}],"source":["import os\n","import json\n","import numpy as np\n","from tqdm import tqdm, trange\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import train_test_split\n","from keras.utils import pad_sequences\n","#from keras_preprocessing.sequence import pad_sequences\n","\n","import transformers\n","from transformers import BertTokenizer, BertConfig\n","from transformers import BertForTokenClassification, TFBertForTokenClassification, AdamW\n","from transformers import get_linear_schedule_with_warmup\n","\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","\n","torch.cuda.get_device_name(0)"]},{"cell_type":"markdown","metadata":{"id":"5kKaWwaX6Pos"},"source":["## Download NCBI Disease CoNLL Dataset"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"iBEuXQnw6VPG","executionInfo":{"status":"ok","timestamp":1681380581503,"user_tz":-180,"elapsed":1110,"user":{"displayName":"Monster C","userId":"08787989274818793476"}}},"outputs":[],"source":["!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/data/NER_NCBIconlltrain.txt\n","!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/data/NER_NCBIconlltest.txt"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"hO0VnBGa81Wb","executionInfo":{"status":"ok","timestamp":1681381366634,"user_tz":-180,"elapsed":617,"user":{"displayName":"Monster C","userId":"08787989274818793476"}}},"outputs":[],"source":["PROJECT_NAME = 'ner_disease_main'\n","\n","train_set =  \"NER_NCBIconlltrain.txt\"\n","test_set = \"NER_NCBIconlltest.txt\"\n","\n","test_metrics = True\n","\n","# select any Bert model from >> https://huggingface.co/models?pipeline_tag=token-classification&sort=downloads&search=bert\n","\n","MODEL_TO_TRAIN = 'dmis-lab/biobert-base-cased-v1.2'\n","# emilyalsentzer/Bio_ClinicalBERT\n","\n","# Defining some key variables that will be used later on in the training\n","MAX_LEN = 128 # 512\n","TRAIN_BATCH_SIZE = 64 # 8\n","VALID_BATCH_SIZE = 64 # 8\n","EPOCHS = 5\n","LEARNING_RATE = 2e-05\n","\n","!mkdir -p {PROJECT_NAME}\n","!mkdir -p {PROJECT_NAME}/logs"]},{"cell_type":"markdown","metadata":{"id":"iebvp7r-t4tl"},"source":["## Run the follwing cells with no change"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"Ct9KClpFaZCe","executionInfo":{"status":"ok","timestamp":1681381367452,"user_tz":-180,"elapsed":3,"user":{"displayName":"Monster C","userId":"08787989274818793476"}}},"outputs":[],"source":["from pyspark.sql import DataFrame\n","import pyspark.sql.functions as F\n","import pyspark.sql.types as T\n","import pyspark.sql as SQL\n","from pyspark import keyword_only"]},{"cell_type":"code","source":["def get_conll_df(pth):\n","  data = nlp.CoNLL().readDataset(spark, pth)\n","  data = data.withColumn(\"sentence_idx\", F.monotonically_increasing_id())\n","  data = data.withColumn('unique', F.array_distinct(\"label.result\"))\\\n","              .withColumn('c', F.size('unique'))\\\n","              .filter(F.col('c')>1)\n","\n","  df = data.select('sentence_idx', F.explode(F.arrays_zip(data.token.result,data.label.result,data.pos.result)).alias(\"cols\")) \\\n","          .select('sentence_idx',\n","                  F.expr(\"cols['0']\").alias(\"word\"),\n","                  F.expr(\"cols['1']\").alias(\"tag\"),\n","                  F.expr(\"cols['2']\").alias(\"pos\")).toPandas()\n","  \n","  return df\n","\n","\n","train_data_df = get_conll_df(train_set)\n","test_data_df = get_conll_df(test_set)\n","\n","print ('=== TRAINING SET DISTRIBUTION ===')\n","print (train_data_df['tag'].value_counts())\n","\n","print ('=== TEST SET DISTRIBUTION ===')\n","print (test_data_df['tag'].value_counts())\n","\n","if not test_metrics:\n","\n","  train_data_df = pd.concat([train_data_df, test_data_df])\n","\n","\n","## convert conll file to sentences\n","\n","class SentenceGetter(object):\n","    \n","    def __init__(self, dataset):\n","        self.n_sent = 1\n","        self.dataset = dataset\n","        self.empty = False\n","        agg_func = lambda s: [(w,p, t) for w,p, t in zip(s[\"word\"].values.tolist(),\n","                                                       s['pos'].values.tolist(),\n","                                                        s[\"tag\"].values.tolist())]\n","        self.grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","    \n","    def get_next(self):\n","        try:\n","            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None\n","\n","train_getter = SentenceGetter(train_data_df)\n","\n","if test_metrics:  \n","  test_getter = SentenceGetter(test_data_df)\n","\n","\n","print ('=== Getting sentences and labels ===')\n","\n","# Sentences \n","train_sentences = [[word[0] for word in sentence] for sentence in train_getter.sentences]\n","print(\"Example of train sentence:\")\n","print (train_sentences[5])\n","\n","if test_metrics:\n","  test_sentences = [[word[0] for word in sentence] for sentence in test_getter.sentences]\n","  print(\"Example of test sentence:\")\n","  print (test_sentences[5])\n","\n","# Labels\n","train_labels = [[s[2] for s in sentence] for sentence in train_getter.sentences]\n","print(\"Example of train sentence:\")\n","print(train_labels[5])\n","\n","if test_metrics:\n","  test_labels = [[s[2] for s in sentence] for sentence in test_getter.sentences]\n","  print(\"Example of test sentence:\")\n","  print(test_labels[5])\n","\n","\n","tag_values = list(set(train_data_df[\"tag\"].values))\n","tag_values.append(\"PAD\")\n","tag2idx = {t: i for i, t in enumerate(tag_values)}\n","\n","print(tag_values[:10])\n","print(tag2idx)\n","\n","tokenizer = BertTokenizer.from_pretrained(MODEL_TO_TRAIN, do_lower_case=False)\n","\n","\n","def tokenize_and_preserve_labels(sentence, text_labels):\n","    tokenized_sentence = []\n","    labels = []\n","\n","    for word, label in zip(sentence, text_labels):\n","\n","        # Tokenize the word and count # of subwords the word is broken into\n","        tokenized_word = tokenizer.tokenize(word)\n","        n_subwords = len(tokenized_word)\n","\n","        # Add the tokenized word to the final tokenized word list\n","        tokenized_sentence.extend(tokenized_word)\n","\n","        # Add the same label to the new list of labels `n_subwords` times\n","        labels.extend([label] * n_subwords)\n","\n","    return tokenized_sentence, labels\n","\n","\n","train_tokenized_texts_and_labels = [\n","    tokenize_and_preserve_labels(sent, labs)\n","    for sent, labs in zip(train_sentences, train_labels)\n","]\n","\n","if test_metrics:\n","\n","  test_tokenized_texts_and_labels = [\n","      tokenize_and_preserve_labels(sent, labs)\n","      for sent, labs in zip(test_sentences, test_labels)\n","  ]\n","\n","train_tokenized_texts_tokens = [token_label_pair[0] for token_label_pair in train_tokenized_texts_and_labels]\n","\n","if test_metrics:\n","  test_tokenized_texts_tokens = [token_label_pair[0] for token_label_pair in test_tokenized_texts_and_labels]\n","  print(test_tokenized_texts_tokens[5])\n","\n","train_tokenized_texts_labels = [token_label_pair[1] for token_label_pair in train_tokenized_texts_and_labels]\n","\n","if test_metrics:\n","  test_tokenized_texts_labels = [token_label_pair[1] for token_label_pair in test_tokenized_texts_and_labels]\n","  print(test_tokenized_texts_labels[5])\n","\n","\n","\n","train_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in train_tokenized_texts_tokens],\n","                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n","                          truncating=\"post\", padding=\"post\")\n","\n","if test_metrics:\n","\n","  test_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in test_tokenized_texts_tokens],\n","                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n","                          truncating=\"post\", padding=\"post\")\n","\n","train_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in train_tokenized_texts_labels],\n","                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n","                     dtype=\"long\", truncating=\"post\")\n","\n","train_attention_masks = [[float(i != 0.0) for i in ii] for ii in train_input_ids]\n","\n","if test_metrics:\n","\n","  test_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in test_tokenized_texts_labels],\n","                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n","                     dtype=\"long\", truncating=\"post\")\n","  test_attention_masks = [[float(i != 0.0) for i in ii] for ii in test_input_ids]\n","\n","\n","\n","\n","tr_inputs = torch.tensor(train_input_ids)\n","tr_tags = torch.tensor(train_tags)\n","tr_masks = torch.tensor(train_attention_masks)\n","\n","if test_metrics:\n","\n","  val_inputs = torch.tensor(test_input_ids)\n","  val_tags = torch.tensor(test_tags)\n","  val_masks = torch.tensor(test_attention_masks)\n","\n","\n","train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=TRAIN_BATCH_SIZE)\n","\n","if test_metrics:\n","\n","  valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n","  valid_sampler = SequentialSampler(valid_data)\n","  valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=TRAIN_BATCH_SIZE)\n","\n","\n","\n","model = BertForTokenClassification.from_pretrained(\n","    MODEL_TO_TRAIN,\n","    num_labels=len(tag2idx),\n","    output_attentions = False,\n","    output_hidden_states = False\n",")\n","model.to(device)\n","\n","FULL_FINETUNING = True\n","if FULL_FINETUNING:\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = ['bias', 'gamma', 'beta']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","         'weight_decay_rate': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","         'weight_decay_rate': 0.0}\n","    ]\n","else:\n","    param_optimizer = list(model.classifier.named_parameters())\n","    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n","\n","optimizer = AdamW(\n","    optimizer_grouped_parameters,\n","    lr=3e-5,\n","    eps=1e-8\n",")\n","\n","\n","epochs = EPOCHS\n","max_grad_norm = 1.0\n","\n","# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=total_steps\n",")\n","\n","\n","## Store the average loss after each epoch so we can plot them.\n","loss_values, validation_loss_values = [], []\n","\n","for EPOCH in trange(epochs, desc=\"Epoch\"):\n","    # Put the model into training mode.\n","    model.train()\n","    # Reset the total loss for this epoch.\n","    total_loss = 0\n","\n","    # Training loop\n","    for step, batch in enumerate(train_dataloader):\n","        # add batch to gpu\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        # Always clear any previously calculated gradients before performing a backward pass.\n","        model.zero_grad()\n","        # forward pass\n","        # This will return the loss (rather than the model output)\n","        # because we have provided the `labels`.\n","        outputs = model(b_input_ids, token_type_ids=None,\n","                        attention_mask=b_input_mask, labels=b_labels)\n","        # get the loss\n","        loss = outputs[0]\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","        # track train loss\n","        total_loss += loss.item()\n","        # Clip the norm of the gradient\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n","        # update parameters\n","        optimizer.step()\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)\n","    tr_loss = f\"Average train loss: {str(avg_train_loss)}\\n\"\n","\n","    # Saving partial models (this creates the folder too)    \n","    tokenizer.save_pretrained(f'{PROJECT_NAME}/{str(EPOCH)}/tokenizer/')\n","    model.save_pretrained(save_directory=f'{PROJECT_NAME}/{str(EPOCH)}/',\n","                          save_config=True, state_dict=model.state_dict())\n","    \n","    # Saving checkpoint in case it crashes, to restore work\n","    torch.save({\n","        'epoch': EPOCH,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': avg_train_loss,\n","        }, f'{PROJECT_NAME}/{str(EPOCH)}/checkpoint.pth')\n","\n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","    \n","    if test_metrics:\n","\n","      # Put the model into evaluation mode\n","      model.eval()\n","      # Reset the validation loss for this epoch.\n","      eval_loss, eval_accuracy = 0, 0\n","      nb_eval_steps, nb_eval_examples = 0, 0\n","      predictions , true_labels = [], []\n","      for batch in valid_dataloader:\n","          batch = tuple(t.to(device) for t in batch)\n","          b_input_ids, b_input_mask, b_labels = batch\n","\n","          # Telling the model not to compute or store gradients,\n","          # saving memory and speeding up validation\n","          with torch.no_grad():\n","              # Forward pass, calculate logit predictions.\n","              # This will return the logits rather than the loss because we have not provided labels.\n","              outputs = model(b_input_ids, token_type_ids=None,\n","                              attention_mask=b_input_mask, labels=b_labels)\n","          # Move logits and labels to CPU\n","          logits = outputs[1].detach().cpu().numpy()\n","          label_ids = b_labels.to('cpu').numpy()\n","\n","          # Calculate the accuracy for this batch of test sentences.\n","          eval_loss += outputs[0].mean().item()\n","          predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","          true_labels.extend(label_ids)\n","\n","      eval_loss = eval_loss / len(valid_dataloader)\n","      validation_loss_values.append(eval_loss)\n","\n","      val_loss = f\"Validation loss: {str(eval_loss)}\\n\"\n","      \n","    # Saving losses log\n","    with open(f'{PROJECT_NAME}/logs/epoch_' + str(EPOCH) + '_loss.log', 'a') as f:\n","      f.write(tr_loss)\n","      f.write('')\n","      if test_metrics:\n","          f.write(val_loss)\n","\n","    # Calculating metrics\n","    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n","                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n","    valid_tags = [tag_values[l_i] for l in true_labels\n","                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n","    \n","    report = classification_report(valid_tags, pred_tags)\n","    \n","    # Saving metrics\n","    with open(f'{PROJECT_NAME}/logs/epoch_' + str(EPOCH) + '_metrics.log', 'a') as f:\n","      f.write(report)\n","\n","    # Printing also to stdout\n","    print(tr_loss)\n","\n","    if test_metrics:\n","      print(val_loss)\n","      print(report)\n","    "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S2gvXAmZ1EQQ","executionInfo":{"status":"ok","timestamp":1681381588409,"user_tz":-180,"elapsed":220959,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"6d698f9e-cc80-4a10-8983-a19411a5b742"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["=== TRAINING SET DISTRIBUTION ===\n","O            39427\n","I-Disease     3547\n","B-Disease     3093\n","Name: tag, dtype: int64\n","=== TEST SET DISTRIBUTION ===\n","O            9316\n","I-Disease     789\n","B-Disease     708\n","Name: tag, dtype: int64\n","=== Getting sentences and labels ===\n","Example of train sentence:\n","['A', 'common', 'MSH2', 'mutation', 'in', 'English', 'and', 'North', 'American', 'HNPCC', 'families', ':', 'origin', ',', 'phenotypic', 'expression', ',', 'and', 'sex', 'specific', 'differences', 'in', 'colorectal', 'cancer', '.']\n","Example of test sentence:\n","['Two', 'of', 'seventeen', 'mutated', 'T', '-', 'PLL', 'samples', 'had', 'a', 'previously', 'reported', 'A', '-', 'T', 'allele', '.']\n","Example of train sentence:\n","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Disease', 'I-Disease', 'O']\n","Example of test sentence:\n","['O', 'O', 'O', 'O', 'B-Disease', 'I-Disease', 'I-Disease', 'O', 'O', 'O', 'O', 'O', 'B-Disease', 'I-Disease', 'I-Disease', 'O', 'O']\n","['B-Disease', 'O', 'I-Disease', 'PAD']\n","{'B-Disease': 0, 'O': 1, 'I-Disease': 2, 'PAD': 3}\n","['Two', 'of', 'seventeen', 'm', '##uta', '##ted', 'T', '-', 'P', '##LL', 'samples', 'had', 'a', 'previously', 'reported', 'A', '-', 'T', 'all', '##ele', '.']\n","['O', 'O', 'O', 'O', 'O', 'O', 'B-Disease', 'I-Disease', 'I-Disease', 'I-Disease', 'O', 'O', 'O', 'O', 'O', 'B-Disease', 'I-Disease', 'I-Disease', 'O', 'O', 'O']\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Epoch:  20%|â–ˆâ–ˆ        | 1/5 [00:41<02:47, 41.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Average train loss: 0.3819158504958506\n","\n","Validation loss: 0.11982738120215279\n","\n","              precision    recall  f1-score   support\n","\n","   B-Disease       0.71      0.60      0.65      1718\n","   I-Disease       0.71      0.65      0.68      1560\n","           O       0.95      0.95      0.95     11654\n","         PAD       0.00      0.00      0.00         0\n","\n","    accuracy                           0.88     14932\n","   macro avg       0.59      0.55      0.57     14932\n","weighted avg       0.90      0.88      0.89     14932\n","\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [01:23<02:05, 41.93s/it]"]},{"output_type":"stream","name":"stdout","text":["Average train loss: 0.10452845471876639\n","\n","Validation loss: 0.07361122167536191\n","\n","              precision    recall  f1-score   support\n","\n","   B-Disease       0.82      0.79      0.80      1718\n","   I-Disease       0.82      0.82      0.82      1560\n","           O       0.98      0.95      0.96     11654\n","         PAD       0.00      0.00      0.00         0\n","\n","    accuracy                           0.92     14932\n","   macro avg       0.65      0.64      0.65     14932\n","weighted avg       0.94      0.92      0.93     14932\n","\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 3/5 [02:06<01:24, 42.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Average train loss: 0.04682124447491434\n","\n","Validation loss: 0.043770931128944666\n","\n","              precision    recall  f1-score   support\n","\n","   B-Disease       0.87      0.82      0.84      1718\n","   I-Disease       0.81      0.91      0.86      1560\n","           O       0.98      0.97      0.98     11654\n","\n","    accuracy                           0.95     14932\n","   macro avg       0.89      0.90      0.89     14932\n","weighted avg       0.95      0.95      0.95     14932\n","\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [02:48<00:42, 42.06s/it]"]},{"output_type":"stream","name":"stdout","text":["Average train loss: 0.026773365904335624\n","\n","Validation loss: 0.03965195082128048\n","\n","              precision    recall  f1-score   support\n","\n","   B-Disease       0.88      0.84      0.86      1718\n","   I-Disease       0.84      0.90      0.87      1560\n","           O       0.98      0.98      0.98     11654\n","\n","    accuracy                           0.95     14932\n","   macro avg       0.90      0.91      0.90     14932\n","weighted avg       0.95      0.95      0.95     14932\n","\n"]},{"output_type":"stream","name":"stderr","text":["Epoch: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [03:30<00:00, 42.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Average train loss: 0.022350820609264903\n","\n","Validation loss: 0.03927617360438619\n","\n","              precision    recall  f1-score   support\n","\n","   B-Disease       0.88      0.86      0.87      1718\n","   I-Disease       0.85      0.91      0.88      1560\n","           O       0.98      0.98      0.98     11654\n","\n","    accuracy                           0.96     14932\n","   macro avg       0.90      0.91      0.91     14932\n","weighted avg       0.96      0.96      0.96     14932\n","\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","execution_count":32,"metadata":{"id":"uCXnULJwMwW-","executionInfo":{"status":"ok","timestamp":1681381588898,"user_tz":-180,"elapsed":503,"user":{"displayName":"Monster C","userId":"08787989274818793476"}}},"outputs":[],"source":["!rm -rf /content/ner_disease_main/0\n","!rm -rf /content/ner_disease_main/1\n","!rm -rf /content/ner_disease_main/2\n","!rm -rf /content/ner_disease_main/3"]},{"cell_type":"markdown","metadata":{"id":"AJQEdfA0Npmz"},"source":["## Load the model as TF and save properly\n"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"er8xdlkKNqZ4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681381590196,"user_tz":-180,"elapsed":1300,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"7d685e9f-7075-410b-f1db-c1aae757f817"},"outputs":[{"output_type":"stream","name":"stdout","text":["Last successfull epoch: 4\n","model_epoch_4_pytorch\n","model_epoch_4_tf\n"]}],"source":["last_successfull_epoch = len(loss_values) - 1\n","if last_successfull_epoch < 0:\n","  last_successfull_epoch = None\n","\n","if last_successfull_epoch is None:\n","  print(\"No epochs finished successfully.\")\n","else:\n","  print(f\"Last successfull epoch: {str(last_successfull_epoch)}\")\n","\n","# first save the model as pytorch model (we'll cast later)\n","MODEL_NAME_PYTORCH = 'model_epoch_'+str(last_successfull_epoch)+'_pytorch'\n","MODEL_NAME_TF = 'model_epoch_'+str(last_successfull_epoch)+'_tf'\n","\n","print(MODEL_NAME_PYTORCH)\n","print(MODEL_NAME_TF)\n","\n","tokenizer.save_pretrained(f'./{PROJECT_NAME}/{MODEL_NAME_PYTORCH}_tokenizer/')\n","model.save_pretrained(f'./{PROJECT_NAME}/{MODEL_NAME_PYTORCH}', saved_model=True, save_format='tf')\n"]},{"cell_type":"code","source":["import tensorflow as tf\n","from transformers import TFBertForTokenClassification\n","\n","# now load the model as TF and save properly\n","\n","loaded_model = TFBertForTokenClassification.from_pretrained(f'./{PROJECT_NAME}/{MODEL_NAME_PYTORCH}', from_pt=True)\n","\n","# Define TF Signature\n","@tf.function(\n","  input_signature=[\n","      {\n","          \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),\n","          \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),\n","          \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),\n","      }\n","  ]\n",")\n","def serving_fn(input):\n","    return loaded_model(input)\n","loaded_model.save_pretrained(f'./{PROJECT_NAME}/{MODEL_NAME_TF}', saved_model=True, signatures={\"serving_default\": serving_fn})\n","labels = sorted(tag2idx, key=tag2idx.get)\n","\n","print (labels)\n","\n","with open(f'./{PROJECT_NAME}/{MODEL_NAME_TF}/saved_model/1/assets/labels.txt', 'w') as f:\n","    f.write('\\n'.join(labels))\n","\n","vocab_pth = f\"./{PROJECT_NAME}/{MODEL_NAME_PYTORCH}_tokenizer/vocab.txt\"\n","saved_model_pth = f'./{PROJECT_NAME}/{MODEL_NAME_TF}/saved_model/1/assets/'\n","\n","! cp $vocab_pth $saved_model_pth"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5MelKBsKsZEg","executionInfo":{"status":"ok","timestamp":1681381629801,"user_tz":-180,"elapsed":39607,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"fa1e4cc1-9697-4438-a763-3eab5cd2f86d"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForTokenClassification: ['bert.embeddings.position_ids']\n","- This IS expected if you are initializing TFBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n","WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, LayerNorm_layer_call_fn while saving (showing 5 of 416). These functions will not be directly callable after loading.\n"]},{"output_type":"stream","name":"stdout","text":["['B-Disease', 'O', 'I-Disease', 'PAD']\n"]}]},{"cell_type":"markdown","metadata":{"id":"IcQNjWOeN83V"},"source":["## Load the saved model in Spark NLP and save it properlyÂ¶\n"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"A7lvrYkvOC3L","executionInfo":{"status":"ok","timestamp":1681381644599,"user_tz":-180,"elapsed":14809,"user":{"displayName":"Monster C","userId":"08787989274818793476"}}},"outputs":[],"source":["tokenClassifier = nlp.BertForTokenClassification.loadSavedModel(\n","      f'./{PROJECT_NAME}/{MODEL_NAME_TF}/saved_model/1',\n","      spark)\\\n","    .setInputCols([\"sentence\",'token'])\\\n","    .setOutputCol(\"ner\")\\\n","    .setCaseSensitive(True)\\\n","    .setMaxSentenceLength(128) # 512\n","\n","tokenClassifier.write().overwrite().save(f\"./{PROJECT_NAME}/{MODEL_NAME_TF}_spark_nlp\")"]},{"cell_type":"markdown","metadata":{"id":"tYDq9D-cOGbo"},"source":["## Test the imported model in Spark NLPÂ¶\n"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"fugqP283OG6u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681381657699,"user_tz":-180,"elapsed":13103,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"34ba4984-8500-44a3-dbd4-4ae6591317b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["sentence_detector_dl download started this may take some time.\n","Approximate size to download 354.6 KB\n","[OK!]\n"]}],"source":["documentAssembler = nlp.DocumentAssembler()\\\n","    .setInputCol(\"text\")\\\n","    .setOutputCol(\"document\")\n","\n","sentenceDetector = nlp.SentenceDetectorDLModel.pretrained()\\\n","    .setInputCols([\"document\"])\\\n","    .setOutputCol(\"sentence\")\n","\n","tokenizer = nlp.Tokenizer()\\\n","    .setInputCols(\"sentence\")\\\n","    .setOutputCol(\"token\")\n","\n","tokenClassifier = nlp.BertForTokenClassification.load(f\"./{PROJECT_NAME}/{MODEL_NAME_TF}_spark_nlp\")\\\n","    .setInputCols(\"token\", \"sentence\")\\\n","    .setOutputCol(\"label\")\\\n","    .setCaseSensitive(True)\n","\n","ner_converter = medical.NerConverterInternal()\\\n","    .setInputCols([\"sentence\",\"token\",\"label\"])\\\n","    .setOutputCol(\"ner_chunk\")\n","\n","\n","pipeline =  nlp.Pipeline(\n","    stages=[\n","        documentAssembler,\n","        sentenceDetector,\n","        tokenizer,\n","        tokenClassifier,\n","        ner_converter\n","    ]\n",")\n","\n","p_model = pipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\"))"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"k808iCsakEfn","executionInfo":{"status":"ok","timestamp":1681381657699,"user_tz":-180,"elapsed":12,"user":{"displayName":"Monster C","userId":"08787989274818793476"}}},"outputs":[],"source":["text = 'A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced pancreatitis three years prior to presentation , associated with an acute hepatitis , and obesity with a body mass index ( BMI ) of 33.5 kg/m2 , presented with a one-week history of polyuria , polydipsia , poor appetite , and vomiting . Two weeks prior to presentation , she was treated with a five-day course of amoxicillin for a respiratory tract infection . She was on metformin , glipizide , and dapagliflozin for T2DM and atorvastatin and gemfibrozil for HTG . She had been on dapagliflozin for six months at the time of presentation . Physical examination on presentation was significant for dry oral mucosa ; significantly , her abdominal examination was benign with no tenderness , guarding , or rigidity . Pertinent laboratory findings on admission were : serum glucose 111 mg/dl , bicarbonate 18 mmol/l , anion gap 20 , creatinine 0.4 mg/dL , triglycerides 508 mg/dL , total cholesterol 122 mg/dL , glycated hemoglobin ( HbA1c ) 10% , and venous pH 7.27 . Serum lipase was normal at 43 U/L . Serum acetone levels could not be assessed as blood samples kept hemolyzing due to significant lipemia . The patient was initially admitted for starvation ketosis , as she reported poor oral intake for three days prior to admission . However , serum chemistry obtained six hours after presentation revealed her glucose was 186 mg/dL , the anion gap was still elevated at 21 , serum bicarbonate was 16 mmol/L , triglyceride level peaked at 2050 mg/dL , and lipase was 52 U/L . The Î²-hydroxybutyrate level was obtained and found to be elevated at 5.29 mmol/L - the original sample was centrifuged and the chylomicron layer removed prior to analysis due to interference from turbidity caused by lipemia again . The patient was treated with an insulin drip for euDKA and HTG with a reduction in the anion gap to 13 and triglycerides to 1400 mg/dL , within 24 hours . Her euDKA was thought to be precipitated by her respiratory tract infection in the setting of SGLT2 inhibitor use . The patient was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day . It was determined that all SGLT2 inhibitors should be discontinued indefinitely . She had close follow-up with endocrinology post discharge .'\n","\n","result = p_model.transform(spark.createDataFrame([[text]]).toDF('text'))"]},{"cell_type":"code","source":["tokenClassifier.getClasses()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DpUONv6BHuOG","executionInfo":{"status":"ok","timestamp":1681381657699,"user_tz":-180,"elapsed":11,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"01925139-1a86-4970-9285-609199c1a04a"},"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['O', 'PAD', 'I-Disease', 'B-Disease']"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","execution_count":39,"metadata":{"id":"mx1Qio5_LzKx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681381663292,"user_tz":-180,"elapsed":5603,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"4c02fd0c-016d-4dbf-d59e-ff0cb97b22ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["+------------+---------+\n","|token       |label    |\n","+------------+---------+\n","|A           |O        |\n","|28-year-old |O        |\n","|female      |O        |\n","|with        |O        |\n","|a           |O        |\n","|history     |O        |\n","|of          |O        |\n","|gestational |B-Disease|\n","|diabetes    |I-Disease|\n","|mellitus    |I-Disease|\n","|diagnosed   |O        |\n","|eight       |O        |\n","|years       |O        |\n","|prior       |O        |\n","|to          |O        |\n","|presentation|O        |\n","|and         |O        |\n","|subsequent  |O        |\n","|type        |B-Disease|\n","|two         |I-Disease|\n","|diabetes    |I-Disease|\n","|mellitus    |I-Disease|\n","|(           |O        |\n","|T2DM        |B-Disease|\n","|),          |O        |\n","|one         |O        |\n","|prior       |O        |\n","|episode     |O        |\n","|of          |O        |\n","|HTG-induced |B-Disease|\n","|pancreatitis|I-Disease|\n","|three       |O        |\n","|years       |O        |\n","|prior       |O        |\n","|to          |O        |\n","|presentation|O        |\n","|,           |O        |\n","|associated  |O        |\n","|with        |O        |\n","|an          |O        |\n","|acute       |B-Disease|\n","|hepatitis   |B-Disease|\n","|,           |O        |\n","|and         |O        |\n","|obesity     |O        |\n","|with        |O        |\n","|a           |O        |\n","|body        |O        |\n","|mass        |O        |\n","|index       |O        |\n","+------------+---------+\n","only showing top 50 rows\n","\n"]}],"source":["result.select(F.explode(F.arrays_zip(result.token.result, \n","                                     result.label.result)).alias(\"cols\")) \\\n","      .select(F.expr(\"cols['0']\").alias(\"token\"),\n","              F.expr(\"cols['1']\").alias(\"label\")).show(50, truncate=False)"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"pPtJkXRlkHD7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1681381665823,"user_tz":-180,"elapsed":2542,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"5a880484-68cb-4329-c2c8-fa69bb26d35a"},"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------------------+---------+\n","|chunk                        |ner_label|\n","+-----------------------------+---------+\n","|gestational diabetes mellitus|Disease  |\n","|type two diabetes mellitus   |Disease  |\n","|T2DM                         |Disease  |\n","|HTG-induced pancreatitis     |Disease  |\n","|acute                        |Disease  |\n","|hepatitis                    |Disease  |\n","|polyuria                     |Disease  |\n","|polydipsia                   |Disease  |\n","|appetite                     |Disease  |\n","|vomiting                     |Disease  |\n","|respiratory                  |Disease  |\n","|T2DM                         |Disease  |\n","|HTG                          |Disease  |\n","|tenderness                   |Disease  |\n","|guarding                     |Disease  |\n","|rigidity                     |Disease  |\n","|HbA1c                        |Disease  |\n","|lipemia                      |Disease  |\n","|starvation                   |Disease  |\n","|ketosis                      |Disease  |\n","+-----------------------------+---------+\n","only showing top 20 rows\n","\n"]}],"source":["result.select(F.explode(F.arrays_zip(result.ner_chunk.result, \n","                                     result.ner_chunk.metadata)).alias(\"cols\")) \\\n","      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n","              F.expr(\"cols['1']['entity']\").alias(\"ner_label\")).show(truncate=False)"]},{"cell_type":"code","source":[],"metadata":{"id":"t-_ctP3qHdEI","executionInfo":{"status":"ok","timestamp":1681381665823,"user_tz":-180,"elapsed":4,"user":{"displayName":"Monster C","userId":"08787989274818793476"}}},"execution_count":40,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}