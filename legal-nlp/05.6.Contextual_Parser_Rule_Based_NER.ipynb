{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8W51t04BN6B"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "21lTnEqRBd0s"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/legal-nlp/05.6.Contextual_Parser_Rule_Based_NER.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3-vwpSj3BSbj"
      },
      "source": [
        "# ContextualParser (Rule Based NER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "gk3kZHmNj51v"
      },
      "source": [
        "# Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_914itZsj51v",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "! pip install -q johnsnowlabs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPsbAnNoPt0Z"
      },
      "source": [
        "## Automatic Installation\n",
        "Using my.johnsnowlabs.com SSO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY0lcShkj51w",
        "pycharm": {
          "is_executing": true
        }
      },
      "outputs": [],
      "source": [
        "from johnsnowlabs import nlp, legal\n",
        "\n",
        "# nlp.install(force_browser=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsJvn_WWM2GL"
      },
      "source": [
        "## Manual downloading\n",
        "If you are not registered in my.johnsnowlabs.com, you received a license via e-email or you are using Safari, you may need to do a manual update of the license.\n",
        "\n",
        "- Go to my.johnsnowlabs.com\n",
        "- Download your license\n",
        "- Upload it using the following command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i57QV3-_P2sQ"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "print('Please Upload your John Snow Labs License using the button below')\n",
        "license_keys = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGgNdFzZP_hQ"
      },
      "source": [
        "- Install it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfmmPqknP4rR"
      },
      "outputs": [],
      "source": [
        "nlp.install()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCl5ErZkNNLk"
      },
      "source": [
        "# Starting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRXTnNl3j51w"
      },
      "outputs": [],
      "source": [
        "spark = nlp.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZzN6yNBvb2b"
      },
      "source": [
        "# How the ContextualParser Works"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSO62FEY0iof"
      },
      "source": [
        "Spark NLP's `ContextualParser` is a licensed annotator that allows users to extract entities from a document based on pattern matching. It provides more functionality than its open-source counterpart `EntityRuler` by allowing users to customize specific characteristics for pattern matching. You're able to find entities using regex rules for full and partial matches, a dictionary with normalizing options and context parameters to take into account things such as token distances. \n",
        "\n",
        "There are 3 components necessary to understand when using the `ContextualParser` annotator:\n",
        "\n",
        "1. `ContextualParser` annotator's parameters\n",
        "2. JSON configuration file\n",
        "3. Dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0_3bAzaPbip"
      },
      "source": [
        "## 1. ContextualParser Annotator Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTSYc7RUQgKh"
      },
      "source": [
        "Here are all the parameters available to use with the `ContextualParserApproach`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vF4_Dm7qVTty"
      },
      "source": [
        "```\n",
        "contextualParser = legal.ContextualParserApproach() \\\n",
        "    .setInputCols([\"sentence\", \"token\"]) \\\n",
        "    .setOutputCol(\"entity\") \\\n",
        "    .setCaseSensitive(True) \\\n",
        "    .setJsonPath(\"context_config.json\") \\\n",
        "    .setPrefixAndSuffixMatch(True) \\\n",
        "    .setCompleteContextMatch(True) \\\n",
        "    .setDictionary(\"dictionary.tsv\", options={\"orientation\":\"vertical\"})\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVmRKjoBZ7HQ"
      },
      "source": [
        "We will dive deeper into the details of each parameter, but here's a quick overview:\n",
        "\n",
        "- `setCaseSensitive`: do you want the matching to be case sensitive (applies to all JSON properties apart from the regex property)\n",
        "- `setJsonPath`: the path to your JSON configuration file\n",
        "- `setPrefixAndSuffixMatch`: do you want to match using both the prefix AND suffix properties from the JSON configuration file\n",
        "- `setCompleteContextMatch`: do you want an exact match of prefix and suffix.\n",
        "- `setDictionary`: the path to your dictionary, used for normalizing entities\n",
        "\n",
        "Let's start by looking at the JSON configuration file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVO5m215TjXf"
      },
      "source": [
        "## 2. JSON Configuration File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNJr5ISlaJsl"
      },
      "source": [
        "Here is a fully utilized JSON configuration file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q1UuczZVhD_"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"entity\": \"Header\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"\\d\\.\\d+\\.?[A-Z-,; a-z]+\",\n",
        "  \"completeMatchRegex\": \"true\",\n",
        "  \"matchScope\": \"token\",\n",
        "  \"prefix\": [\"PART\"],\n",
        "  \"suffix\": [\"contract\"],\n",
        "  \"contextLength\": 100,\n",
        "  \"contextException\": [\"of\"],\n",
        "  \"exceptionDistance\": 40\n",
        " }\n",
        " ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GChnk1cXaUIZ"
      },
      "source": [
        "### 2.1. Basic Properties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-kJhmUe0f13"
      },
      "source": [
        "There are 5 basic properties you can set in your JSON configuration file:\n",
        "\n",
        "- `entity`\n",
        "- `ruleScope`\n",
        "- `regex`\n",
        "- `completeMatchRegex`\n",
        "- `matchScope`\n",
        "\n",
        "Let's first look at the 3 most essential properties to set:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RP8mwtgVkcj"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"entity\": \"Digit\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"\\\\d+\" # Note here: backslashes are escape characters in JSON, so for regex pattern \"\\d+\" we need to write it out as \"\\\\d+\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHSsBgoNcJiw"
      },
      "source": [
        "Here, we're looking for tokens in our text that match the regex: \"`\\d+`\" and assign the \"`Digit`\" entity to those tokens. When `ruleScope` is set to \"`sentence`\", we're looking for a match on each *token* of a **sentence**. You can change it to \"`document`\" to look for a match on each *sentence* of a **document**. The latter is particularly useful when working with multi-word matches, but we'll explore this at a later stage.\n",
        "\n",
        "The next properties to look at are `completeMatchRegex` and `matchScope`. To understand their use case, let's take a look at an example where we're trying to match all digits in our text. \n",
        "\n",
        "Let's say we come across the following string: ***XYZ987***\n",
        "\n",
        "Depending on how we set the `completeMatchRegex` and `matchScope` properties, we'll get the following results:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOxfFn8_VngD"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"entity\": \"Digit\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"\\\\d+\",\n",
        "  \"completeMatchRegex\": \"false\",\n",
        "  \"matchScope\": \"token\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZzkfdtDyavl"
      },
      "source": [
        "`OUTPUT: [XYZ987]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K37Ucw75Vrog"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"entity\": \"Digit\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"\\\\d+\",  \n",
        "  \"completeMatchRegex\": \"false\",\n",
        "  \"matchScope\": \"sub-token\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkOfYVHGyb20"
      },
      "source": [
        "`OUTPUT: [987]`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Jpr2IkFVwKw"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"entity\": \"Digit\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"\\\\d+\",\n",
        "  \"completeMatchRegex\": \"true\"\n",
        "  # matchScope is ignored here\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiYD0oF7gJtw"
      },
      "source": [
        "`OUTPUT: []`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFE9Ri2N4xxT"
      },
      "source": [
        "`\"completeMatchRegex\": \"true\"` will only return an output if our string was modified in the following way (to get a complete, exact match): **XYZ 987**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-_sXg5l5NBg"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"entity\": \"Digit\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"\\\\d+\",  \n",
        "  \"completeMatchRegex\": \"true\",\n",
        "  \"matchScope\": \"token\" # Note here: sub-token would return the same output\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUeuct_05p3f"
      },
      "source": [
        "`OUTPUT: [987]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlIUAPKazpT9"
      },
      "source": [
        "### 2.2. Context Awareness Properties"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D2UfFFBz7fX"
      },
      "source": [
        "There are 5 properties related to context awareness:\n",
        "\n",
        "- `contextLength`\n",
        "- `prefix`\n",
        "- `suffix`\n",
        "- `contextException`\n",
        "- `exceptionDistance`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcUlaqmgDL9R"
      },
      "source": [
        "Let's look at a similar example. Say we have the following text: ***At birth, the typical XYZ Corporation is growing slightly faster than the typical ABC Inc., but growth rates become equal at about seven months.***\n",
        "\n",
        "If we want to match the company that grows faster at birth, we can start by defining our regex: \"`XYZ|ABC`\"\n",
        "\n",
        "Next, we add a prefix (\"`birth`\") and suffix (\"`faster`\") to ask the parser to match the regex only if the word \"`birth`\" comes before and only if the word \"`faster`\" comes after. Finally, we will need to set the `contextLength` - this is the maximum number of tokens after the prefix and before the suffix that will be searched to find a regex match.\n",
        "\n",
        "Here's what the JSON configuration file would look like:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Tq5OJBVzCL"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"entity\": \"Company\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"XYZ|ABC\",\n",
        "  \"contextLength\": 50,\n",
        "  \"prefix\": [\"birth\"],\n",
        "  \"suffix\": [\"faster\"]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5Y5r9pO92B0"
      },
      "source": [
        "`OUTPUT: [XYZ]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXchjZZC_Gm0"
      },
      "source": [
        "If you remember, the annotator has a `setPrefixAndSuffixMatch()` parameter. If you set it to `True`, the previous output would remain as is. However, if you had set it to `False` and used the following JSON configuration:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO64udOnV1GJ"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"entity\": \"Company\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"XYZ|ABC\",\n",
        "  \"contextLength\": 50,\n",
        "  \"prefix\": [\"birth\"],\n",
        "  \"suffix\": [\"faster\", \"rates\"]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xm-y_c_RAJpF"
      },
      "source": [
        "`OUTPUT: [XYZ,ABC]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk0nox1sAdw_"
      },
      "source": [
        "The parser now takes into account either the prefix OR suffix, only one of the condition has to be fulfilled for a match to count."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyTdPDlgLmxL"
      },
      "source": [
        "If you remember, the annotator has a `setCompleteContextMatch()` parameter. If you set it to `True`, and used the following JSON configuration :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ycrvCMpMJ_J"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"entity\": \"Company\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"XYZ|ABC\",\n",
        "  \"contextLength\": 50,\n",
        "  \"prefix\": [\"birth\"],\n",
        "  \"suffix\": [\"fast\"]\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5A67yBZMj2N"
      },
      "source": [
        "`OUTPUT: []`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1duAqATM-ir"
      },
      "source": [
        "However if we set `setCompleteContextMatch()` as `False`, and use the same JSON configuration as above, we get the following output :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0vd_jfsNi7y"
      },
      "source": [
        "`OUTPUT: [XYZ]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdEFJZM1DGQ1"
      },
      "source": [
        "Here's the sentence again: ***At birth, the typical XYZ Corporation is growing slightly faster than the typical ABC Inc., but growth rates become equal at about seven months.***\n",
        "\n",
        "The last 2 properties related to context awareness are `contextException` and `exceptionDistance`. This rules out matches based on a given exception:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JluGupMV5SR"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"entity\": \"Company\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"XYZ|ABC\",\n",
        "  \"contextLength\": 50,\n",
        "  \"prefix\": [\"birth\"],\n",
        "  \"suffix\": [\"faster\", \"rates\"],\n",
        "  \"contextException\": [\"At\"],\n",
        "  \"exceptionDistance\": 5\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnFzqpHlC3Qz"
      },
      "source": [
        "`OUTPUT: [ABC]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JT09xrE-DCiO"
      },
      "source": [
        "Here we've asked the parser to ignore a match if the token \"`At`\" is within 5 tokens of the matched regex. This caused the token \"`XYZ`\" to be ignored."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQ9-Fkr94qAH"
      },
      "source": [
        "If the annotator's `setOptionalContextRules` parameter is set `True`, it allows us to output regex matches regardless of context match (prefix, suffix configuration). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBZ1D92g5aEB"
      },
      "source": [
        "When `shortestContextMatch` parameter is set to `True`, it will stop finding for matches when one of prefix and suffix data is found in the text.\",\n",
        "                                "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41wdqkIX5WAy"
      },
      "source": [
        "Confidence Value Scenarios:\n",
        "* When there is regex match only, the confidence value will be 0.5.\n",
        "* When there are regex and prefix matches together, the confidence value will be > 0.5 depending on the distance between target token and the prefix.\n",
        "* When there are regex and suffix matches together, the confidence value will be > 0.5 depending on the distance between target token and the suffix.\n",
        "* When there are regex, prefix, and suffix matches all together, the confidence value will be > than the other scenarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzFSjw7aVO2b"
      },
      "source": [
        "## 3. Dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NPiJZx-Va8b"
      },
      "source": [
        "Another key feature of the `ContextualParser` annotator is the use of dictionaries. You can specify a path to a dictionary in `tsv` or `csv` format using the `setDictionary()` parameter. Using a dictionary is a useful when you have a list of exact words that you want the parser to pick up when processing some text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmB9hadyXYeM"
      },
      "source": [
        "### 3.1. Orientation\n",
        "\n",
        "The first feature to be aware of when it comes to feeding dictionaries is the format of the dictionaries. The `ContextualParser` annotator will accept dictionaries in the horizontal format and in a vertical format. This is how they would look in practice:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyDmIqLmWGRy"
      },
      "source": [
        "Horizontal:\n",
        "\n",
        "| normalize | word1 | word2 | word3     |\n",
        "|-----------|-------|-------|-----------|\n",
        "| country    | US | Spain  |  India      |\n",
        "| Company   | Amazon   | Google   | John Snow Labs |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToFbLxsDYInk"
      },
      "source": [
        "\n",
        "Vertical:\n",
        "\n",
        "| country    | company |\n",
        "|-----------|-----------|\n",
        "| US     | Amazon     |\n",
        "| India      | Google     |\n",
        "| Spain      | John Snow Labs     | "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCrIw4CRYeBC"
      },
      "source": [
        "As you can see, your dictionary needs to have a `normalize` field that lets the annotator know which entity labels to use, and another field that lets the annotator know a list of words it should be looking to match. Here's how to set the format that your dictionary uses:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_epZRQiV85Y"
      },
      "source": [
        "```\n",
        "contextualParser = legal.ContextualParserApproach() \\\n",
        "    .setDictionary(\"dictionary.tsv\", options={\"orientation\":\"vertical\"}) # default is horizontal\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CvM-FgLZgKk"
      },
      "source": [
        "### 3.2. Dictionary-related JSON Properties\n",
        "\n",
        "When working with dictionaries, there are 2 properties in the JSON configuration file to be aware of:\n",
        "\n",
        "- `ruleScope`\n",
        "- `matchScope`\n",
        "\n",
        "This is especially true when you have multi-word entities in your dictionary.\n",
        "\n",
        "Let's take an example of a dictionary that contains a list of cities, sometimes made up of multiple words:\n",
        "\n",
        "| normalize | word1 | word2 | word3     |\n",
        "|-----------|-------|-------|-----------|\n",
        "| City      | New York | Salt Lake City  | Washington      |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5weyyiIMzNr"
      },
      "source": [
        "Let's say we're working with the following text: ***I love New York. Salt Lake City is nice too.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg8eJwuTJcB6"
      },
      "source": [
        "With the following JSON properties, here's what you would get:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1TNWtywoGWb"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"entity\": \"City\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"matchScope\": \"sub-token\",\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wweg5_C9JuWm"
      },
      "source": [
        "`OUTPUT: []`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNYWPr2eLJ5d"
      },
      "source": [
        "When `ruleScope` is set to `\"sentence\"`, the annotator attempts to find matches at the token level, parsing through each token in the sentence one by one, looking for a match with the dictionary items. Since `\"New York\"` and `\"Salt Lake City\"` are made up of multiple tokens, the annotator would never find a match from the dictionary. Let's change `ruleScope` to `\"document\"`:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xib9zHN7oBvV"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"entity\": \"City\",\n",
        "  \"ruleScope\": \"document\",\n",
        "  \"matchScope\": \"sub-token\",\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYmkGdtALXzK"
      },
      "source": [
        "`OUTPUT: [New York, Salt Lake City]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nmc7rvfsdFK9"
      },
      "source": [
        "When `ruleScope` is set to `\"document\"`, the annotator attempts to find matches by parsing through each sentence in the document one by one, looking for a match with the dictionary items. Beware of how you set `matchScope`. Taking the previous example, if we were to set `matchScope` to `\"token\"` instead of `\"sub-token\"`, here's what would happen:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkzDTcgen9aS"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"entity\": \"City\",\n",
        "  \"ruleScope\": \"document\",\n",
        "  \"matchScope\": \"token\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np7rBSQmetA9"
      },
      "source": [
        "`OUTPUT: [I love New York., Salt Lake City is nice too.]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5nNtvLaeOv9"
      },
      "source": [
        "As you can see, when `ruleScope` is at the document level, if you set your `matchScope` to the token level, the annotator will output each sentence containing the matched entities as individual chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zTtAFwqQdNy"
      },
      "source": [
        "### 3.3. Working with Multi-Word Matches\n",
        "\n",
        "Although not directly related to dictionaries, if we build on top of what we've just seen, there is a use-case that is particularly in demand when working with the `ContextualParser` annotator: finding regex matches for chunks of words that span across multiple tokens. \n",
        "\n",
        "Let's re-iterate how the `ruleScope` property works: when `ruleScope` is set to `\"sentence\"`, we're looking for a match on each token of a sentence. When `ruleScope` is set to `\"document\"`, we're looking for a match on each sentence of a document. \n",
        "\n",
        "So now let's imagine you're parsing through legal documents trying to tag the *John Snow* headers in those documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJWXRrg0Qu0q"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"entity\": \"John Snow\",\n",
        "  \"regex\": \"[j|J]ohn\\s+[s|S]now\",  \n",
        "  \"ruleScope\": \"document\",\n",
        "  \"matchScope\": \"sub-token\"\n",
        "}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP4H1t3CQyg5"
      },
      "source": [
        "`OUTPUT: [John Snow, john snow, John snow]`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8PgD_N9RFTP"
      },
      "source": [
        "If you had set `ruleScope` to  `\"sentence\"`, here's what would have happened:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljqiVjWaRJPe"
      },
      "source": [
        "```\n",
        "{\n",
        "  \"entity\": \"John Snow\",\n",
        "  \"regex\": \"[j|J]ohn\\s+[s|S]now\", \n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"matchScope\": \"sub-token\"\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfcGKW81RMKN"
      },
      "source": [
        "`OUTPUT: []`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMMgQdK5RPYb"
      },
      "source": [
        "Since John Snow is divided into two different tokens, the annotator will never find a match since it's now looking for a match on each token of a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehWiHjziPfGV"
      },
      "source": [
        "# Running a Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGz-nXwCDLgb"
      },
      "source": [
        "## Example 1: Detecting DOC, ALIAS, PARTY, Subheaders from a Credit agreement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thaF2bObwDXd"
      },
      "source": [
        "Let's try running through some examples to build on top of what you've learned so far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcWKcPZO-upM"
      },
      "outputs": [],
      "source": [
        "# Here's a credit agreement\n",
        "sample_text = \"\"\"\n",
        "1.1 RESTATED CREDIT AGREEMENT\n",
        "THIS TWELFTH AMENDMENT TO AMENDED AND RESTATED CREDIT AGREEMENT , (\"Twelfth Amendment\") is made as of the 27th day of December, 2007 , by\n",
        "and between CULP , INC. , a North Carolina corporation (together with its\n",
        "successors and permitted assigns, the \"Borrower\"), and WACHOVIA BANK , NATIONAL ASSOCIATION (formerly, Wachovia Bank , N.A ), a National banking association , as\n",
        "Agent and as a Bank (together with its endorsees, successors and assigns, the \"Bank\" ).\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4Ipms-4PwoN",
        "outputId": "41cc9267-4ce4-4306-86ab-43e52b08c8f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "date\n",
            "27th day of December, 2007"
          ]
        }
      ],
      "source": [
        "# Create a dictionary to detect date\n",
        "date = '''date\\n27th day of December, 2007'''\n",
        "\n",
        "with open('date.tsv', 'w') as f:\n",
        "    f.write(date)\n",
        "\n",
        "# Check what dictionary looks like\n",
        "!cat date.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jvi1mbqY_LeA"
      },
      "outputs": [],
      "source": [
        "# Create JSON file\n",
        "date= {\n",
        "  \"entity\": \"EFFDATE\",\n",
        "  \"ruleScope\": \"document\", \n",
        "  \"matchScope\":\"sub-token\",\n",
        "  \"completeMatchRegex\": \"true\"\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('date.json', 'w') as f:\n",
        "    json.dump(date, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ur4bvI-ppMIn",
        "outputId": "34deba54-7ec4-4364-cd10-eebf8469de4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "doc\n",
            "TWELFTH AMENDMENT TO AMENDED AND RESTATED CREDIT AGREEMENT"
          ]
        }
      ],
      "source": [
        "# Create a dictionary to detect doc\n",
        "doc= '''doc\\nTWELFTH AMENDMENT TO AMENDED AND RESTATED CREDIT AGREEMENT'''\n",
        "\n",
        "with open('doc.tsv', 'w') as f:\n",
        "    f.write(doc)\n",
        "\n",
        "# Check what dictionary looks like\n",
        "!cat doc.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5bDhxSkpMK1"
      },
      "outputs": [],
      "source": [
        "# Create JSON file\n",
        "doc= {\n",
        "  \"entity\": \"Doc\",\n",
        "  \"ruleScope\": \"document\", \n",
        "  \"matchScope\":\"sub-token\",\n",
        "  \"completeMatchRegex\": \"true\"\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('doc.json', 'w') as f:\n",
        "    json.dump(doc, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmq_6Gz5pMNV",
        "outputId": "5946e0de-a8cd-4e6e-f7e5-826b5a1747c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "alias\n",
            "Borrower"
          ]
        }
      ],
      "source": [
        "# Create a dictionary to detect alias\n",
        "alias = '''alias\\nBorrower'''\n",
        "\n",
        "with open('alias.tsv', 'w') as f:\n",
        "    f.write(alias)\n",
        "\n",
        "# Check what dictionary looks like\n",
        "!cat alias.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R2qAtDRnu03W"
      },
      "outputs": [],
      "source": [
        "# Create JSON file\n",
        "alias= {\n",
        "  \"entity\": \"ALIAS\",\n",
        "  \"ruleScope\": \"document\", \n",
        "  \"matchScope\":\"sub-token\",\n",
        "  \"completeMatchRegex\": \"true\"\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('alias.json', 'w') as f:\n",
        "    json.dump(alias, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3mn3yzSyQDG"
      },
      "outputs": [],
      "source": [
        "# Create JSON file for sub header\n",
        "sub_header = {\n",
        "  \"entity\": \"SUBHEADER\",\n",
        "  \"ruleScope\": \"document\", \n",
        "  \"completeMatchRegex\": \"true\",\n",
        "  \"regex\":\"\\d\\.\\d+\\.?[A-Z-,; a-z]+\",\n",
        "  \"matchScope\": \"sub-token\",\n",
        "  \"contextLength\": 100\n",
        "}\n",
        "# \\d\\.+[A-Z ]+ --->header\n",
        "#\"^(\\d\\.?\\d.*?)$\" ---> subheader\n",
        "import json\n",
        "with open('sub_header.json', 'w') as f:\n",
        "    json.dump(sub_header, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cc_QyGcR07hc",
        "outputId": "c9482b6a-d4b9-4672-b3a3-92661bdbcb6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "party\n",
            "CULP , INC. , a North Carolina corporation\n",
            "WACHOVIA BANK , NATIONAL ASSOCIATION"
          ]
        }
      ],
      "source": [
        "# Create a dictionary to detect party\n",
        "party = '''party\\nCULP , INC. , a North Carolina corporation\\nWACHOVIA BANK , NATIONAL ASSOCIATION'''\n",
        "\n",
        "with open('party.tsv', 'w') as f:\n",
        "    f.write(party)\n",
        "\n",
        "# Check what dictionary looks like\n",
        "!cat party.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jl23zXVr07sL"
      },
      "outputs": [],
      "source": [
        "# Create JSON file\n",
        "party= {\n",
        "  \"entity\": \"party\",\n",
        "  \"ruleScope\": \"document\", \n",
        "  \"matchScope\":\"sub-token\",\n",
        "  \"completeMatchRegex\": \"true\"\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('party.json', 'w') as f:\n",
        "    json.dump(party, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OIGPcRwQJVW",
        "outputId": "65fac4a0-e8b0-4bfd-ef4b-91bd2a889a89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "former_name\n",
            "Wachovia Bank , N.A"
          ]
        }
      ],
      "source": [
        "# Create a dictionary to detect former_name\n",
        "former_name = '''former_name\\nWachovia Bank , N.A'''\n",
        "\n",
        "with open('former_name.tsv', 'w') as f:\n",
        "    f.write(former_name)\n",
        "\n",
        "# Check what dictionary looks like\n",
        "!cat former_name.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAxBcQPCQJVW"
      },
      "outputs": [],
      "source": [
        "# Create JSON file\n",
        "former_name= {\n",
        "  \"entity\": \"former_name\",\n",
        "  \"ruleScope\": \"document\", \n",
        "  \"matchScope\":\"sub-token\",\n",
        "  \"completeMatchRegex\": \"true\"\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('former_name.json', 'w') as f:\n",
        "    json.dump(former_name, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjQmEjWNQHvx"
      },
      "outputs": [],
      "source": [
        "# Build pipeline\n",
        "document_assembler = nlp.DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "# sentence_detector = nlp.SentenceDetector() \\\n",
        "#     .setInputCols([\"document\"]) \\\n",
        "#     .setOutputCol(\"sentence\")\n",
        "\n",
        "tokenizer = nlp.Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "date_contextual_parser = legal.ContextualParserApproach() \\\n",
        "    .setInputCols([\"document\", \"token\"])\\\n",
        "    .setOutputCol(\"entity_date\")\\\n",
        "    .setJsonPath(\"date.json\")\\\n",
        "    .setDictionary('date.tsv', options={\"orientation\":\"vertical\"})\\\n",
        "    .setPrefixAndSuffixMatch(False)\\\n",
        "    .setShortestContextMatch(True)\\\n",
        "    .setOptionalContextRules(False) \n",
        "\n",
        "doc_contextual_parser = legal.ContextualParserApproach() \\\n",
        "    .setInputCols([\"document\", \"token\"])\\\n",
        "    .setOutputCol(\"entity_doc\")\\\n",
        "    .setJsonPath(\"doc.json\")\\\n",
        "    .setDictionary('doc.tsv', options={\"orientation\":\"vertical\"})\\\n",
        "    .setPrefixAndSuffixMatch(False)\\\n",
        "    .setShortestContextMatch(True)\\\n",
        "    .setOptionalContextRules(False)\\\n",
        "    .setCaseSensitive(True)\n",
        "\n",
        "alias_contextual_parser = legal.ContextualParserApproach() \\\n",
        "    .setInputCols([\"document\", \"token\"])\\\n",
        "    .setOutputCol(\"entity_alias\")\\\n",
        "    .setJsonPath(\"alias.json\")\\\n",
        "    .setDictionary('alias.tsv', options={\"orientation\":\"vertical\"})\\\n",
        "    .setPrefixAndSuffixMatch(False)\\\n",
        "    .setShortestContextMatch(True)\\\n",
        "    .setOptionalContextRules(False)\\\n",
        "    .setCaseSensitive(True)\n",
        "\n",
        "title_parser = legal.ContextualParserApproach() \\\n",
        "    .setInputCols([\"document\", \"token\"]) \\\n",
        "    .setOutputCol(\"title\")\\\n",
        "    .setJsonPath(\"sub_header.json\") \\\n",
        "    .setCaseSensitive(True) \\\n",
        "    .setPrefixAndSuffixMatch(False)\\\n",
        "    .setOptionalContextRules(False)\n",
        "\n",
        "party_contextual_parser = legal.ContextualParserApproach() \\\n",
        "    .setInputCols([\"document\", \"token\"])\\\n",
        "    .setOutputCol(\"entity_party\")\\\n",
        "    .setJsonPath(\"party.json\")\\\n",
        "    .setDictionary('party.tsv', options={\"orientation\":\"vertical\"})\\\n",
        "    .setPrefixAndSuffixMatch(False)\\\n",
        "    .setShortestContextMatch(True)\\\n",
        "    .setOptionalContextRules(False)\\\n",
        "    .setCaseSensitive(True)\n",
        "\n",
        "former_name_contextual_parser = legal.ContextualParserApproach() \\\n",
        "    .setInputCols([\"document\", \"token\"])\\\n",
        "    .setOutputCol(\"entity_former_name\")\\\n",
        "    .setJsonPath(\"former_name.json\")\\\n",
        "    .setDictionary('former_name.tsv', options={\"orientation\":\"vertical\"})\\\n",
        "    .setPrefixAndSuffixMatch(False)\\\n",
        "    .setShortestContextMatch(True)\\\n",
        "    .setOptionalContextRules(False)\\\n",
        "    .setCaseSensitive(True)\n",
        "\n",
        "chunk_converter = legal.ChunkMergeApproach() \\\n",
        "    .setInputCols([\"entity_date\", \"entity_doc\",\"entity_alias\",'title','entity_party','entity_former_name']) \\\n",
        "    .setOutputCol(\"ner_chunk\")\n",
        "\n",
        "parserPipeline = nlp.Pipeline(stages=[\n",
        "        document_assembler, \n",
        "        tokenizer,\n",
        "        doc_contextual_parser,\n",
        "        date_contextual_parser,\n",
        "        alias_contextual_parser,\n",
        "        title_parser,\n",
        "        party_contextual_parser,\n",
        "        former_name_contextual_parser,\n",
        "        chunk_converter,\n",
        "        ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxPVaa4fYCb5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a lightpipeline model\n",
        "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "\n",
        "parserModel = parserPipeline.fit(empty_data)\n",
        "\n",
        "light_model = nlp.LightPipeline(parserModel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3T1xnn0vYOMA"
      },
      "outputs": [],
      "source": [
        "# Annotate the sample text\n",
        "annotations = light_model.fullAnnotate(sample_text)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NY5zFoeSb8fg",
        "outputId": "cbb2eec7-a487-4c7f-bfaf-ea693c0b1f02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Annotation(chunk, 1, 29, 1.1 RESTATED CREDIT AGREEMENT, {'tokenIndex': '0', 'entity': 'SUBHEADER', 'field': 'SUBHEADER', 'chunk': '0', 'normalized': '', 'sentence': '0', 'confidenceValue': '0.50'}),\n",
              " Annotation(chunk, 36, 93, TWELFTH AMENDMENT TO AMENDED AND RESTATED CREDIT AGREEMENT, {'tokenIndex': '5', 'entity': 'Doc', 'field': 'Doc', 'chunk': '1', 'normalized': 'doc', 'sentence': '0', 'confidenceValue': '0.50'}),\n",
              " Annotation(chunk, 137, 162, 27th day of december, 2007, {'tokenIndex': '23', 'entity': 'EFFDATE', 'field': 'EFFDATE', 'chunk': '2', 'normalized': 'date', 'sentence': '0', 'confidenceValue': '0.50'}),\n",
              " Annotation(chunk, 181, 222, CULP , INC. , a North Carolina corporation, {'tokenIndex': '33', 'entity': 'party', 'field': 'party', 'chunk': '3', 'normalized': 'party', 'sentence': '0', 'confidenceValue': '0.50'}),\n",
              " Annotation(chunk, 282, 289, Borrower, {'tokenIndex': '53', 'entity': 'ALIAS', 'field': 'ALIAS', 'chunk': '4', 'normalized': 'alias', 'sentence': '0', 'confidenceValue': '0.50'}),\n",
              " Annotation(chunk, 298, 333, WACHOVIA BANK , NATIONAL ASSOCIATION, {'tokenIndex': '56', 'entity': 'party', 'field': 'party', 'chunk': '5', 'normalized': 'party', 'sentence': '0', 'confidenceValue': '0.50'}),\n",
              " Annotation(chunk, 346, 364, Wachovia Bank , N.A, {'tokenIndex': '64', 'entity': 'former_name', 'field': 'former_name', 'chunk': '6', 'normalized': 'former_name', 'sentence': '0', 'confidenceValue': '0.50'})]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check outputs\n",
        "annotations.get('ner_chunk')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "BBRie-vWXzK6",
        "outputId": "583dbaa4-bb09-4160-bbe9-1d7b0d2f11dd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n",
              "    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n",
              "    \n",
              "    .spark-nlp-display-scroll-entities {\n",
              "        border: 1px solid #E7EDF0;\n",
              "        border-radius: 3px;\n",
              "        text-align: justify;\n",
              "        \n",
              "    }\n",
              "    .spark-nlp-display-scroll-entities span {  \n",
              "        font-size: 14px;\n",
              "        line-height: 24px;\n",
              "        color: #536B76;\n",
              "        font-family: 'Montserrat', sans-serif !important;\n",
              "    }\n",
              "    \n",
              "    .spark-nlp-display-entity-wrapper{\n",
              "    \n",
              "        display: inline-grid;\n",
              "        text-align: center;\n",
              "        border-radius: 4px;\n",
              "        margin: 0 2px 5px 2px;\n",
              "        padding: 1px\n",
              "    }\n",
              "    .spark-nlp-display-entity-name{\n",
              "        font-size: 14px;\n",
              "        line-height: 24px;\n",
              "        font-family: 'Montserrat', sans-serif !important;\n",
              "        \n",
              "        background: #f1f2f3;\n",
              "        border-width: medium;\n",
              "        text-align: center;\n",
              "        \n",
              "        font-weight: 400;\n",
              "        \n",
              "        border-radius: 5px;\n",
              "        padding: 2px 5px;\n",
              "        display: block;\n",
              "        margin: 3px 2px;\n",
              "    \n",
              "    }\n",
              "    .spark-nlp-display-entity-type{\n",
              "        font-size: 14px;\n",
              "        line-height: 24px;\n",
              "        color: #ffffff;\n",
              "        font-family: 'Montserrat', sans-serif !important;\n",
              "        \n",
              "        text-transform: uppercase;\n",
              "        \n",
              "        font-weight: 500;\n",
              "\n",
              "        display: block;\n",
              "        padding: 3px 5px;\n",
              "    }\n",
              "    \n",
              "    .spark-nlp-display-entity-resolution{\n",
              "        font-size: 14px;\n",
              "        line-height: 24px;\n",
              "        color: #ffffff;\n",
              "        font-family: 'Vistol Regular', sans-serif !important;\n",
              "        \n",
              "        text-transform: uppercase;\n",
              "        \n",
              "        font-weight: 500;\n",
              "\n",
              "        display: block;\n",
              "        padding: 3px 5px;\n",
              "    }\n",
              "    \n",
              "    .spark-nlp-display-others{\n",
              "        font-size: 14px;\n",
              "        line-height: 24px;\n",
              "        font-family: 'Montserrat', sans-serif !important;\n",
              "        \n",
              "        font-weight: 400;\n",
              "    }\n",
              "\n",
              "</style>\n",
              " <span class=\"spark-nlp-display-others\" style=\"background-color: white\"><br></span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #3B95AF\"><span class=\"spark-nlp-display-entity-name\">1.1 RESTATED CREDIT AGREEMENT </span><span class=\"spark-nlp-display-entity-type\">SUBHEADER</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"><br>THIS </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #2C6D3C\"><span class=\"spark-nlp-display-entity-name\">TWELFTH AMENDMENT TO AMENDED AND RESTATED CREDIT AGREEMENT </span><span class=\"spark-nlp-display-entity-type\">Doc</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> , (\"Twelfth Amendment\") is made as of the </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #5607A7\"><span class=\"spark-nlp-display-entity-name\">27th day of December, 2007 </span><span class=\"spark-nlp-display-entity-type\">EFFDATE</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> , by<br>and between </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #1D373F\"><span class=\"spark-nlp-display-entity-name\">CULP , INC. , a North Carolina corporation </span><span class=\"spark-nlp-display-entity-type\">party</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> (together with its<br>successors and permitted assigns, the \"</span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #944F69\"><span class=\"spark-nlp-display-entity-name\">Borrower </span><span class=\"spark-nlp-display-entity-type\">ALIAS</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">\"), and </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #1D373F\"><span class=\"spark-nlp-display-entity-name\">WACHOVIA BANK , NATIONAL ASSOCIATION </span><span class=\"spark-nlp-display-entity-type\">party</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> (formerly, </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #C2BCA5\"><span class=\"spark-nlp-display-entity-name\">Wachovia Bank , N.A </span><span class=\"spark-nlp-display-entity-type\">former_name</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> ), a National banking association , as<br>Agent and as a Bank (together with its endorsees, successors and assigns, the \"Bank\" ).<br></span></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Visualize outputs\n",
        "# from sparknlp_display import NerVisualizer\n",
        "\n",
        "visualiser = nlp.viz.NerVisualizer()\n",
        "\n",
        "visualiser.display(annotations, label_col='ner_chunk', document_col='document', save_path=\"display_result.html\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtD1HJ7SABU7"
      },
      "source": [
        "Feel free to experiment with the annotator parameters and JSON properties to see how the output might change."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
