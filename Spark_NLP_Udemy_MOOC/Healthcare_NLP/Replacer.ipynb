{"cells":[{"cell_type":"markdown","metadata":{"id":"8V_Ow3cAVEYe"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"]},{"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/Spark_NLP_Udemy_MOOC/Healthcare_NLP/Replacer.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"Y0fJRpNJBslT"},"source":["# **Replacer**"]},{"cell_type":"markdown","metadata":{"id":"qeclGJmrVLjX"},"source":["\n","This notebook will cover the `Replacer` annotator.\n","\n","`Replacer` allows to replace entities in the original text with the ones extracted by the annotators `NameChunkObfuscatorApproach` or `DateNormalizer`.\n","\n","\n","\n","\n","**📖 Learning Objectives:**\n","\n","1. Understand how `Replacer` works.\n","\n","2. Understand how `Replacer` can be used to with the `DateNormalizer` annotator and in the deintification process.\n","\n","3. Become comfortable using the `setUseReplacement` parameter of the annotator.\n","\n","\n","**🔗 Helpful Links:**\n","\n","- Documentation : [Replacer](https://nlp.johnsnowlabs.com/docs/en/licensed_annotators#replacer)\n","\n","- Python Docs : [Replacer](https://nlp.johnsnowlabs.com/licensed/api/python/reference/autosummary/sparknlp_jsl/annotator/deid/replacer/index.html#sparknlp_jsl.annotator.deid.replacer.Replacer)\n","\n","- Scala Docs : [Replacer](https://nlp.johnsnowlabs.com/licensed/api/com/johnsnowlabs/nlp/annotators/deid/Replacer.html)\n","\n","- For extended examples of usage, see the [Clinical Deidentification](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/4.Clinical_DeIdentification.ipynb#scrollTo=9alThnhZeOvn) and [Date Normalizer](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/healthcare-nlp/13.0.Date_Normalizer.ipynb#scrollTo=yX57W_6SLiWz) notebooks.\n"]},{"cell_type":"markdown","metadata":{"id":"OV0hPSCWXslc"},"source":["## **📜 Background**"]},{"cell_type":"markdown","metadata":{"id":"ms9xwzYACPtU"},"source":["`Replacer` is most often used in conjunction with the `DateNormalizer` annotator or in deidentification pipelines.\n","\n","With the dates, the `Replacer` annotator is used to replace specific tokens in a text with another token or string. The `DateNormalizer` annotator, on the other hand, is used to normalize dates and times to a standardized format.\n","\n","Obfuscation in healthcare is the act of making healthcare data difficult to understand or use without authorization. This can be done by replacing or removing identifying information, such as names, dates of birth, and Social Security numbers. Obfuscation can also be used to hide the contents of healthcare records, such as diagnoses, medications, and treatment plans.\n","\n","In the **deidentification** process, the `Replacer` annotator is used to replace certain tokens or patterns in the text with specified values. For example, it can be used to replace all instances of a person's name with a placeholder like \"PERSON\".\n","\n","The `NameChunkObfuscatorApproach` annotator is used to identify and obfuscate sensitive named entities in the text, such as people's names, addresses, dates of birth, SSNs etc.\n"]},{"cell_type":"markdown","metadata":{"id":"E8qy2MI2XySv"},"source":["## **🎬 Colab Setup**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":73752,"status":"ok","timestamp":1718187486713,"user":{"displayName":"Bugra","userId":"12474083375271086689"},"user_tz":-180},"id":"HuLFt0OdBkuo","outputId":"294b58eb-bca9-4395-eaf9-f4458dbb3440"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.2/265.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m676.2/676.2 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# Install the johnsnowlabs library to access Spark-OCR and Spark-NLP for Healthcare, Finance, and Legal.\n","! pip install -q johnsnowlabs"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"executionInfo":{"elapsed":231710,"status":"ok","timestamp":1718187718404,"user":{"displayName":"Bugra","userId":"12474083375271086689"},"user_tz":-180},"id":"URRyJvTMBtXQ","outputId":"3a217db1-5021-48df-bf53-a7c08009f33c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Please Upload your John Snow Labs License using the button below\n"]},{"data":{"text/html":["\n","     <input type=\"file\" id=\"files-b6ad899c-4817-4b53-8dbe-9879e1b307c3\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-b6ad899c-4817-4b53-8dbe-9879e1b307c3\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Saving 5.3.3.spark_nlp_for_healthcare.json to 5.3.3.spark_nlp_for_healthcare.json\n"]}],"source":["from google.colab import files\n","print('Please Upload your John Snow Labs License using the button below')\n","license_keys = files.upload()"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":81540,"status":"ok","timestamp":1718187799931,"user":{"displayName":"Bugra","userId":"12474083375271086689"},"user_tz":-180},"id":"8OocQcx0I7ky","outputId":"7b7291ca-04b2-43d2-8cb5-52b57af661b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["👌 Detected license file /content/5.3.3.spark_nlp_for_healthcare.json\n","🚨 Outdated Medical Secrets in license file. Version=5.3.3 but should be Version=5.3.2\n","🚨 Outdated OCR Secrets in license file. Version=5.1.2 but should be Version=5.3.2\n","👷 Trying to install compatible secrets. Use nlp.settings.enforce_versions=False if you want to install outdated secrets.\n","📋 Stored John Snow Labs License in /root/.johnsnowlabs/licenses/license_number_0_for_Spark-Healthcare_Spark-OCR.json\n","👷 Setting up  John Snow Labs home in /root/.johnsnowlabs, this might take a few minutes.\n","Downloading 🐍+🚀 Python Library spark_nlp-5.3.2-py2.py3-none-any.whl\n","Downloading 🐍+💊 Python Library spark_nlp_jsl-5.3.2-py3-none-any.whl\n","Downloading 🫘+🚀 Java Library spark-nlp-assembly-5.3.2.jar\n","Downloading 🫘+💊 Java Library spark-nlp-jsl-5.3.2.jar\n","🙆 JSL Home setup in /root/.johnsnowlabs\n","👌 Detected license file /content/5.3.3.spark_nlp_for_healthcare.json\n","👷 Trying to install compatible secrets. Use nlp.settings.enforce_versions=False if you want to install outdated secrets.\n","Installing /root/.johnsnowlabs/py_installs/spark_nlp_jsl-5.3.2-py3-none-any.whl to /usr/bin/python3\n","Installed 1 products:\n","💊 Spark-Healthcare==5.3.2 installed! ✅ Heal the planet with NLP! \n"]}],"source":["from johnsnowlabs import nlp, medical\n","\n","# After uploading your license run this to install all licensed Python Wheels and pre-download Jars the Spark Session JVM\n","nlp.install()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":271},"executionInfo":{"elapsed":27100,"status":"ok","timestamp":1718187827713,"user":{"displayName":"Bugra","userId":"12474083375271086689"},"user_tz":-180},"id":"ua2KKPdVI7iB","outputId":"e5a930fb-f103-4236-943c-114e1a5652af"},"outputs":[{"name":"stdout","output_type":"stream","text":["👌 Detected license file /content/5.3.3.spark_nlp_for_healthcare.json\n","👷 Trying to install compatible secrets. Use nlp.settings.enforce_versions=False if you want to install outdated secrets.\n","👌 Launched \u001b[92mcpu optimized\u001b[39m session with with: 🚀Spark-NLP==5.3.2, 💊Spark-Healthcare==5.3.2, running on ⚡ PySpark==3.4.0\n"]},{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://5c427d78c1df:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.4.0</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>John-Snow-Labs-Spark-Session 🚀 with Jars for: 🚀Spark-NLP==5.3.2, 💊Spark-Healthcare==5.3.2, running on ⚡ PySpark==3.4.0</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7fa06b88a3b0>"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark.sql.functions as F\n","\n","# Automatically load license data and start a session with all jars user has access to\n","spark = nlp.start()\n","spark"]},{"cell_type":"markdown","metadata":{"id":"f_4conOIYj6D"},"source":["## **🖨️ Input/Output Annotation Types**"]},{"cell_type":"markdown","metadata":{"id":"0Fc_3iRwYk8N"},"source":["- Input: `DOCUMENT`, `CHUNK`\n","\n","- Output: `DOCUMENT`"]},{"cell_type":"markdown","metadata":{"id":"rgCCuKLdepW9"},"source":["## **🔎 Parameters**"]},{"cell_type":"markdown","metadata":{"id":"Y4tqQGMsj1BQ"},"source":["- `setUseReplacement`: (Boolean) Select what output format should be used. By default it will use the current day.   \n","\n"]},{"cell_type":"markdown","metadata":{"id":"LW_kZcwMeK2h"},"source":["## **💻 Deidentification Pipeline**"]},{"cell_type":"markdown","metadata":{"id":"xX16Jq-fyRuZ"},"source":["`Obfuscation` refers to the process of making data unclear, confusing, or difficult to understand or interpret. The goal of obfuscation is to hide or protect **sensitive information** by altering it in a way that makes it challenging for unauthorized parties to access or comprehend.\n","\n","\n","\n","\n","The `NameChunkObfuscatorApproach` annotator contains all the methods for training a NameChunkObfuscator model. This module can replace name entities with consistent fakers."]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1718187827714,"user":{"displayName":"Bugra","userId":"12474083375271086689"},"user_tz":-180},"id":"A2jHRADpwjR-"},"outputs":[],"source":["names = \"\"\"Mitchell#NAME\n","Clifford#NAME\n","Jeremiah#NAME\n","Lawrence#NAME\n","Brittany#NAME\n","Patricia#NAME\n","Samantha#NAME\n","Jennifer#NAME\n","Jackson#NAME\n","Leonard#NAME\n","Randall#NAME\n","Camacho#NAME\n","Ferrell#NAME\n","Mueller#NAME\n","Bowman#NAME\n","Hansen#NAME\n","Acosta#NAME\n","Gillespie#NAME\n","Zimmerman#NAME\n","Gillespie#NAME\n","Chandler#NAME\n","Bradshaw#NAME\n","Ferguson#NAME\n","Jacobson#NAME\n","Figueroa#NAME\n","Chandler#NAME\n","Schaefer#NAME\n","Matthews#NAME\n","Ferguson#NAME\n","Bradshaw#NAME\n","Figueroa#NAME\n","Delacruz#NAME\n","Gallegos#NAME\n","Villarreal#NAME\n","Williamson#NAME\n","Montgomery#NAME\n","Mclaughlin#NAME\n","Blankenship#NAME\n","Fitzpatrick#NAME\n","\"\"\"\n","\n","with open('names_test.txt', 'w') as file:\n","    file.write(names)"]},{"cell_type":"markdown","metadata":{"id":"AQqfGttI17lp"},"source":["### `setUseReplacement`\n","\n","<br/>\n","\n","This parameter is used to enable or disable replacement of entities.\n","\n","True is for Replacing, False for otherwise."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":136137,"status":"ok","timestamp":1718187963846,"user":{"displayName":"Bugra","userId":"12474083375271086689"},"user_tz":-180},"id":"eaN4PazxUuLN","outputId":"6209b719-8b49-4359-ff54-9823b2b110c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["embeddings_clinical download started this may take some time.\n","Approximate size to download 1.6 GB\n","[OK!]\n","ner_deid_generic_augmented download started this may take some time.\n","[OK!]\n"]}],"source":["# Annotator that transforms a text column from dataframe into an Annotation ready for NLP\n","documentAssembler = nlp.DocumentAssembler()\\\n","  .setInputCol(\"text\")\\\n","  .setOutputCol(\"sentence\")\\\n","\n","# Tokenizer splits words in a relevant format for NLP\n","tokenizer = nlp.Tokenizer()\\\n","  .setInputCols(\"sentence\")\\\n","  .setOutputCol(\"token\")\\\n","\n","# Clinical word embeddings trained on PubMED dataset\n","word_embeddings = nlp.WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n","    .setInputCols([\"sentence\", \"token\"])\\\n","    .setOutputCol(\"embeddings\")\n","\n","# NER model trained on n2c2 (de-identification and Heart Disease Risk Factors Challenge) datasets)\n","clinical_ner = medical.NerModel.pretrained(\"ner_deid_generic_augmented\", \"en\", \"clinical/models\") \\\n","    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n","    .setOutputCol(\"ner\")\n","\n","ner_converter_name = medical.NerConverterInternal()\\\n","    .setInputCols([\"sentence\",\"token\",\"ner\"])\\\n","    .setOutputCol(\"ner_chunk\")\n","\n","nameChunkObfuscator = medical.NameChunkObfuscatorApproach()\\\n","  .setInputCols(\"ner_chunk\")\\\n","  .setOutputCol(\"replacement\")\\\n","  .setRefFileFormat(\"csv\")\\\n","  .setObfuscateRefFile(\"names_test.txt\")\\\n","  .setRefSep(\"#\")\\\n","\n","replacer_name = medical.Replacer()\\\n","  .setInputCols(\"replacement\",\"sentence\")\\\n","  .setOutputCol(\"obfuscated_document_name\")\\\n","  .setUseReplacement(True)\n","\n","nlpPipeline = nlp.Pipeline(stages=[\n","    documentAssembler,\n","    tokenizer,\n","    word_embeddings,\n","    clinical_ner,\n","    ner_converter_name,\n","    nameChunkObfuscator,\n","    replacer_name,\n","    ])\n","\n","empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n","\n","model = nlpPipeline.fit(empty_data)\n","\n","result = model.transform(empty_data)"]},{"cell_type":"markdown","metadata":{"id":"8Y8jLywKIoG2"},"source":["Let’s use LightPipeline here to extract the entities and make the replacements.\n","\n","[LightPipeline](https://nlp.johnsnowlabs.com/docs/en/concepts#using-spark-nlps-lightpipeline) is a Spark NLP specific Pipeline class equivalent to the Spark ML Pipeline, which achieves fast results when dealing with small amounts of data."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":5490,"status":"ok","timestamp":1718187969317,"user":{"displayName":"Bugra","userId":"12474083375271086689"},"user_tz":-180},"id":"xESKA6nXv3wQ"},"outputs":[],"source":["sample_text = \"John Davies is a 62 y.o. patient admitted. Mr. Davies was seen by attending physician Dr. Lorand and was scheduled for emergency assessment.\"\n","\n","lmodel = nlp.LightPipeline(model)\n","\n","res = lmodel.fullAnnotate(sample_text)"]},{"cell_type":"markdown","metadata":{"id":"Eqgco_PTMdhM"},"source":["The original text and the output of the `Replacer` annotator is shown below. All the names were replaced with values defined in the `names_test.txt` file."]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1718187969318,"user":{"displayName":"Bugra","userId":"12474083375271086689"},"user_tz":-180},"id":"jpr8THxIxrFT","outputId":"57f981e8-3bac-4a73-e2d9-71be9362062d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original text.  :  John Davies is a 62 y.o. patient admitted. Mr. Davies was seen by attending physician Dr. Lorand and was scheduled for emergency assessment.\n","Obfuscated text :  Patriciaann is a 62 y.o. patient admitted. Mr. Noella was seen by attending physician Dr. Genice and was scheduled for emergency assessment.\n"]}],"source":["print(\"Original text.  : \", res[0]['sentence'][0].result)\n","print(\"Obfuscated text : \", res[0]['obfuscated_document_name'][0].result)"]},{"cell_type":"markdown","metadata":{"id":"n9DMErh6KVdS"},"source":["This time, change the `setUseReplacement` parameter setting to **False** and see the difference."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":1343,"status":"ok","timestamp":1718187970656,"user":{"displayName":"Bugra","userId":"12474083375271086689"},"user_tz":-180},"id":"PFg_YbgFzqfF"},"outputs":[],"source":["replacer_name = medical.Replacer()\\\n","  .setInputCols(\"replacement\",\"sentence\")\\\n","  .setOutputCol(\"obfuscated_document_name\")\\\n","  .setUseReplacement(False)\n","\n","nlpPipeline = nlp.Pipeline(stages=[\n","    documentAssembler,\n","    tokenizer,\n","    word_embeddings,\n","    clinical_ner,\n","    ner_converter_name,\n","    nameChunkObfuscator,\n","    replacer_name,\n","    ])\n","\n","model = nlpPipeline.fit(empty_data)\n","\n","result = model.transform(empty_data)"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":662,"status":"ok","timestamp":1718187971315,"user":{"displayName":"Bugra","userId":"12474083375271086689"},"user_tz":-180},"id":"raEvepICzqcG"},"outputs":[],"source":["lmodel = nlp.LightPipeline(model)\n","\n","res = lmodel.fullAnnotate(sample_text)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1718187971316,"user":{"displayName":"Bugra","userId":"12474083375271086689"},"user_tz":-180},"id":"SbuuW99xzqZp","outputId":"5c0a3766-528a-4045-8a58-0c32fce65a7d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original text.  :  John Davies is a 62 y.o. patient admitted. Mr. Davies was seen by attending physician Dr. Lorand and was scheduled for emergency assessment.\n","Obfuscated text :  John Davies is a 62 y.o. patient admitted. Mr. Davies was seen by attending physician Dr. Lorand and was scheduled for emergency assessment.\n"]}],"source":["print(\"Original text.  : \", res[0]['sentence'][0].result)\n","print(\"Obfuscated text : \", res[0]['obfuscated_document_name'][0].result)"]},{"cell_type":"markdown","metadata":{"id":"6W9_SqmDNAX6"},"source":["As you can see, the names in the text are unaffected because of the change in the setting."]},{"cell_type":"markdown","metadata":{"id":"btb7vPf4K4oQ"},"source":["## **💻 Date Normalizer Pipeline**\n","\n","\n","The `DateNormalizer` annotator transforms date mentions to a common standard format: YYYY/MM/DD. It is useful when using data from different sources, sometimes from different countries that has different formats to represent dates.\n","\n","For the relative dates (next year, past month, etc.), it is possible to define an anchor date to create the normalized date by setting the parameters anchorDateYear, anchorDateMonth, and anchorDateDay.\n","\n","The `Replacer` annotator will use the output of the `DateNormalizer` annotator, which replaced the extracted date entity by the standard date format, and provide a new text by replacing the original date entity with this value."]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7219,"status":"ok","timestamp":1718187978530,"user":{"displayName":"Bugra","userId":"12474083375271086689"},"user_tz":-180},"id":"oEObt46_wPwu","outputId":"cd341293-72af-48fa-ed5c-ae0cedee8593"},"outputs":[{"name":"stdout","output_type":"stream","text":["embeddings_clinical download started this may take some time.\n","Approximate size to download 1.6 GB\n","[OK!]\n","ner_deid_generic_augmented download started this may take some time.\n","[OK!]\n"]}],"source":["# Annotator that transforms a text column from dataframe into an Annotation ready for NLP\n","documentAssembler = nlp.DocumentAssembler()\\\n","    .setInputCol(\"text\")\\\n","    .setOutputCol(\"document\")\n","\n","# Sentence Detector annotator, processes various sentences per line\n","sentenceDetector = nlp.SentenceDetector()\\\n","    .setInputCols([\"document\"])\\\n","    .setOutputCol(\"sentence\")\n","\n","# Tokenizer splits words in a relevant format for NLP\n","tokenizer = nlp.Tokenizer()\\\n","    .setInputCols([\"sentence\"])\\\n","    .setOutputCol(\"token\")\n","\n","# Clinical word embeddings trained on PubMED dataset\n","word_embeddings = nlp.WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n","    .setInputCols([\"sentence\", \"token\"])\\\n","    .setOutputCol(\"embeddings\")\n","\n","# NER model trained on n2c2 (de-identification and Heart Disease Risk Factors Challenge) datasets)\n","clinical_ner = medical.NerModel.pretrained(\"ner_deid_generic_augmented\", \"en\", \"clinical/models\") \\\n","    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n","    .setOutputCol(\"ner\")\n","\n","ner_converter = medical.NerConverterInternal()\\\n","    .setInputCols([\"sentence\", \"token\", \"ner\"])\\\n","    .setOutputCol(\"date_chunk\")\\\n","    .setWhiteList([\"DATE\"])\n","\n","date_normalizer = medical.DateNormalizer()\\\n","    .setInputCols('date_chunk')\\\n","    .setOutputCol('normalized_date')\n","\n","replacer = medical.Replacer()\\\n","    .setInputCols([\"normalized_date\",\"document\"])\\\n","    .setOutputCol(\"replaced_document\")\\\n","    .setUseReplacement(True)\n","\n","nlpPipeline = nlp.Pipeline(stages=[\n","      documentAssembler,\n","      sentenceDetector,\n","      tokenizer,\n","      word_embeddings,\n","      clinical_ner,\n","      ner_converter,\n","      date_normalizer,\n","      replacer])\n","\n","empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n","\n","model = nlpPipeline.fit(empty_data)\n"]},{"cell_type":"markdown","metadata":{"id":"bnKbW3ZDRcIm"},"source":["Let's define 7 texts; normalize the date entities and then replace the normalized entities with the original dates in the document by using the `Replacer` annotator."]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5390,"status":"ok","timestamp":1718188281106,"user":{"displayName":"Bugra","userId":"12474083375271086689"},"user_tz":-180},"id":"1f1r2RdjwUzL","outputId":"dfae1e0d-ff4f-4267-a653-b796b02ea7ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------------------------------------------------------------------------------------------------------------+\n","|text                                                                                                                 |\n","+---------------------------------------------------------------------------------------------------------------------+\n","|She has a history of pericarditis and pericardectomy on 08/02/2018 and developed a cough with right-sided chest pain.|\n","|She has been receiving gemcitabine and she receives three cycles of this with the last one being given on 11/2018.   |\n","|She was last seen in the clinic on 11/01/2018by Dr. Y.                                                               |\n","|Chris Brown was discharged on 12Mar2021                                                                              |\n","|Last INR was on Tuesday, Jan 30, 2018, and her INR was 2.3. 2. Amiodarone 100 mg p.o. daily.                         |\n","|We reviewed the pathology obtained from the pericardectomy on 13.04.1999, which was diagnostic of mesothelioma       |\n","|A review of her CT scan on 3 April2020 prior to her pericardectomy, already shows bilateral plural effusions.        |\n","+---------------------------------------------------------------------------------------------------------------------+\n","\n"]}],"source":["from pyspark.sql.types import StringType\n","\n","dates = [\n","'She has a history of pericarditis and pericardectomy on 08/02/2018 and developed a cough with right-sided chest pain.' ,\n","'She has been receiving gemcitabine and she receives three cycles of this with the last one being given on 11/2018. ',\n","'She was last seen in the clinic on 11/01/2018by Dr. Y.',\n","'Chris Brown was discharged on 12Mar2021',\n","'Last INR was on Tuesday, Jan 30, 2018, and her INR was 2.3. 2. Amiodarone 100 mg p.o. daily. ',\n","'We reviewed the pathology obtained from the pericardectomy on 13.04.1999, which was diagnostic of mesothelioma',\n","'A review of her CT scan on 3 April2020 prior to her pericardectomy, already shows bilateral plural effusions. ',\n","]\n","\n","df_dates = spark.createDataFrame(dates,StringType()).toDF('text')\n","\n","df_dates.show(truncate=False)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5717,"status":"ok","timestamp":1718188286819,"user":{"displayName":"Bugra","userId":"12474083375271086689"},"user_tz":-180},"id":"UVdefBtoLi9t","outputId":"a35d6788-1e8e-4391-c2e0-ffd1a44c189b"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------------------------------------------------------------------------------------------+---------------+----------------------------------------------------------------------------------------------------+\n","|                                                                                                text|normalized_date|                                                                                   replaced_document|\n","+----------------------------------------------------------------------------------------------------+---------------+----------------------------------------------------------------------------------------------------+\n","|She has a history of pericarditis and pericardectomy on 08/02/2018 and developed a cough with rig...|     2018/08/02|She has a history of pericarditis and pericardectomy on 2018/08/02 and developed a cough with rig...|\n","|She has been receiving gemcitabine and she receives three cycles of this with the last one being ...|     2018/11/15|She has been receiving gemcitabine and she receives three cycles of this with the last one being ...|\n","|                                              She was last seen in the clinic on 11/01/2018by Dr. Y.|     2018/11/01|                                                She was last seen in the clinic on 2018/11/01 Dr. Y.|\n","|                                                             Chris Brown was discharged on 12Mar2021|     2021/03/12|                                                            Chris Brown was discharged on 2021/03/12|\n","|       Last INR was on Tuesday, Jan 30, 2018, and her INR was 2.3. 2. Amiodarone 100 mg p.o. daily. |     2018/01/30|                  Last INR was on 2018/01/30, and her INR was 2.3. 2. Amiodarone 100 mg p.o. daily. |\n","|We reviewed the pathology obtained from the pericardectomy on 13.04.1999, which was diagnostic of...|     1999/04/13|We reviewed the pathology obtained from the pericardectomy on 1999/04/13, which was diagnostic of...|\n","|A review of her CT scan on 3 April2020 prior to her pericardectomy, already shows bilateral plura...|     2020/04/03|A review of her CT scan on 2020/04/03 prior to her pericardectomy, already shows bilateral plural...|\n","+----------------------------------------------------------------------------------------------------+---------------+----------------------------------------------------------------------------------------------------+\n","\n"]}],"source":["result = model.transform(df_dates)\n","\n","result_df = result.select(\"text\",F.explode(F.arrays_zip(result.date_chunk.result,\n","                                                        result.normalized_date.result,\n","                                                        result.replaced_document.result)).alias(\"cols\")) \\\n","                  .select(\"text\",F.expr(\"cols['1']\").alias(\"normalized_date\"),\n","                                 F.expr(\"cols['2']\").alias(\"replaced_document\"))\n","\n","result_df.show(truncate=100)"]},{"cell_type":"markdown","metadata":{"id":"axbEolr6Q8pT"},"source":["The dataframe above shows the original texts, normalized dates and `replaced_document` involving the normalized dates."]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
