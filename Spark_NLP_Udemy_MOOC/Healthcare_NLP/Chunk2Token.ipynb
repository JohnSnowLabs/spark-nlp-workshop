{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n"],"metadata":{"id":"K1cAykkQkWF-"}},{"cell_type":"markdown","source":["# **Chunk2Token**"],"metadata":{"id":"bewC1SWN-6jB"}},{"cell_type":"markdown","source":["This notebook will cover the different parameters and usages of `Chunk2Token` annotator.\n","\n","**ðŸ“– Learning Objectives:**\n","\n","1. Understand how to use `Chunk2Token`.\n","\n","2. Become comfortable using the different parameters of the annotator.\n","\n","\n","\n","**ðŸ”— Helpful Links:**\n","\n","- Documentation : [Chunk2Token](https://nlp.johnsnowlabs.com/docs/en/licensed_annotators#chunk2token)\n","\n","- Python Docs : [Chunk2Token]()\n","\n","- Scala Docs : [Chunk2Token](https://nlp.johnsnowlabs.com/licensed/api/com/johnsnowlabs/nlp/annotators/Chunk2Token.html)\n","\n","- For extended examples of usage, see the [Spark NLP Workshop repository]()."],"metadata":{"id":"m048uDkB69Rv"}},{"cell_type":"markdown","source":["## **ðŸ“œ Background**\n"],"metadata":{"id":"B9Wo54MT8jKU"}},{"cell_type":"markdown","source":["`Chunk2Token`  a feature transformer that converts the input array of strings (annotatorType CHUNK) into an array of chunk-based tokens (annotatorType TOKEN).\n","\n","When the input is empty, an empty array is returned.\n","\n","This annotator is specially convenient when using `NGramGenerator` annotations as inputs to WordEmbeddingsModels.\n"],"metadata":{"id":"yaDBNKJsAovm"}},{"cell_type":"markdown","source":["## **ðŸŽ¬ Colab Setup**"],"metadata":{"id":"A4hMnkhd_ik9"}},{"cell_type":"code","source":["# Install the johnsnowlabs library to access Spark-NLP for Healthcare\n","! pip install -q johnsnowlabs"],"metadata":{"id":"xrdvNxjD_yQI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","print('Please Upload your John Snow Labs License using the button below')\n","license_keys = files.upload()"],"metadata":{"id":"3WU-z_e8jYpO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from johnsnowlabs import nlp, medical\n","\n","# After uploading your license run this to install all licensed Python Wheels and pre-download Jars the Spark Session JVM\n","nlp.settings.enforce_versions=False\n","nlp.install(refresh_install=True)"],"metadata":{"id":"bUDfeNzUjaO-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from johnsnowlabs import nlp, medical\n","import pandas as pd\n","\n","# Automatically load license data and start a session with all jars user has access to\n","spark = nlp.start()"],"metadata":{"id":"19b2mPsZjbI3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1691915151888,"user_tz":-180,"elapsed":26305,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"0da8c2fc-5ccf-4579-9578-2bb87f0774bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ‘Œ Detected license file /content/5.0.1.spark_nlp_for_healthcare.json\n","ðŸ‘Œ Launched \u001b[92mcpu optimized\u001b[39m session with with: ðŸš€Spark-NLP==5.0.1, ðŸ’ŠSpark-Healthcare==5.0.1, running on âš¡ PySpark==3.1.2\n"]}]},{"cell_type":"code","source":["spark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":240},"id":"aFMD5KCS9Xih","executionInfo":{"status":"ok","timestamp":1691915151888,"user_tz":-180,"elapsed":6,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"1af402bd-14b8-47c8-ed7d-d57bf69b453c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x78ca5b1026e0>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://411d8b3affb9:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>John-Snow-Labs-Spark-Session ðŸš€ with Jars for: ðŸš€Spark-NLP==5.0.1, ðŸ’ŠSpark-Healthcare==5.0.1, running on âš¡ PySpark==3.1.2</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["from pyspark.sql import DataFrame\n","import pyspark.sql.functions as F\n","import pyspark.sql.types as T"],"metadata":{"id":"1SquZfvA_OAX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **ðŸ–¨ï¸ Input/Output Annotation Types**"],"metadata":{"id":"fdTHvykf8wni"}},{"cell_type":"markdown","source":["- Input: `CHUNK`\n","\n","- Output: `TOKEN`"],"metadata":{"id":"ejYVNcX98y5j"}},{"cell_type":"markdown","source":["## **ðŸ”Ž Parameters**\n"],"metadata":{"id":"YVa72oJd9Bk_"}},{"cell_type":"markdown","source":["- `inputCols`: The name of the columns containing the input annotations. It can read either a String column or an Array.\n","- `outputCol`: The name of the column in Document type that is generated. We can specify only one column here.\n","\n","\n","All the parameters can be set using the corresponding set method in camel case. For example, `.setInputcols()`."],"metadata":{"id":"AUbv3YL59D8Q"}},{"cell_type":"markdown","source":["### `inputCols` and `outputCol`"],"metadata":{"id":"EpBqGk7oNc3C"}},{"cell_type":"markdown","source":["Define the column names containing the `DOCUMENT` and `TOKEN` annotations needed as input to the `ContextualParser` and the name of the new column containg the identified entities.\n","\n","Let's define a pipeline to process raw texts into `DOCUMENT` and `TOKEN` annotations:"],"metadata":{"id":"wOGfzMDiOAXE"}},{"cell_type":"code","source":["document = nlp.DocumentAssembler()\\\n","    .setInputCol(\"text\")\\\n","    .setOutputCol(\"document\")\n","\n","sentenceDetector = nlp.SentenceDetector()\\\n","    .setInputCols([\"document\"])\\\n","    .setOutputCol(\"sentence\")\n","\n","token = nlp.Tokenizer()\\\n","    .setInputCols([\"sentence\"])\\\n","    .setOutputCol(\"token\")\n","\n","ngrammer = nlp.NGramGenerator() \\\n","    .setN(2) \\\n","    .setEnableCumulative(False) \\\n","    .setInputCols([\"token\"]) \\\n","    .setOutputCol(\"ngrams\") \\\n","    .setDelimiter(\"_\")\n","\n","# Stage to convert n-gram CHUNKS to TOKEN type\n","chunk2Token = medical.Chunk2Token()\\\n","    .setInputCols([\"ngrams\"])\\\n","    .setOutputCol(\"ngram_tokens\")\n","\n","pipeline = nlp.Pipeline(stages=[\n","    document,\n","    sentenceDetector,\n","    token,\n","    ngrammer,\n","    chunk2Token])\n"],"metadata":{"id":"c23AbFZ7OV6B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text = \"The patient is a 41-year-old Vietnamese female with a nonproductive cough that started last week.\"\n","\n","data = spark.createDataFrame([[text]]).toDF(\"text\")\n","\n","result = pipeline.fit(data).transform(data)"],"metadata":{"id":"KB0WFZo5Esw4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result.select(\"ngram_tokens\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lnJzoiE_oLcz","executionInfo":{"status":"ok","timestamp":1687295361721,"user_tz":-180,"elapsed":1202,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"6a8edee6-b428-46ed-8460-195541f884ee"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|ngram_tokens                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|[{token, 0, 10, The_patient, {sentence -> 0, chunk -> 0}, []}, {token, 4, 13, patient_is, {sentence -> 0, chunk -> 1}, []}, {token, 12, 15, is_a, {sentence -> 0, chunk -> 2}, []}, {token, 15, 27, a_41-year-old, {sentence -> 0, chunk -> 3}, []}, {token, 17, 38, 41-year-old_Vietnamese, {sentence -> 0, chunk -> 4}, []}, {token, 29, 45, Vietnamese_female, {sentence -> 0, chunk -> 5}, []}, {token, 40, 50, female_with, {sentence -> 0, chunk -> 6}, []}, {token, 47, 52, with_a, {sentence -> 0, chunk -> 7}, []}, {token, 52, 66, a_nonproductive, {sentence -> 0, chunk -> 8}, []}, {token, 54, 72, nonproductive_cough, {sentence -> 0, chunk -> 9}, []}, {token, 68, 77, cough_that, {sentence -> 0, chunk -> 10}, []}, {token, 74, 85, that_started, {sentence -> 0, chunk -> 11}, []}, {token, 79, 90, started_last, {sentence -> 0, chunk -> 12}, []}, {token, 87, 95, last_week, {sentence -> 0, chunk -> 13}, []}, {token, 92, 96, week_., {sentence -> 0, chunk -> 14}, []}]|\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n"]}]},{"cell_type":"code","source":["result_df = result.select(F.explode(F.arrays_zip(result.ngram_tokens.result,\n","                                                 result.ngram_tokens.annotatorType,\n","                                                 result.ngram_tokens.metadata)).alias(\"cols\"))\\\n","                  .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n","                          F.expr(\"cols['1']\").alias(\"annotatorType\"),\n","                          F.expr(\"cols['2']\").alias(\"metadata\"))\n","\n","result_df.show(50, truncate=False)"],"metadata":{"id":"KfueJP-rALU_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687295363754,"user_tz":-180,"elapsed":2035,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"acfe5ed3-ad9a-4ff6-9e43-aa2c1ac81962"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------------------+-------------+----------------------------+\n","|chunk                 |annotatorType|metadata                    |\n","+----------------------+-------------+----------------------------+\n","|The_patient           |token        |{sentence -> 0, chunk -> 0} |\n","|patient_is            |token        |{sentence -> 0, chunk -> 1} |\n","|is_a                  |token        |{sentence -> 0, chunk -> 2} |\n","|a_41-year-old         |token        |{sentence -> 0, chunk -> 3} |\n","|41-year-old_Vietnamese|token        |{sentence -> 0, chunk -> 4} |\n","|Vietnamese_female     |token        |{sentence -> 0, chunk -> 5} |\n","|female_with           |token        |{sentence -> 0, chunk -> 6} |\n","|with_a                |token        |{sentence -> 0, chunk -> 7} |\n","|a_nonproductive       |token        |{sentence -> 0, chunk -> 8} |\n","|nonproductive_cough   |token        |{sentence -> 0, chunk -> 9} |\n","|cough_that            |token        |{sentence -> 0, chunk -> 10}|\n","|that_started          |token        |{sentence -> 0, chunk -> 11}|\n","|started_last          |token        |{sentence -> 0, chunk -> 12}|\n","|last_week             |token        |{sentence -> 0, chunk -> 13}|\n","|week_.                |token        |{sentence -> 0, chunk -> 14}|\n","+----------------------+-------------+----------------------------+\n","\n"]}]},{"cell_type":"code","source":["chunk2Token.extractParamMap()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gylS8A9cn4Q1","executionInfo":{"status":"ok","timestamp":1687295363755,"user_tz":-180,"elapsed":6,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"c0e24f49-7645-421b-dabb-88a799195f2a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{Param(parent='Chunk2Token_8ba3bf035cec', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n"," Param(parent='Chunk2Token_8ba3bf035cec', name='inputCols', doc='previous annotations columns, if renamed'): ['ngrams'],\n"," Param(parent='Chunk2Token_8ba3bf035cec', name='outputCol', doc='output annotation column. can be left default.'): 'ngram_tokens'}"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":[],"metadata":{"id":"TpEg0TK-obBp"},"execution_count":null,"outputs":[]}]}