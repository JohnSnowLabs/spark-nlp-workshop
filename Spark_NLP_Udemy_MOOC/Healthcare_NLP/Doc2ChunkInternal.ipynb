{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMF1PdZ68V9fY4AhThdmT4P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n"],"metadata":{"id":"Dm9w05ifTEnq"}},{"cell_type":"markdown","source":["# **Doc2ChunkInternal**"],"metadata":{"id":"bewC1SWN-6jB"}},{"cell_type":"markdown","source":["This notebook will cover the different parameters and usages of `Doc2ChunkInternal` annotator.\n","\n","**ðŸ“– Learning Objectives:**\n","\n","1. Understand how to use `Doc2ChunkInternal`.\n","\n","2. Become comfortable using the different parameters of the annotator.\n","\n","\n","\n","\n","**ðŸ”— Helpful Links:**\n","\n","- Documentation : [Doc2ChunkInternal](https://nlp.johnsnowlabs.com/docs/en/licensed_annotators#doc2chunkinternal)\n","\n","- Python Docs : [Doc2ChunkInternal](https://nlp.johnsnowlabs.com/licensed/api/python/reference/autosummary/sparknlp_jsl/annotator/doc2_chunk_internal/index.html#sparknlp_jsl.annotator.doc2_chunk_internal.Doc2ChunkInternal)\n","\n","- Scala Docs : [Doc2ChunkInternal](https://nlp.johnsnowlabs.com/licensed/api/com/johnsnowlabs/annotator/Doc2ChunkInternal.html)\n","\n"],"metadata":{"id":"m048uDkB69Rv"}},{"cell_type":"markdown","source":["## **ðŸ“œ Background**\n"],"metadata":{"id":"B9Wo54MT8jKU"}},{"cell_type":"markdown","source":["`Doc2ChunkInternal` Converts DOCUMENT type annotations into CHUNK type with the contents of a chunkCol.\n","\n","\n","Converts `DOCUMENT`, `TOKEN` typed annotations into `CHUNK` type with the contents of a chunkCol. Chunk text must be contained within input `DOCUMENT`. May be either StringType or `ArrayType[StringType]` (using `setIsArray`). Useful for annotators that require a `CHUNK` type input."],"metadata":{"id":"yaDBNKJsAovm"}},{"cell_type":"markdown","source":["## **ðŸŽ¬ Colab Setup**"],"metadata":{"id":"A4hMnkhd_ik9"}},{"cell_type":"code","source":["# Install the johnsnowlabs library to access Spark-NLP for Healthcare\n","! pip install -q johnsnowlabs"],"metadata":{"id":"xrdvNxjD_yQI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","print('Please Upload your John Snow Labs License using the button below')\n","license_keys = files.upload()"],"metadata":{"id":"3WU-z_e8jYpO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from johnsnowlabs import nlp, medical\n","\n","# After uploading your license run this to install all licensed Python Wheels and pre-download Jars the Spark Session JVM\n","nlp.settings.enforce_versions=False\n","nlp.install(refresh_install=True)"],"metadata":{"id":"bUDfeNzUjaO-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from johnsnowlabs import nlp, medical\n","import pandas as pd\n","\n","# Automatically load license data and start a session with all jars user has access to\n","spark = nlp.start()"],"metadata":{"id":"19b2mPsZjbI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["spark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"aFMD5KCS9Xih","executionInfo":{"status":"ok","timestamp":1687295965127,"user_tz":-180,"elapsed":335,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"c0548641-5949-4441-8c91-ff61075bffe9"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7f7a62d392d0>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://91b521ae9b2a:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>John-Snow-Labs-Spark-Session ðŸš€ with Jars for: ðŸš€Spark-NLP==4.4.1, ðŸ’ŠSpark-Healthcare==4.4.4, running on âš¡ PySpark==3.1.2</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["from pyspark.sql import DataFrame\n","import pyspark.sql.functions as F\n","import pyspark.sql.types as T"],"metadata":{"id":"1SquZfvA_OAX","executionInfo":{"status":"ok","timestamp":1687295966373,"user_tz":-180,"elapsed":2,"user":{"displayName":"Monster C","userId":"08787989274818793476"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## **ðŸ–¨ï¸ Input/Output Annotation Types**"],"metadata":{"id":"fdTHvykf8wni"}},{"cell_type":"markdown","source":["- Input: `DOCUMENT`, `CHUNK`\n","\n","- Output: `CHUNK`"],"metadata":{"id":"ejYVNcX98y5j"}},{"cell_type":"markdown","source":["## **ðŸ”Ž Parameters**\n"],"metadata":{"id":"YVa72oJd9Bk_"}},{"cell_type":"markdown","source":["- `inputCols`: The name of the columns containing the input annotations. It can read either a String column or an Array.\n","- `outputCol`: The name of the column in Document type that is generated. We can specify only one column here.\n","\n","\n","All the parameters can be set using the corresponding set method in camel case. For example, `.setInputcols()`."],"metadata":{"id":"AUbv3YL59D8Q"}},{"cell_type":"markdown","source":["### `inputCols` and `outputCol`"],"metadata":{"id":"EpBqGk7oNc3C"}},{"cell_type":"markdown","source":["Define the column names containing the `DOCUMENT` and `TOKEN` annotations needed as input to the `Doc2ChunkInternal ` and the name of the new column containg the identified entities.\n","\n","Let's define a pipeline to process raw texts into `DOCUMENT` and `TOKEN` annotations:"],"metadata":{"id":"wOGfzMDiOAXE"}},{"cell_type":"code","source":["documentAssembler = nlp.DocumentAssembler()\\\n","    .setInputCol(\"text\")\\\n","    .setOutputCol(\"document\")\n","\n","tokenizer = nlp.Tokenizer()\\\n","    .setInputCols(\"document\")\\\n","    .setOutputCol(\"token\")\n","\n","chunkAssembler = medical.Doc2ChunkInternal()\\\n","    .setInputCols(\"document\", \"token\")\\\n","    .setChunkCol(\"target\")\\\n","    .setOutputCol(\"chunk\")\\\n","    .setIsArray(True)\n","\n","pipeline = nlp.Pipeline().setStages([documentAssembler, tokenizer, chunkAssembler])"],"metadata":{"id":"c23AbFZ7OV6B","executionInfo":{"status":"ok","timestamp":1687295974157,"user_tz":-180,"elapsed":291,"user":{"displayName":"Monster C","userId":"08787989274818793476"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["data = spark.createDataFrame([[\n","    \"Spark NLP is an open-source text processing library for advanced natural language processing.\", [\"Spark NLP\", \"text processing library\", \"natural language processing\"],\n","    ]]).toDF(\"text\", \"target\")\n","\n","data.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b_Q4lQpa1bUN","executionInfo":{"status":"ok","timestamp":1687295986109,"user_tz":-180,"elapsed":8670,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"5d925b2b-5b8b-4162-ea1c-1ae82c192c35"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------+\n","|text                                                                                         |target                                                           |\n","+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------+\n","|Spark NLP is an open-source text processing library for advanced natural language processing.|[Spark NLP, text processing library, natural language processing]|\n","+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------+\n","\n"]}]},{"cell_type":"code","source":["result = pipeline.fit(data).transform(data)\n","result.selectExpr(\"chunk.result\", \"chunk.annotatorType\").show(truncate=False)"],"metadata":{"id":"iihpMyAv1lEi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687295991279,"user_tz":-180,"elapsed":5177,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"92e9b49b-e24b-4d66-abe0-f21166cc58ea"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------------------------------------------------------+---------------------+\n","|result                                                           |annotatorType        |\n","+-----------------------------------------------------------------+---------------------+\n","|[Spark NLP, text processing library, natural language processing]|[chunk, chunk, chunk]|\n","+-----------------------------------------------------------------+---------------------+\n","\n"]}]},{"cell_type":"code","source":["result.selectExpr(\"chunk\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2MkrLO-U4utZ","executionInfo":{"status":"ok","timestamp":1687295992014,"user_tz":-180,"elapsed":747,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"6f76078c-e61e-442b-c224-7b512438cdb2"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|chunk                                                                                                                                                                                                                |\n","+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|[{chunk, 0, 8, Spark NLP, {sentence -> 0, chunk -> 0}, []}, {chunk, 28, 50, text processing library, {sentence -> 0, chunk -> 1}, []}, {chunk, 65, 91, natural language processing, {sentence -> 0, chunk -> 2}, []}]|\n","+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n"]}]},{"cell_type":"code","source":["result_df = result.select(F.explode(F.arrays_zip(result.chunk.result,\n","                                                 result.chunk.annotatorType,\n","                                                 result.chunk.metadata)).alias(\"cols\"))\\\n","                  .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n","                          F.expr(\"cols['1']\").alias(\"annotatorType\"),\n","                          F.expr(\"cols['2']\").alias(\"metadata\"))\n","\n","result_df.show(50, truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2cwA-24e4kYd","executionInfo":{"status":"ok","timestamp":1687296289173,"user_tz":-180,"elapsed":1330,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"d085ab59-df10-4c7b-81ef-bcfa0f5a5f8d"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------------------------+-------------+---------------------------+\n","|chunk                      |annotatorType|metadata                   |\n","+---------------------------+-------------+---------------------------+\n","|Spark NLP                  |chunk        |{sentence -> 0, chunk -> 0}|\n","|text processing library    |chunk        |{sentence -> 0, chunk -> 1}|\n","|natural language processing|chunk        |{sentence -> 0, chunk -> 2}|\n","+---------------------------+-------------+---------------------------+\n","\n"]}]},{"cell_type":"code","source":["chunkAssembler.extractParamMap()"],"metadata":{"id":"vg44awUQrv0Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687296289174,"user_tz":-180,"elapsed":4,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"0cae3d42-cfbf-4c6d-9f47-8c9764451cec"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{Param(parent='Doc2ChunkInternal_b4e2c856bc68', name='isArray', doc='whether the chunkCol is an array of strings'): True,\n"," Param(parent='Doc2ChunkInternal_b4e2c856bc68', name='inputCols', doc='previous annotations columns, if renamed'): ['document',\n","  'token'],\n"," Param(parent='Doc2ChunkInternal_b4e2c856bc68', name='chunkCol', doc='column that contains string. Must be part of DOCUMENT'): 'target',\n"," Param(parent='Doc2ChunkInternal_b4e2c856bc68', name='outputCol', doc='output annotation column. can be left default.'): 'chunk'}"]},"metadata":{},"execution_count":17}]}]}