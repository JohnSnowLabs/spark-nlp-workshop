{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMlSroS1etL7Fd3DjpEJayq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"],"metadata":{"id":"8V_Ow3cAVEYe"}},{"cell_type":"markdown","source":["# **NerChunker**"],"metadata":{"id":"Y0fJRpNJBslT"}},{"cell_type":"markdown","source":["This notebook will cover the uses and the `RegexParsers` parameter of `NerChunker`. This annotator extracts phrases that fits into a known pattern using the NER tags. \n","\n","\n","\n","\n","**📖 Learning Objectives:**\n","\n","1. Understand how `NerChunker` works.\n","\n","2. Become comfortable using the Regex parameter of the annotator.\n","\n","\n","**🔗 Helpful Links:**\n","\n","- Documentation : [NerChunker](https://nlp.johnsnowlabs.com/docs/en/licensed_annotators#nerchunker)\n","\n","- Python Docs : [NerChunker](https://nlp.johnsnowlabs.com/licensed/api/python/reference/autosummary/sparknlp_jsl/annotator/ner/ner_chunker/index.html#sparknlp_jsl.annotator.ner.ner_chunker.NerChunker)\n","\n","- Scala Docs : [NerChunker](https://nlp.johnsnowlabs.com/licensed/api/com/johnsnowlabs/nlp/annotators/ner/NerChunker.html)\n","\n","- For extended examples of usage, see [Spark NLP Workshop repository](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/healthcare-nlp/01.0.Clinical_Named_Entity_Recognition_Model.ipynb).\n"],"metadata":{"id":"qeclGJmrVLjX"}},{"cell_type":"markdown","source":["## **📜 Background**"],"metadata":{"id":"OV0hPSCWXslc"}},{"cell_type":"markdown","source":["The `NerChunker` annotator is a component of the Spark NLP library that performs chunking of named entities that fit into a pattern defined by the Regex Parameter - `setRegexParsers`.\n","\n","Named Entity Recognition (NER) is the process of identifying named entities such as people, organizations, locations, and other entities in unstructured text data. Chunking is the process of grouping together contiguous tokens in a sentence based on their relationships.\n","\n","`NerChunker` annotator in Spark NLP combines these two tasks by first identifying named entities in a sentence using a model trained on annotated data, and then grouping them into chunks based on their type and position in the sentence. \n","\n","The output of the `NerChunker` annotator is a set of annotations that label each token in a sentence with its named entity type and chunk label, as well as any additional metadata such as confidence scores or start/end offsets. This can be useful for a variety of natural language processing tasks such as information extraction, entity linking, and text classification."],"metadata":{"id":"_-hGkZ7_cAZE"}},{"cell_type":"markdown","source":["## **🎬 Colab Setup**"],"metadata":{"id":"E8qy2MI2XySv"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"HuLFt0OdBkuo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1686078925982,"user_tz":240,"elapsed":41026,"user":{"displayName":"Gursev Pirge","userId":"01579888832874245157"}},"outputId":"51deac4f-5e20-498b-bb9d-da2195a584e7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.2/84.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.4/212.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.7/486.7 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m639.9/639.9 kB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.4/95.4 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.5/83.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["# Install the johnsnowlabs library to access Spark-OCR and Spark-NLP for Healthcare, Finance, and Legal.\n","! pip install -q johnsnowlabs "]},{"cell_type":"code","source":["from google.colab import files\n","print('Please Upload your John Snow Labs License using the button below')\n","license_keys = files.upload()"],"metadata":{"id":"HJGAUiIqBI94","colab":{"base_uri":"https://localhost:8080/","height":90},"executionInfo":{"status":"ok","timestamp":1686078935054,"user_tz":240,"elapsed":6943,"user":{"displayName":"Gursev Pirge","userId":"01579888832874245157"}},"outputId":"dc030462-b61a-45b9-ab15-8f969ff82382"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Please Upload your John Snow Labs License using the button below\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-20cca54d-4396-4449-ac11-30880313cbba\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-20cca54d-4396-4449-ac11-30880313cbba\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving 4.4.3.spark_nlp_for_healthcare.json to 4.4.3.spark_nlp_for_healthcare.json\n"]}]},{"cell_type":"code","source":["from johnsnowlabs import nlp, medical, visual\n","\n","# After uploading your license run this to install all licensed Python Wheels and pre-download Jars the Spark Session JVM\n","nlp.install()"],"metadata":{"id":"SBhRUP21BI7R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from johnsnowlabs import nlp, medical, visual\n","import pandas as pd\n","\n","# Automatically load license data and start a session with all jars user has access to\n","spark = nlp.start()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_1HS4iCB5ie","executionInfo":{"status":"ok","timestamp":1684431950839,"user_tz":240,"elapsed":18582,"user":{"displayName":"Gursev Pirge","userId":"01579888832874245157"}},"outputId":"ef915955-1a28-4958-d32b-e3beece5390c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["👌 Detected license file /content/4.4.1.spark_nlp_for_healthcare.json\n","👌 Launched \u001b[92mcpu optimized\u001b[39m session with with: 🚀Spark-NLP==4.4.1, 💊Spark-Healthcare==4.4.1, running on ⚡ PySpark==3.1.2\n"]}]},{"cell_type":"code","source":["from pyspark.sql import DataFrame\n","import pyspark.sql.functions as F\n","import pyspark.sql.types as T\n","import pyspark.sql as SQL\n","from pyspark import keyword_only"],"metadata":{"id":"dycus2LCBI4R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **🖨️ Input/Output Annotation Types**"],"metadata":{"id":"f_4conOIYj6D"}},{"cell_type":"markdown","source":["- Input: `DOCUMENT`, `NAMED_ENTITY`\n","\n","- Output: `CHUNK`"],"metadata":{"id":"0Fc_3iRwYk8N"}},{"cell_type":"markdown","source":["## **🔎 Parameters**"],"metadata":{"id":"rgCCuKLdepW9"}},{"cell_type":"markdown","source":["- `setRegexParsers`: Array of grammar based chunk parsers.   \n","\n"],"metadata":{"id":"Y4tqQGMsj1BQ"}},{"cell_type":"markdown","source":["### `setRegexParsers`"],"metadata":{"id":"LW_kZcwMeK2h"}},{"cell_type":"markdown","source":["## **💻 Pipeline**"],"metadata":{"id":"pagXc73HWidS"}},{"cell_type":"markdown","source":["Let us define pipeline for extracting posology related entities by using the [ner_posology](https://nlp.johnsnowlabs.com/2020/04/15/ner_posology_en.html) model. \n","\n","This model will extract the following entities:\n","\n","`DOSAGE`, `DRUG`, `DURATION`, `FORM`, `FREQUENCY`, `ROUTE`, `STRENGTH`"],"metadata":{"id":"wHcZWID_aHzy"}},{"cell_type":"code","source":["# Annotator that transforms a text column from dataframe into an Annotation ready for NLP\n","documentAssembler = nlp.DocumentAssembler()\\\n","    .setInputCol(\"text\")\\\n","    .setOutputCol(\"document\")\n","        \n","sentenceDetector = nlp.SentenceDetectorDLModel.pretrained(\"sentence_detector_dl_healthcare\",\"en\",\"clinical/models\")\\\n","    .setInputCols([\"document\"])\\\n","    .setOutputCol(\"sentence\")\n"," \n","# Tokenizer splits words in a relevant format for NLP\n","tokenizer = nlp.Tokenizer()\\\n","    .setInputCols([\"sentence\"])\\\n","    .setOutputCol(\"token\")\n","\n","# Clinical word embeddings trained on PubMED dataset\n","word_embeddings = nlp.WordEmbeddingsModel.pretrained(\"embeddings_clinical\",\"en\",\"clinical/models\")\\\n","    .setInputCols([\"sentence\",\"token\"])\\\n","    .setOutputCol(\"embeddings\")\n","\n","# NER model trained for pextracting entities related to posology\n","posology_ner = medical.NerModel.pretrained(\"ner_posology\", \"en\", \"clinical/models\") \\\n","    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n","    .setOutputCol(\"ner\")\n","\n","nlpPipeline = nlp.Pipeline(stages=[\n","    documentAssembler, \n","    sentenceDetector,\n","    tokenizer,\n","    word_embeddings,\n","    posology_ner])\n","\n","empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n","\n","ner_model = nlpPipeline.fit(empty_data)"],"metadata":{"id":"sZKRtor7c_eJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684438438041,"user_tz":240,"elapsed":8887,"user":{"displayName":"Gursev Pirge","userId":"01579888832874245157"}},"outputId":"c86a8549-1a69-49d8-8aeb-5e0f3dd3484e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sentence_detector_dl_healthcare download started this may take some time.\n","Approximate size to download 367.3 KB\n","[OK!]\n","embeddings_clinical download started this may take some time.\n","Approximate size to download 1.6 GB\n","[OK!]\n","ner_posology download started this may take some time.\n","[OK!]\n"]}]},{"cell_type":"markdown","source":["Define a sample text about the usage of drugs, convert the text to Pyspark dataframe and get predictions for posology-related entity extraction by using `.transform`."],"metadata":{"id":"FY7P-dn8a4D6"}},{"cell_type":"code","source":["sample_text = \"\"\"The patient was prescribed 1 capsule of Advil for 5 days. \n","He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals, metformin 1000 mg two times a day.\"\"\"\n","\n","data = spark.createDataFrame([[sample_text]]).toDF(\"text\")\n","\n","result = ner_model.transform(data)\n","\n","result.select('text', 'token.result', 'ner.result').show(truncate = 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OsNGbZTqWJM2","executionInfo":{"status":"ok","timestamp":1684439696735,"user_tz":240,"elapsed":1881,"user":{"displayName":"Gursev Pirge","userId":"01579888832874245157"}},"outputId":"060e9639-1f68-48d3-8fdd-6ec13088a5dd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------+\n","|                                                        text|                                                      result|                                                      result|\n","+------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------+\n","|The patient was prescribed 1 capsule of Advil for 5 days....|[The, patient, was, prescribed, 1, capsule, of, Advil, fo...|[O, O, O, O, B-DOSAGE, B-FORM, O, B-DRUG, B-DURATION, I-D...|\n","+------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["The result in the dataframe below shows the entities extracted by the posology model."],"metadata":{"id":"ce17F7Tveiyx"}},{"cell_type":"code","source":["result.select('ner.result').show(truncate = False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qWqtJCcmeWlk","executionInfo":{"status":"ok","timestamp":1684440461339,"user_tz":240,"elapsed":1592,"user":{"displayName":"Gursev Pirge","userId":"01579888832874245157"}},"outputId":"99f2fddd-d1d6-4fae-9cb8-2cbcee854026"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|result                                                                                                                                                                                                                                                                                                                                             |\n","+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|[O, O, O, O, B-DOSAGE, B-FORM, O, B-DRUG, B-DURATION, I-DURATION, I-DURATION, O, O, O, O, O, O, O, O, O, O, O, O, O, B-DOSAGE, I-DOSAGE, O, B-DRUG, I-DRUG, B-FREQUENCY, I-FREQUENCY, O, B-DOSAGE, I-DOSAGE, O, B-DRUG, I-DRUG, B-FREQUENCY, I-FREQUENCY, O, B-DRUG, B-STRENGTH, I-STRENGTH, B-FREQUENCY, I-FREQUENCY, I-FREQUENCY, I-FREQUENCY, O]|\n","+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["Explode the results to include the tokens, labels predicted by the model and the confidence score to get a better picture."],"metadata":{"id":"XYY6GELmbkB2"}},{"cell_type":"code","source":["result.select(F.explode(F.arrays_zip(result.token.result, \n","                                              result.ner.result, \n","                                              result.ner.metadata)).alias(\"cols\")) \\\n","               .select(F.expr(\"cols['0']\").alias(\"token\"),\n","                       F.expr(\"cols['1']\").alias(\"ner_label\"),\n","                       F.expr(\"cols['2']['confidence']\").alias(\"confidence\"))\\\n","               .filter(\"ner_label!='O'\")\\\n","               .show(30, truncate=100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":242},"id":"ZNOqSm7tWJJ8","executionInfo":{"status":"error","timestamp":1686079371849,"user_tz":240,"elapsed":253,"user":{"displayName":"Gursev Pirge","userId":"01579888832874245157"}},"outputId":"7d4929ca-1a9f-466f-cd80-aadfb151b572"},"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-789f5d46de78>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m result.select(F.explode(F.arrays_zip(result.token.result, \n\u001b[0m\u001b[1;32m      2\u001b[0m                                               \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                                               result.ner.metadata)).alias(\"cols\")) \\\n\u001b[1;32m      4\u001b[0m                .select(F.expr(\"cols['0']\").alias(\"token\"),\n\u001b[1;32m      5\u001b[0m                        \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cols['1']\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ner_label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'result' is not defined"]}]},{"cell_type":"markdown","source":["### `setRegexParsers`"],"metadata":{"id":"Opw8cfSSXmfc"}},{"cell_type":"markdown","source":["This parameter is used to define a list of regex patterns to match chunks.\n","\n","\n","Let's say we want to extract `DRUG` and `FREQUENCY` together as a single chunk even if there are some unwanted tokens between them. "],"metadata":{"id":"wfPKyJLrYW78"}},{"cell_type":"code","source":["# To extract drug and frequency together as a single chunk even if there are some unwanted tokens between them.\n","ner_chunker = medical.NerChunker()\\\n","    .setInputCols([\"sentence\",\"ner\"])\\\n","    .setOutputCol(\"ner_chunk\")\\\n","    .setRegexParsers([\"<DRUG>.*<FREQUENCY>\"])\n","\n","nlpPipeline = nlp.Pipeline(stages=[\n","    documentAssembler, \n","    sentenceDetector,\n","    tokenizer,\n","    word_embeddings,\n","    posology_ner,\n","    ner_chunker])\n","\n","ner_chunker_model = nlpPipeline.fit(empty_data)"],"metadata":{"id":"bT3jVwiiWAnG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sample_text = \"\"\"The patient was prescribed 1 capsule of Advil for 5 days . He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night, 12 units of insulin lispro with meals , metformin 1000 mg two times a day.\"\"\"\n","\n","result = ner_chunker_model.transform(data)"],"metadata":{"id":"xuAZ7pUtpHb8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result.select('ner_chunk.result').show(truncate = False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"40NU4_nReHf-","executionInfo":{"status":"ok","timestamp":1684440410649,"user_tz":240,"elapsed":193,"user":{"displayName":"Gursev Pirge","userId":"01579888832874245157"}},"outputId":"da5eec31-e15a-40e5-b8b6-ebf48ea7687f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------------------------------------------------------------------------------------------+\n","|result                                                                                               |\n","+-----------------------------------------------------------------------------------------------------+\n","|[insulin glargine at night, 12 units of insulin lispro with meals, metformin 1000 mg two times a day]|\n","+-----------------------------------------------------------------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["The result shows the chunks, defined by the `setRegexParsers` parameter, including all the tokens between those entitiy types.\n","\n","In this case, `DRUG` and `FREQUENCY`."],"metadata":{"id":"7Q4IIl_TeH_-"}},{"cell_type":"markdown","source":["### LightPipeline\n","\n","Let’s use `LightPipeline` here to extract the entities. \n","\n","[LightPipeline](https://nlp.johnsnowlabs.com/docs/en/concepts#using-spark-nlps-lightpipeline) is a Spark NLP specific Pipeline class equivalent to the Spark ML Pipeline, which achieves fast results when dealing with small amounts of data."],"metadata":{"id":"8_NNMSUBGLKu"}},{"cell_type":"code","source":["light_model = nlp.LightPipeline(ner_chunker_model)\n","\n","light_result = light_model.annotate(sample_text)\n","\n","list(zip(light_result['token'], light_result['ner']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZyW0E4EGOD3","executionInfo":{"status":"ok","timestamp":1684439886379,"user_tz":240,"elapsed":489,"user":{"displayName":"Gursev Pirge","userId":"01579888832874245157"}},"outputId":"c4e3f3ec-7221-4078-b500-9a5ec2c32a62"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('The', 'O'),\n"," ('patient', 'O'),\n"," ('was', 'O'),\n"," ('prescribed', 'O'),\n"," ('1', 'B-DOSAGE'),\n"," ('capsule', 'B-FORM'),\n"," ('of', 'O'),\n"," ('Advil', 'B-DRUG'),\n"," ('for', 'B-DURATION'),\n"," ('5', 'I-DURATION'),\n"," ('days', 'I-DURATION'),\n"," ('.', 'O'),\n"," ('He', 'O'),\n"," ('was', 'O'),\n"," ('seen', 'O'),\n"," ('by', 'O'),\n"," ('the', 'O'),\n"," ('endocrinology', 'O'),\n"," ('service', 'O'),\n"," ('and', 'O'),\n"," ('she', 'O'),\n"," ('was', 'O'),\n"," ('discharged', 'O'),\n"," ('on', 'O'),\n"," ('40', 'B-DOSAGE'),\n"," ('units', 'I-DOSAGE'),\n"," ('of', 'O'),\n"," ('insulin', 'B-DRUG'),\n"," ('glargine', 'I-DRUG'),\n"," ('at', 'B-FREQUENCY'),\n"," ('night', 'I-FREQUENCY'),\n"," (',', 'O'),\n"," ('12', 'B-DOSAGE'),\n"," ('units', 'I-DOSAGE'),\n"," ('of', 'O'),\n"," ('insulin', 'B-DRUG'),\n"," ('lispro', 'I-DRUG'),\n"," ('with', 'B-FREQUENCY'),\n"," ('meals', 'I-FREQUENCY'),\n"," (',', 'O'),\n"," ('metformin', 'B-DRUG'),\n"," ('1000', 'B-STRENGTH'),\n"," ('mg', 'I-STRENGTH'),\n"," ('two', 'B-FREQUENCY'),\n"," ('times', 'I-FREQUENCY'),\n"," ('a', 'I-FREQUENCY'),\n"," ('day', 'I-FREQUENCY'),\n"," ('.', 'O')]"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["light_result[\"ner_chunk\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MT-QEL6dGoN2","executionInfo":{"status":"ok","timestamp":1684439891137,"user_tz":240,"elapsed":163,"user":{"displayName":"Gursev Pirge","userId":"01579888832874245157"}},"outputId":"61ae2bc8-ac32-4173-e293-85056a7416ca"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['insulin glargine at night, 12 units of insulin lispro with meals , metformin 1000 mg two times a day']"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","source":["The result shows the chunks, defined by the `setRegexParsers` parameter, including all the tokens between those entity types.\n","\n","In this case, `DRUG` and `FREQUENCY`."],"metadata":{"id":"4BLIkAbXdjRd"}},{"cell_type":"code","source":[],"metadata":{"id":"QXf5yzXTe7Hi"},"execution_count":null,"outputs":[]}]}