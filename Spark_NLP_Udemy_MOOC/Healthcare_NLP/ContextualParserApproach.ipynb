{"cells":[{"cell_type":"markdown","metadata":{"id":"klIak_Gb_OPJ"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n"]},{"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/Spark_NLP_Udemy_MOOC/Healthcare_NLP/ContextualParserApproach.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"bewC1SWN-6jB"},"source":["# **ContextualParserApproach**"]},{"cell_type":"markdown","metadata":{"id":"m048uDkB69Rv"},"source":["This notebook will cover the different parameters and usages of `ContextualParserApproach` annotator.\n","\n","**ğŸ“– Learning Objectives:**\n","\n","1. Understand how to use `ContextualParserApproach`.\n","\n","2. Become comfortable using the different parameters of the annotator.\n","\n","3. Train an `ContextualParserModel` based on pattern matching.\n","\n","\n","**ğŸ”— Helpful Links:**\n","\n","- Documentation : [ContextualParserApproach](https://nlp.johnsnowlabs.com/docs/en/licensed_annotators#contextualparser)\n","\n","- Python Docs : [ContextualParserApproach](https://nlp.johnsnowlabs.com/licensed/api/python/reference/autosummary/sparknlp_jsl/annotator/context/contextual_parser/index.html#sparknlp_jsl.annotator.context.contextual_parser.ContextualParserApproach)\n","\n","- Scala Docs : [ContextualParserApproach](https://nlp.johnsnowlabs.com/licensed/api/com/johnsnowlabs/nlp/annotators/context/ContextualParserApproach.html)\n","\n","- For extended examples of usage, see the [Spark NLP Workshop repository](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/healthcare-nlp/01.5.Contextual_Parser_Rule_Based_NER.ipynb)."]},{"cell_type":"markdown","metadata":{"id":"B9Wo54MT8jKU"},"source":["## **ğŸ“œ Background**\n"]},{"cell_type":"markdown","metadata":{"id":"yaDBNKJsAovm"},"source":["`ContextualParser` annotator extracts entities from texts based on pattern matching. It provides more functionality than its open-source counterpart `EntityRuler` by allowing users to customize specific characteristics for pattern matching.\n","\n","It allows setting regex rules for full and partial matches, a dictionary with normalizing options and context parameters to take into account specific conditions such as token distances.\n","\n","`ContextualParserApproach` annotator learns the patterns given by JSON/TSV/CSV file to define a new `ContextualParserModel`."]},{"cell_type":"markdown","metadata":{"id":"A4hMnkhd_ik9"},"source":["## **ğŸ¬ Colab Setup**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":66745,"status":"ok","timestamp":1718057490645,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"xrdvNxjD_yQI","outputId":"f60d95a3-1490-4ca1-944e-960cee63b276"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m265.2/265.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m565.0/565.0 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m676.2/676.2 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q johnsnowlabs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":91},"executionInfo":{"elapsed":7630,"status":"ok","timestamp":1718057498266,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"zbUfCKmmiVCG","outputId":"d8cb7f1d-c002-4916-f448-2ce7d841dc47"},"outputs":[{"name":"stdout","output_type":"stream","text":["Please Upload your John Snow Labs License using the button below\n"]},{"data":{"text/html":["\n","     <input type=\"file\" id=\"files-3ac11c67-74b3-4c95-8afc-5fe84f5ff529\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-3ac11c67-74b3-4c95-8afc-5fe84f5ff529\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Saving spark_nlp_for_healthcare_spark_ocr_8734_532.json to spark_nlp_for_healthcare_spark_ocr_8734_532.json\n"]}],"source":["from google.colab import files\n","print('Please Upload your John Snow Labs License using the button below')\n","license_keys = files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":66758,"status":"ok","timestamp":1718057565020,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"NulWi4_f4GN5","outputId":"10261d07-be2f-4593-888d-c1276c727dd0"},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ‘Œ Detected license file /content/spark_nlp_for_healthcare_spark_ocr_8734_532.json\n","ğŸ“‹ Stored John Snow Labs License in /root/.johnsnowlabs/licenses/license_number_0_for_Spark-Healthcare_Spark-OCR.json\n","ğŸ‘· Setting up  John Snow Labs home in /root/.johnsnowlabs, this might take a few minutes.\n","Downloading ğŸ+ğŸš€ Python Library spark_nlp-5.3.2-py2.py3-none-any.whl\n","Downloading ğŸ+ğŸ’Š Python Library spark_nlp_jsl-5.3.2-py3-none-any.whl\n","Downloading ğŸ«˜+ğŸš€ Java Library spark-nlp-assembly-5.3.2.jar\n","Downloading ğŸ«˜+ğŸ’Š Java Library spark-nlp-jsl-5.3.2.jar\n","ğŸ™† JSL Home setup in /root/.johnsnowlabs\n","ğŸ‘Œ Detected license file /content/spark_nlp_for_healthcare_spark_ocr_8734_532.json\n","Installing /root/.johnsnowlabs/py_installs/spark_nlp_jsl-5.3.2-py3-none-any.whl to /usr/bin/python3\n","Installed 1 products:\n","ğŸ’Š Spark-Healthcare==5.3.2 installed! âœ… Heal the planet with NLP! \n"]}],"source":["from johnsnowlabs import nlp, medical\n","\n","nlp.install()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18867,"status":"ok","timestamp":1718057583881,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"2GLXxe1Q0Iln","outputId":"ba7173a7-1d05-4748-a4cc-2f99cf1f3a6a"},"outputs":[{"name":"stdout","output_type":"stream","text":["ğŸ‘Œ Detected license file /content/spark_nlp_for_healthcare_spark_ocr_8734_532.json\n","ğŸ‘Œ Launched \u001b[92mcpu optimized\u001b[39m session with with: ğŸš€Spark-NLP==5.3.2, ğŸ’ŠSpark-Healthcare==5.3.2, running on âš¡ PySpark==3.4.0\n"]}],"source":["spark = nlp.start()\n","spark"]},{"cell_type":"markdown","metadata":{"id":"fdTHvykf8wni"},"source":["## **ğŸ–¨ï¸ Input/Output Annotation Types**"]},{"cell_type":"markdown","metadata":{"id":"ejYVNcX98y5j"},"source":["- Input: `DOCUMENT`, `TOKEN`\n","\n","- Output: `CHUNK`"]},{"cell_type":"markdown","metadata":{"id":"YVa72oJd9Bk_"},"source":["## **ğŸ” Parameters**\n"]},{"cell_type":"markdown","metadata":{"id":"AUbv3YL59D8Q"},"source":["- `inputCols`: The name of the columns containing the input annotations. It can read either a String column or an Array.\n","- `outputCol`: The name of the column in Document type that is generated. We can specify only one column here.\n","- `jsonPath`: Path to json file containing regex patterns and rules to match the entities.\n","- `dictionary`: Path to dictionary file in tsv or csv format.\n","- `caseSensitive`: Whether to use case sensitive when matching values.\n","- `prefixAndSuffixMatch`: Whether to match both prefix and suffix to annotate the match.\n","- `optionalContextRules`: When set to true, it will output regex match regardless of context matches.\n","- `shortestContextMatch`: When set to true, it will stop finding for matches when prefix/suffix data is found in the text.\n","- `completeContextMatch`: Whether to do an exact match of prefix and suffix.\n","\n","All the parameters can be set using the corresponding set method in camel case. For example, `.setInputcols()`."]},{"cell_type":"markdown","metadata":{"id":"EpBqGk7oNc3C"},"source":["### `inputCols` and `outputCol`"]},{"cell_type":"markdown","metadata":{"id":"wOGfzMDiOAXE"},"source":["Define the column names containing the `DOCUMENT` and `TOKEN` annotations needed as input to the `ContextualParser` and the name of the new column containg the identified entities.\n","\n","Let's define a pipeline to process raw texts into `DOCUMENT` and `TOKEN` annotations:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c23AbFZ7OV6B"},"outputs":[],"source":["document_assembler = (\n","    nlp.DocumentAssembler()\n","    .setInputCol(\"text\")\n","    .setOutputCol(\"document\")\n",")\n","\n","tokenizer = (\n","    nlp.Tokenizer().setInputCols(\"document\").setOutputCol(\"token\")\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"J05F4CcZA30R"},"source":["Then, we use the defined column names of the previous stages to define the `ContextualParserApproach` input columns and define a name for the output column:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KfueJP-rALU_"},"outputs":[],"source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"document\", \"token\"])\n","    .setOutputCol(\"entity\")\n",")"]},{"cell_type":"markdown","metadata":{"id":"7T_3otzSBllg"},"source":["### `jsonPath`"]},{"cell_type":"markdown","metadata":{"id":"V-NurY5XB8lK"},"source":["Defines the path to the JSON file containing the rules to match the entities on the text. The file needs to define the following information:\n","\n","- `entity`: type of the entity that will be matched\n","- `ruleScope`: Scope to search the pattern. Can be one of the following:\n","  `document`: Match the entities on the entire document (useful for matching multi-token/phrases entities).\n","  - `sentence`: Match the entities in sentences in a token level (match words).\n","- `regex`: The pattern to match (backslashes are escape characters in JSON, so for regex pattern \"\\d+\" we need to write it out as \"\\\\d+\").\n","- `completeMatchRegex`: Whether to consider only the exact matches on full tokens. If set to `True`, the parameter `matchScope` is ignored.\n","- `matchScope`: The return level of the match:\n"," - `token`: Returns the entire token containing the matched rule.\n"," - `sub-token`: Returns the part of the token where the rule matches.\n","- `prefix`: List of prefixes to be cosidered on the matches.\n","- `suffix`: List of suffixes to be cosidered on the matches.\n","- `contextLength`: Maximum length to be used as context.\n","- `contextException`: List of exceptions on the context.\n","- `exceptionDistance`: Maximum distance of the exception (Default to 40).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TK_XNeVOnGVp"},"source":["Let's see how to use a JSON file in an example:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15855,"status":"ok","timestamp":1718057599731,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"OYYFxn4Qo7RS","outputId":"e448a624-351a-4ca8-d586-25016d8b9591"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+\n","|                text|\n","+--------------------+\n","|Peter Parker is a...|\n","+--------------------+\n","\n"]}],"source":["sample_text = \"\"\"Peter Parker is a nice guy and lives in New York . Bruce Wayne is also a nice guy and lives in San Antonio and Gotham City . \"\"\"\n","\n","sample_df = spark.createDataFrame([[sample_text]]).toDF(\"text\")\n","sample_df.show()\n"]},{"cell_type":"markdown","metadata":{"id":"gTtKlUsnpM4h"},"source":["We will preprocess the example sentences with the required input annotations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lef4k3DrnSN6"},"outputs":[],"source":["document_assembler = (\n","    nlp.DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",")\n","\n","sentence_detector = (\n","    nlp.SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentence\")\n",")\n","\n","tokenizer = nlp.Tokenizer().setInputCols([\"sentence\"]).setOutputCol(\"token\")\n","\n","\n","preprocessPipeline = nlp.Pipeline(\n","    stages=[document_assembler, sentence_detector, tokenizer]\n",")\n","\n","\n","preprocessModel = preprocessPipeline.fit(sample_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4526,"status":"ok","timestamp":1718057607078,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"lsJqJEbPpUWg","outputId":"70a1790e-748a-4104-fc38-31641b13e6e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+--------------------+--------------------+\n","|                text|            document|            sentence|               token|\n","+--------------------+--------------------+--------------------+--------------------+\n","|Peter Parker is a...|[{document, 0, 12...|[{document, 0, 49...|[{token, 0, 4, Pe...|\n","+--------------------+--------------------+--------------------+--------------------+\n","\n"]}],"source":["processed_df = preprocessModel.transform(sample_df)\n","processed_df.show()"]},{"cell_type":"markdown","metadata":{"id":"vvtfiAzlpnHn"},"source":["Create a sample JSON file:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2QkxOQeHBmup"},"outputs":[],"source":["import json\n","\n","\n","cities = {\n","    \"entity\": \"City\",\n","    \"ruleScope\": \"document\",\n","    \"matchScope\": \"sub-token\",\n","    \"completeMatchRegex\": \"false\",\n","    \"regex\": \"([A-Z]{1}[a-z]+ [A-Z]{1}[a-z]+)\" # Find two consecutive words in title case.\n","}\n","\n","with open(\"cities.json\", \"w\") as f:\n","    json.dump(cities, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6120,"status":"ok","timestamp":1718057613185,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"iOsSgnSEocEF","outputId":"833e2bbb-c119-4ca4-af89-eaed97eb10e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------------------------------------------------------+\n","|result                                                         |\n","+---------------------------------------------------------------+\n","|[Peter Parker, New York, Bruce Wayne, San Antonio, Gotham City]|\n","+---------------------------------------------------------------+\n","\n"]}],"source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setJsonPath(\"cities.json\")\n",")\n","\n","contextual_parser.fit(processed_df).transform(processed_df).select(\"entity.result\").show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"BtxGj5RB0eXW"},"source":["Using the context to find matches:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bPI8xOqY0hwf"},"outputs":[],"source":["context_example = \"At birth, the typical boy is growing slightly faster than the typical girl, but growth rates become equal at about seven months.\"\n","context_df = spark.createDataFrame([[context_example]]).toDF(\"text\")\n","processed_context = preprocessModel.transform(context_df)\n","\n","\n","context_rules = {\n","  \"entity\": \"Gender\",\n","  \"ruleScope\": \"sentence\",\n","  \"regex\": \"girl|boy\",\n","  \"contextLength\": 50,\n","  \"prefix\": [\"birth\"],\n","  \"suffix\": [\"faster\", \"rates\"]\n","}\n","\n","with open(\"context.json\", \"w\") as f:\n","    json.dump(context_rules, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1354,"status":"ok","timestamp":1718057614968,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"6p7UZm1K3S9S","outputId":"b17b2edf-901b-4256-d247-1d451cdf2a08"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+\n","|result     |\n","+-----------+\n","|[boy, girl]|\n","+-----------+\n","\n"]}],"source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setJsonPath(\"context.json\")\n",")\n","\n","contextual_parser.fit(processed_context).transform(processed_context).select(\"entity.result\").show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"ceu-4opwsKQr"},"source":["### `dictionary`"]},{"cell_type":"markdown","metadata":{"id":"AE7tDhBHsN2G"},"source":["The dictionary parameter can be used to define to define entities as a list to be found on the text.\n","\n","When setting the dictionary file, we can use the parameter `orientation` that indicates whether the file is to be read horizontally or vertically.\n","\n","Horizontal:\n","\n","| normalize | word1 | word2 | word3     |\n","|-----------|-------|-------|-----------|\n","| female    | woman | girl  | lady      |\n","| male      | man   | boy   | gentleman |\n","\n","\n","Vertical:\n","\n","| female    | normalize |\n","|-----------|-----------|\n","| woman     | word1     |\n","| girl      | word2     |\n","| lady      | word3     |\n","\n","</br>\n","\n","JSON path needs to be set."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1718057614968,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"quMhYlmwpzfI","outputId":"859d4cce-4255-4394-b4d8-f2e47ceb0a9b"},"outputs":[{"name":"stdout","output_type":"stream","text":["City\n","New York\n","Gotham City\n","San Antonio\n","Salt Lake City"]}],"source":["# Create a dictionary to detect cities\n","cities = \"\"\"City\\nNew York\\nGotham City\\nSan Antonio\\nSalt Lake City\"\"\"\n","\n","\n","# TSV or CSV\n","with open('cities.tsv', 'w') as f:\n","    f.write(cities)\n","\n","# Check what dictionary looks like\n","!cat cities.tsv"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1255,"status":"ok","timestamp":1718057616218,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"CUzSUhF7stCa","outputId":"4764a771-ae80-40ba-c1bb-c39487ae3a69"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------+\n","|result                              |\n","+------------------------------------+\n","|[new york, san antonio, gotham city]|\n","+------------------------------------+\n","\n"]}],"source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setJsonPath(\"cities.json\")\n","    .setDictionary('cities.tsv', options={\"orientation\": \"vertical\"})\n",")\n","\n","contextual_parser.fit(processed_df).transform(processed_df).select(\"entity.result\").show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"hP-gvAZ-wqpb"},"source":["### `caseSensitive`"]},{"cell_type":"markdown","metadata":{"id":"aJkjo7WywsC7"},"source":["Defines whether the mathces should be case sensitive or not."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1359,"status":"ok","timestamp":1718057617574,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"UVtoG_iPuecH","outputId":"e50d6b25-b1e7-4948-fe0e-beffdb2de8a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------+\n","|result                              |\n","+------------------------------------+\n","|[new york, san antonio, gotham city]|\n","+------------------------------------+\n","\n"]}],"source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setJsonPath(\"cities.json\")\n","    .setCaseSensitive(False)\n","    .setDictionary('cities.tsv', options={\"orientation\": \"vertical\"})\n",")\n","\n","contextual_parser.fit(processed_df).transform(processed_df).select(\"entity.result\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1058,"status":"ok","timestamp":1718057618630,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"cPo0xsNVw0vQ","outputId":"ec60772a-546a-49e5-bdcd-34d8e919a321"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------+\n","|result                              |\n","+------------------------------------+\n","|[New York, San Antonio, Gotham City]|\n","+------------------------------------+\n","\n"]}],"source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setJsonPath(\"cities.json\")\n","    .setCaseSensitive(True)\n","    .setDictionary('cities.tsv', options={\"orientation\": \"vertical\"})\n",")\n","\n","contextual_parser.fit(processed_df).transform(processed_df).select(\"entity.result\").show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"8UHN3W2Jw-zl"},"source":["### `prefixAndSuffixMatch`\n","\n","Whether to match both prefix and suffix to annotate the match."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":898,"status":"ok","timestamp":1718057619526,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"XF8tXMG13rPj","outputId":"301ab698-f6f8-46a7-cddc-e59873fef9db"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+\n","|result|\n","+------+\n","|[boy] |\n","+------+\n","\n"]}],"source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setPrefixAndSuffixMatch(True)\n","    .setJsonPath(\"context.json\")\n",")\n","\n","contextual_parser.fit(processed_context).transform(processed_context).select(\"entity.result\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1167,"status":"ok","timestamp":1718057620691,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"IZAMc2arxDHa","outputId":"9903cb66-97b0-4f9a-eaab-3824e8f45d95"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+\n","|result     |\n","+-----------+\n","|[boy, girl]|\n","+-----------+\n","\n"]}],"source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setPrefixAndSuffixMatch(False)\n","    .setJsonPath(\"context.json\")\n",")\n","\n","contextual_parser.fit(processed_context).transform(processed_context).select(\"entity.result\").show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"a4-dec0jxDYA"},"source":["### `optionalContextRules`\n","\n","When set to true, it will output regex match regardless of context matches."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":929,"status":"ok","timestamp":1718057621617,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"kmDj3Jp_w2v4","outputId":"f1d1e4ca-7afe-4b3e-ec7a-5618350b96b0"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+\n","|result     |\n","+-----------+\n","|[boy, girl]|\n","+-----------+\n","\n"]}],"source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setPrefixAndSuffixMatch(True)\n","    .setJsonPath(\"context.json\")\n","    .setOptionalContextRules(True)\n",")\n","\n","contextual_parser.fit(processed_context).transform(processed_context).select(\"entity.result\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":817,"status":"ok","timestamp":1718057622432,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"nmmI8QsexGT5","outputId":"e39d55b9-e89a-43f2-a2bb-640fa3e112fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+\n","|result|\n","+------+\n","|[boy] |\n","+------+\n","\n"]}],"source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setPrefixAndSuffixMatch(True)\n","    .setJsonPath(\"context.json\")\n","    .setOptionalContextRules(False)\n",")\n","\n","contextual_parser.fit(processed_context).transform(processed_context).select(\"entity.result\").show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"HGt5jqlCxGql"},"source":["### `shortestContextMatch`\n","\n","When set to true, it will stop finding for matches when prefix/suffix data is found in the text."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":860,"status":"ok","timestamp":1718057623289,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"mXgn9LzUxGPt","outputId":"0936790d-9bdd-4851-9d9a-ed307614221e"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+\n","|result     |\n","+-----------+\n","|[boy, girl]|\n","+-----------+\n","\n"]}],"source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setPrefixAndSuffixMatch(False)\n","    .setJsonPath(\"context.json\")\n","    .setShortestContextMatch(False)\n",")\n","\n","contextual_parser.fit(processed_context).transform(processed_context).select(\"entity.result\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1046,"status":"ok","timestamp":1718057624333,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"OE_d0a_sxJWs","outputId":"8ac7a376-5a23-48bc-f372-93c13d5583b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+\n","|result     |\n","+-----------+\n","|[boy, girl]|\n","+-----------+\n","\n"]}],"source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setPrefixAndSuffixMatch(False)\n","    .setJsonPath(\"context.json\")\n","    .setShortestContextMatch(True)\n",")\n","\n","contextual_parser.fit(processed_context).transform(processed_context).select(\"entity.result\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SeBzqLUxxJUa"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Pr5-zcl8xJ4I"},"source":["### `completeContextMatch`\n","\n","Whether to do an exact match of prefix and suffix on the entire context or not."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"--dg1uz299Hm"},"outputs":[],"source":["context_rules_complete = {\n","  \"entity\": \"Gender\",\n","  \"ruleScope\": \"sentence\",\n","  \"regex\": \"girl|boy\",\n","  \"contextLength\": 50,\n","  \"prefix\": [\"birth\"],\n","  \"suffix\": [\"fast\"]\n","}\n","\n","with open(\"context_complete.json\", \"w\") as f:\n","  json.dump(context_rules_complete, f)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1259,"status":"ok","timestamp":1718057626013,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"1auProjuxLZv","outputId":"9bcbdb2b-c7e5-4cc1-d9e0-29e26af9cb35"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+\n","|result|\n","+------+\n","|[boy] |\n","+------+\n","\n"]}],"source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setPrefixAndSuffixMatch(True)\n","    .setJsonPath(\"context_complete.json\")\n","    .setCompleteContextMatch(False)\n",")\n","\n","contextual_parser.fit(processed_context).transform(processed_context).select(\"entity.result\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1407,"status":"ok","timestamp":1718057627418,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"KsNs3T_2-LiM","outputId":"39274d06-cf52-4af1-b784-d17a18785fbc"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+\n","|result|\n","+------+\n","|[]    |\n","+------+\n","\n"]}],"source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setPrefixAndSuffixMatch(True)\n","    .setJsonPath(\"context_complete.json\")\n","    .setCompleteContextMatch(True)\n",")\n","\n","contextual_parser.fit(processed_context).transform(processed_context).select(\"entity.result\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"djcj2eVW-NU9"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1M91oRaz13uQc9yB05O6AMISVRzh5LxC8","timestamp":1683051971314},{"file_id":"1gAmDc0oIjjTYctFbdplGlPbexp5AstuO","timestamp":1681680414646},{"file_id":"1k1SVJ-vVIc_vLS4-3ex7f687zjmHp8gR","timestamp":1672339943946}],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
