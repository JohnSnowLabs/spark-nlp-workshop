{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1M91oRaz13uQc9yB05O6AMISVRzh5LxC8","timestamp":1683051971314},{"file_id":"1gAmDc0oIjjTYctFbdplGlPbexp5AstuO","timestamp":1681680414646},{"file_id":"1k1SVJ-vVIc_vLS4-3ex7f687zjmHp8gR","timestamp":1672339943946}],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n"],"metadata":{"id":"klIak_Gb_OPJ"}},{"cell_type":"markdown","source":["# **ContextualParserApproach**"],"metadata":{"id":"bewC1SWN-6jB"}},{"cell_type":"markdown","source":["This notebook will cover the different parameters and usages of `ContextualParserApproach` annotator.\n","\n","**ğŸ“– Learning Objectives:**\n","\n","1. Understand how to use `ContextualParserApproach`.\n","\n","2. Become comfortable using the different parameters of the annotator.\n","\n","3. Train an `ContextualParserModel` based on pattern matching.\n","\n","\n","**ğŸ”— Helpful Links:**\n","\n","- Documentation : [ContextualParserApproach](https://nlp.johnsnowlabs.com/docs/en/licensed_annotators#contextualparser)\n","\n","- Python Docs : [ContextualParserApproach](https://nlp.johnsnowlabs.com/licensed/api/python/reference/autosummary/sparknlp_jsl/annotator/context/contextual_parser/index.html#sparknlp_jsl.annotator.context.contextual_parser.ContextualParserApproach)\n","\n","- Scala Docs : [ContextualParserApproach](https://nlp.johnsnowlabs.com/licensed/api/com/johnsnowlabs/nlp/annotators/context/ContextualParserApproach.html)\n","\n","- For extended examples of usage, see the [Spark NLP Workshop repository](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/healthcare-nlp/01.5.Contextual_Parser_Rule_Based_NER.ipynb)."],"metadata":{"id":"m048uDkB69Rv"}},{"cell_type":"markdown","source":["## **ğŸ“œ Background**\n"],"metadata":{"id":"B9Wo54MT8jKU"}},{"cell_type":"markdown","source":["`ContextualParser` annotator extracts entities from texts based on pattern matching. It provides more functionality than its open-source counterpart `EntityRuler` by allowing users to customize specific characteristics for pattern matching.\n","\n","It allows setting regex rules for full and partial matches, a dictionary with normalizing options and context parameters to take into account specific conditions such as token distances.\n","\n","`ContextualParserApproach` annotator learns the patterns given by JSON/TSV/CSV file to define a new `ContextualParserModel`."],"metadata":{"id":"yaDBNKJsAovm"}},{"cell_type":"markdown","source":["## **ğŸ¬ Colab Setup**"],"metadata":{"id":"A4hMnkhd_ik9"}},{"cell_type":"code","source":["!pip install -q johnsnowlabs"],"metadata":{"id":"xrdvNxjD_yQI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718057490645,"user_tz":180,"elapsed":66745,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"f60d95a3-1490-4ca1-944e-960cee63b276"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m265.2/265.2 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m565.0/565.0 kB\u001b[0m \u001b[31m28.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m676.2/676.2 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m95.6/95.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","print('Please Upload your John Snow Labs License using the button below')\n","license_keys = files.upload()"],"metadata":{"id":"zbUfCKmmiVCG","colab":{"base_uri":"https://localhost:8080/","height":91},"executionInfo":{"status":"ok","timestamp":1718057498266,"user_tz":180,"elapsed":7630,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"d8cb7f1d-c002-4916-f448-2ce7d841dc47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Please Upload your John Snow Labs License using the button below\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-3ac11c67-74b3-4c95-8afc-5fe84f5ff529\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-3ac11c67-74b3-4c95-8afc-5fe84f5ff529\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving spark_nlp_for_healthcare_spark_ocr_8734_532.json to spark_nlp_for_healthcare_spark_ocr_8734_532.json\n"]}]},{"cell_type":"code","source":["from johnsnowlabs import nlp, medical\n","\n","nlp.install()"],"metadata":{"id":"NulWi4_f4GN5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718057565020,"user_tz":180,"elapsed":66758,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"10261d07-be2f-4593-888d-c1276c727dd0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ‘Œ Detected license file /content/spark_nlp_for_healthcare_spark_ocr_8734_532.json\n","ğŸ“‹ Stored John Snow Labs License in /root/.johnsnowlabs/licenses/license_number_0_for_Spark-Healthcare_Spark-OCR.json\n","ğŸ‘· Setting up  John Snow Labs home in /root/.johnsnowlabs, this might take a few minutes.\n","Downloading ğŸ+ğŸš€ Python Library spark_nlp-5.3.2-py2.py3-none-any.whl\n","Downloading ğŸ+ğŸ’Š Python Library spark_nlp_jsl-5.3.2-py3-none-any.whl\n","Downloading ğŸ«˜+ğŸš€ Java Library spark-nlp-assembly-5.3.2.jar\n","Downloading ğŸ«˜+ğŸ’Š Java Library spark-nlp-jsl-5.3.2.jar\n","ğŸ™† JSL Home setup in /root/.johnsnowlabs\n","ğŸ‘Œ Detected license file /content/spark_nlp_for_healthcare_spark_ocr_8734_532.json\n","Installing /root/.johnsnowlabs/py_installs/spark_nlp_jsl-5.3.2-py3-none-any.whl to /usr/bin/python3\n","Installed 1 products:\n","ğŸ’Š Spark-Healthcare==5.3.2 installed! âœ… Heal the planet with NLP! \n"]}]},{"cell_type":"code","source":["spark = nlp.start()\n","spark"],"metadata":{"id":"2GLXxe1Q0Iln","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1718057583881,"user_tz":180,"elapsed":18867,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"ba7173a7-1d05-4748-a4cc-2f99cf1f3a6a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ğŸ‘Œ Detected license file /content/spark_nlp_for_healthcare_spark_ocr_8734_532.json\n","ğŸ‘Œ Launched \u001b[92mcpu optimized\u001b[39m session with with: ğŸš€Spark-NLP==5.3.2, ğŸ’ŠSpark-Healthcare==5.3.2, running on âš¡ PySpark==3.4.0\n"]}]},{"cell_type":"markdown","source":["## **ğŸ–¨ï¸ Input/Output Annotation Types**"],"metadata":{"id":"fdTHvykf8wni"}},{"cell_type":"markdown","source":["- Input: `DOCUMENT`, `TOKEN`\n","\n","- Output: `CHUNK`"],"metadata":{"id":"ejYVNcX98y5j"}},{"cell_type":"markdown","source":["## **ğŸ” Parameters**\n"],"metadata":{"id":"YVa72oJd9Bk_"}},{"cell_type":"markdown","source":["- `inputCols`: The name of the columns containing the input annotations. It can read either a String column or an Array.\n","- `outputCol`: The name of the column in Document type that is generated. We can specify only one column here.\n","- `jsonPath`: Path to json file containing regex patterns and rules to match the entities.\n","- `dictionary`: Path to dictionary file in tsv or csv format.\n","- `caseSensitive`: Whether to use case sensitive when matching values.\n","- `prefixAndSuffixMatch`: Whether to match both prefix and suffix to annotate the match.\n","- `optionalContextRules`: When set to true, it will output regex match regardless of context matches.\n","- `shortestContextMatch`: When set to true, it will stop finding for matches when prefix/suffix data is found in the text.\n","- `completeContextMatch`: Whether to do an exact match of prefix and suffix.\n","\n","All the parameters can be set using the corresponding set method in camel case. For example, `.setInputcols()`."],"metadata":{"id":"AUbv3YL59D8Q"}},{"cell_type":"markdown","source":["### `inputCols` and `outputCol`"],"metadata":{"id":"EpBqGk7oNc3C"}},{"cell_type":"markdown","source":["Define the column names containing the `DOCUMENT` and `TOKEN` annotations needed as input to the `ContextualParser` and the name of the new column containg the identified entities.\n","\n","Let's define a pipeline to process raw texts into `DOCUMENT` and `TOKEN` annotations:"],"metadata":{"id":"wOGfzMDiOAXE"}},{"cell_type":"code","source":["document_assembler = (\n","    nlp.DocumentAssembler()\n","    .setInputCol(\"text\")\n","    .setOutputCol(\"document\")\n",")\n","\n","tokenizer = (\n","    nlp.Tokenizer().setInputCols(\"document\").setOutputCol(\"token\")\n",")\n"],"metadata":{"id":"c23AbFZ7OV6B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then, we use the defined column names of the previous stages to define the `ContextualParserApproach` input columns and define a name for the output column:"],"metadata":{"id":"J05F4CcZA30R"}},{"cell_type":"code","source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"document\", \"token\"])\n","    .setOutputCol(\"entity\")\n",")"],"metadata":{"id":"KfueJP-rALU_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### `jsonPath`"],"metadata":{"id":"7T_3otzSBllg"}},{"cell_type":"markdown","source":["Defines the path to the JSON file containing the rules to match the entities on the text. The file needs to define the following information:\n","\n","- `entity`: type of the entity that will be matched\n","- `ruleScope`: Scope to search the pattern. Can be one of the following:\n","  `document`: Match the entities on the entire document (useful for matching multi-token/phrases entities).\n","  - `sentence`: Match the entities in sentences in a token level (match words).\n","- `regex`: The pattern to match (backslashes are escape characters in JSON, so for regex pattern \"\\d+\" we need to write it out as \"\\\\d+\").\n","- `completeMatchRegex`: Whether to consider only the exact matches on full tokens. If set to `True`, the parameter `matchScope` is ignored.\n","- `matchScope`: The return level of the match:\n"," - `token`: Returns the entire token containing the matched rule.\n"," - `sub-token`: Returns the part of the token where the rule matches.\n","- `prefix`: List of prefixes to be cosidered on the matches.\n","- `suffix`: List of suffixes to be cosidered on the matches.\n","- `contextLength`: Maximum length to be used as context.\n","- `contextException`: List of exceptions on the context.\n","- `exceptionDistance`: Maximum distance of the exception (Default to 40).\n","\n"],"metadata":{"id":"V-NurY5XB8lK"}},{"cell_type":"markdown","source":["Let's see how to use a JSON file in an example:"],"metadata":{"id":"TK_XNeVOnGVp"}},{"cell_type":"code","source":["sample_text = \"\"\"Peter Parker is a nice guy and lives in New York . Bruce Wayne is also a nice guy and lives in San Antonio and Gotham City . \"\"\"\n","\n","sample_df = spark.createDataFrame([[sample_text]]).toDF(\"text\")\n","sample_df.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OYYFxn4Qo7RS","executionInfo":{"status":"ok","timestamp":1718057599731,"user_tz":180,"elapsed":15855,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"e448a624-351a-4ca8-d586-25016d8b9591"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+\n","|                text|\n","+--------------------+\n","|Peter Parker is a...|\n","+--------------------+\n","\n"]}]},{"cell_type":"markdown","source":["We will preprocess the example sentences with the required input annotations."],"metadata":{"id":"gTtKlUsnpM4h"}},{"cell_type":"code","source":["document_assembler = (\n","    nlp.DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",")\n","\n","sentence_detector = (\n","    nlp.SentenceDetector().setInputCols([\"document\"]).setOutputCol(\"sentence\")\n",")\n","\n","tokenizer = nlp.Tokenizer().setInputCols([\"sentence\"]).setOutputCol(\"token\")\n","\n","\n","preprocessPipeline = nlp.Pipeline(\n","    stages=[document_assembler, sentence_detector, tokenizer]\n",")\n","\n","\n","preprocessModel = preprocessPipeline.fit(sample_df)"],"metadata":{"id":"Lef4k3DrnSN6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["processed_df = preprocessModel.transform(sample_df)\n","processed_df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lsJqJEbPpUWg","executionInfo":{"status":"ok","timestamp":1718057607078,"user_tz":180,"elapsed":4526,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"70a1790e-748a-4104-fc38-31641b13e6e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+--------------------+--------------------+--------------------+\n","|                text|            document|            sentence|               token|\n","+--------------------+--------------------+--------------------+--------------------+\n","|Peter Parker is a...|[{document, 0, 12...|[{document, 0, 49...|[{token, 0, 4, Pe...|\n","+--------------------+--------------------+--------------------+--------------------+\n","\n"]}]},{"cell_type":"markdown","source":["Create a sample JSON file:"],"metadata":{"id":"vvtfiAzlpnHn"}},{"cell_type":"code","source":["import json\n","\n","\n","cities = {\n","    \"entity\": \"City\",\n","    \"ruleScope\": \"document\",\n","    \"matchScope\": \"sub-token\",\n","    \"completeMatchRegex\": \"false\",\n","    \"regex\": \"([A-Z]{1}[a-z]+ [A-Z]{1}[a-z]+)\" # Find two consecutive words in title case.\n","}\n","\n","with open(\"cities.json\", \"w\") as f:\n","    json.dump(cities, f)"],"metadata":{"id":"2QkxOQeHBmup"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setJsonPath(\"cities.json\")\n",")\n","\n","contextual_parser.fit(processed_df).transform(processed_df).select(\"entity.result\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iOsSgnSEocEF","executionInfo":{"status":"ok","timestamp":1718057613185,"user_tz":180,"elapsed":6120,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"833e2bbb-c119-4ca4-af89-eaed97eb10e5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---------------------------------------------------------------+\n","|result                                                         |\n","+---------------------------------------------------------------+\n","|[Peter Parker, New York, Bruce Wayne, San Antonio, Gotham City]|\n","+---------------------------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["Using the context to find matches:"],"metadata":{"id":"BtxGj5RB0eXW"}},{"cell_type":"code","source":["context_example = \"At birth, the typical boy is growing slightly faster than the typical girl, but growth rates become equal at about seven months.\"\n","context_df = spark.createDataFrame([[context_example]]).toDF(\"text\")\n","processed_context = preprocessModel.transform(context_df)\n","\n","\n","context_rules = {\n","  \"entity\": \"Gender\",\n","  \"ruleScope\": \"sentence\",\n","  \"regex\": \"girl|boy\",\n","  \"contextLength\": 50,\n","  \"prefix\": [\"birth\"],\n","  \"suffix\": [\"faster\", \"rates\"]\n","}\n","\n","with open(\"context.json\", \"w\") as f:\n","    json.dump(context_rules, f)"],"metadata":{"id":"bPI8xOqY0hwf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setJsonPath(\"context.json\")\n",")\n","\n","contextual_parser.fit(processed_context).transform(processed_context).select(\"entity.result\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6p7UZm1K3S9S","executionInfo":{"status":"ok","timestamp":1718057614968,"user_tz":180,"elapsed":1354,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"b17b2edf-901b-4256-d247-1d451cdf2a08"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+\n","|result     |\n","+-----------+\n","|[boy, girl]|\n","+-----------+\n","\n"]}]},{"cell_type":"markdown","source":["### `dictionary`"],"metadata":{"id":"ceu-4opwsKQr"}},{"cell_type":"markdown","source":["The dictionary parameter can be used to define to define entities as a list to be found on the text.\n","\n","When setting the dictionary file, we can use the parameter `orientation` that indicates whether the file is to be read horizontally or vertically.\n","\n","Horizontal:\n","\n","| normalize | word1 | word2 | word3     |\n","|-----------|-------|-------|-----------|\n","| female    | woman | girl  | lady      |\n","| male      | man   | boy   | gentleman |\n","\n","\n","Vertical:\n","\n","| female    | normalize |\n","|-----------|-----------|\n","| woman     | word1     |\n","| girl      | word2     |\n","| lady      | word3     |\n","\n","</br>\n","\n","JSON path needs to be set."],"metadata":{"id":"AE7tDhBHsN2G"}},{"cell_type":"code","source":["# Create a dictionary to detect cities\n","cities = \"\"\"City\\nNew York\\nGotham City\\nSan Antonio\\nSalt Lake City\"\"\"\n","\n","\n","# TSV or CSV\n","with open('cities.tsv', 'w') as f:\n","    f.write(cities)\n","\n","# Check what dictionary looks like\n","!cat cities.tsv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"quMhYlmwpzfI","executionInfo":{"status":"ok","timestamp":1718057614968,"user_tz":180,"elapsed":8,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"859d4cce-4255-4394-b4d8-f2e47ceb0a9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["City\n","New York\n","Gotham City\n","San Antonio\n","Salt Lake City"]}]},{"cell_type":"code","source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setJsonPath(\"cities.json\")\n","    .setDictionary('cities.tsv', options={\"orientation\": \"vertical\"})\n",")\n","\n","contextual_parser.fit(processed_df).transform(processed_df).select(\"entity.result\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CUzSUhF7stCa","executionInfo":{"status":"ok","timestamp":1718057616218,"user_tz":180,"elapsed":1255,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"4764a771-ae80-40ba-c1bb-c39487ae3a69"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------------------+\n","|result                              |\n","+------------------------------------+\n","|[new york, san antonio, gotham city]|\n","+------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["### `caseSensitive`"],"metadata":{"id":"hP-gvAZ-wqpb"}},{"cell_type":"markdown","source":["Defines whether the mathces should be case sensitive or not."],"metadata":{"id":"aJkjo7WywsC7"}},{"cell_type":"code","source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setJsonPath(\"cities.json\")\n","    .setCaseSensitive(False)\n","    .setDictionary('cities.tsv', options={\"orientation\": \"vertical\"})\n",")\n","\n","contextual_parser.fit(processed_df).transform(processed_df).select(\"entity.result\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UVtoG_iPuecH","executionInfo":{"status":"ok","timestamp":1718057617574,"user_tz":180,"elapsed":1359,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"e50d6b25-b1e7-4948-fe0e-beffdb2de8a5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------------------+\n","|result                              |\n","+------------------------------------+\n","|[new york, san antonio, gotham city]|\n","+------------------------------------+\n","\n"]}]},{"cell_type":"code","source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setJsonPath(\"cities.json\")\n","    .setCaseSensitive(True)\n","    .setDictionary('cities.tsv', options={\"orientation\": \"vertical\"})\n",")\n","\n","contextual_parser.fit(processed_df).transform(processed_df).select(\"entity.result\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cPo0xsNVw0vQ","executionInfo":{"status":"ok","timestamp":1718057618630,"user_tz":180,"elapsed":1058,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"ec60772a-546a-49e5-bdcd-34d8e919a321"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------------------+\n","|result                              |\n","+------------------------------------+\n","|[New York, San Antonio, Gotham City]|\n","+------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["### `prefixAndSuffixMatch`\n","\n","Whether to match both prefix and suffix to annotate the match."],"metadata":{"id":"8UHN3W2Jw-zl"}},{"cell_type":"code","source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setPrefixAndSuffixMatch(True)\n","    .setJsonPath(\"context.json\")\n",")\n","\n","contextual_parser.fit(processed_context).transform(processed_context).select(\"entity.result\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XF8tXMG13rPj","executionInfo":{"status":"ok","timestamp":1718057619526,"user_tz":180,"elapsed":898,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"301ab698-f6f8-46a7-cddc-e59873fef9db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+\n","|result|\n","+------+\n","|[boy] |\n","+------+\n","\n"]}]},{"cell_type":"code","source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setPrefixAndSuffixMatch(False)\n","    .setJsonPath(\"context.json\")\n",")\n","\n","contextual_parser.fit(processed_context).transform(processed_context).select(\"entity.result\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IZAMc2arxDHa","executionInfo":{"status":"ok","timestamp":1718057620691,"user_tz":180,"elapsed":1167,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"9903cb66-97b0-4f9a-eaab-3824e8f45d95"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+\n","|result     |\n","+-----------+\n","|[boy, girl]|\n","+-----------+\n","\n"]}]},{"cell_type":"markdown","source":["### `optionalContextRules`\n","\n","When set to true, it will output regex match regardless of context matches."],"metadata":{"id":"a4-dec0jxDYA"}},{"cell_type":"code","source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setPrefixAndSuffixMatch(True)\n","    .setJsonPath(\"context.json\")\n","    .setOptionalContextRules(True)\n",")\n","\n","contextual_parser.fit(processed_context).transform(processed_context).select(\"entity.result\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kmDj3Jp_w2v4","executionInfo":{"status":"ok","timestamp":1718057621617,"user_tz":180,"elapsed":929,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"f1d1e4ca-7afe-4b3e-ec7a-5618350b96b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+\n","|result     |\n","+-----------+\n","|[boy, girl]|\n","+-----------+\n","\n"]}]},{"cell_type":"code","source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setPrefixAndSuffixMatch(True)\n","    .setJsonPath(\"context.json\")\n","    .setOptionalContextRules(False)\n",")\n","\n","contextual_parser.fit(processed_context).transform(processed_context).select(\"entity.result\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nmmI8QsexGT5","executionInfo":{"status":"ok","timestamp":1718057622432,"user_tz":180,"elapsed":817,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"e39d55b9-e89a-43f2-a2bb-640fa3e112fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+\n","|result|\n","+------+\n","|[boy] |\n","+------+\n","\n"]}]},{"cell_type":"markdown","source":["### `shortestContextMatch`\n","\n","When set to true, it will stop finding for matches when prefix/suffix data is found in the text."],"metadata":{"id":"HGt5jqlCxGql"}},{"cell_type":"code","source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setPrefixAndSuffixMatch(False)\n","    .setJsonPath(\"context.json\")\n","    .setShortestContextMatch(False)\n",")\n","\n","contextual_parser.fit(processed_context).transform(processed_context).select(\"entity.result\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mXgn9LzUxGPt","executionInfo":{"status":"ok","timestamp":1718057623289,"user_tz":180,"elapsed":860,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"0936790d-9bdd-4851-9d9a-ed307614221e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+\n","|result     |\n","+-----------+\n","|[boy, girl]|\n","+-----------+\n","\n"]}]},{"cell_type":"code","source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setPrefixAndSuffixMatch(False)\n","    .setJsonPath(\"context.json\")\n","    .setShortestContextMatch(True)\n",")\n","\n","contextual_parser.fit(processed_context).transform(processed_context).select(\"entity.result\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OE_d0a_sxJWs","executionInfo":{"status":"ok","timestamp":1718057624333,"user_tz":180,"elapsed":1046,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"8ac7a376-5a23-48bc-f372-93c13d5583b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+\n","|result     |\n","+-----------+\n","|[boy, girl]|\n","+-----------+\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"SeBzqLUxxJUa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### `completeContextMatch`\n","\n","Whether to do an exact match of prefix and suffix on the entire context or not."],"metadata":{"id":"Pr5-zcl8xJ4I"}},{"cell_type":"code","source":["context_rules_complete = {\n","  \"entity\": \"Gender\",\n","  \"ruleScope\": \"sentence\",\n","  \"regex\": \"girl|boy\",\n","  \"contextLength\": 50,\n","  \"prefix\": [\"birth\"],\n","  \"suffix\": [\"fast\"]\n","}\n","\n","with open(\"context_complete.json\", \"w\") as f:\n","  json.dump(context_rules_complete, f)"],"metadata":{"id":"--dg1uz299Hm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setPrefixAndSuffixMatch(True)\n","    .setJsonPath(\"context_complete.json\")\n","    .setCompleteContextMatch(False)\n",")\n","\n","contextual_parser.fit(processed_context).transform(processed_context).select(\"entity.result\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1auProjuxLZv","executionInfo":{"status":"ok","timestamp":1718057626013,"user_tz":180,"elapsed":1259,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"9bcbdb2b-c7e5-4cc1-d9e0-29e26af9cb35"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+\n","|result|\n","+------+\n","|[boy] |\n","+------+\n","\n"]}]},{"cell_type":"code","source":["contextual_parser = (\n","    medical.ContextualParserApproach()\n","    .setInputCols([\"sentence\", \"token\"])\n","    .setOutputCol(\"entity\")\n","    .setPrefixAndSuffixMatch(True)\n","    .setJsonPath(\"context_complete.json\")\n","    .setCompleteContextMatch(True)\n",")\n","\n","contextual_parser.fit(processed_context).transform(processed_context).select(\"entity.result\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KsNs3T_2-LiM","executionInfo":{"status":"ok","timestamp":1718057627418,"user_tz":180,"elapsed":1407,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"39274d06-cf52-4af1-b784-d17a18785fbc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+\n","|result|\n","+------+\n","|[]    |\n","+------+\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"djcj2eVW-NU9"},"execution_count":null,"outputs":[]}]}