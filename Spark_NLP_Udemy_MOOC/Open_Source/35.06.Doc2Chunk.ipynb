{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"fsFAlvKJBBus"},"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"sXatvRX899i0"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/Spark_NLP_Udemy_MOOC/Open_Source/35.06.Doc2Chunk.ipynb)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"AOn8d1tcBkK3"},"source":["# **Doc2Chunk**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"WimihSwD3vtH"},"source":["This notebook will cover the different parameters and usages of `Doc2Chunk`. This annotator converts `DOCUMENT` type annotations into `CHUNK` type. The text to be transformed into chunks must be contained within the input `DOCUMENT`. It may be either `StringType` or `ArrayType[StringType]` (using `setIsArray`). The `Doc2Chunk` annotator is used in conjunction with annotators that require a `CHUNK` type input.\n","\n","**📖 Learning Objectives:**\n","\n","1. Understand the usage of the annotator.\n","\n","2. Become comfortable using the different parameters of the annotator.\n","\n","3. Become comfortable using the annotator in several examples.\n","\n","\n","**🔗 Helpful Links:**\n","\n","- Documentation : [Doc2Chunk](https://nlp.johnsnowlabs.com/docs/en/annotators#doc2chunk)\n","\n","- Python Docs : [Doc2Chunk](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/base/doc2_chunk/index.html#sparknlp.base.doc2_chunk.Doc2Chunk)\n","\n","- Scala Docs : [Doc2Chunk](https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/Doc2Chunk)\n","\n","- For extended examples of usage, see the [Spark NLP Workshop repository](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qL9lcISyFSLv"},"source":["## **📜 Background**\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TjDKOoZ4Fc8G"},"source":["In Spark ML, the machine learning algorithms are grouped in two classes: Estimators and Transformers. An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. A Transformer is an algorithm which can transform one DataFrame into another DataFrame.\n","\n","Similarily, in Spark NLP, there are two types of annotators: AnnotatorApproach and AnnotatorModel. \n","The AnnotatorApproach extends the Estimator from Spark ML, and is meant to be trained through fit(). The AnnotatorModel extends the Transformer and is meant to transform data frames through transform().\n","\n","Each annotator accepts certain types of columns and outputs new columns in another type (we call this AnnotatorType).\n","\n","In Spark NLP, we have five different transformers that are mainly used for getting the data in or transforming the data from one AnnotatorType to another. `Doc2Chunk` is one of them, it transforms the `DOCUMENT` type into `CHUNK` type."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"MfkkKkbVF309"},"source":["## **🎬 Colab Setup**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33189,"status":"ok","timestamp":1673883401000,"user":{"displayName":"Silvia Onofrei","userId":"08518681359279549405"},"user_tz":360},"id":"iMkMQtZNF2n-","outputId":"c5eee394-0133-4350-e58b-779fe447956d"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.4/212.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m448.4/448.4 KB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 KB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q pyspark==3.1.2  spark-nlp==4.2.4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NulWi4_f4GN5"},"outputs":[],"source":["import sparknlp\n","from sparknlp.base import *\n","from sparknlp.annotator import *\n","from pyspark.ml import Pipeline\n","\n","spark = sparknlp.start()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9Fbbk1bqcuA5"},"source":["## **🖨️ Input/Output Annotation Types**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0yFIrr5acsiU"},"source":["- Input: `DOCUMENT`\n","\n","- Output: `CHUNK`"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"b2YJehUKMhb0"},"source":["## **🔎 Parameters**\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"oidLDoS94asU"},"source":["- `inputCols`: (String) Previous annotations columns.\n","\n","- `outputCol`: (String) Output annotation column. \n","\n","- `chunkCol`: (String) --> Column that contains string, must be part of `DOCUMENT`.\n","\n","- `failOnMissing`: (BooleanParam) --> Whether to fail the job if a chunk is not found within document, return empty otherwise (Default: false).\n","\n","- `isArray`: (BooleanParam) --> Whether the chunkCol is an array of strings (Default: false).\n","\n","- `lowerCase`: (BooleanParam) --> Whether to lower case for matching case (Default: true).\n","\n","- `startCol`: (String) --> Column that has a reference of where the chunk begins.\n","\n","- `startColByTokenIndex`: (BooleanParam) --> Whether start column is prepended by whitespace tokens (Default: false).\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"sVcaX9eBO1x6"},"source":["## `Basic Usage Example`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":728,"status":"ok","timestamp":1673884660323,"user":{"displayName":"Silvia Onofrei","userId":"08518681359279549405"},"user_tz":360},"id":"fyUGCjxGQZP8","outputId":"c1ba0e50-bd6b-45fe-a1b1-aafc866d163f"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------------------+-------------+\n","|result                                |annotatorType|\n","+--------------------------------------+-------------+\n","|[advanced natural language processing]|[chunk]      |\n","+--------------------------------------+-------------+\n","\n"]}],"source":["# Convert data into SparNLP compatible format\n","documentAssembler = DocumentAssembler() \\\n","        .setInputCol(\"text\") \\\n","        .setOutputCol(\"document\")\n","\n","# Transform document type into chunk type\n","chunkAssembler = Doc2Chunk() \\\n","        .setInputCols(\"document\") \\\n","        .setOutputCol(\"chunk\") \n","\n","# Data sample saved as a Spark dataframe\n","data = spark.createDataFrame([[\n","    \"advanced natural language processing\"\n","    ]]).toDF(\"text\")\n","\n","# Basic pipeline model\n","pipeline = Pipeline() \\\n","        .setStages([\n","            documentAssembler, \n","            chunkAssembler]) \\\n","        .fit(data)\n","\n","# Obtain and extract the results\n","result = pipeline.transform(data)\n","result.selectExpr(\"chunk.result\", \"chunk.annotatorType\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mvqoj92KTTKa"},"source":["## `Example with Provided Chunk Column`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":729,"status":"ok","timestamp":1673884844442,"user":{"displayName":"Silvia Onofrei","userId":"08518681359279549405"},"user_tz":360},"id":"FcSWfE42L8X_","outputId":"1574b3c4-c941-4348-f7bc-d60675d8f6f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------------------------------------------------------------+---------------------+\n","|result                                                           |annotatorType        |\n","+-----------------------------------------------------------------+---------------------+\n","|[Spark NLP, text processing library, natural language processing]|[chunk, chunk, chunk]|\n","+-----------------------------------------------------------------+---------------------+\n","\n"]}],"source":["# Convert data into SparNLP compatible format\n","documentAssembler = DocumentAssembler() \\\n","        .setInputCol(\"text\") \\\n","        .setOutputCol(\"document\")\n","\n","# Transform document type into chunk type\n","chunkAssembler = Doc2Chunk() \\\n","        .setInputCols(\"document\") \\\n","        .setChunkCol(\"target\") \\\n","        .setOutputCol(\"chunk\") \\\n","        .setIsArray(True)\n","\n","# Save sample data as a Spark dataframe\n","data = spark.createDataFrame([[\n","    \"Spark NLP is an open-source text processing library for advanced natural language processing.\",\n","    [\"Spark NLP\", \"text processing library\", \"natural language processing\"]\n","]]).toDF(\"text\", \"target\")\n","\n","# Basic pipeline model\n","pipeline = Pipeline() \\\n","        .setStages([\n","            documentAssembler, \n","            chunkAssembler]) \\\n","        .fit(data)\n","\n","# Obtain and extract the results\n","result = pipeline.transform(data)\n","result.selectExpr(\"chunk.result\", \"chunk.annotatorType\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Jy4yxgxIAmnq"},"source":["## Example with Extraneous Chunks"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":668,"status":"ok","timestamp":1673885395056,"user":{"displayName":"Silvia Onofrei","userId":"08518681359279549405"},"user_tz":360},"id":"7qjj0psgUQ9z","outputId":"11539b0c-973e-4e9c-e1da-37ed8bdcd127"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------------------------------------------------------------+---------------------+\n","|result                                                           |annotatorType        |\n","+-----------------------------------------------------------------+---------------------+\n","|[Spark NLP, text processing library, natural language processing]|[chunk, chunk, chunk]|\n","+-----------------------------------------------------------------+---------------------+\n","\n"]}],"source":["# Convert data into SparNLP compatible format\n","documentAssembler = DocumentAssembler() \\\n","        .setInputCol(\"text\") \\\n","        .setOutputCol(\"document\")\n","\n","# Transform document type into chunk type\n","chunkAssembler = Doc2Chunk() \\\n","        .setInputCols(\"document\") \\\n","        .setChunkCol(\"target\") \\\n","        .setOutputCol(\"chunk\") \\\n","        .setIsArray(True) \\\n","        .setFailOnMissing(False)\n","\n","# Save sample data as a Spark dataframe\n","data = spark.createDataFrame([[\n","    \"Spark NLP is an open-source text processing library for advanced natural language processing.\",\n","    [\"python\", \"Spark NLP\", \"text processing library\", \"natural language processing\"]\n","]]).toDF(\"text\", \"target\")\n","\n","# Basic pipeline model\n","pipeline = Pipeline() \\\n","        .setStages([\n","            documentAssembler, \n","            chunkAssembler]) \\\n","        .fit(data)\n","\n","# Obtain and extract the results\n","result = pipeline.transform(data)\n","result.selectExpr(\"chunk.result\", \"chunk.annotatorType\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"zizxR6zrVI1_"},"source":["**Comment**:\n","\n","The chunk _`python`_ is not found in the sample text. If we set `setFailOnMissing` to default value `False`, the pipeline will ignore the chunk and output the results. If this parameter is set to `True` we get a `Py4JJavaError`."]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"141VhFB3xvWVKAlul6bVWVL3WahUH6ftf","timestamp":1673638505160},{"file_id":"1L8GdQ-yorjVk_V8Um6Rw6aEzFVT31OCO","timestamp":1673032420252},{"file_id":"1VVV4jTagH47UZiKFqXoP-Abq5ozZM1BV","timestamp":1672918708428},{"file_id":"https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb","timestamp":1671914287039}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}
