{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"13hdDuFsr-7lHijJLzkSzVhX8X0laC6XJ","timestamp":1674827969232},{"file_id":"1f0hJ6HS5dJlCA7RzYT_X6BH7yVBxWfBt","timestamp":1674826423494}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"],"metadata":{"id":"PJbcjC-samnF"}},{"cell_type":"markdown","source":["# WordSegmenter"],"metadata":{"id":"90XYFSVbblyp"}},{"cell_type":"markdown","source":["This notebook will cover the different parameters and usages of `WordSegmenter`.\n","\n","**ğŸ“– Learning Objectives:**\n","\n","1. Be able to split text into words in diffferent languages.\n","\n","2. Understand how to use the `WordSegmenter` annotator.\n","\n","3. Become comfortable using the different parameters of the annotator.\n"],"metadata":{"id":"5BRq-xE4blyp"}},{"cell_type":"markdown","source":["**ğŸ”— Helpful Links:**\n","\n","- Documentation : [WordSegmenter](https://nlp.johnsnowlabs.com/docs/en/annotators#wordsegmenter)\n","\n","- Python Docs : [WordSegmenter](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/ws/word_segmenter/index.html#sparknlp.annotator.ws.word_segmenter.WordSegmenterModel)\n","\n","- Scala Docs : [WordSegmenter](https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/ws/WordSegmenterModel.html)\n","\n","- For extended examples of usage, see the [Spark NLP Workshop repository](https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/tutorials/Certification_Trainings/Public)."],"metadata":{"id":"3d7jx_8Ubq6C"}},{"cell_type":"markdown","source":["## **ğŸ“œ Background**"],"metadata":{"id":"S0ERC5daa22T"}},{"cell_type":"markdown","source":["An important part of text preprocessing is to split texts into an array of words that can be further used on many NLP tasks.\n","\n","This task is relatively more difficult in some languages such as Chinese, Japanese, Korean, and Thai where the words in a text are not separated by a white space (or other delimiter). \n","\n","For example, check the following text in Chinese:\n","\n","> æˆ‘ä»¬éƒ½å¾ˆå–œæ¬¢è‡ªç„¶è¯­è¨€å¤„ç†ï¼\n","\n","We can identify that the Chinese words are all together without any separation, so how can we identify which composition of ideograms form a word? \n","\n","In this example, the words are:\n","\n","- æˆ‘ä»¬ (we, composition of two ideograms)\n","- éƒ½ (all, only one ideogram)\n","- å¾ˆ (very, only one ideogram)\n","- å–œæ¬¢ (like, composition of two ideograms)\n","- è‡ªç„¶è¯­è¨€å¤„ç† (NLP, composition of six ideograms), which can be breaked down to:\n","  - è‡ªç„¶ (Natural)\n","  - è¯­è¨€ (Language)\n","  - å¤„ç† (Processing)\n","\n","But there is no easy way to programmatically identify them! Thus, we need help from Machine Learning models. \n","\n","In this notebook, we will introduce the Spark NLP annotators that can identify the words in this kind of texts, either by using pretrained models or training new ones.\n","\n","John Snow Labs currently has pretrained models for Chinese, Japanese, Korean, and Thai."],"metadata":{"id":"fJydwdDsa5Ky"}},{"cell_type":"markdown","source":["## **ğŸ¬ Colab Setup**"],"metadata":{"id":"4aqI2eoZbM1G"}},{"cell_type":"markdown","source":["Before going through the annotators, let's set up the environment and start a `spark` session."],"metadata":{"id":"B61TLXIWbOpS"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"EC4nWgnbblyr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676570970724,"user_tz":180,"elapsed":37728,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"b8215d13-a9af-4a0f-b2b5-4cb4e027da28"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m212.4/212.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m448.4/448.4 KB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m198.6/198.6 KB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q pyspark==3.1.2  spark-nlp==4.2.4"]},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","from pyspark.ml import Pipeline\n","\n","import sparknlp\n","from sparknlp.annotator import (\n","    Wav2Vec2ForCTC\n",")\n","from sparknlp.base import DocumentAssembler, LightPipeline\n"],"metadata":{"id":"eAOeQqzXblyr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Starting the spark session:"],"metadata":{"id":"2gPrL2jUbXvL"}},{"cell_type":"code","source":["spark = sparknlp.start()\n","\n","print(\"Spark NLP version\", sparknlp.version())\n","print(\"Apache Spark version:\", spark.version)\n","\n","spark"],"metadata":{"id":"7jv6xvuMbXHw","colab":{"base_uri":"https://localhost:8080/","height":254},"executionInfo":{"status":"ok","timestamp":1676571254974,"user_tz":180,"elapsed":61328,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"78def9f8-35f9-4c12-97e3-e93eddfb964c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Spark NLP version 4.2.4\n","Apache Spark version: 3.1.2\n"]},{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7f6f662dc3d0>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cdd024103d78:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Spark NLP</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["## **ğŸ–¨ï¸ Input/Output Annotation Types**"],"metadata":{"id":"xmgxwrSCcF6o"}},{"cell_type":"markdown","source":["- Input: `DOCUMENT`\n","\n","- Output: `TOKEN`"],"metadata":{"id":"EM7O2BDycHgA"}},{"cell_type":"markdown","source":["## **ğŸ” Parameters**"],"metadata":{"id":"xmxq8Xi_cJFe"}},{"cell_type":"markdown","source":["- **model**: Part-of-Speech model.\n","\n","- **pattern**: Regex pattern used to match delimiters (Default: \"\\\\s+\").\n","\n","- **toLowercase**: Indicates whether to convert all characters to lowercase before tokenizing (Default: false). Useful when multilanguage is present in the text."],"metadata":{"id":"IqpCn0BPcK5Q"}},{"cell_type":"markdown","source":["### âœŒ Using pretrained models\n","\n","We can use pretrained model with the `WordSegmenterModel` annotator. For a list of available models, check [NLP Models Hub](https://nlp.johnsnowlabs.com/models?task=Word+Segmentation).\n","\n","This annotator acts like the `Tokenizer` annotator, for languages where the words don't have a clear separator (like white space).\n","\n","We will show how to use the Chinese pretrained model ``."],"metadata":{"id":"cDLILHrc_dKj"}},{"cell_type":"code","source":["# Chinese example\n","example_sentence = r\"æˆ‘ä»¬éƒ½å¾ˆå–œæ¬¢è‡ªç„¶è¯­è¨€å¤„ç†ï¼\""],"metadata":{"id":"laUdYcblcczE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create the pipeline"],"metadata":{"id":"XtNbuop_mSkQ"}},{"cell_type":"code","source":["document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n","\n","# Model trained on the Chinese Treebank 9 dataset\n","word_segmenter = (\n","    WordSegmenterModel.pretrained(\"wordseg_ctb9\", \"zh\")\n","    .setInputCols([\"document\"])\n","    .setOutputCol(\"words_segmented\")\n","    .setPattern(\"\\\\s+\")\n",")\n","\n","\n","pipeline = Pipeline(stages=[document_assembler, word_segmenter])\n","example = spark.createDataFrame([[example_sentence]]).toDF(\"text\")\n","\n","model = pipeline.fit(example)\n","result = model.transform(example)\n","result.select(F.explode(\"words_segmented.result\").alias(\"word\")).show(truncate=False)"],"metadata":{"id":"oC3cekia2XLd","executionInfo":{"status":"ok","timestamp":1676572192363,"user_tz":180,"elapsed":35625,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"48caff47-7a15-4ed9-9644-8e2730370346"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["wordseg_ctb9 download started this may take some time.\n","Approximate size to download 2.2 MB\n","[OK!]\n","+----+\n","|word|\n","+----+\n","|æˆ‘ä»¬|\n","|éƒ½  |\n","|å¾ˆ  |\n","|å–œæ¬¢|\n","|è‡ªç„¶|\n","|è¯­è¨€|\n","|å¤„ç†|\n","|ï¼  |\n","+----+\n","\n"]}]},{"cell_type":"markdown","source":["## Fast inference with [LightPipelines](https://nlp.johnsnowlabs.com/docs/en/concepts#using-spark-nlps-lightpipeline)"],"metadata":{"id":"os6n3ivso_YM"}},{"cell_type":"markdown","source":["We can use Spark NLP's `LightPipeline` to run fast inference directly on text (or list of text) instead of using spark data frames. \n","\n","Let's check how to do that."],"metadata":{"id":"TyPScBbfo_WE"}},{"cell_type":"code","source":["# Simply define the LightPipeline on the PipelineModel\n","lp = LightPipeline(model)\n","\n","lp.annotate(example_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dYsy6129vIEP","executionInfo":{"status":"ok","timestamp":1676572271731,"user_tz":180,"elapsed":1076,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"66e1c6fc-5fa0-42ca-8093-b06a381608ff"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'document': ['æˆ‘ä»¬éƒ½å¾ˆå–œæ¬¢è‡ªç„¶è¯­è¨€å¤„ç†ï¼'],\n"," 'words_segmented': ['æˆ‘ä»¬', 'éƒ½', 'å¾ˆ', 'å–œæ¬¢', 'è‡ªç„¶', 'è¯­è¨€', 'å¤„ç†', 'ï¼']}"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["for i, token in enumerate(lp.annotate(example_sentence)[\"words_segmented\"]):\n","  print(f\"Word {i}: {token}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tAYJ_a2Z66JE","executionInfo":{"status":"ok","timestamp":1676572278730,"user_tz":180,"elapsed":329,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"1512556c-3a3b-4c55-a23b-81cf96fe62ad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Word 0: æˆ‘ä»¬\n","Word 1: éƒ½\n","Word 2: å¾ˆ\n","Word 3: å–œæ¬¢\n","Word 4: è‡ªç„¶\n","Word 5: è¯­è¨€\n","Word 6: å¤„ç†\n","Word 7: ï¼\n"]}]},{"cell_type":"markdown","source":["Easy as that!"],"metadata":{"id":"KiwhnbaI7JiD"}},{"cell_type":"markdown","source":["## âš¡ **Training a new WordSegmenterModel**"],"metadata":{"id":"YqJ2QjEwd5sx"}},{"cell_type":"markdown","source":["To train a new model, we need to use the `WordSegmenterApproach` annotator.\n","\n","The parameters of the annotator are:\n","\n","- **model**: Part-of-Speech model.\n","- **pattern**: Regex pattern used to match delimiters (Default: \"\\\\s+\").\n","- **toLowercase**: Indicates whether to convert all characters to lowercase before tokenizing (Default: false). Useful when multilanguage is present in the text.\n","- **ambiguityThreshold**: How much percentage of total amount of words are covered to be marked as frequent (Default: 0.97)\n","- **frequencyThreshold**: How many times at least a tag on a word to be marked as frequent (Default: 20)\n","- **nIterations**: Number of iterations in training, converges to better accuracy (Default: 5)\n","- **posCol**: Name of the column containing the POS tags that match tokens\n","\n","The implemented model is a modification of the following reference paper:\n","\n","> [Chinese Word Segmentation as Character Tagging (Xue, IJCLCLP 2003)](https://aclanthology.org/O03-4002/)\n"],"metadata":{"id":"z2sj_F-KdWNO"}},{"cell_type":"markdown","source":["### Training data"],"metadata":{"id":"ffYGrIlw1NMF"}},{"cell_type":"markdown","source":["The training data for the `WordSegmenterApproach` annotator is a text file in the same format used to train `Part-of-Speech` (POS) models, meaning that each ideogram/character is tagged with a label and are separated by a delimiter.\n","\n","We will use the following as training data to train a simple Korean Word Segmenter model (character and tag are separated by `| ` and characters-tag are separated by white space):\n","\n","> ìš°|LL ë¦¬|RR ëª¨|LL ë‘|MM ëŠ”|RR ì|LLì—°|MM ì–´|MM ì²˜|MM ë¦¬|MM ë¥¼|RR ì¢‹|LL ì•„|MM í•©|MM ë‹ˆ|MM ë‹¤|RR !|LR \n","\n","\n","Where the labels are:\n","\n","* `LL`: The beginning of the word\n","* `MM`: Middle part of the word\n","* `RR`: The end of the word\n","* `LR`: A word formed of only one character"],"metadata":{"id":"X-0MCS-2dWK5"}},{"cell_type":"markdown","source":["Create a text file with the training data:"],"metadata":{"id":"424E9bqC3A47"}},{"cell_type":"code","source":["with open(\"train_data.txt\", \"w\", encoding=\"utf8\") as f:\n","  f.write(\"ìš°|LL ë¦¬|RR ëª¨|LL ë‘|MM ëŠ”|RR ì|LL ì—°|MM ì–´|RR ì²˜|LL ë¦¬|MM ë¥¼|RR ì¢‹|LL ì•„|MM í•©|MM ë‹ˆ|MM ë‹¤|RR !|LR \")"],"metadata":{"id":"rB68D0Ww2tQx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To read this kind of dataset, you can use the helper class [POS](https://nlp.johnsnowlabs.com/docs/en/training#pos-dataset)."],"metadata":{"id":"xNglzdBo2Ovf"}},{"cell_type":"code","source":["from sparknlp.training import POS"],"metadata":{"id":"i-LJuo2oeYiY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data = POS().readDataset(spark, \"train_data.txt\")\n","train_data.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FylSPmpV2ij9","executionInfo":{"status":"ok","timestamp":1676572532797,"user_tz":180,"elapsed":2049,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"d1249ba8-9667-4cd5-876d-21681369702b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------------------+--------------------+--------------------+\n","|                         text|            document|                tags|\n","+-----------------------------+--------------------+--------------------+\n","|ìš° ë¦¬ ëª¨ ë‘ ëŠ” ì ì—° ì–´ ì²˜...|[{document, 0, 32...|[{pos, 0, 0, LL, ...|\n","+-----------------------------+--------------------+--------------------+\n","\n"]}]},{"cell_type":"markdown","source":["Build the pipeline for training"],"metadata":{"id":"q-RSm_933Ewh"}},{"cell_type":"code","source":["documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n","\n","wordSegmenter = (\n","    WordSegmenterApproach()\n","    .setInputCols([\"document\"])\n","    .setOutputCol(\"token\")\n","    .setPosColumn(\"tags\") # Name in the training data obtained with POS class\n","    .setNIterations(10)\n","    .setFrequencyThreshold(1) # Since our data is very small\n",")\n","\n","pipeline = Pipeline().setStages([documentAssembler, wordSegmenter])"],"metadata":{"id":"xdUoJ0BueaY4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","pipelineModel = pipeline.fit(train_data)"],"metadata":{"id":"trjcuaRfefT1","executionInfo":{"status":"ok","timestamp":1676572573104,"user_tz":180,"elapsed":2219,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ca0ad18f-7f24-496a-d91d-7b621b059a48"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CPU times: user 21.7 ms, sys: 6.07 ms, total: 27.7 ms\n","Wall time: 1.06 s\n"]}]},{"cell_type":"markdown","source":["Try the trained model:"],"metadata":{"id":"_LW6ssbDfBmR"}},{"cell_type":"code","source":["lp = LightPipeline(pipelineModel)\n","\n","lp.annotate(\"ìš°ë¦¬ëª¨ë‘ëŠ”ìì—°ì–´ì²˜ë¦¬ë¥¼ì¢‹ì•„í•©ë‹ˆë‹¤!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eFIvgPHAfa9d","executionInfo":{"status":"ok","timestamp":1676572586957,"user_tz":180,"elapsed":349,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"}},"outputId":"96e9750f-afc6-4513-9eb8-79e9e70a174d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'document': ['ìš°ë¦¬ëª¨ë‘ëŠ”ìì—°ì–´ì²˜ë¦¬ë¥¼ì¢‹ì•„í•©ë‹ˆë‹¤!'],\n"," 'token': ['ìš°ë¦¬', 'ëª¨ë‘ëŠ”', 'ìì—°ì–´', 'ì²˜ë¦¬ë¥¼', 'ì¢‹ì•„í•©ë‹ˆë‹¤', '!']}"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["That's it! Now you know how to train a new Word Segmenter model, as well as how to use a pretrained one!"],"metadata":{"id":"g7Bx9piT54rA"}},{"cell_type":"code","source":[],"metadata":{"id":"CCZb8Dy56AW5"},"execution_count":null,"outputs":[]}]}