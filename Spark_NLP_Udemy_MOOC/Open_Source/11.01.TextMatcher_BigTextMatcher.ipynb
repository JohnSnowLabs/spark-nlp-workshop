{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AVkXGNCrQjbe"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/Spark_NLP_Udemy_MOOC/Open_Source/11.01.TextMatcher_BigTextMatcher.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr-_QuBgQncC"
      },
      "source": [
        "# **TextMatcher / BigTextMatcher**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "K5q7Px9ZSUYS"
      },
      "source": [
        "The objective of this notebook is to explore the different parameters and usage of the TextMatcher and BigTextMatcher annotators in Spark NLP.\n",
        "\n",
        "**ðŸ“– Learning Objectives:**\n",
        "\n",
        "1. Learn how to use TextMatcher and BigTextMatcher annotators in Spark NLP for text matching tasks, including loading pre-trained models and configuring the matching pipeline.\n",
        "\n",
        "2. Understand the parameters and options available for the TextMatcher and BigTextMatcher annotators to customize the matching process based on specific use cases.\n",
        "\n",
        "**ðŸ”— Helpful Links:**\n",
        "\n",
        "- Documentation : [TextMatcher](https://sparknlp.org/docs/en/annotators#textmatcher), [BigTextMatcher](https://sparknlp.org/docs/en/annotators#bigtextmatcher)\n",
        "\n",
        "- Python Docs : [TextMatcher](https://sparknlp.org/api/python/reference/autosummary/sparknlp/annotator/matcher/text_matcher/index.html), [BigTextMatcher](https://sparknlp.org/api/python/reference/autosummary/sparknlp/annotator/matcher/big_text_matcher/index.html)\n",
        "\n",
        "- Scala Docs : [TextMatcher](https://sparknlp.org/api/com/johnsnowlabs/nlp/annotators/TextMatcher.html), [BigTextMatcher](https://sparknlp.org/api/com/johnsnowlabs/nlp/annotators/btm/BigTextMatcher.html)\n",
        "\n",
        "- For extended examples of usage, see the [TextMatcher](https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/main/scala/com/johnsnowlabs/nlp/annotators/TextMatcher.scala), [BigTextMatcher](https://github.com/JohnSnowLabs/spark-nlp/blob/master/src/main/scala/com/johnsnowlabs/nlp/annotators/btm/BigTextMatcher.scala)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "dbK3uWVxYLE8"
      },
      "source": [
        "## **ðŸ“œ Background**\n",
        "\n",
        "`TextMatcher` and `BigTextMatcher` are powerful annotators in Spark NLP used for matching and extracting text patterns from a document. `TextMatcher` works by defining rules that specify the patterns to match and how to match them, while `BigTextMatcher` is optimized for larger datasets. Both annotators use similar rules to match patterns and are customizable, allowing users to adjust the matching process to meet specific use case requirements. They are widely used in various natural language processing applications, including information retrieval, sentiment analysis, and content categorization. By using these annotators, organizations can quickly and accurately match text patterns, retrieve relevant information, and improve decision-making, leading to better customer experiences."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hKLnA3jiLfcE"
      },
      "source": [
        "## **ðŸŽ¬ Colab Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROMTLbCELeRj",
        "outputId": "a4fe5b2d-0d76-4c2d-95fa-baa08c6451f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spark-nlp\n",
            "  Downloading spark_nlp-4.4.0-py2.py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m486.4/486.4 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-4.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317145 sha256=272c87d48ad65679895ab9697e1831396dcfdf23ea0bf45dbab4137dbdbd7810\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/34/a4/159aa12d0a510d5ff7c8f0220abbea42e5d81ecf588c4fd884\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install spark-nlp\n",
        "!pip install pyspark"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1dzhMeaUMN9c"
      },
      "source": [
        "## âš’ï¸ Setup and Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfbYC4OcLipN"
      },
      "outputs": [],
      "source": [
        "import sparknlp\n",
        "from sparknlp.base import LightPipeline, Pipeline, ReadAs, Finisher\n",
        "from sparknlp.annotator import SentenceDetector, Tokenizer, DocumentAssembler, TextMatcher, BigTextMatcher\n",
        "from pyspark.sql import functions as F\n",
        "import pandas as pd\n",
        "\n",
        "# Start Spark Session\n",
        "spark = sparknlp.start()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1va0IhU6YRrN"
      },
      "source": [
        "##  ðŸ“‘ **`TextMatcher`**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kFIreUmPY8gJ"
      },
      "source": [
        "`TextMatcher` is a Spark NLP annotator that matches exact phrases in a document using tokens from a provided file. It requires `DOCUMENT` and `TOKEN` as input and produces CHUNK as output."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NfndVWsmdCJo"
      },
      "source": [
        "### **ðŸ–¨ï¸ Input/Output Annotation Types**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hzXgwQhndG0q"
      },
      "source": [
        "- Input: `DOCUMENT`, `TOKEN`\n",
        "\n",
        "- Output: `CHUNK`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_soae9d7dbUM"
      },
      "source": [
        "### **ðŸ”ŽParameters**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_RMDmMq-dgaP"
      },
      "source": [
        "- `setEntities`: Sets the external resource for the entities.\n",
        "- `setEntityValue`: Sets value for the entity metadata field.\n",
        "- `setCaseSensitive`: Sets whether to match regardless of case, by default True.\n",
        "- `setMergeOverlapping`: Sets whether to merge overlapping matched chunks, by default False.\n",
        "- `setBuildFromTokens`: Sets whether the `TextMatcher` should take the `CHUNK` from `TOKEN` or not."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jXvGAt1yhx5e"
      },
      "source": [
        "#### `setEntities`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5B8d6dldh263"
      },
      "source": [
        "- `setEntities` is a parameter in the `TextMatcher` component of Spark NLP that allows you to associate entities with the patterns you are matching. It takes a dictionary where the keys are the pattern names and the values are lists of entity names associated with that pattern.\n",
        "\n",
        "- `setEntities(path, read_as=ReadAs.TEXT, options={'format': 'text'})`\n",
        "\n",
        "  Parameters:\n",
        "\n",
        "  **path**: str\n",
        "\n",
        "  **read_as**: str, optional, by default  ReadAs.TEXT\n",
        "\n",
        "  **options**: dict, optional, by default {â€œformatâ€: â€œtextâ€}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vXcEw5TQ5clp"
      },
      "source": [
        "Here is an example usage of `setEntities`. First, letâ€™s create a dataframe of a sample text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX2FWGD67nhP"
      },
      "outputs": [],
      "source": [
        "# Create a dataframe from the sample_text\n",
        "data = spark.createDataFrame([\n",
        "[\"\"\"As she traveled across the world, Emma visited many different places\n",
        "and met many fascinating people. She walked the busy streets of Tokyo,\n",
        "hiked the rugged mountains of Nepal, and swam in the crystal-clear waters\n",
        "of the Caribbean. Along the way, she befriended locals like Akira, Rajesh,\n",
        "and Maria, each with their own unique stories to tell. Emma's travels took her\n",
        "to many cities, including New York, Paris, and Hong Kong, where she savored\n",
        "delicious foods and explored vibrant cultures. No matter where she went,\n",
        "Emma always found new wonders to discover and memories to cherish.\"\"\"]\n",
        "]).toDF(\"text\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "O1VQZtAl8xpu"
      },
      "source": [
        "Letâ€™s define the names and locations that we seek to match and save them as text files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xypd2pT7n8y"
      },
      "outputs": [],
      "source": [
        "# PERSON\n",
        "person_matches = \"\"\"\n",
        "Emma\n",
        "Akira\n",
        "Rajesh\n",
        "Maria\n",
        "\"\"\"\n",
        "\n",
        "with open('person_matches.txt', 'w') as f:\n",
        "    f.write(person_matches)\n",
        "\n",
        "# LOCATION\n",
        "location_matches = \"\"\"\n",
        "Tokyo\n",
        "Nepal\n",
        "Caribbean\n",
        "New York\n",
        "Paris\n",
        "Hong Kong\n",
        "\"\"\"\n",
        "\n",
        "with open('location_matches.txt', 'w') as f:\n",
        "    f.write(location_matches)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "P0CBXNLTAhGN"
      },
      "source": [
        "Create the pipeline, and define `setEntities` in `TextMatcher()` to match the input text with PERSON and LOCATION entities above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0eeG_Jv83jK"
      },
      "outputs": [],
      "source": [
        "# Step 1: Transforms raw texts to `document` annotation\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "# Step 2: Gets the tokens of the text\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols(\"document\") \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "# Step 3: PERSON matcher\n",
        "person_extractor = TextMatcher() \\\n",
        "    .setInputCols(\"document\", \"token\") \\\n",
        "    .setEntities(\"person_matches.txt\", ReadAs.TEXT) \\\n",
        "    .setOutputCol(\"person_entity\")\n",
        "\n",
        "# Step 4: LOCATION matcher\n",
        "location_extractor = TextMatcher() \\\n",
        "    .setInputCols(\"document\", \"token\") \\\n",
        "    .setEntities(\"location_matches.txt\", ReadAs.TEXT) \\\n",
        "    .setOutputCol(\"location_entity\")\n",
        "\n",
        "\n",
        "pipeline = Pipeline().setStages([document_assembler,\n",
        "                                 tokenizer,\n",
        "                                 person_extractor,\n",
        "                                 location_extractor\n",
        "                                 ])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PEp3CrUJA3u0"
      },
      "source": [
        "Fit and transform:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SAzEmeLABPq",
        "outputId": "3c8c7cb5-d2f7-4991-8a74-dac975f6f77b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------+-----------------------------------------------------+\n",
            "|result                            |result                                               |\n",
            "+----------------------------------+-----------------------------------------------------+\n",
            "|[Emma, Akira, Rajesh, Maria, Emma]|[Tokyo, Nepal, Caribbean, New York, Paris, Hong Kong]|\n",
            "+----------------------------------+-----------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Fit and transform to get a prediction\n",
        "results = pipeline.fit(data).transform(data)\n",
        "\n",
        "# Display the results\n",
        "results.selectExpr(\"person_entity.result\", \"location_entity.result\").show(truncate=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "etneHm_JA_Z3"
      },
      "source": [
        "#### `setEntityValue`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6CF_rSOuEWPp"
      },
      "source": [
        "- In Spark NLP's `TextMatcher`, the `setEntityValue` function allows you to set a custom value for the \"entity\" metadata field of the matched phrases. This can be particularly useful when you want to assign a specific label or category to the matched phrases in the output.\n",
        "\n",
        "- The \"entity\" metadata field is a part of the output annotations that `TextMatcher` produces. By default, the value of the \"entity\" field is set to the matched phrase itself. However, you may want to assign a more meaningful label or category to the matched phrases to better understand or process them in later stages of your NLP pipeline.\n",
        "\n",
        "- `setEntityValue(b)`\n",
        "\n",
        "    **b**: str\n",
        "    \n",
        "    Value for the entity metadata field, by default entity"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eYnXSArmEoa6"
      },
      "source": [
        "To see this lets look at the metadata of the previous example results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAcbelkrEMrx",
        "outputId": "b63cfbcd-a73b-4688-cdff-5dbb3e499fc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|metadata                                                                                                                                                                                                                                   |metadata                                                                                                                                                                                                                                                                                  |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[{entity -> entity, sentence -> 0, chunk -> 0}, {entity -> entity, sentence -> 0, chunk -> 1}, {entity -> entity, sentence -> 0, chunk -> 2}, {entity -> entity, sentence -> 0, chunk -> 3}, {entity -> entity, sentence -> 0, chunk -> 4}]|[{entity -> entity, sentence -> 0, chunk -> 0}, {entity -> entity, sentence -> 0, chunk -> 1}, {entity -> entity, sentence -> 0, chunk -> 2}, {entity -> entity, sentence -> 0, chunk -> 3}, {entity -> entity, sentence -> 0, chunk -> 4}, {entity -> entity, sentence -> 0, chunk -> 5}]|\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Fit and transform to get a prediction\n",
        "results = pipeline.fit(data).transform(data)\n",
        "\n",
        "# Display the results\n",
        "results.selectExpr(\"person_entity.metadata\", \"location_entity.metadata\").show(truncate=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DUIxP2o7FLl1"
      },
      "source": [
        "It can be seen that the metadata field is assigned as entity by default. Lets update both \"entity\" metadata field of the matched phrases by changing the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Imy0Uxe8A_Lp"
      },
      "outputs": [],
      "source": [
        "# Step 1: Transforms raw texts to `document` annotation\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "# Step 2: Gets the tokens of the text\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols(\"document\") \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "# Step 3: PERSON matcher\n",
        "person_extractor = TextMatcher() \\\n",
        "    .setInputCols(\"document\", \"token\") \\\n",
        "    .setEntities(\"person_matches.txt\", ReadAs.TEXT) \\\n",
        "    .setEntityValue(\"PERSON\") \\\n",
        "    .setOutputCol(\"person_entity\")\n",
        "\n",
        "# Step 4: LOCATION matcher\n",
        "location_extractor = TextMatcher() \\\n",
        "    .setInputCols(\"document\", \"token\") \\\n",
        "    .setEntities(\"location_matches.txt\", ReadAs.TEXT) \\\n",
        "    .setEntityValue(\"LOCATION\") \\\n",
        "    .setOutputCol(\"location_entity\")\n",
        "\n",
        "\n",
        "pipeline = Pipeline().setStages([document_assembler,\n",
        "                                 tokenizer,\n",
        "                                 person_extractor,\n",
        "                                 location_extractor\n",
        "                                 ])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hfHxuLk5HGra"
      },
      "source": [
        "Phrases match with person_matches.txt file and phrases match with location_matches.txt file are assigned using `setEntityValue(\"PERSON\")` and `setEntityValue(\"LOCATION\")`, respectively. Therefore metadata results are updated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axGbiFcQCSSw",
        "outputId": "f906ed09-cdf0-4304-90e8-5b3205ab7318"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|metadata                                                                                                                                                                                                                                   |metadata                                                                                                                                                                                                                                                                                              |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[{entity -> PERSON, sentence -> 0, chunk -> 0}, {entity -> PERSON, sentence -> 0, chunk -> 1}, {entity -> PERSON, sentence -> 0, chunk -> 2}, {entity -> PERSON, sentence -> 0, chunk -> 3}, {entity -> PERSON, sentence -> 0, chunk -> 4}]|[{entity -> LOCATION, sentence -> 0, chunk -> 0}, {entity -> LOCATION, sentence -> 0, chunk -> 1}, {entity -> LOCATION, sentence -> 0, chunk -> 2}, {entity -> LOCATION, sentence -> 0, chunk -> 3}, {entity -> LOCATION, sentence -> 0, chunk -> 4}, {entity -> LOCATION, sentence -> 0, chunk -> 5}]|\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Fit and transform to get a prediction\n",
        "results = pipeline.fit(data).transform(data)\n",
        "\n",
        "# Display the results\n",
        "results.selectExpr(\"person_entity.metadata\", \"location_entity.metadata\").show(truncate=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcGAQLlCHfSV"
      },
      "source": [
        "#### `setCaseSensitive`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "f_ndYy7RI5EF"
      },
      "source": [
        "- The `setCaseSensitive` option in Spark NLP's `TextMatcher` is used to regulate the matching process's case sensitivity while looking for certain words or phrases in the input text. It accepts a boolean value in which:\n",
        "\n",
        "- True: The `TextMatcher` takes the case into account while matching the text. In this situation, the `TextMatcher` must receive keywords or phrases that precisely match the case of the input text. For instance, the `TextMatcher` won't match \"Apple\" or \"APPLE\" in the input text if you're seeking for the term \"apple.\"\n",
        "\n",
        "- False: Case insensitivity will not be a factor in the TextMatcher's matching. This implies that regardless of how the keywords or phrases are presented in the input text, it will match them. In the same example, the TextMatcher will match \"apple,\" \"Apple,\" and \"APPLE\" in the input text if you are seeking for the keyword \"apple.\"\n",
        "\n",
        "- `setCaseSensitive(b)`\n",
        "\n",
        "    **b**: bool\n",
        "\n",
        "    Whether to match regardless of case, by default True"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zMNRNuXtM4oN"
      },
      "source": [
        "Let's see this by changing the names and locations files above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkvIdtNUHgaU"
      },
      "outputs": [],
      "source": [
        "# PERSON\n",
        "person_matches = \"\"\"\n",
        "emma\n",
        "Akira\n",
        "rajesh\n",
        "MARIA\n",
        "\"\"\"\n",
        "\n",
        "with open('person_matches.txt', 'w') as f:\n",
        "    f.write(person_matches)\n",
        "\n",
        "# LOCATION\n",
        "location_matches = \"\"\"\n",
        "Tokyo\n",
        "nepal\n",
        "CARIBBEAN\n",
        "New York\n",
        "Paris\n",
        "hong Kong\n",
        "\"\"\"\n",
        "\n",
        "with open('location_matches.txt', 'w') as f:\n",
        "    f.write(location_matches)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxb9LOAfNa6m"
      },
      "outputs": [],
      "source": [
        "# Step 1: Transforms raw texts to `document` annotation\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "# Step 2: Gets the tokens of the text\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols(\"document\") \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "# Step 3: PERSON matcher\n",
        "person_extractor = TextMatcher() \\\n",
        "    .setInputCols(\"document\", \"token\") \\\n",
        "    .setEntities(\"person_matches.txt\", ReadAs.TEXT) \\\n",
        "    .setEntityValue(\"PERSON\") \\\n",
        "    .setOutputCol(\"person_entity\")\n",
        "\n",
        "# Step 4: LOCATION matcher\n",
        "location_extractor = TextMatcher() \\\n",
        "    .setInputCols(\"document\", \"token\") \\\n",
        "    .setEntities(\"location_matches.txt\", ReadAs.TEXT) \\\n",
        "    .setEntityValue(\"LOCATION\") \\\n",
        "    .setOutputCol(\"location_entity\")\\\n",
        "    .setCaseSensitive(False)\n",
        "\n",
        "\n",
        "pipeline = Pipeline().setStages([document_assembler,\n",
        "                                 tokenizer,\n",
        "                                 person_extractor,\n",
        "                                 location_extractor\n",
        "                                 ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "273xlqgGNnI3",
        "outputId": "d4c365dc-2a68-4935-f325-882104ebbbda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----------------------------------------------------+\n",
            "|result |result                                               |\n",
            "+-------+-----------------------------------------------------+\n",
            "|[Akira]|[Tokyo, Nepal, Caribbean, New York, Paris, Hong Kong]|\n",
            "+-------+-----------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Fit and transform to get a prediction\n",
        "results = pipeline.fit(data).transform(data)\n",
        "\n",
        "# Display the results\n",
        "results.selectExpr(\"person_entity.result\", \"location_entity.result\").show(truncate=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a3Xs6QubN0ha"
      },
      "source": [
        "It can be seen that the result of person_entity is able to match the text with only Akira since its `setCaseSensitive` is set to `True` by default. On the other hand, `setCaseSensitive` for location-TextMatcher is set to `False`, and the result of location_entity is able to match the each entity defined in the location_matches.txt file with the input text."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8Bf6qcGIcUhk"
      },
      "source": [
        "#### `setMergeOverlapping`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8_B-E_B4fyeV"
      },
      "source": [
        "- In Spark NLP, the `setMergeOverlapping` parameter of the `TextMatcher` determines whether overlapping matched chunks should be merged. By default, this value is set to `False,` meaning overlapping matches will be kept separate entities.\n",
        "\n",
        "- If `setMergeOverlapping` is `True,` the `TextMatcher` will merge overlapping matches into a single chunk. This is particularly useful when you have phrases with shared words or characters and want to consider them a single match.\n",
        "\n",
        "- `setMergeOverlapping(b)`\n",
        "\n",
        "    **b**: bool\n",
        "\n",
        "    Whether to merge overlapping matched chunks\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H-ooCKcHgxXR"
      },
      "source": [
        "Here is an example to show how to use `setMergeOverlapping`, and its effect on the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmP1qCYSiUI0"
      },
      "outputs": [],
      "source": [
        "# Create a dataframe from the sample_text\n",
        "data = spark.createDataFrame([\n",
        "    (\"\"\"The new AI technology is making great strides in areas like machine learning, natural language processing, and computer vision.\"\"\",)\n",
        "]).toDF(\"text\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mwAp9lFm1XIz"
      },
      "source": [
        "Define the names that we seek to match and save them as text files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Y5OT9611inC"
      },
      "outputs": [],
      "source": [
        "entities_matches = \"\"\"\n",
        "AI\n",
        "AI technology\n",
        "machine learning\n",
        "natural language processing\n",
        "language processing\n",
        "computer vision\n",
        "\"\"\"\n",
        "\n",
        "with open('entities.txt', 'w') as f:\n",
        "    f.write(entities_matches)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2kfF2kUi1kPf"
      },
      "source": [
        "Create pipeline, `setMergeOverlapping()` is set to `False` by default, the Finisher will clean the annotations and exclude the metadata."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAq9uG-qNqn_",
        "outputId": "08be4aea-17b8-4772-91cc-eceeae1a3f1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------------------------------------------------+\n",
            "|matched_entities                                                                                        |\n",
            "+--------------------------------------------------------------------------------------------------------+\n",
            "|[AI, AI technology, machine learning, natural language processing, language processing, computer vision]|\n",
            "+--------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Transforms raw texts to `document` annotation\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "  .setInputCol(\"text\")\\\n",
        "  .setOutputCol(\"document\")\n",
        "\n",
        "# Step 2: Detects sentences within the document\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "  .setInputCols(\"document\")\\\n",
        "  .setOutputCol(\"sentence\")\n",
        "\n",
        "# Step 3: Tokenizes the words within the document\n",
        "tokenizer = Tokenizer()\\\n",
        "  .setInputCols([\"document\"])\\\n",
        "  .setOutputCol(\"token\")\n",
        "\n",
        "# Step 4: Matches the tokens with the entities defined in the `entities.txt` file\n",
        "extractor = TextMatcher()\\\n",
        "  .setEntities(\"entities.txt\")\\\n",
        "  .setInputCols(\"token\", \"sentence\")\\\n",
        "  .setCaseSensitive(False)\\\n",
        "  .setOutputCol(\"entities\")\n",
        "\n",
        "# Step 5: Extracts only the matched entities from the `entities` column\n",
        "finisher = Finisher() \\\n",
        "    .setInputCols(\"entities\") \\\n",
        "    .setOutputCols(\"matched_entities\")\\\n",
        "    .setIncludeMetadata(False) \\\n",
        "    .setCleanAnnotations(True)\n",
        "\n",
        "# Create a pipeline containing all the stages\n",
        "pipeline = Pipeline(\n",
        "    stages = [\n",
        "    documentAssembler,\n",
        "    sentenceDetector,\n",
        "    tokenizer,\n",
        "    extractor,\n",
        "    finisher\n",
        "  ])\n",
        "\n",
        "# Fit and transform the DataFrame\n",
        "result_setMergeOverlapping_False = pipeline.fit(data).transform(data)\n",
        "\n",
        "# Show the results\n",
        "result_setMergeOverlapping_False.select(\"matched_entities\").show(truncate=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0EvpVPkQ20HY"
      },
      "source": [
        "Create the same pipeline while setting `setMergeOverlapping` to be `True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJi2lbEH3M6t",
        "outputId": "b6cb70e0-2185-4e68-9338-9c01ce9aca31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------------------------------------------------------+\n",
            "|matched_entities                                                               |\n",
            "+-------------------------------------------------------------------------------+\n",
            "|[AI technology, machine learning, natural language processing, computer vision]|\n",
            "+-------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Transforms raw texts to `document` annotation\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "  .setInputCol(\"text\")\\\n",
        "  .setOutputCol(\"document\")\n",
        "\n",
        "# Step 2: Detects sentences within the document\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "  .setInputCols(\"document\")\\\n",
        "  .setOutputCol(\"sentence\")\n",
        "\n",
        "# Step 3: Tokenizes the words within the document\n",
        "tokenizer = Tokenizer()\\\n",
        "  .setInputCols(\"document\")\\\n",
        "  .setOutputCol(\"token\")\n",
        "\n",
        "# Step 4: Matches the tokens with the entities defined in the `entities.txt` file\n",
        "extractor = TextMatcher()\\\n",
        "  .setEntities(\"entities.txt\")\\\n",
        "  .setInputCols(\"token\", \"sentence\")\\\n",
        "  .setOutputCol(\"entities\")\\\n",
        "  .setCaseSensitive(False)\\\n",
        "  .setMergeOverlapping(True)\n",
        "\n",
        "# Step 5: Extracts only the matched entities from the `entities` column\n",
        "finisher = Finisher() \\\n",
        "    .setInputCols(\"entities\") \\\n",
        "    .setOutputCols(\"matched_entities\")\\\n",
        "    .setIncludeMetadata(False) \\\n",
        "    .setCleanAnnotations(True)\n",
        "\n",
        "# Create a pipeline containing all the stages\n",
        "pipeline = Pipeline(\n",
        "    stages = [\n",
        "    documentAssembler,\n",
        "    sentenceDetector,\n",
        "    tokenizer,\n",
        "    extractor,\n",
        "    finisher\n",
        "  ])\n",
        "\n",
        "# Fit and transform the DataFrame\n",
        "result_setMergeOverlapping_True = pipeline.fit(data).transform(data)\n",
        "\n",
        "# Show the results\n",
        "result_setMergeOverlapping_True.select(\"matched_entities\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "cU_636qx4Eco",
        "outputId": "02ac0126-3771-4921-f657-362d5b443e9c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-3846afc0-f4dc-4edd-a892-d9bc6a3c5d32\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>matched_entities_no_merge</th>\n",
              "      <th>matched_entities_with_merge</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[AI, AI technology, machine learning, natural language processing, language processing, computer vision]</td>\n",
              "      <td>[AI technology, machine learning, natural language processing, computer vision]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3846afc0-f4dc-4edd-a892-d9bc6a3c5d32')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3846afc0-f4dc-4edd-a892-d9bc6a3c5d32 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3846afc0-f4dc-4edd-a892-d9bc6a3c5d32');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                                                  matched_entities_no_merge  \\\n",
              "0  [AI, AI technology, machine learning, natural language processing, language processing, computer vision]   \n",
              "\n",
              "                                                       matched_entities_with_merge  \n",
              "0  [AI technology, machine learning, natural language processing, computer vision]  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert Spark DataFrames to pandas DataFrames\n",
        "result_setMergeOverlapping_False_pd = result_setMergeOverlapping_False.select(\"matched_entities\").toPandas()\n",
        "result_setMergeOverlapping_True_pd = result_setMergeOverlapping_True.select(\"matched_entities\").toPandas()\n",
        "\n",
        "# Rename columns to distinguish between the two sets of results\n",
        "result_setMergeOverlapping_False_pd = result_setMergeOverlapping_False_pd.rename(columns={\"matched_entities\": \"matched_entities_no_merge\"})\n",
        "result_setMergeOverlapping_True_pd = result_setMergeOverlapping_True_pd.rename(columns={\"matched_entities\": \"matched_entities_with_merge\"})\n",
        "\n",
        "# Concatenate the two pandas DataFrames, set max_colwidth for pandas\n",
        "combined_results = pd.concat([result_setMergeOverlapping_False_pd, result_setMergeOverlapping_True_pd], axis=1)\n",
        "pd.set_option('max_colwidth', None)\n",
        "\n",
        "# Display the combined results\n",
        "combined_results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "E8MeSfk99ypo"
      },
      "source": [
        "#### `setBuildFromTokens`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ONIoORx3_HWh"
      },
      "source": [
        "- The `setBuildFromTokens` parameter in `TextMatcher` is used to determine whether the `TextMatcher` should build chunks from tokens or not. By deafult it is set to `False`, meaning the `TextMatcher` will not build chunks from tokens.\n",
        "\n",
        "- If `setBuildFromTokens` is set to be `True`, the `TextMatcher` will consider individual tokens as potential matches for the provided entities. This can be useful when you want to match your entity list with the tokens in the text, rather than searching for the exact phrase.\n",
        "\n",
        "- `setBuildFromTokens(b)\n",
        "\n",
        "   **b**: bool\n",
        "\n",
        "   Whether the `TextMatcher` should take the `CHUNK` from `TOKEN` or not"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uV9bPnqwMz5L"
      },
      "source": [
        "##  ðŸ“‘ **`BigTextMatcher`**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vQCJ8BHZQ-mT"
      },
      "source": [
        "`BigTextMatcher` is an extension of Spark NLP's `TextMatcher`, designed for matching and extracting patterns from massive documents or corpora. It efficiently handles datasets too large for memory and performs distributed pattern matching using Spark NLP. The tool builds a data structure with input words or phrases, enabling quick matching against large datasets, surpassing `TextMatcher` in speed.\n",
        "\n",
        "A text file of predefined phrases must be provided with `setStoragePath`."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "56rPV2FDRG_k"
      },
      "source": [
        "### **ðŸ–¨ï¸ Input/Output Annotation Types**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q63tLDJrRNBe"
      },
      "source": [
        "- Input: `DOCUMENT`, `TOKEN`\n",
        "\n",
        "- Output: `CHUNK`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uiBA8j_VRRHE"
      },
      "source": [
        "### **ðŸ”ŽParameters**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i2v5ShOrRVZw"
      },
      "source": [
        "- `setEntities`: Sets the external resource for the entities.\n",
        "- `setCaseSensitive`: Sets whether to match regardless of case, by default True.\n",
        "- `setMergeOverlapping`: Sets whether to merge overlapping matched chunks, by default False.\n",
        "- `setTokenizer`: Sets TokenizerModel to use to tokenize input file for building a Trie."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iXnnVvoIT5_K"
      },
      "source": [
        "The parameters `setEntities`, `setCaseSensitive`, and `setMergeOverlapping` in `BigTextMatcher` are used in the same way as they are used in `TextMatcher`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCeErpOOr3Ng"
      },
      "outputs": [],
      "source": [
        "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_train.csv\n",
        "\n",
        "news_df = spark.read \\\n",
        "            .option(\"header\", True) \\\n",
        "            .csv(\"news_category_train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Qi6-oLtr3vk",
        "outputId": "69d3da32-5285-4df4-859c-fc51de23f763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+--------------------------------------------------+\n",
            "|category|                                       description|\n",
            "+--------+--------------------------------------------------+\n",
            "|Business| Short sellers, Wall Street's dwindling band of...|\n",
            "|Business| Private investment firm Carlyle Group, which h...|\n",
            "|Business| Soaring crude prices plus worries about the ec...|\n",
            "|Business| Authorities have halted oil export flows from ...|\n",
            "|Business| Tearaway world oil prices, toppling records an...|\n",
            "+--------+--------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "news_df.show(5, truncate=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--Ype8-Ht9xY"
      },
      "outputs": [],
      "source": [
        " # write the target entities to txt file\n",
        "\n",
        "entities = ['Wall Street', 'USD', 'stock', 'NYSE']\n",
        "with open ('financial_entities.txt', 'w') as f:\n",
        "    for i in entities:\n",
        "        f.write(i+'\\n')\n",
        "\n",
        "\n",
        "entities = ['soccer', 'world cup', 'Messi', 'FC Barcelona']\n",
        "with open ('sport_entities.txt', 'w') as f:\n",
        "    for i in entities:\n",
        "        f.write(i+'\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXA6RjqruCVh"
      },
      "outputs": [],
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"description\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "financial_entity_extractor = BigTextMatcher() \\\n",
        "    .setInputCols([\"document\",'token'])\\\n",
        "    .setOutputCol(\"financial_entities\")\\\n",
        "    .setStoragePath(\"financial_entities.txt\", ReadAs.TEXT)\\\n",
        "    .setCaseSensitive(False)\n",
        "\n",
        "sport_entity_extractor = BigTextMatcher() \\\n",
        "    .setInputCols([\"document\",'token'])\\\n",
        "    .setOutputCol(\"sport_entities\")\\\n",
        "    .setStoragePath(\"sport_entities.txt\", ReadAs.TEXT)\\\n",
        "    .setCaseSensitive(False)\n",
        "\n",
        "nlpPipeline = Pipeline(\n",
        "    stages=[\n",
        "        documentAssembler,\n",
        "        tokenizer,\n",
        "        financial_entity_extractor,\n",
        "        sport_entity_extractor\n",
        "        ])\n",
        "\n",
        "result = nlpPipeline.fit(news_df).transform(news_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmTPLPsguRk4",
        "outputId": "81e0b760-ca14-4286-c6d0-56335f300edc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------+----------------------------------+-------------------+\n",
            "|                                                                  text|                 financial_matches|      sport_matches|\n",
            "+----------------------------------------------------------------------+----------------------------------+-------------------+\n",
            "|\"Company launched the biggest electronic auction of stock in Wall S...|              [stock, Wall Street]|                 []|\n",
            "|Google, Inc. significantly cut the expected share price for its ini...|                    [stock, stock]|                 []|\n",
            "|Google, Inc. significantly cut the expected share price this mornin...|                    [stock, stock]|                 []|\n",
            "| Shares of Air Canada  (AC.TO) fell by more than half on Wednesday,...|                    [Stock, stock]|                 []|\n",
            "|Stock prices are lower in moderate trading. The Dow Jones Industria...|                    [Stock, Stock]|                 []|\n",
            "|The bad news just keeps pouring in for mutual fund manager Janus Ca...|                      [NYSE, NYSE]|                 []|\n",
            "|  Shaun Wright Phillips scored in his international debut as Englan...|                                []|[soccer, World Cup]|\n",
            "|NEWCASTLE, ENGLAND - England deservedly beat Ukraine 3-0 today in t...|                                []|[soccer, World Cup]|\n",
            "|MONTREAL (Reuters) - Shares of Air Canada (AC.TO: Quote, Profile, R...|                    [Stock, stock]|                 []|\n",
            "|\"SAN JOSE, California - On the cusp of its voyage into public tradi...|[stock, Wall Street, stock, Stock]|                 []|\n",
            "|\"Shortly before noon today, Google Inc. stock began trading under t...|                    [stock, stock]|                 []|\n",
            "|roundup Plus: EA to take World Cup soccer to Xbox...IBM chalks up t...|                                []|[World Cup, soccer]|\n",
            "|The U.S. Securities and Exchange Commission yesterday approved Goog...|                    [stock, stock]|                 []|\n",
            "|After a bumpy ride toward becoming a publicly traded company, Googl...|                    [stock, stock]|                 []|\n",
            "|In the most highly anticipated Wall Street debut since the heady da...|              [Wall Street, stock]|                 []|\n",
            "|NEW YORK Despite voluble skepticism among investors, Google #39;s s...|                    [stock, stock]|                 []|\n",
            "|If only the rest of my investments worked out this way. One week ag...|                    [stock, stock]|                 []|\n",
            "| U.S. stocks to watch: GOOGLE INC. (GOOG.O) Google shares jumped 18...|                    [stock, stock]|                 []|\n",
            "|\" U.S. stocks to watch: GOOGLE INC.  &lt;A HREF=\"\"http://www.invest...|                    [stock, stock]|                 []|\n",
            "|roundup Plus: KDE updates Linux desktop...EA to take World Cup socc...|                                []|[World Cup, soccer]|\n",
            "+----------------------------------------------------------------------+----------------------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select('description','financial_entities.result','sport_entities.result')\\\n",
        "      .toDF('text','financial_matches','sport_matches').filter((F.size('financial_matches')>1) | (F.size('sport_matches')>1))\\\n",
        "      .show(truncate=70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_l0SlOVEq_8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
