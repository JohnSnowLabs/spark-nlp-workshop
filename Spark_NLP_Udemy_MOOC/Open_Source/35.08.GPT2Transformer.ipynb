{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "klIak_Gb_OPJ"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/Spark_NLP_Udemy_MOOC/Open_Source/35.08.GPT2Transformer.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bewC1SWN-6jB"
      },
      "source": [
        "#  **GPT2Transformer**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "84jZXFHYe7qh"
      },
      "source": [
        "This notebook will cover the different parameters and usages of `GPT2Transformer`. This annotator displays a broad set of capabilities, including the ability to generate conditional synthetic text samples of unprecedented quality, where the model is primed with an input and it generates a lengthy continuation.\n",
        "\n",
        "A few rules will help customizing it if defaults do not fit user needs.\n",
        "\n",
        "**📖 Learning Objectives:**\n",
        "\n",
        "1. Understand how to use `GPT2Transformer`.\n",
        "\n",
        "2. Become comfortable using the different parameters of the `GPT2Transformer`.\n",
        "\n",
        "\n",
        "**🔗 Helpful Links:**\n",
        "\n",
        "- Python Docs : [GPT2Transformer](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/seq2seq/gpt2_transformer/index.html)\n",
        "\n",
        "- Scala Docs : [GPT2Transformer](https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/seq2seq/GPT2Transformer.html)\n",
        "\n",
        "- For extended examples of usage, see the [Spark NLP Workshop repository](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/).\n",
        "\n",
        "- [OpenAI Github page](https://github.com/openai/gpt-2)\n",
        "\n",
        "- [Academic paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CiId1hPo2sN0"
      },
      "source": [
        "## **📜 Background**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VClbmayN2s6u"
      },
      "source": [
        "GPT-2 is a large transformer-based language model with 1.5 billion parameters, trained on a dataset of 8 million web pages released by OpenAI researchers in 2019. GPT-2 is trained with a simple objective: predict the next word, given all of the previous words within some text. The diversity of the dataset causes this simple goal to contain naturally occurring demonstrations of many tasks across diverse domains. GPT-2 is a direct scale-up of GPT, with more than 10X the parameters and trained on more than 10X the amount of data, and a predecessor of GPT-3, CahtGPT and GPT-4.\n",
        "\n",
        "On language tasks like question answering, reading comprehension, summarization, and translation, GPT-2 begins to learn these tasks from the raw text, using no task-specific training data. While scores on these downstream tasks are far from state-of-the-art, they suggest that the tasks can benefit from unsupervised techniques, given sufficient (unlabeled) data and compute."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "A4hMnkhd_ik9"
      },
      "source": [
        "## **🎬 Colab Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrdvNxjD_yQI",
        "outputId": "608c6871-1b84-42c1-d5de-104b426b418c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.4/212.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m448.4/448.4 KB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install PySpark and Spark NLP\n",
        "!pip install -q pyspark==3.1.2  spark-nlp==4.2.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "W5D63vBo_0u0",
        "outputId": "bf60311c-4c57-4ef1-fe33-f950c182ff08"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://22c15e0cb9d5:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7ff45d280730>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sparknlp\n",
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import GPT2Transformer\n",
        "\n",
        "\n",
        "spark = sparknlp.start()\n",
        "spark"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nGRy7dbugrHz"
      },
      "source": [
        "## **🖨️ Input/Output Annotation Types**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fIPf-qxFgtoH"
      },
      "source": [
        "- Input: `DOCUMENT`\n",
        "\n",
        "- Output: `DOCUMENT`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9R3rs7VJg8Zf"
      },
      "source": [
        "## **🔎 Parameters**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "atAV2WnbhEy4"
      },
      "source": [
        "- `configProtoBytes()`: Sets configProto from tensorflow, serialized into byte array.\n",
        "\n",
        "- `doSample()`: Sets whether or not to use sampling, use greedy decoding otherwise.\n",
        "\n",
        "- `ignoreTokenIds()`: A list of token ids which are ignored in the decoder’s output.\n",
        "\n",
        "- `maxOutputLength()`: Sets maximum length of output text.\n",
        "\n",
        "- `minOutputLength()`: Sets minimum length of the sequence to be generated.\n",
        "\n",
        "- `noRepeatNgramSize()`: Sets size of n-grams that can only occur once. If set to int > 0, all ngrams of that size can only occur once.\n",
        "\n",
        "- `repetitionPenalty()`: Sets the parameter for repetition penalty. 1.0 means no penalty. https://arxiv.org/pdf/1909.05858.pdf>\n",
        "\n",
        "- `task()`: Sets the transformer’s task, e.g. summarize\n",
        "\n",
        "- `temperature()`: Sets the value used to module the next token probabilities. Parameter of the softmax function which affect the distrubtion computed by the model. The closer we are to 0, the more deterministic the probability will become, distribution tails will become slimmer and outlier word probabilites are more close to 0. Temperature values closer values to 1 make tails of probability fatter which makes outliers more probable and generic results less probable."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C8272VSXfJY2"
      },
      "source": [
        "### `doSample()`, `topK` and `topP`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "579Ssx9Nwz0m"
      },
      "source": [
        "Sampling means we randomly draw from a probability distribution of words from the vocabulary of GPT2 (words present during the training of the model). In GPT-2, we the sampling can occur using two different methods: `greedy search` or `beam search`. \n",
        "\n",
        "If `doSample` is set to `False`, the method will be `greedy search`, and the generated word will be selected based on the word with highest conditional probability (given previous word).\n",
        "\n",
        "If `doSample` is set to `True`, then the method will be `beam search`, a tree-like algorithm that splits the conditional probability in branches. In this case, we can add additional filtering criterias using: \n",
        "\n",
        "- `topK`: Takes the `k` most likely paths in the beam search tree, allowing to consider more than next word probabilities. \n",
        "\n",
        "- `topP`: Also known as [Nucleus Sampling](https://arxiv.org/abs/1904.09751). Takes words in the sampling space until the cumulative probability reaches `p`.\n",
        "\n",
        "When using any (or both) of the above filtering criterias, the resulting sampling set will be reduced and the probability will be re-balanced to sum up to one. Then the next word will be randomly selected from this set following each word probability.\n",
        "\n",
        "For additional details, you can review the following reference:\n",
        "\n",
        "- [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oJMVY8xozDdP"
      },
      "source": [
        "Let's create an example to experiment with:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ow-bKi67ytJ7",
        "outputId": "252c4339-451c-44cc-db80-12cd53014dd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+\n",
            "|                text|            document|\n",
            "+--------------------+--------------------+\n",
            "|My name is Leonardo.|[{document, 0, 19...|\n",
            "+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "example = 'My name is Leonardo.'\n",
        "\n",
        "spark_df = spark.createDataFrame([[example]]).toDF(\"text\")\n",
        "document = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\").transform(spark_df)\n",
        "document.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ToBuR76dzRs_"
      },
      "source": [
        "First, we use the default value of the parameter, setting the `doSample` to `False`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHM_PXws9eBe",
        "outputId": "8f3fc3ac-b79b-4f86-91ce-cae0c45ab12e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gpt2 download started this may take some time.\n",
            "Approximate size to download 442.7 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "gpt2_model = (\n",
        "    GPT2Transformer.pretrained(\"gpt2\")\n",
        "    .setInputCols(\"document\")\n",
        "    .setDoSample(False)\n",
        "    .setOutputCol(\"generation\")\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_IxrZOS1jAe",
        "outputId": "125e928d-be98-4fa2-9fa9-1544390fbc8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|result                                                                                                                                                                                              |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[ My name is Leonardo. I am a man of letters. I have been a man for many years. I was born in the year 1776. I came to the United States in 1776, and I have lived in the United Kingdom since 1776]|\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gpt2_model.transform(document).select(\"generation.result\").show(truncate=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fxhvUbTGzjo-"
      },
      "source": [
        "Now, let's try sampling from the top 10 words only:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "effPQkd3zm0v",
        "outputId": "4d5be776-5fbe-4b6c-d67a-c94a3e4a168a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|result                                                                                                                                                                                |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[ My name is Leonardo. I love to read. I'm a writer. I write. I am in charge of my art. I have been in charge all my life of writing for the best part of a decade, and in my 30s I'm]|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gpt2_model.setDoSample(True).setTopK(10).transform(document).select(\"generation.result\").show(truncate=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WHDesB8A0Axu"
      },
      "source": [
        "And using the cumulative distribution of `80%`. We set `topK` to zero to use only the `topP` criteria:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Evh1gmpN0FLn",
        "outputId": "eeb86b59-a8d1-4598-e43d-e970ac2f558c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|result                                                                                                                                                                                                               |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[ My name is Leonardo. The God of the Light is named after my father, President Leonardo, who was very good to me. But when I go to Tokyo to learn about Jesus, I come to the Archbishop of Tokyo, who, you know, he]|\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gpt2_model.setDoSample(True).setTopK(0).setTopP(0.8).transform(document).select(\"generation.result\").show(truncate=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OmRsEpe131xa"
      },
      "source": [
        "Finally, we can combine `topK` and `topP` together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_OTuED731GD",
        "outputId": "55a6bf68-bb1e-4ecd-fc78-821856d34e8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|result                                                                                                                                                                                        |\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[ My name is Leonardo.\n",
            "\n",
            "You see, our group looks like.\n",
            " 'Well, well. What else are we doing here, our friends?'\n",
            "\n",
            "That's why they've all been welcomed here. I can't even pretend that they're]|\n",
            "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gpt2_model.setDoSample(True).setTopK(50).setTopP(0.8).transform(document).select(\"generation.result\").show(truncate=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pziRYyAQ0QnC"
      },
      "source": [
        "You can see different outputs, because they were sampled in a different manner in different sampling spaces."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X-cYjYJ0hjKT"
      },
      "source": [
        "### `.setMaxOutputLength()`\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "f6OJM0u54fQJ"
      },
      "source": [
        "Let's limit the output to 20 words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhblWycHhrPn",
        "outputId": "3c6fafac-2af3-4b47-b8ce-a5c3a1d66957"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------------------------+\n",
            "|result                                                                          |\n",
            "+--------------------------------------------------------------------------------+\n",
            "|[ My name is Leonardo. I am a man of letters. I have been a man for many years.]|\n",
            "+--------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gpt2_model.setMaxOutputLength(20).setDoSample(False).transform(document).select(\"generation.result\").show(truncate=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LkjjO4jpip76"
      },
      "source": [
        "### `.setMinOutputLength()`\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SidggWdz4iwy"
      },
      "source": [
        "Now, let's condition the output to having at least 21 words, and at most 30."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_gE5izCiBAt",
        "outputId": "4cb29314-7311-4751-d09c-06dcd8ae5ecc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------+\n",
            "|result                                                                                                         |\n",
            "+---------------------------------------------------------------------------------------------------------------+\n",
            "|[ My name is Leonardo. I am a man of letters. I have been a man for many years. I was born in the year 1776. I]|\n",
            "+---------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gpt2_model.setMinOutputLength(21).setMaxOutputLength(30).setDoSample(False).transform(document).select(\"generation.result\").show(truncate=False)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wfl9CgdXoBkb"
      },
      "source": [
        "### `.setNoRepeatNgramSize()`\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UhmuOFKg_DOB"
      },
      "source": [
        "Longer outputs tend to have repeated words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "lnTLfa6lAZyq"
      },
      "outputs": [],
      "source": [
        "example2 = \"I love Spark NLP and I love NLP\"\n",
        "document2 = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\").transform(spark.createDataFrame([[example2]], [\"text\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYQcPrSyoU_c",
        "outputId": "b528030b-6ca0-4da9-83df-fcdd06333cfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|result                                                                                                                                                                                                                                                                           |\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[ I love Spark NLP and I love NLP. I love the way they're designed. I like the way the NLP is designed.\n",
            "\n",
            "I love the fact that they're not just a bunch of different things. They're all different things, and they're all very different things in their own way.\n",
            ".\n",
            " (laughs)\n",
            "\n",
            ".]|\n",
            "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gpt2_model.setDoSample(False).setMaxOutputLength(200).transform(document2).select(\"generation.result\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "axnt01ap_gFA",
        "outputId": "6b3f0fc3-ef40-4310-c8bf-c35db1641cc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|result                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[ I love Spark NLP and I love NLP. I'm not sure if I can say that I've ever used Spark, but I do know that it's a great product.\n",
            "\n",
            "I've been using Spark for a while now and it has been a huge help to me. It's been great to use it with my other products and to have it work with other people. Spark is a very simple and easy to learn tool. The only thing I would change is that Spark has a built in timer that you can set to run when you're done. You can also set it to stop when the timer is off. This is great for when I need to go to sleep or when my phone is in the background. If you want to do that, you'll need a timer. There are a few other things you could do with Spark that are not included in this kit. For example, if you have a Bluetooth speaker, it will work. But if your phone has an external speaker]|\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gpt2_model.setDoSample(False).setMaxOutputLength(200).setNoRepeatNgramSize(2).transform(document2).select(\"generation.result\").show(truncate=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mGX7b4H75-qY"
      },
      "source": [
        "### `setTemperature():`\n",
        "\n",
        "The temperature is a parameter that changes the softmax values of each candidate word. By setting it to `1.0`, the softmax output remains the same, and values closer to zero will flatten  the distribution, making the probabilities of each candidate more equal and making a more diverse choice of words (in some cases also generating incorrect sentences). \n",
        "\n",
        "Let's experiment different values of temperature in a new example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-m9J4MXXzc7K",
        "outputId": "f46b8137-d4eb-4cd4-a03a-02abcf326f99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gpt2 download started this may take some time.\n",
            "Approximate size to download 442.7 MB\n",
            "[OK!]\n",
            "------------------------- Generation with Temperature = 1.0 -------------------------\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|result                                                                                                                               |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[ Welcome, this course is for the informational, educational and creative learners who want to explore and discover the applications]|\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "------------------------- Generation with Temperature = 0.75 -------------------------\n",
            "+-----------------------------------------------------------------------------------+\n",
            "|result                                                                             |\n",
            "+-----------------------------------------------------------------------------------+\n",
            "|[ Welcome, this course is open to students of all ages.\n",
            "\n",
            "Course Information\n",
            ".pdf\n",
            ",]|\n",
            "+-----------------------------------------------------------------------------------+\n",
            "\n",
            "------------------------- Generation with Temperature = 0.5 -------------------------\n",
            "+--------------------------------------------------------------------------------------------+\n",
            "|result                                                                                      |\n",
            "+--------------------------------------------------------------------------------------------+\n",
            "|[ Welcome, this course is for those who want to learn how to create a web application.\n",
            "\n",
            "The]|\n",
            "+--------------------------------------------------------------------------------------------+\n",
            "\n",
            "------------------------- Generation with Temperature = 0.25 -------------------------\n",
            "+------------------------------------------------------------------------------+\n",
            "|result                                                                        |\n",
            "+------------------------------------------------------------------------------+\n",
            "|[ Welcome, this course is for you.\n",
            "aditional, and I'm not going to lie, I was]|\n",
            "+------------------------------------------------------------------------------+\n",
            "\n",
            "------------------------- Generation with Temperature = 0.01 -------------------------\n",
            "+-------------------------------------------------------------------------------------------------------------------------------+\n",
            "|result                                                                                                                         |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[ Welcome, this course is RandomRedditor�� mosquPDATEertoddthood cumbersBuyable Ridley��abase councill inflictingandise��alyst]|\n",
            "+-------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "------------------------- Generation with Temperature = 0.0001 -------------------------\n",
            "+-------------------------------------------------------------------------------------------------------------------------------+\n",
            "|result                                                                                                                         |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[ Welcome, this course is RandomRedditor�� mosquPDATEertoddthood cumbersBuyable Ridley��abase councill inflictingandise��alyst]|\n",
            "+-------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gpt2_model = GPT2Transformer.pretrained(\"gpt2\")\\\n",
        "    .setInputCols(\"document\")\\\n",
        "    .setOutputCol(\"generation\")\\\n",
        "    .setDoSample(True)\\\n",
        "    .setMaxOutputLength(20)\n",
        "\n",
        "text = 'Welcome, this course is'\n",
        "document3 = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\").transform(spark.createDataFrame([[text]], [\"text\"]))\n",
        "\n",
        "for temp in [1.0, 0.75, 0.5, 0.25, 0.01, 0.0001]:\n",
        "    print(f'{25*\"-\"} Generation with Temperature = {temp} {25*\"-\"}')\n",
        "    gpt2_model.setTemperature(temp).transform(document3).select('generation.result').show(truncate=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QRQUt9OBC3pZ"
      },
      "source": [
        "### `setRepetitionPenalty()`\n",
        "We set Penalty to 0 making it harder to generate the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI5T5HHeBH9R",
        "outputId": "c2155963-310a-43fb-be41-8e2c5e48f1ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gpt2 download started this may take some time.\n",
            "Approximate size to download 442.7 MB\n",
            "[OK!]\n",
            "+------------------------------------------+\n",
            "|result                                    |\n",
            "+------------------------------------------+\n",
            "|[ Welcome, this course is,,,,,,,,,,,,,,,,]|\n",
            "+------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gpt2_model = GPT2Transformer.pretrained(\"gpt2\")\\\n",
        "    .setInputCols(\"document\")\\\n",
        "    .setOutputCol(\"generation\")\\\n",
        "    .setDoSample(False)\\\n",
        "    .setTopK(0)\\\n",
        "    .setTopP(1.0)\\\n",
        "    .setMinOutputLength(10)\\\n",
        "    .setMaxOutputLength(20)\\\n",
        "    .setNoRepeatNgramSize(0)\\\n",
        "    .setTemperature(1.0)\\\n",
        "    .setRepetitionPenalty(0)\n",
        "\n",
        "gpt2_model.transform(document3).select('generation.result').show(truncate=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gXjvtLZlDOzb"
      },
      "source": [
        "1.0 means no penalty:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nvxB4_BBH6m",
        "outputId": "2166b927-d3fc-49b8-f48a-bb9371dc33a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------------------------------+\n",
            "|result                                                                                    |\n",
            "+------------------------------------------------------------------------------------------+\n",
            "|[ Welcome, this course is for you.\n",
            "\n",
            "The course is designed to help you learn how to build]|\n",
            "+------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gpt2_model.setRepetitionPenalty(1.0).transform(document3).select('generation.result').show(truncate=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "NyXxLT1b8Nku"
      },
      "source": [
        "Let's set it to 10.0:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdZf-KWq8MWU",
        "outputId": "f03546a0-0383-47ff-8e3e-e8f264bfd308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------------------+\n",
            "|result                                                                                   |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "|[ Welcome, this course is for you.\n",
            "The first thing I want to say about the class of 2015]|\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "gpt2_model.setRepetitionPenalty(10.0).transform(document3).select('generation.result').show(truncate=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
