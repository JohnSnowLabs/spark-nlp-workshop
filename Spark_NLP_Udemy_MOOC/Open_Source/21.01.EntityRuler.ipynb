{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6EDOvaSaGNnS"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/Spark_NLP_Udemy_MOOC/Open_Source/21.01.EntityRuler.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "StKew2JGGb88"
      },
      "source": [
        "# **EntityRuler**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "geFwPWERGsje"
      },
      "source": [
        "This notebook will cover the different parameter and usage of **EntityRuler**. There are 2 annotators to perform this task in Spark NLP; `EntityRulerApproach` and `EntityRulerModel`. <br/>\n",
        "\n",
        "- `EntityRulerApproach` fits an Annotator to match exact strings or regex patterns provided in a file against a Document and assigns them an named entity. The definitions can contain any number of named entities. \n",
        "- `EntityRulerModel` is instantiated model of the `EntityRulerApproach`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cj_4JrsnHwkg"
      },
      "source": [
        "**üìñ Learning Objectives:**\n",
        "\n",
        "1. Understand how to extract entities with predefined regex patterns or match predefined exact strings. \n",
        "\n",
        "2. Understand the difference between the `EntityRulerApproach` and `EntityRulerModel`.\n",
        "\n",
        "3. Become comfortable using the different parameters of these annotators.\n",
        "\n",
        "**üîó Helpful Links:**\n",
        "\n",
        "Documentation: [EntityRuler](https://nlp.johnsnowlabs.com/docs/en/annotators#entityruler)\n",
        "\n",
        "Python Docs: [EntityRulerApproach](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/er/entity_ruler/index.html#sparknlp.annotator.er.entity_ruler.EntityRulerApproach), [EntityRulerModel](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/er/entity_ruler/index.html#sparknlp.annotator.er.entity_ruler.EntityRulerModel)\n",
        "\n",
        "Scala Docs: [EntityRulerApproach](https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/er/EntityRulerApproach.html), [EntityRulerModel](https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/er/EntityRulerModel.html)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "b4UKGnXjKcXO"
      },
      "source": [
        "## **üìú Background**\n",
        "\n",
        "Extracting entities is a significant task in NLP area. In Spark NLP, `EntityRulerApproach` and `EntityRulerModel` can be used to perform this task based on predefined custom file instead of building a high level machine learning/deep learning models. <br/>\n",
        "\n",
        "There are multiple ways and formats to set the extraction resource. It is possible to set it either as a ‚ÄúJSON‚Äù, ‚ÄúJSONL‚Äù or ‚ÄúCSV‚Äù file. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gdN797PJLzuD"
      },
      "source": [
        "## **üé¨ Colab Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zinsxDwXEzFG"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyspark==3.1.2  spark-nlp==4.2.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "lcUfVXyIEzCC",
        "outputId": "157f26ed-566c-4433-bda0-6cc266366070"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://c3bf2420e9cd:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f66d530b340>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import sparknlp\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "spark = sparknlp.start()\n",
        "spark"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rylGHPQTc0ix"
      },
      "source": [
        "# `EntityRulerApproach`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pMnO7zINNIS1"
      },
      "source": [
        "## **üñ®Ô∏è Input/Output Annotation Types**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1Qod38w3coqW"
      },
      "source": [
        "- Input: `DOCUMENT`, `TOKEN`\n",
        "- Output: `CHUNK`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "358nMjL7cwnu"
      },
      "source": [
        "## **üîé Parameters**\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rcZ86AZ6c9HN"
      },
      "source": [
        "- `caseSensitive`: (Boolean) Whether to ignore case in index lookups (Default depends on model)\n",
        "- `patternsResource` (String) Sets Resource in JSON or CSV format to map entities to patterns.\n",
        "- `useStorage` (Boolean) Whether to use RocksDB storage to serialize patterns (Default: true).\n",
        "- `SentenceMatch` (Boolean) Sets whether to find match at sentence level.\n",
        "- `AlphabetResource` (String) Alphabet Resource (a simple plain text with all language characters)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "a_CcQ4B9rxFn"
      },
      "source": [
        "### `setPatternsResource()`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jWBDuSTDq4Zi"
      },
      "source": [
        "There are multiple ways and formats to set the extraction resource. It is possible to set it either as a ‚ÄúJSON‚Äù, ‚ÄúJSONL‚Äù or ‚ÄúCSV‚Äù file. A path to the file needs to be provided to `setPatternsResource()` parameter. <br/>\n",
        "\n",
        "The file format needs to be set as the `format` field in the `option` parameter map and depending on the file type, additional parameters might need to be set."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "q2gL0beYvdDQ"
      },
      "source": [
        "In our first example, we will define keywords and their labels to be matched by `EntityRulerApproach`. <br/>\n",
        "\n",
        "If the file is in a JSON format, then the rule definitions need to be given in a list with the fields ‚Äúlabel‚Äù and ‚Äúpatterns‚Äù:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1bJHjp29sCTs"
      },
      "outputs": [],
      "source": [
        "#Defining the source JSON file and saving\n",
        "import json\n",
        "\n",
        "keywords = [\n",
        "          {\n",
        "            \"label\": \"PERSON\",\n",
        "            \"patterns\": [\"Jon\", \"John\", \"John Snow\", \"Jon Snow\"]\n",
        "          },\n",
        "          {\n",
        "            \"label\": \"PERSON\",\n",
        "            \"patterns\": [\"Eddard\", \"Eddard Stark\"]\n",
        "          },\n",
        "          {\n",
        "            \"label\": \"LOCATION\",\n",
        "            \"patterns\": [\"Winterfell\"]\n",
        "          },\n",
        "         ]\n",
        "\n",
        "with open('keywords.json', 'w') as jsonfile:\n",
        "    json.dump(keywords, jsonfile)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GkyPFKVRtJgc"
      },
      "source": [
        "Sample dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iWdnqAqqtPM",
        "outputId": "d4ae372c-4e7e-47c9-c063-ac9605da0dc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------+\n",
            "|text                                                                         |\n",
            "+-----------------------------------------------------------------------------+\n",
            "|Lord Eddard Stark was the head of House Stark. John Snow lives in Winterfell.|\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = spark.createDataFrame([[\"Lord Eddard Stark was the head of House Stark. John Snow lives in Winterfell.\"]]).toDF(\"text\")\n",
        "data.show(truncate=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Lw5canrotSkl"
      },
      "source": [
        "Building a pipeline with `EntityRulerApproach()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "juq5IUtztiRF"
      },
      "outputs": [],
      "source": [
        "documenter = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "    .setInputCols('document')\\\n",
        "    .setOutputCol('sentence')\n",
        "\n",
        "entity_ruler = EntityRulerApproach() \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"entity\") \\\n",
        "    .setPatternsResource(\"keywords.json\") \n",
        "\n",
        "pipeline = Pipeline(stages=[documenter, \n",
        "                            sentenceDetector, \n",
        "                            entity_ruler])\n",
        "\n",
        "pipeline_model = pipeline.fit(data)\n",
        "result= pipeline_model.transform(data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3lEbTjnivW54"
      },
      "source": [
        "Checking matched entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBkoDPcsEy-J",
        "outputId": "a655c01a-0314-44b9-d715-d712047f3800"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|entity                                                                                                                                                                                                        |\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[{chunk, 5, 16, Eddard Stark, {entity -> PERSON, sentence -> 0}, []}, {chunk, 47, 55, John Snow, {entity -> PERSON, sentence -> 1}, []}, {chunk, 66, 75, Winterfell, {entity -> LOCATION, sentence -> 1}, []}]|\n",
            "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(\"entity\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbkPGyLsv6Su",
        "outputId": "e024a50f-c148-4781-f8f5-8f4328861cd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+--------+\n",
            "|     keyword|   label|\n",
            "+------------+--------+\n",
            "|Eddard Stark|  PERSON|\n",
            "|   John Snow|  PERSON|\n",
            "|  Winterfell|LOCATION|\n",
            "+------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(F.explode(F.arrays_zip('entity.result', 'entity.metadata')).alias('col'))\\\n",
        "      .select(F.expr(\"col['0']\").alias(\"keyword\"),\n",
        "              F.expr(\"col['1']['entity']\").alias(\"label\")).show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vhYk9mFDvzDu"
      },
      "source": [
        "As seen above, keywords that we defined in the JSON file were matched. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o1pwCQjvyZHj"
      },
      "source": [
        "We can define an \"id\" field to identify entities as the example below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yuBXA9sIvUV0"
      },
      "outputs": [],
      "source": [
        "keywords = [\n",
        "            {\n",
        "              \"id\": \"names-with-j\",\n",
        "              \"label\": \"PERSON\",\n",
        "              \"patterns\": [\"Jon\", \"John\", \"John Snow\", \"Jon Snow\"]\n",
        "            },\n",
        "            {\n",
        "              \"id\": \"names-with-e\",\n",
        "              \"label\": \"PERSON\",\n",
        "              \"patterns\": [\"Eddard\", \"Eddard Stark\"]\n",
        "            },\n",
        "            {\n",
        "              \"id\": \"locations\",\n",
        "              \"label\": \"LOCATION\",\n",
        "              \"patterns\": [\"Winterfell\"]\n",
        "            },\n",
        "         ]\n",
        "\n",
        "with open('keywords_with_id.json', 'w') as jsonfile:\n",
        "    json.dump(keywords, jsonfile)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6NN5wZGrywPi"
      },
      "source": [
        "Defining the `EntityRulerApproach()` again with the new JSON."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "712bQ0KJvUQG"
      },
      "outputs": [],
      "source": [
        "entity_ruler = EntityRulerApproach() \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"entity\") \\\n",
        "    .setPatternsResource(\"keywords_with_id.json\") \n",
        "\n",
        "pipeline = Pipeline(stages=[documenter, \n",
        "                            sentenceDetector, \n",
        "                            entity_ruler])\n",
        "\n",
        "pipeline_model = pipeline.fit(data)\n",
        "result= pipeline_model.transform(data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bZftCmJQzlyW"
      },
      "source": [
        "Checking the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0U9DnXevM46",
        "outputId": "d597a925-8acf-4bc6-f211-e9a3ae7e086b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|entity                                                                                                                                                                                                                                                                 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[{chunk, 5, 16, Eddard Stark, {entity -> PERSON, sentence -> 0, id -> names-with-e}, []}, {chunk, 47, 55, John Snow, {entity -> PERSON, sentence -> 1, id -> names-with-j}, []}, {chunk, 66, 75, Winterfell, {entity -> LOCATION, sentence -> 1, id -> locations}, []}]|\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(\"entity\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhvaHkDDzYdT",
        "outputId": "2a166289-f04c-46a0-a46e-4436401582d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+------------+--------+\n",
            "|          id|     keyword|   label|\n",
            "+------------+------------+--------+\n",
            "|names-with-e|Eddard Stark|  PERSON|\n",
            "|names-with-j|   John Snow|  PERSON|\n",
            "|   locations|  Winterfell|LOCATION|\n",
            "+------------+------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(F.explode(F.arrays_zip('entity.result', 'entity.metadata')).alias('col'))\\\n",
        "      .select(F.expr(\"col['1']['id']\").alias(\"id\"),\n",
        "              F.expr(\"col['0']\").alias(\"keyword\"),\n",
        "              F.expr(\"col['1']['entity']\").alias(\"label\")).show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Dngg12tsznlr"
      },
      "source": [
        "As seen above, we succesfully defined the \"id\" section. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8DK699pW0HFq"
      },
      "source": [
        "Now, we will do an example with a source file in CSV format. For the CSV file we use the following configuration:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9ATrrNCMz3XC"
      },
      "outputs": [],
      "source": [
        "with open('keywords.csv', 'w') as csvfile:\n",
        "    csvfile.write('PERSON|Jon\\n')\n",
        "    csvfile.write('PERSON|John\\n')\n",
        "    csvfile.write('PERSON|John Snow\\n')\n",
        "    csvfile.write('LOCATION|Winterfell')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an-iCst2z3Ov",
        "outputId": "94d69e78-1a08-4570-8a55-7201c7e67d8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PERSON|Jon\n",
            "PERSON|John\n",
            "PERSON|John Snow\n",
            "LOCATION|Winterfell"
          ]
        }
      ],
      "source": [
        "! cat keywords.csv"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UjZ_Vatb0YyJ"
      },
      "source": [
        "Building `EntityRulerApproach()` with the CSV source file: <br/>\n",
        "We will also set the **format** of the file as a CSV and will specify the **delimiter** for the CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Rp7mUQDjz3Gt"
      },
      "outputs": [],
      "source": [
        "entity_ruler_csv = EntityRulerApproach() \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"entity\") \\\n",
        "    .setPatternsResource(\"keywords.csv\", options={\"format\": \"csv\", \"delimiter\": \"\\\\|\"}) \\\n",
        "\n",
        "pipeline = Pipeline(stages=[documenter, \n",
        "                            sentenceDetector, \n",
        "                            entity_ruler])\n",
        "\n",
        "pipeline_model = pipeline.fit(data)\n",
        "result= pipeline_model.transform(data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oDMlKWln00Cb"
      },
      "source": [
        "Checking the results. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRxasPJM01_r",
        "outputId": "84cfb8fe-f293-40c2-e3a8-26e63c10bac7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|entity                                                                                                                                                                                                                                                                 |\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[{chunk, 5, 16, Eddard Stark, {entity -> PERSON, sentence -> 0, id -> names-with-e}, []}, {chunk, 47, 55, John Snow, {entity -> PERSON, sentence -> 1, id -> names-with-j}, []}, {chunk, 66, 75, Winterfell, {entity -> LOCATION, sentence -> 1, id -> locations}, []}]|\n",
            "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(\"entity\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb2Nc130za4-",
        "outputId": "8a5d8532-addd-457f-bf34-e6b7b2f6dc6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+--------+\n",
            "|     keyword|   label|\n",
            "+------------+--------+\n",
            "|Eddard Stark|  PERSON|\n",
            "|   John Snow|  PERSON|\n",
            "|  Winterfell|LOCATION|\n",
            "+------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(F.explode(F.arrays_zip('entity.result', 'entity.metadata')).alias('col'))\\\n",
        "      .select(F.expr(\"col['0']\").alias(\"keyword\"),\n",
        "              F.expr(\"col['1']['entity']\").alias(\"label\")).show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "m2hrbOMR0-Y6"
      },
      "source": [
        "As you see above, we successfully defined a CSV file as a source and used it with the `EntityRulerApproach()`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6O0VRpYa1dym"
      },
      "source": [
        "### Using Regular Expressions \n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1QZGd1L71kBQ"
      },
      "source": [
        "If you need to set a regex pattern for the matching, you can specify a key value pair as `\"regex\": true` in the source JSON file. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdosgHsR1j2l",
        "outputId": "ef7959da-a05d-47c8-ae59-9e5226b86e46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------+\n",
            "|text                               |\n",
            "+-----------------------------------+\n",
            "|The address is 123456 in Winterfell|\n",
            "+-----------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Sample data\n",
        "data = spark.createDataFrame([[\"The address is 123456 in Winterfell\"]]).toDF(\"text\")\n",
        "data.show(truncate=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rWZXHUnZ2Aq3"
      },
      "source": [
        "We will define a JSON source file which has a regex rule for matching the 'id' and defined keywords for 'location'. <br/>\n",
        "\n",
        "We will specify \"id\", \"label\", \"pattern\" and \"regex\" keys in the JSON. The \"regex\" should be set as 'True' for regex patterns. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JI11csEK08ZV"
      },
      "outputs": [],
      "source": [
        "import json \n",
        "\n",
        "patterns_string = \"\"\"\n",
        "[\n",
        "  {\n",
        "    \"id\": \"id-regex\",\n",
        "    \"label\": \"ID\",\n",
        "    \"patterns\": [\"[0-9]+\"],\n",
        "    \"regex\": true\n",
        "      },\n",
        "  {\n",
        "    \"id\": \"locations-words\",\n",
        "    \"label\": \"LOCATION\",\n",
        "    \"patterns\": [\"Winterfell\"],\n",
        "    \"regex\": false\n",
        "\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "patterns_obj = json.loads(patterns_string)\n",
        "with open('regex_patterns.json', 'w') as jsonfile:\n",
        "    json.dump(patterns_obj, jsonfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "383NqKG62pmD",
        "outputId": "531af565-ea4c-4b6c-b7cc-6d7cf799fadb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{\"id\": \"id-regex\", \"label\": \"ID\", \"patterns\": [\"[0-9]+\"], \"regex\": true}, {\"id\": \"locations-words\", \"label\": \"LOCATION\", \"patterns\": [\"Winterfell\"], \"regex\": false}]"
          ]
        }
      ],
      "source": [
        "!cat regex_patterns.json"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "i-Fbwyzj2zLN"
      },
      "source": [
        "**Note:** When defining a regex pattern, we need to define Tokenizer annotator in the pipeline!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yiMmn5S72pCA"
      },
      "outputs": [],
      "source": [
        "documenter = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "    .setInputCols('document')\\\n",
        "    .setOutputCol('sentence')\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols(\"sentence\") \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "regex_entity_ruler = EntityRulerApproach() \\\n",
        "    .setInputCols([\"sentence\", \"token\"]) \\\n",
        "    .setOutputCol(\"entity\") \\\n",
        "    .setPatternsResource(\"regex_patterns.json\") \n",
        "\n",
        "pipeline = Pipeline(stages=[documenter, \n",
        "                            sentenceDetector, \n",
        "                            tokenizer,\n",
        "                            regex_entity_ruler])\n",
        "\n",
        "pipeline_model = pipeline.fit(data)\n",
        "result= pipeline_model.transform(data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9r7JmC_v3hZk"
      },
      "source": [
        "Checking the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19Eyffx_3RbL",
        "outputId": "e8822252-6bc9-48aa-c461-75908af012d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|entity                                                                                                                                                                   |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[{chunk, 15, 20, 123456, {entity -> ID, id -> id-regex, sentence -> 0}, []}, {chunk, 25, 34, Winterfell, {entity -> LOCATION, sentence -> 0, id -> locations-words}, []}]|\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(\"entity\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywh3YdZW3g26",
        "outputId": "2d2b6f93-9e78-4b79-9d69-0957e67f8bd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------+--------+\n",
            "|   keyword|   label|\n",
            "+----------+--------+\n",
            "|    123456|      ID|\n",
            "|Winterfell|LOCATION|\n",
            "+----------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(F.explode(F.arrays_zip('entity.result', 'entity.metadata')).alias('col'))\\\n",
        "      .select(F.expr(\"col['0']\").alias(\"keyword\"),\n",
        "              F.expr(\"col['1']['entity']\").alias(\"label\")).show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "N9-gqVGf4SJ6"
      },
      "source": [
        "As seen above, we matched '123456' as an id as we defined with a regex in the source JSON file. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fS9OGhTi5pcT"
      },
      "source": [
        "### `setSentenceMatch()`\n",
        "This parameter is used to set whether finding a match at sentence level. When we set `setSentenceMatch(False)`, we're looking for a match on each token of a sentence. You can change it to \"True\" to look for a match on each sentence of a document. The latter is particularly useful when working with multi-word matches <br/>\n",
        "\n",
        "**Please note that this parameter only works for Regex Patterns!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTxFqvIBt7Xm",
        "outputId": "f0ba6ca0-6e5a-43f7-fd4f-edcc75c4f2ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------+\n",
            "|text                          |\n",
            "+------------------------------+\n",
            "|Patrick lives in New York City|\n",
            "+------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#sample data\n",
        "data = spark.createDataFrame([[\"Patrick lives in New York City\"]]).toDF(\"text\")\n",
        "data.show(truncate=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLOaN-Sk8UWY"
      },
      "source": [
        "In this example, we will extract \"New York City\" by defining a regex. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BTvPQFcgtFIf"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "patterns_string = \"\"\"\n",
        "[\n",
        "  {\n",
        "    \"id\": \"locations-words\",\n",
        "    \"label\": \"LOCATION\",\n",
        "    \"patterns\": [\"\\\\\\\\bNew York City\\\\\\\\b\"],\n",
        "    \"regex\": true\n",
        "      },\n",
        "  {\n",
        "    \"id\": \"name-words\",\n",
        "    \"label\": \"NAME\",\n",
        "    \"patterns\": [\"Patrick\"],\n",
        "    \"regex\": false\n",
        "\n",
        "  }\n",
        "]\n",
        "\"\"\"\n",
        "\n",
        "patterns_obj = json.loads(patterns_string)\n",
        "with open('patterns.json', 'w') as jsonfile:\n",
        "    json.dump(patterns_obj, jsonfile)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "65kxQIXR9aby"
      },
      "source": [
        "**`setSentenceMatch(False)`**: We do not expect any matching for the \"New York City\" since it is multi-token entity and we are not checking sentence level match. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YGg7uWKK6SjF"
      },
      "outputs": [],
      "source": [
        "documenter = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "    .setInputCols('document')\\\n",
        "    .setOutputCol('sentence')\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols(\"sentence\") \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "regex_entity_ruler = EntityRulerApproach() \\\n",
        "    .setInputCols([\"sentence\", \"token\"]) \\\n",
        "    .setOutputCol(\"entity\") \\\n",
        "    .setPatternsResource(\"patterns.json\") \\\n",
        "    .setSentenceMatch(False)\n",
        "\n",
        "pipeline = Pipeline(stages=[documenter, \n",
        "                            sentenceDetector, \n",
        "                            tokenizer,\n",
        "                            regex_entity_ruler])\n",
        "\n",
        "pipeline_model = pipeline.fit(data)\n",
        "result= pipeline_model.transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ay7IBsB57m35",
        "outputId": "1ae6e4bf-909a-4429-991e-09058fd87766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------------------------------------------------------+\n",
            "|entity                                                                         |\n",
            "+-------------------------------------------------------------------------------+\n",
            "|[{chunk, 0, 6, Patrick, {entity -> NAME, sentence -> 0, id -> name-words}, []}]|\n",
            "+-------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(\"entity\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cv8dFGja9x9k",
        "outputId": "b388e122-6fa8-4818-c307-3be5352f1230"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----+\n",
            "|keyword|label|\n",
            "+-------+-----+\n",
            "|Patrick| NAME|\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(F.explode(F.arrays_zip('entity.result', 'entity.metadata')).alias('col'))\\\n",
        "      .select(F.expr(\"col['0']\").alias(\"keyword\"),\n",
        "              F.expr(\"col['1']['entity']\").alias(\"label\")).show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tjkwd2PG97Se"
      },
      "source": [
        "As you see, we could only matched \"Patrick\". "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "x8B3SsxX-CS3"
      },
      "source": [
        "**`setSentenceMatch(True)`**: Now, we expect a match for the \"New York City\" since we will check for the sentence level match. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "W86qjhqL92DB"
      },
      "outputs": [],
      "source": [
        "regex_entity_ruler = EntityRulerApproach() \\\n",
        "    .setInputCols([\"sentence\", \"token\"]) \\\n",
        "    .setOutputCol(\"entity\") \\\n",
        "    .setPatternsResource(\"patterns.json\") \\\n",
        "    .setSentenceMatch(True)\n",
        "\n",
        "pipeline = Pipeline(stages=[documenter, \n",
        "                            sentenceDetector, \n",
        "                            tokenizer,\n",
        "                            regex_entity_ruler])\n",
        "\n",
        "pipeline_model = pipeline.fit(data)\n",
        "result= pipeline_model.transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbwfJeCc6sJW",
        "outputId": "85369fe8-9a2c-4815-cb67-d11fef1ea5fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|entity                                                                                                                                                                         |\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|[{chunk, 17, 29, New York City, {entity -> LOCATION, id -> locations-words, sentence -> 0}, []}, {chunk, 0, 6, Patrick, {entity -> NAME, sentence -> 0, id -> name-words}, []}]|\n",
            "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(\"entity\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lQcI-vp3k2q",
        "outputId": "7dab649b-f95a-420c-c189-110eabece616"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------+--------+\n",
            "|      keyword|   label|\n",
            "+-------------+--------+\n",
            "|New York City|LOCATION|\n",
            "|      Patrick|    NAME|\n",
            "+-------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(F.explode(F.arrays_zip('entity.result', 'entity.metadata')).alias('col'))\\\n",
        "      .select(F.expr(\"col['0']\").alias(\"keyword\"),\n",
        "              F.expr(\"col['1']['entity']\").alias(\"label\")).show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "r-CV5KBB-Rwg"
      },
      "source": [
        "As seen above, \"New York City\" was matched."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EI8CDiQZ-M23"
      },
      "source": [
        "### `setAlphabetResource`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ9-iE8jVhEQ"
      },
      "source": [
        "Since Spark NLP version 4.2.0, we reduce significantly the latency of Entity Ruler by implementing Aho-Corasick algorithm. This requires defining an alphabet for some cases. For English documents, you won't need to define it because under the hood Entity Ruler annotator uses an English alphabet by default. However, for special use cases we will need to proceed like the example below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FvWjgsA-Mt7",
        "outputId": "3c0c8c5c-b267-4a27-b863-709362a9c20d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------------------+\n",
            "|text                           |\n",
            "+-------------------------------+\n",
            "|Elendil used to live in N√∫menor|\n",
            "+-------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = spark.createDataFrame([[\"Elendil used to live in N√∫menor\"]]).toDF(\"text\")\n",
        "data.show(truncate=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tRCWGJrbVpQQ"
      },
      "source": [
        "The text above has a special character, an accent in vowel u (√∫)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Cu8LYfs0-Mmt"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "locations = [\n",
        "              {\n",
        "                \"id\": \"locations\",\n",
        "                \"label\": \"LOCATION\",\n",
        "                \"patterns\": [\"N√∫menor\", \"Middle-earth\"]\n",
        "              }\n",
        "            ]\n",
        "\n",
        "with open('locations.json', 'w') as jsonlfile:\n",
        "  json.dump(locations, jsonlfile)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-Bc7zwGTV_ja"
      },
      "source": [
        "In addition, a pattern in `locations.json` file has also hyphen punctuation mark (-). So, we need to define our custom alphabet to use Entity Ruler for Tolkien's books. Here, we will define just the 2 special characters for our text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "yjkWiwIMWFUA"
      },
      "outputs": [],
      "source": [
        "alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "\n",
        "with open('custom_alphabet.txt', 'w') as alphabet_file:\n",
        "    alphabet_file.write(alphabet + \"\\n\")\n",
        "    alphabet_file.write(alphabet.upper() + \"\\n\")\n",
        "    alphabet_file.write(\"√∫\")\n",
        "    alphabet_file.write(\"-\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm5Z-03sWFMU",
        "outputId": "1aae0200-db44-4d3e-eeb9-567f6df98b45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "abcdefghijklmnopqrstuvwxyz\n",
            "ABCDEFGHIJKLMNOPQRSTUVWXYZ\n",
            "√∫-"
          ]
        }
      ],
      "source": [
        "!cat custom_alphabet.txt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "lEyilccnWN6H"
      },
      "source": [
        "Now, we will build `EntityRulerApproach()` with that alphabet. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "09ar_fykV-x6"
      },
      "outputs": [],
      "source": [
        "entity_ruler = EntityRulerApproach() \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"entity\") \\\n",
        "    .setPatternsResource(\"locations.json\") \\\n",
        "    .setAlphabetResource(\"/content/custom_alphabet.txt\")\n",
        "\n",
        "pipeline = Pipeline(stages=[documenter, \n",
        "                            sentenceDetector, \n",
        "                            entity_ruler])\n",
        "\n",
        "pipeline_model = pipeline.fit(data)\n",
        "result= pipeline_model.transform(data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CtLVqwwSWxW9"
      },
      "source": [
        "Checking the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hm_PdbsWl9O",
        "outputId": "138f99d0-2049-4dec-eca2-e9d2aefe4c25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------------------------+\n",
            "|entity                                                                              |\n",
            "+------------------------------------------------------------------------------------+\n",
            "|[{chunk, 24, 30, N√∫menor, {entity -> LOCATION, sentence -> 0, id -> locations}, []}]|\n",
            "+------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(\"entity\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN0mnYPUWtBl",
        "outputId": "33c62218-dd56-4e96-cae8-a44dac212116"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------+\n",
            "|keyword|   label|\n",
            "+-------+--------+\n",
            "|N√∫menor|LOCATION|\n",
            "+-------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(F.explode(F.arrays_zip('entity.result', 'entity.metadata')).alias('col'))\\\n",
        "      .select(F.expr(\"col['0']\").alias(\"keyword\"),\n",
        "              F.expr(\"col['1']['entity']\").alias(\"label\")).show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wG7qu_fVWzAI"
      },
      "source": [
        "As seen above, we successfully matched \"N√∫menor\" to LOCATION by defining an alphabet. <br/>\n",
        "\n",
        "If you don't define the required alphabet, you will get this error:\n",
        "\n",
        "```python\n",
        "Py4JJavaError: An error occurred while calling o69.fit.\n",
        ": java.lang.UnsupportedOperationException: Char √∫ not found on alphabet. Please check alphabet\n",
        "```\n",
        "\n",
        "So, the alphabet must have all the characters that can be found in your document.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3Rclttj5XQah"
      },
      "source": [
        "#### Non-English Languages"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AE6-Ln2nXXWN"
      },
      "source": [
        "`EntityRulerApproach` has some predefined alphabets for the most common languages: English, Spanish, French, and German. For example, if you have documents in Spanish, you just need to set an alphabet like the example below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UN-10cUiWs7k",
        "outputId": "d805d085-3361-420d-d224-62c242d7ddb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------+\n",
            "|text                          |\n",
            "+------------------------------+\n",
            "|Elendil sol√≠a vivir en N√∫menor|\n",
            "+------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = spark.createDataFrame([[\"Elendil sol√≠a vivir en N√∫menor\"]]).toDF(\"text\")\n",
        "data.show(truncate=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OBpiKfscXubT"
      },
      "source": [
        "We will define the paramater as `setAlphabetResource(\"spanish\")`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ARwsalKjXoHi"
      },
      "outputs": [],
      "source": [
        "entity_ruler = EntityRulerApproach() \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"entity\") \\\n",
        "    .setPatternsResource(\"locations.json\") \\\n",
        "    .setAlphabetResource(\"spanish\")\n",
        "\n",
        "pipeline = Pipeline(stages=[documenter, \n",
        "                            sentenceDetector, \n",
        "                            entity_ruler])\n",
        "\n",
        "pipeline_model = pipeline.fit(data)\n",
        "result= pipeline_model.transform(data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNgpkUzoX9Qe"
      },
      "source": [
        "Checking the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-akCBskX28Q",
        "outputId": "05b6c11b-3522-4288-db84-b27633766730"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------------------------------------------------------------------------+\n",
            "|entity                                                                              |\n",
            "+------------------------------------------------------------------------------------+\n",
            "|[{chunk, 23, 29, N√∫menor, {entity -> LOCATION, sentence -> 0, id -> locations}, []}]|\n",
            "+------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(\"entity\").show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8636ijNX223",
        "outputId": "680049ed-17b8-48ea-eaed-a42463738cd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+--------+\n",
            "|keyword|   label|\n",
            "+-------+--------+\n",
            "|N√∫menor|LOCATION|\n",
            "+-------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(F.explode(F.arrays_zip('entity.result', 'entity.metadata')).alias('col'))\\\n",
        "      .select(F.expr(\"col['0']\").alias(\"keyword\"),\n",
        "              F.expr(\"col['1']['entity']\").alias(\"label\")).show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JO8uPT7EX_Yu"
      },
      "source": [
        "As seen above, we successfully matched a word from a Spanish text.  "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "smxQx7eZYaRb"
      },
      "source": [
        "If your language is not a predefined alphabet, you will need to define all the characters of your alphabet, as shown in the first example. Keep in mind that an alphabet may require not only letters but also numbers, punctuation marks, and symbol characters."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "uin0PKeXvFcF"
      },
      "source": [
        "### `setCaseSensitive`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9-_NU7UGvFcN"
      },
      "source": [
        "This parameter is used to set whether to ignore case in index lookups. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fzLQFUKjvFcN"
      },
      "source": [
        "Setting example keywords as lowercased:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3w56yCi8vFcO",
        "outputId": "ada292d7-1c48-424a-e6c5-cccdffadf3e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------+\n",
            "|text                                                                         |\n",
            "+-----------------------------------------------------------------------------+\n",
            "|Lord eddard stark was the head of house stark. John snow lives in winterfell.|\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = spark.createDataFrame([[\"Lord eddard stark was the head of house stark. John snow lives in winterfell.\"]]).toDF(\"text\")\n",
        "data.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "jEbHi_jeviMJ"
      },
      "outputs": [],
      "source": [
        "#Defining the source JSON file and saving\n",
        "import json\n",
        "\n",
        "keywords = [\n",
        "          {\n",
        "            \"label\": \"PERSON\",\n",
        "            \"patterns\": [\"Jon\", \"John\", \"John Snow\", \"Jon Snow\"]\n",
        "          },\n",
        "          {\n",
        "            \"label\": \"PERSON\",\n",
        "            \"patterns\": [\"Eddard\", \"Eddard Stark\"]\n",
        "          },\n",
        "          {\n",
        "            \"label\": \"LOCATION\",\n",
        "            \"patterns\": [\"Winterfell\"]\n",
        "          },\n",
        "         ]\n",
        "\n",
        "with open('keywords.json', 'w') as jsonfile:\n",
        "    json.dump(keywords, jsonfile)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Smc55EFovFcQ"
      },
      "source": [
        "**`setCaseSensitive(True)`**: we expect no matching since there is no case match between the entities in the given sentence and the source JSON file. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "9W54UpUGvFcQ"
      },
      "outputs": [],
      "source": [
        "entity_ruler = EntityRulerApproach() \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"entity\") \\\n",
        "    .setPatternsResource(\"keywords.json\") \\\n",
        "    .setCaseSensitive(True)\n",
        "\n",
        "pipeline = Pipeline(stages=[documenter, \n",
        "                            sentenceDetector, \n",
        "                            entity_ruler])\n",
        "\n",
        "pipeline_model = pipeline.fit(data)\n",
        "result= pipeline_model.transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tKaXE4cvFcR",
        "outputId": "a00a9a3d-7a92-40b7-a1d6-07ed7a1e5273"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+-----+\n",
            "|keyword|label|\n",
            "+-------+-----+\n",
            "+-------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(F.explode(F.arrays_zip('entity.result', 'entity.metadata')).alias('col'))\\\n",
        "      .select(F.expr(\"col['0']\").alias(\"keyword\"),\n",
        "              F.expr(\"col['1']['entity']\").alias(\"label\")).show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8vyA_MG8vFcR"
      },
      "source": [
        "**`setCaseSensitive(False)`**: we expect matching even though there is no case match between the entities in the given sentence and the source JSON file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "5OcYMeTwvFcS"
      },
      "outputs": [],
      "source": [
        "entity_ruler = EntityRulerApproach() \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"entity\") \\\n",
        "    .setPatternsResource(\"keywords.json\") \\\n",
        "    .setCaseSensitive(False)\n",
        "\n",
        "pipeline = Pipeline(stages=[documenter, \n",
        "                            sentenceDetector, \n",
        "                            entity_ruler])\n",
        "\n",
        "pipeline_model = pipeline.fit(data)\n",
        "result= pipeline_model.transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puqz-WfRvFcS",
        "outputId": "82a7479d-353d-4672-9bbb-35c32a5f0c59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+--------+\n",
            "|     keyword|   label|\n",
            "+------------+--------+\n",
            "|eddard stark|  PERSON|\n",
            "|   John snow|  PERSON|\n",
            "|  winterfell|LOCATION|\n",
            "+------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(F.explode(F.arrays_zip('entity.result', 'entity.metadata')).alias('col'))\\\n",
        "      .select(F.expr(\"col['0']\").alias(\"keyword\"),\n",
        "              F.expr(\"col['1']['entity']\").alias(\"label\")).show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nMCD5VIhYxwh"
      },
      "source": [
        "### `setUseStorage`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bmtePf8QY_Wf"
      },
      "source": [
        "If this parameter is kept as False(default), the annotator will serialize patterns file data with SparkML parameters when saving the model. <br/>\n",
        "\n",
        "We recommend using the default value `setUseStorage(False)` since the results of our benchmarks reflect that this configuration is faster than `setUseStorage(True)`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BMZ8eyXR1ELB"
      },
      "source": [
        "# `EntityRulerModel`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4-50Tixl2WLt"
      },
      "source": [
        "This annotator is instantiated model of the `EntityRulerApproach`. Once you build an `EntityRulerApproach()`, you can save it and use it with `EntityRulerModel()` via `load()` function. <br/>\n",
        "\n",
        "Let's re-build one of examples that we have done before and save it. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDZqgtLu1MsY",
        "outputId": "c2914789-c957-4144-b6a9-6102d961cc29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------------------------+\n",
            "|text                                                                         |\n",
            "+-----------------------------------------------------------------------------+\n",
            "|Lord Eddard Stark was the head of House Stark. John Snow lives in Winterfell.|\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "data = spark.createDataFrame([[\"Lord Eddard Stark was the head of House Stark. John Snow lives in Winterfell.\"]]).toDF(\"text\")\n",
        "data.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "4zd_7q134dCv"
      },
      "outputs": [],
      "source": [
        "#Defining the source JSON file and saving\n",
        "import json\n",
        "\n",
        "keywords = [\n",
        "          {\n",
        "            \"label\": \"PERSON\",\n",
        "            \"patterns\": [\"Jon\", \"John\", \"John Snow\", \"Jon Snow\"]\n",
        "          },\n",
        "          {\n",
        "            \"label\": \"PERSON\",\n",
        "            \"patterns\": [\"Eddard\", \"Eddard Stark\"]\n",
        "          },\n",
        "          {\n",
        "            \"label\": \"LOCATION\",\n",
        "            \"patterns\": [\"Winterfell\"]\n",
        "          },\n",
        "         ]\n",
        "\n",
        "with open('keywords.json', 'w') as jsonfile:\n",
        "    json.dump(keywords, jsonfile)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "qaBRAtop0zyl"
      },
      "outputs": [],
      "source": [
        "entity_ruler = EntityRulerApproach() \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"entity\") \\\n",
        "    .setPatternsResource(\"keywords.json\") \n",
        "    \n",
        "pipeline = Pipeline(stages=[documenter, \n",
        "                            sentenceDetector, \n",
        "                            entity_ruler])\n",
        "\n",
        "pipeline_model = pipeline.fit(data)\n",
        "result= pipeline_model.transform(data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kRjiiW_I2_m9"
      },
      "source": [
        "Saving the approach to disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "FaGeCIj5YVYC"
      },
      "outputs": [],
      "source": [
        "pipeline_model.stages[2].write().overwrite().save('models/ruler_approach_model')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2I7E0wkV3C5u"
      },
      "source": [
        "Loading the saved model and using it with the `EntityRulerModel()` via `load`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "RHdhw2t-6pmL"
      },
      "outputs": [],
      "source": [
        "entity_ruler = EntityRulerModel.load('/content/models/ruler_approach_model') \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"entity\") \n",
        "\n",
        "pipeline = Pipeline(stages=[documenter, \n",
        "                            sentenceDetector, \n",
        "                            entity_ruler])\n",
        "\n",
        "pipeline_model = pipeline.fit(data)\n",
        "result= pipeline_model.transform(data)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kT6h7Hoy3Qkw"
      },
      "source": [
        "Checking the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_Qr9g_y1uaN",
        "outputId": "b9425469-3096-4c99-8fd5-cd6358d8f0b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+--------+\n",
            "|     keyword|   label|\n",
            "+------------+--------+\n",
            "|Eddard Stark|  PERSON|\n",
            "|   John Snow|  PERSON|\n",
            "|  Winterfell|LOCATION|\n",
            "+------------+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(F.explode(F.arrays_zip('entity.result', 'entity.metadata')).alias('col'))\\\n",
        "      .select(F.expr(\"col['0']\").alias(\"keyword\"),\n",
        "              F.expr(\"col['1']['entity']\").alias(\"label\")).show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "D9nSEExz3STL"
      },
      "source": [
        "As seen above, we built an `EntityRuler`, saved it and used the saved model with `EntityRulerModel`. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
