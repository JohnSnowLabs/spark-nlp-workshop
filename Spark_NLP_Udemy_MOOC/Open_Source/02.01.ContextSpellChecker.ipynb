{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PJbcjC-samnF"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/Spark_NLP_Udemy_MOOC/Open_Source/02.01.ContextSpellChecker.ipynb)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "90XYFSVbblyp"
      },
      "source": [
        "# ContextSpellChecker"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "5BRq-xE4blyp"
      },
      "source": [
        "This notebook will cover the different parameters and usages of `ContextSpellChecker`.\n",
        "\n",
        "**üìñ Learning Objectives:**\n",
        "\n",
        "1. Be able to detect and fix spelling errors in text.\n",
        "\n",
        "2. Understand how to use the `ContextSpellChecker` annotator.\n",
        "\n",
        "3. Become comfortable using the different parameters of the annotator.\n",
        "\n",
        "4. Be able to train a new spell checker model.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3d7jx_8Ubq6C"
      },
      "source": [
        "**üîó Helpful Links:**\n",
        "\n",
        "- Documentation : [ContextSpellChecker](https://nlp.johnsnowlabs.com/docs/en/annotators#contextspellchecker)\n",
        "\n",
        "- Python Docs : [ContextSpellChecker](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/spell_check/context_spell_checker/index.html#sparknlp.annotator.spell_check.context_spell_checker.ContextSpellCheckerModel)\n",
        "\n",
        "- Scala Docs : [ContextSpellChecker](https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/spell/context/ContextSpellCheckerModel.html)\n",
        "\n",
        "- For extended examples of usage, see the [Spark NLP Workshop repository](https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/tutorials/Certification_Trainings/Public/)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "S0ERC5daa22T"
      },
      "source": [
        "## **üìú Background**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fJydwdDsa5Ky"
      },
      "source": [
        "Text data originated from social media or extracted from images using Optical Character Recognition (OCR) usually contains typos, mispellings, spurious symbols, or errors that can impact machine learning models trained on this data. For example, if the word `John` is present in the data both with the correct spelling and `J0hn` (a zero character replaced the o letter), then a model would treat them as two separated words, which could cause unexpected outcomes in its predictions. \n",
        "\n",
        "**Spell Checking** is a very important task in NLP pipelines that can help to fix those kind of errors in the data. Being able to rely on correct data, without spelling problems, reduces vocabulary sizes at different stages in the pipeline, and improves the performance of all the models in the pipeline. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vJ658wIMbA5k"
      },
      "source": [
        "### ü§ì More details"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "c5s2vHRMbBju"
      },
      "source": [
        "Spell Checkers can recommend corrections on three levels: \n",
        "\n",
        "- _Subword_: relative cost of different correction candidates according to the edit operations at the character level it requires.\n",
        "- _Word_: different correction candidates for each word.\n",
        "- _Sentence_: surrounding text of each word, i.e., it‚Äôs context.\n",
        "\n",
        "In the Spark NLP ecosystem, we implemented three types of spell checkers:\n",
        "\n",
        "- [NorvigSweeting](https://nlp.johnsnowlabs.com/docs/en/annotators#norvigsweeting-spellchecker): Retrieves tokens and makes corrections automatically if not found in an English dictionary.\n",
        "- [SymmetricDeleter](https://nlp.johnsnowlabs.com/docs/en/annotators#symmetricdelete-spellchecker): Symmetric Delete spelling correction algorithm.\n",
        "- [ContextSpellChecker](https://nlp.johnsnowlabs.com/docs/en/annotators#contextspellchecker) annotator uses contextual information to both detect errors and produce the best corrections. \n",
        "\n",
        "The first two annotators don't take into account the context around the words, while the last one is a deep-learning model that does consider a window of a few words around them. Taking the context into account can help to determine a correct word even if more than one possible candidates are feasible (they are present in the dictionary). Let's illusterate it with the word `siter`. This word is not part of the English dictionary, so we can check which could be the intended word by making only one change of letter in it:\n",
        "\n",
        "- **sister**, by adding one `s` to the word\n",
        "- **site**, by removing the `r`\n",
        "- **sites**, by replacing `r` by `s`\n",
        "\n",
        "All of these three words exists in the English dictionary, and which one to choose will depend on the context. for example, which one should we use in the sentence \"I will call my siter.\"? By adding the context, the answer is clear.\n",
        "\n",
        "In this notebook, we will focus on the `ContextSpellChecker` annotator, how to train a custom model with the `ContextSpellCheckerApproach` annotator and how to use pretrained models with the `ContextSpellCheckerModel` annotator. Both annotators apply have `TOKEN` type column for input and output.\n",
        "\n",
        "The model uses a deep learning approach to obtain context information, and [Viterbi decoder](https://en.wikipedia.org/wiki/Viterbi_decoder) applied to a [Trellis modulation](https://en.wikipedia.org/wiki/Trellis_modulation) of the candidate words to decide which word to suggest (the one with lowest cost - given by the [Damerau‚ÄìLevenshtein distance](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)). This distance considers three valid operations, each with cost equal to one:\n",
        "  - Add one letter\n",
        "  - Delete one letter\n",
        "  - Replace one letter for another one\n",
        "  - Swaps two adjacent letters"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "H5xLVHWGbKWU"
      },
      "source": [
        "To determine the correction candidates, the model uses two sources:\n",
        "\n",
        "- vocabulary built from the training corpus during model training that remains immutable\n",
        "- special classes for dealing with special types of words like numbers or dates. These are configurable, and you can modify them so they can adjust better to your data. Usual classes are:\n",
        "  - `_AGE_`: age tokens like ‚Äò21-year-old‚Äô.\n",
        "  - `_LOC_`: tokens representing locations like a city, state, country, etc.\n",
        "  - `_DATE_`: tokens representing dates like ‚ÄòJan-03'.\n",
        "  - `_NAME_`: tokens representing names and surnames.\n",
        "  - `_NUM_`: tokens representing numbers, like 22 or twenty-two.\n",
        "\n",
        " These classes can be extended in two ways on the `ContextSpellChekcerModel` annotator:\n",
        "  - Using custom vocabularies and setting with the method `.updateVocabClass(label, vocabulary, append=True)`, by passing the label, a list of words and a bool if the vocabulary should be appended to existing vocabularies or replace them. For example:\n",
        "  ```python\n",
        "  spellModel.updateVocabClass('_NAME_', ['Monika', 'Agnieszka', 'Inga', 'Jowita', 'Melania'], True)\n",
        "  ```\n",
        "  - Using regex patterns and setting with the method `.updateRegexClass(label, regex_pattern)`, by passing the label and the regex pattern to use. This always substitutes the pattern if it already existed. For example:\n",
        "  ```python\n",
        "  spellModel.updateRegexClass('_DATE_', '(january|february|march)-[0-31]')\n",
        "  ```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4aqI2eoZbM1G"
      },
      "source": [
        "## **üé¨ Colab Setup**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "B61TLXIWbOpS"
      },
      "source": [
        "Before going through the annotators, let's set up the environment and start a `spark` session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC4nWgnbblyr",
        "outputId": "8e5017c7-decb-4a56-d94a-fa8d8d64536b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m473.2/473.2 KB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU pyspark  spark-nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAOeQqzXblyr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import array_contains\n",
        "\n",
        "import sparknlp\n",
        "from sparknlp.annotator import (\n",
        "    Tokenizer,\n",
        "    ContextSpellCheckerModel,\n",
        "    ContextSpellCheckerApproach,\n",
        "    SentenceDetector,\n",
        "    NorvigSweetingModel,\n",
        "    SymmetricDeleteModel,\n",
        ")\n",
        "from sparknlp.common import RegexRule\n",
        "from sparknlp.base import DocumentAssembler, LightPipeline\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2gPrL2jUbXvL"
      },
      "source": [
        "Starting the spark session:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "id": "7jv6xvuMbXHw",
        "outputId": "b257d0fd-4144-46f6-a817-2950369240ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark NLP version 4.3.2\n",
            "Apache Spark version: 3.3.2\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e55096ef1002:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.3.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f9b4e48c6d0>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark = sparknlp.start()\n",
        "\n",
        "print(\"Spark NLP version\", sparknlp.version())\n",
        "print(\"Apache Spark version:\", spark.version)\n",
        "\n",
        "spark"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xmgxwrSCcF6o"
      },
      "source": [
        "## **üñ®Ô∏è Input/Output Annotation Types**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EM7O2BDycHgA"
      },
      "source": [
        "- Input: `TOKEN`\n",
        "\n",
        "- Output: `TOKEN`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xmxq8Xi_cJFe"
      },
      "source": [
        "## **üîé Parameters**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IqpCn0BPcK5Q"
      },
      "source": [
        "- **wordMaxDistance**: Integer number representing the maximum edit distance for the generated candidates for every word. Higher values increases the number of candidates and can make the algorithm slower, but small values can cause the algorithm to miss the correction. Default value: `3`, minimum od `1`. \n",
        "- **maxCandidates**: An integer number representing the maximum candidates for every word. This limits the returned list of candidates if they are too many. Default value: `6`\n",
        "- **caseStrategy**: What case combinations to try when generating candidates. Possible choices are:\n",
        "  - 0: use only upper case letters\n",
        "  - 1: First letter is upper, the others are lower case\n",
        "  - 2 (default): uses all letters\n",
        "- **errorThreshold**: Threshold perplexity for a word to be considered as an error. No default value.\n",
        "- **tradeoff**: Tradeoff between the cost of a word error and a transition in the language model.  Default value: `18.0`\n",
        "- **maxWindowLen**: Maximum size for the window used to remember history prior to every correction. Default value: `5`\n",
        "- **configProtoBytes**: ConfigProto from tensorflow, serialized into byte array (see [Tensorflow JVM documentation](https://www.tensorflow.org/jvm/api_docs/java/org/tensorflow/proto/framework/ConfigProto) for details). No default value.\n",
        "- **compareLowcase**: If `True`, will compare tokens with vocabulary tokens all in lowercase (match even if the case is different).  Default value: `False`\n",
        "- **useNewLines**: When set to `True`, new line char `\\n` will be treated as any other character. When set to `False` correction is applied on paragraphs as defined by newline characters. Default value: `False`\n",
        "- **gamma**: Controls the influence of individual word frequency in the decision.  Default value: `120.0`\n",
        "- **vocabFreq**: Frequency words from the vocabulary. Constructed during training, can be overwritten. No default value\n",
        "- **idsVocab**: Mapping of ids to vocabulary.  Constructed during training, can be overwritten. No default value\n",
        "- **vocabIds**: Mapping of vocabulary to ids. Constructed during training, can be overwritten. No default value\n",
        "- **classes**: Classes the spell checker recognizes. Constructed when training, can be overwritten. No default value. Tehre are two different types: **vocabulary** based and **regex** based:\n",
        "  * Vocabulary based classes can propose correction candidates from the provided vocabulary, e.g. a dictionary of names.\n",
        "  * Regex classes are defined by a regular expression, and they can be used to generate correction candidates for items like numbers, dates, etc.\n",
        "- **weights**: Levenshtein weights. Constructed during training, can be overwritten. No default value"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cDLILHrc_dKj"
      },
      "source": [
        "### ‚úå Using pretrained Spellchecking Model\n",
        "\n",
        "We use the `ContextSpellCheckerModel` annotator to load a pretrained model and analyze this sentence:\n",
        "\n",
        "> \"**Plaese** **alliow** me **tao** **introdduce** **myhelf**, I am a man of **waelht** **und** **tiaste**\"\n",
        "\n",
        "The pretrained model is the [spellcheck_dl](https://nlp.johnsnowlabs.com/2022/04/02/spellcheck_dl_en_2_4.html), which is a generic context-aware spell cheker model for English language.\n",
        "\n",
        "To check the list of available pretrained models, visit [NLP Models Hub](https://nlp.johnsnowlabs.com/models?task=Spell+Check)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laUdYcblcczE"
      },
      "outputs": [],
      "source": [
        "example_sentence = \"Plaese alliow me tao introdduce myhelf, I am a man of waelht und tiaste\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC3cekia2XLd"
      },
      "outputs": [],
      "source": [
        "def get_light_pipeline(spellModel):\n",
        "  documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        "\n",
        "  tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
        "  pipeline = Pipeline(stages=[documentAssembler, tokenizer, spellModel])\n",
        "\n",
        "  empty_ds = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "  lp = LightPipeline(pipeline.fit(empty_ds))\n",
        "  return lp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCsYm-QOcpMK",
        "outputId": "138cde0c-34f9-4e82-e1ae-54dbf7db6972"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spellcheck_dl download started this may take some time.\n",
            "Approximate size to download 95.1 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
        "\n",
        "spellModel = (\n",
        "    ContextSpellCheckerModel.pretrained(\"spellcheck_dl\")\n",
        "    .setInputCols(\"token\")\n",
        "    .setOutputCol(\"checked\")\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(stages=[documentAssembler, tokenizer, spellModel])\n",
        "\n",
        "empty_ds = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "lp = LightPipeline(pipeline.fit(empty_ds))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4NZG-4Blc5Ul"
      },
      "source": [
        "This pretrained model has 4 speciall casses, two based on vocabularies (`VocabParser`) and two based on regex rules (`RegexParser`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-TiXZbPc7DG",
        "outputId": "43c06bac-2043-403f-d37f-0f2a9a69eb88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['(_NAME_,VocabParser)',\n",
              " '(_DATE_,RegexParser)',\n",
              " '(_NUM_,RegexParser)',\n",
              " '(_LOC_,VocabParser)']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spellModel.getWordClasses()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e4Fdo37odNNT"
      },
      "source": [
        "Let's check the results obtained by the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95UqBfAQdL8D",
        "outputId": "f810452d-a560-4512-d872-87a5c88beab3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plaese => Please\n",
            "alliow => allow\n",
            "me => me\n",
            "tao => to\n",
            "introdduce => introduce\n",
            "myhelf => myself\n",
            ", => ,\n",
            "I => I\n",
            "am => am\n",
            "a => a\n",
            "man => man\n",
            "of => of\n",
            "waelht => wealth\n",
            "und => and\n",
            "tiaste => taste\n"
          ]
        }
      ],
      "source": [
        "result = lp.annotate(example_sentence)\n",
        "\n",
        "for token, checked in zip(result[\"token\"], result[\"checked\"]):\n",
        "  print(f\"{token} => {checked}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "avrGqPYSdWT6"
      },
      "source": [
        "This pretrained model was able to fix all the mistakes without changing the parameters default values. Let's check what parameters are available. What happens when we change some of the parameters?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEGL_Y4Dddh1",
        "outputId": "b6e956c9-cfa1-4d78-a11b-489497bebafd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spellcheck_dl download started this may take some time.\n",
            "Approximate size to download 95.1 MB\n",
            "[OK!]\n",
            "Plaese => Please\n",
            "alliow => allow\n",
            "me => me\n",
            "tao => tao\n",
            "introdduce => introduce\n",
            "myhelf => myself\n",
            ", => ,\n",
            "I => I\n",
            "am => am\n",
            "a => a\n",
            "man => man\n",
            "of => of\n",
            "waelht => waelht\n",
            "und => und\n",
            "tiaste => taste\n"
          ]
        }
      ],
      "source": [
        "spellModel_modified = (\n",
        "    ContextSpellCheckerModel.pretrained(\"spellcheck_dl\")\n",
        "    .setInputCols(\"token\")\n",
        "    .setOutputCol(\"checked\")\n",
        "    .setWordMaxDistance(1)\n",
        ")\n",
        "\n",
        "lp = get_light_pipeline(spellModel_modified)\n",
        "result = lp.annotate(\"Plaese alliow me tao introdduce myhelf, I am a man of waelht und tiaste\")\n",
        "\n",
        "for token, checked in zip(result[\"token\"], result[\"checked\"]):\n",
        "  print(f\"{token} => {checked}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "FeUi-EHDdWR9"
      },
      "source": [
        "We can see that some words were not fixed. It happened because the model could not find a proper candidate with the changed parameter. We encourge you to play with the parameters to obtain the desired result. \n",
        "\n",
        "Final observations:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YW5yxzbydWPq"
      },
      "source": [
        "1. The parameter `tradeoff` acts as a penalizer to the relevance the word has if it is present in the dictionary. In other words, with smaller values of `tradeoff`, the model will opt to change a word that exists in the dictionary with a candidate one (give more weight to context, less to the vocabulary)\n",
        "2. The parameter `errorThreshold` acts as a filter to determine if the word should be corrected or not. Setting the threshold to lower values can increase theaccuracy of the model, but decreases the prediction speed. For reference, see the benchmark table below:\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "| threshold | total_time | fscore |\n",
        "|-----------|------------|--------|\n",
        "| 8f        | 405s       | 52.69  |\n",
        "| 10f       | 357s       | 52.43  |\n",
        "| 12f       | 279s       | 52.25  |\n",
        "| 14f       | 234s       | 52.14  |\n",
        "\n",
        "**Sometimes, the prediction speed can get a relevant boost with a small impact on accuracy.**\n",
        "</div>\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "YqJ2QjEwd5sx"
      },
      "source": [
        "## ‚ö° **Training a context-aware spell checker**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "z2sj_F-KdWNO"
      },
      "source": [
        "To train a new model, we need to use the `ContextSpellCheckerApproach` annotator.\n",
        "\n",
        "The parameters of the class are:\n",
        "\n",
        "- **wordMaxDistance**: Integer number representing the maximum edit distance for the generated candidates for every word. Higher values increases the number of candidates and can make the algorithm slower, but small values can cause the algorithm to miss the correction. Default value: `3`, minimum of `1`. \n",
        "- **maxCandidates**: An integer number representing the maximum candidates for every word. This limits the returned list of candidates if they are too many. Default value: `6`\n",
        "- **caseStrategy**: What case combinations to try when generating candidates. Possible choices are:\n",
        "  - 0: use only upper case letters\n",
        "  - 1: First letter is upper, the others are lower case\n",
        "  - 2 (default): uses all letters\n",
        "- **errorThreshold**: Threshold perplexity for a word to be considered as an error. No default value.\n",
        "- **tradeoff**: Tradeoff between the cost of a word error and a transition in the language model.  Default value: `18.0`\n",
        "- **maxWindowLen**: Maximum size for the window used to remember history prior to every correction. Default value: `5`\n",
        "- **configProtoBytes**: ConfigProto from tensorflow, serialized into byte array (see [Tensorflow JVM documentation](https://www.tensorflow.org/jvm/api_docs/java/org/tensorflow/proto/framework/ConfigProto) for details). No default value.\n",
        "- **languageModelClasses**: Number of classes to use during factorization of the softmax output in the language model. No default value, depends on the vocabulary (learned during training). Can be overwritten.\n",
        "- **epochs**: Number of epochs to train the language model. Default value: `2`\n",
        "- **batchSize**: Batch size for the training phase. Default value: `24`\n",
        "- **initialRate**: Initial learning rate for the training phase. Default value: `0.7`\n",
        "- **finalRate**: Final learning rate. Default value: `0.0005`\n",
        "- **validationFraction**: Percentage of datapoints to use for validation. Default value: `0.1`\n",
        "- **minCount**: Minimum number of times a token should appear to be included in vocabulary. Default value: `3.0`\n",
        "- **compoundCount**: Minimum number of times a compound word should appear to be included in vocabulary. Default value: `5`\n",
        "- **classCount**: Minimum number of times the word need to appear in corpus to not be considered of a special class. Default value: `15.0`\n",
        "- **maxSentLen**: Maximum length for a sentence - internal use during training. Default value: `250`\n",
        "- **graphFolder**: Folder path that contain external graph files. No default value"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "X-0MCS-2dWK5"
      },
      "source": [
        "The training data for the `ContextSpellCheckerApproach` annotator is just a collection of texts. We don't need labeled data to train this model as it uses unsupervised training to generate a language model. As usual, bigger corpus will obtain better models when the data is a good sample of the real-world scenario you plan to use the model for.\n",
        "\n",
        "We will use the Arthur Conan Doyle's first book of Sherlok Holmes as sample data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-LJuo2oeYiY"
      },
      "outputs": [],
      "source": [
        "# Download the book\n",
        "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/holmes.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdUoJ0BueaY4",
        "outputId": "b1ba1286-c7f0-42a4-851a-2c1cb7f075d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------------------------------+\n",
            "|                                                                                                text|\n",
            "+----------------------------------------------------------------------------------------------------+\n",
            "|THE ADVENTURES OF SHERLOCK HOLMESArthur Conan Doyle Table of contents A Scandal in Bohemia The Re...|\n",
            "+----------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "path = \"holmes.txt\"\n",
        "\n",
        "corpus = spark.read.text(path).toDF(\"text\")\n",
        "corpus.show(truncate=100)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "JKdi3yg6dWIK"
      },
      "source": [
        "We will transform the text into our Spark NLP basic structure: `document`, and then we will split it into tokens with the `Tokenizer` annotator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trjcuaRfefT1"
      },
      "outputs": [],
      "source": [
        "documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        "\n",
        "# Splits sentences into tokens\n",
        "tokenizer = Tokenizer().setInputCols(\"document\").setOutputCol(\"token\")\n",
        "\n",
        "spellChecker = (\n",
        "    ContextSpellCheckerApproach()\n",
        "    .setInputCols(\"token\")\n",
        "    .setOutputCol(\"checked\")\n",
        "    .setBatchSize(1) # Batch size 1 to run in Colab\n",
        "    .setEpochs(1)\n",
        "    .setWordMaxDistance(3) # Maximum edit distance to consider\n",
        "    .setMaxWindowLen(3) # important to find context\n",
        "    .setMinCount(3.0) # Removes words that appear less than that from the vocabulary\n",
        "    .setCompoundCount(5) # Removes compound words that appear less than that from the vocabulary\n",
        "    .setClassCount(10.0) # Minimun occurrences of a class\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(stages=[documentAssembler, tokenizer, spellChecker])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fHqTRl1benvE"
      },
      "source": [
        "Let's try to train the model directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znZpF8hfelcL",
        "outputId": "c364a7a6-d63b-47cd-bc66-3ca3ccd9d222"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "requirement failed: We couldn't find any suitable graph for 2000 classes, vocabSize: 3080\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  model = pipeline.fit(corpus)\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "0jMfhRx2dWD9"
      },
      "source": [
        "We see the error message saying that we don't have the Tensorflow graph for this specific corpus containing 2000 classes and vocabulary size 3080. The vocabulary size depends on several parameters of the annotator, such as the `minCount`, `compoundCount`, `classCount`, and, off course, the corpus itself. We will see later how we can also extend the classes with manually chosen ones using vocabulary (for example, names, products, etc.) or regex (identifies dates, numbers, etc.).\n",
        "\n",
        "To create the Tensorflow graph for any specific corpus, you can follow the steps on [this notebook](https://github.com/JohnSnowLabs/spark-nlp/blob/master/python/tensorflow/spellchecker/create_spell_model.ipynb). In the meanwhile, we will set the parameter `languageModelClasses` to `1650` so that we can use an existing graph.\n",
        "\n",
        "> Please note that we will limit the number of sentences so that our deep learning model can be trained in Colab without crashing. It is for educatinal purposes only. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UyeMfHDBfBsM"
      },
      "source": [
        "### Preparing the corpus for training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SF9ngxnMfEmk"
      },
      "source": [
        "We will use the `SentenceDetector` annotator to split the book into sentences. Then we will sample a number of sentences that Colab is able to process. As a deep learning model, it demands heavy computation during training. For big datasets, it is recommended to use spark clusters to train efficiently."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39BFz0sQfDk2",
        "outputId": "af229d65-4773-4c2d-c07d-354877f6fca0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "561"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Limit the size of the data so that we can run it on Colab\n",
        "\n",
        "documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        "sentenceDetector = SentenceDetector().setInputCols(\"document\").setOutputCol(\"sentence\")\n",
        "\n",
        "sentences = sentenceDetector.transform(documentAssembler.transform(corpus))\n",
        "\n",
        "# Get 10% of the senteces only\n",
        "sample = sentences.select(F.explode(\"sentence.result\").alias(\"sentence\")).sample(fraction=0.1, seed=42)\n",
        "sample.count()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xqkdK2C6fBqY"
      },
      "source": [
        "### Training the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft8MXRdefBoj"
      },
      "source": [
        "Create a new pipeline to process this sample from the beginning (DocumentAssembler -> ContextSpellChecker)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i4eOvaLUfS25"
      },
      "outputs": [],
      "source": [
        "# Note that we se `sentence` as input column name\n",
        "documentAssembler = DocumentAssembler().setInputCol(\"sentence\").setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer().setInputCols(\"document\").setOutputCol(\"token\")\n",
        "\n",
        "spellChecker = (\n",
        "    ContextSpellCheckerApproach()\n",
        "    .setInputCols(\"token\")\n",
        "    .setOutputCol(\"checked\")\n",
        "    .setBatchSize(1) # Batch size 1 to run in Colab\n",
        "    .setEpochs(1)\n",
        "    .setWordMaxDistance(3) # Maximum edit distance to consider\n",
        "    .setMaxWindowLen(3) # important to find context\n",
        "    .setMinCount(3.0) # Removes words that appear less than that from the vocabulary\n",
        "    .setCompoundCount(5) # Removes compound words that appear less than that from the vocabulary\n",
        "    .setClassCount(10.0) # Minimun occurrences of a class\n",
        "    .setLanguageModelClasses(1650) # Value taht we have a TF graph available\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(stages=[documentAssembler, tokenizer, spellChecker])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HagnM-zBfVR_",
        "outputId": "59eac56c-efc4-45fa-9630-2d577eea52b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 449 ms, sys: 55.6 ms, total: 505 ms\n",
            "Wall time: 1min 29s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "try:\n",
        "  model = pipeline.fit(sample)\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_LW6ssbDfBmR"
      },
      "source": [
        "Try the trained model on an example sentence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFIvgPHAfa9d",
        "outputId": "672fb3be-d13d-4e4c-866e-000242880462"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sherlok => Sherlock\n",
            "Hlmes => Holmes\n",
            "founds => found\n",
            "the => the\n",
            "solution => solution\n",
            "to => to\n",
            "the => the\n",
            "mistrey => mystery\n"
          ]
        }
      ],
      "source": [
        "lp = LightPipeline(model)\n",
        "\n",
        "test = lp.annotate(\"Sherlok Hlmes founds the solution to the mistrey\")\n",
        "for token, checked in zip(test[\"token\"], test[\"checked\"]):\n",
        "  print(f\"{token} => {checked}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "stXOeZ4mfBj5"
      },
      "source": [
        "> The model was trained with 500+ sentences only, but got some understanding of Conan Doyle's universe."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UYzYLqFPfBiP"
      },
      "source": [
        "## Additional resources"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6NWcNOhtfBgG"
      },
      "source": [
        "For further details, check the following resources:\n",
        "\n",
        "- [Applying Context Aware Spell Checking in Spark NLP](https://medium.com/spark-nlp/applying-context-aware-spell-checking-in-spark-nlp-3c29c46963bc)\n",
        "-[Training a Contextual Spell Checker for Italian Language](https://towardsdatascience.com/training-a-contextual-spell-checker-for-italian-language-66dda528e4bf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgDaSDgU-fc9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
