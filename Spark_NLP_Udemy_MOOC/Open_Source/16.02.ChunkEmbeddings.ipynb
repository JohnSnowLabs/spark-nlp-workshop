{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"sXatvRX899i0"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/Spark_NLP_Udemy_MOOC/Open_Source/16.02.ChunkEmbeddings.ipynb)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"AOn8d1tcBkK3"},"source":["# **ChunkEmbeddings**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"WimihSwD3vtH"},"source":["This notebook will cover the different parameters and usages of `ChunkEmbeddings`. This annotator utilizes WordEmbeddings, BertEmbeddings, etc. to generate chunk embeddings from either [Chunker](https://nlp.johnsnowlabs.com/docs/en/annotators#chunker), [NGramGenerator](https://nlp.johnsnowlabs.com/docs/en/annotators#ngramgenerator) or [NerConverter](https://nlp.johnsnowlabs.com/docs/en/annotators#nerconverter) outputs. \n","\n","**üìñ Learning Objectives:**\n","\n","1. Gain an understanding of how to generate embeddings for various annotator chunk outputs.\n","\n","2. Learn how to generate embeddings for the chunks extracted by a NER model.\n","\n","3. Become comfortable using the different parameters of the annotator.\n","\n","\n","**üîó Helpful Links:**\n","\n","- Documentation : [ChunkEmbeddings](https://nlp.johnsnowlabs.com/docs/en/annotators#chunkembeddings)\n","\n","- Python Docs : [ChunkEmbeddings](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/embeddings/chunk_embeddings/index.html)\n","\n","- Scala Docs : [ChunkEmbeddings](https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/embeddings/ChunkEmbeddings)\n","\n","- For extended examples of usage, see the [Spark NLP Workshop repository](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qL9lcISyFSLv"},"source":["## **üìú Background**\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TjDKOoZ4Fc8G"},"source":["In NLP, embeddings refer to methods which map words (or groups of words) from a corpus to a high-dimensional vector space, in such a way that semantically similar words are mapped to vectors that are close in this space. The distance between vectors can be measured in various ways, the most common are the Euclidean distance and the cosine distance. \n","\n","The input text is split into relevant tokens by the *Tokenizer* and embeddings are created for these tokens. Next, meaningful phrases (chunks) are extracted using either one of the annotators *Chunker*, *NGramGenerator* or *NerConverter*. The corresponding chunk embeddings are calculated by aggregating the individual vectors of the tokens in each chunk, using either mean or sum pooling strategy.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"MfkkKkbVF309"},"source":["## **üé¨ Colab Setup**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iMkMQtZNF2n-"},"outputs":[],"source":["!pip install -q pyspark==3.1.2  spark-nlp==4.2.4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NulWi4_f4GN5"},"outputs":[],"source":["import sparknlp\n","from sparknlp.base import *\n","from sparknlp.annotator import *\n","from pyspark.ml import Pipeline\n","\n","spark = sparknlp.start()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9Fbbk1bqcuA5"},"source":["## **üñ®Ô∏è Input/Output Annotation Types**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0yFIrr5acsiU"},"source":["- Input: `CHUNK`, `WORD_EMBEDDINGS`\n","\n","- Output: `WORD_EMBEDDINGS`"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"b2YJehUKMhb0"},"source":["## **üîé Parameters**\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"oidLDoS94asU"},"source":["- `lazyAnnotator`: (BooleanParam) Whether this AnnotatorModel acts as lazy in RecursivePipelines (Default: false).\n","\n","- `poolingStrategy`: (String) The aggregation method for the word embeddings to create the chunk embeddings, options are AVERAGE or SUM (Default: AVERAGE).\n","\n","- `inputCols`: (Array) Previous annotations columns to retrieve the chunks and the word embeddings to be aggregated, for example ['chunk', 'word_embeddings'].\n","\n","- `outputCol`: (String) Output annotation column, can be left default, for example 'chunk_embeddings'.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"sVcaX9eBO1x6"},"source":["### ChunkEmbeddings for Chunker output"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vFLlDMWgBjqx"},"source":["The [Chunker](https://nlp.johnsnowlabs.com/docs/en/annotators#chunker) annotator matches a pattern of part-of-speech tags in order to return meaningful phrases from the document. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27534,"status":"ok","timestamp":1673460278978,"user":{"displayName":"Silvia Onofrei","userId":"08518681359279549405"},"user_tz":360},"id":"o8k8GUSH7uBm","outputId":"a719499e-8961-4796-eba6-9ec07f1864ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["pos_anc download started this may take some time.\n","Approximate size to download 3.9 MB\n","[OK!]\n","glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n"]}],"source":["documentAssembler = DocumentAssembler() \\\n","      .setInputCol(\"text\") \\\n","      .setOutputCol(\"document\")\n","\n","sentenceDetector = SentenceDetector() \\\n","      .setInputCols([\"document\"]) \\\n","      .setOutputCol(\"sentence\")\n","\n","tokenizer = Tokenizer() \\\n","      .setInputCols([\"sentence\"]) \\\n","      .setOutputCol(\"token\")\n","\n","posTagger = PerceptronModel.pretrained() \\\n","      .setInputCols(\"sentence\", \"token\") \\\n","      .setOutputCol(\"pos\")\n","\n","chunker = Chunker() \\\n","      .setInputCols(\"sentence\", \"pos\") \\\n","      .setOutputCol(\"chunk\") \\\n","      .setRegexParsers([\"<DT>?<JJ>*<NN>+\"])\n","\n","wordEmbeddings = WordEmbeddingsModel.pretrained() \\\n","      .setInputCols([\"sentence\", \"token\"]) \\\n","      .setOutputCol(\"word_embeddings\") \\\n","      .setCaseSensitive(False)\n","\n","chunkEmbeddings = ChunkEmbeddings() \\\n","      .setInputCols([\"chunk\", \"word_embeddings\"]) \\\n","      .setOutputCol(\"chunk_embeddings\") \\\n","      .setPoolingStrategy(\"AVERAGE\")\n","\n","pipeline = Pipeline().setStages([\n","          documentAssembler,\n","          sentenceDetector,\n","          tokenizer,\n","          posTagger,\n","          chunker,\n","          wordEmbeddings,\n","          chunkEmbeddings\n","      ])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AvJ_7_z9784k"},"outputs":[],"source":["data = spark.createDataFrame([[\"This is a sentence.\"]]).toDF(\"text\")\n","result = pipeline.fit(data).transform(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4372,"status":"ok","timestamp":1673460312053,"user":{"displayName":"Silvia Onofrei","userId":"08518681359279549405"},"user_tz":360},"id":"40ochV4QQdjr","outputId":"e1fba509-e13d-48f2-cf91-a88e2481437f"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------+-----+---+----------+------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n","|  annotatorType|begin|end|    result|                                                                            metadata|                                                                                          embeddings|\n","+---------------+-----+---+----------+------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n","|word_embeddings|    8| 17|a sentence|{chunk -> 0, pieceId -> -1, isWordStart -> true, token -> a sentence, sentence -> 0}|[0.17417, 0.095253006, -0.0530925, -0.218465, 0.714395, 0.79860497, 0.012999997, 0.48517, -0.5434...|\n","+---------------+-----+---+----------+------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n","\n"]}],"source":["result.selectExpr(\"explode(chunk_embeddings) as result\") \\\n","    .select(\"result.annotatorType\", \"result.begin\", \"result.end\", \"result.result\", \"result.metadata\", \"result.embeddings\") \\\n","    .show(6, 100)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DQ88eDCxTsD3"},"source":["## ChunkEmbeddings for NGramGenerator output"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8EFZsxV1HxXb"},"source":["[NGramGenerator](https://nlp.johnsnowlabs.com/docs/en/annotators#ngramgenerator) is a feature transformer that converts the input array of strings into an array of n-grams. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4204,"status":"ok","timestamp":1673460326217,"user":{"displayName":"Silvia Onofrei","userId":"08518681359279549405"},"user_tz":360},"id":"0hWNC0Y_RS27","outputId":"875a4f27-e6be-4b51-c598-043ce373ee69"},"outputs":[{"name":"stdout","output_type":"stream","text":["glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n"]}],"source":["documentAssembler = DocumentAssembler() \\\n","    .setInputCol(\"text\") \\\n","    .setOutputCol(\"document\")\n","\n","sentenceDetector = SentenceDetector() \\\n","    .setInputCols([\"document\"]) \\\n","    .setOutputCol(\"sentence\")\n","\n","tokenizer = Tokenizer() \\\n","    .setInputCols([\"sentence\"]) \\\n","    .setOutputCol(\"token\")\n","\n","nGrams = NGramGenerator() \\\n","    .setInputCols([\"token\"]) \\\n","    .setOutputCol(\"chunk\") \\\n","    .setN(2)\n","\n","embeddings = WordEmbeddingsModel.pretrained() \\\n","    .setInputCols([\"sentence\", \"token\"]) \\\n","    .setOutputCol(\"word_embeddings\") \\\n","    .setCaseSensitive(False)\n","\n","chunkEmbeddings = ChunkEmbeddings() \\\n","    .setInputCols([\"chunk\", \"word_embeddings\"]) \\\n","    .setOutputCol(\"chunk_embeddings\") \\\n","    .setPoolingStrategy(\"AVERAGE\")\n","\n","pipeline = Pipeline().setStages([\n","      documentAssembler,\n","      sentenceDetector,\n","      tokenizer,\n","      nGrams,\n","      embeddings,\n","      chunkEmbeddings\n","    ])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b70EEUry-Aqe"},"outputs":[],"source":["data = spark.createDataFrame([[\"This is a sentence.\"]]).toDF(\"text\")\n","result = pipeline.fit(data).transform(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":840,"status":"ok","timestamp":1673460336573,"user":{"displayName":"Silvia Onofrei","userId":"08518681359279549405"},"user_tz":360},"id":"kv2ODITT9_1I","outputId":"0833e1f6-198f-4245-a1d7-ff9a1af96430"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------+-----+---+----------+------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n","|  annotatorType|begin|end|    result|                                                                            metadata|                                                                                          embeddings|\n","+---------------+-----+---+----------+------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n","|word_embeddings|    0|  6|   This is|   {chunk -> 0, pieceId -> -1, isWordStart -> true, token -> This is, sentence -> 0}|[-0.55661, 0.42829502, 0.86661, -0.409785, 0.06316501, 0.120775, -0.0732005, 0.47754502, 0.033937...|\n","|word_embeddings|    5|  8|      is a|      {chunk -> 1, pieceId -> -1, isWordStart -> true, token -> is a, sentence -> 0}|[-0.40674996, 0.22938299, 0.50597, -0.288195, 0.555655, 0.465145, 0.140118, 0.47235, -0.107687, 0...|\n","|word_embeddings|    8| 17|a sentence|{chunk -> 2, pieceId -> -1, isWordStart -> true, token -> a sentence, sentence -> 0}|[0.17417, 0.095253006, -0.0530925, -0.218465, 0.714395, 0.79860497, 0.012999997, 0.48517, -0.5434...|\n","|word_embeddings|   10| 18|sentence .|{chunk -> 3, pieceId -> -1, isWordStart -> true, token -> sentence ., sentence -> 0}|[0.139705, 0.177955, 0.1887775, -0.45545, 0.20030999, 0.461557, -0.07891501, 0.32937, -0.16221501...|\n","+---------------+-----+---+----------+------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n","\n"]}],"source":["result.selectExpr(\"explode(chunk_embeddings) as result\") \\\n","    .select(\"result.annotatorType\", \"result.begin\", \"result.end\", \"result.result\", \"result.metadata\", \"result.embeddings\") \\\n","    .show(6, 100)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ta3-PG7m75Iu"},"source":["## ChunkEmbeddings for NerConverter output"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"NjWJaUcxJRXu"},"source":["In this case, the chunks are NER entities, extracted by a pipeline built around a NER model. The [NerConverter](https://nlp.johnsnowlabs.com/docs/en/annotators#nerconverter) transforms an IOB or IOB2 representation of NER to a user-friendly one, by associating the tokens of recognized entities and their labels. "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fujulxvA-TRC"},"source":["### Extract NER entities using a CRF model\n","\n","For more information regarding the NER model used below, see [CrfModel](https://nlp.johnsnowlabs.com/docs/en/annotators#nercrf) and for details on how to choose the pipeline stages see this [Spark NLP Colab Notebook](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/annotation/english/model-downloader/Running_Pretrained_pipelines.ipynb)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20109,"status":"ok","timestamp":1673460361270,"user":{"displayName":"Silvia Onofrei","userId":"08518681359279549405"},"user_tz":360},"id":"6luoqaJy4B2A","outputId":"8fc98588-ed67-476b-bf6f-bb7f6d39e0b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n","pos_anc download started this may take some time.\n","Approximate size to download 3.9 MB\n","[OK!]\n","ner_crf download started this may take some time.\n","Approximate size to download 10.2 MB\n","[OK!]\n"]}],"source":["documentAssembler = DocumentAssembler() \\\n","    .setInputCol(\"text\") \\\n","    .setOutputCol(\"document\")\n","\n","sentenceDetector = SentenceDetector() \\\n","    .setInputCols([\"document\"]) \\\n","    .setOutputCol(\"sentence\")\n","\n","tokenizer = Tokenizer() \\\n","    .setInputCols([\"sentence\"]) \\\n","    .setOutputCol(\"token\")\n","\n","wordEmbeddings = WordEmbeddingsModel.pretrained() \\\n","    .setInputCols([\"sentence\", \"token\"]) \\\n","    .setOutputCol(\"word_embeddings\") \\\n","    .setCaseSensitive(False)\n","\n","posTagger = PerceptronModel.pretrained() \\\n","    .setInputCols([\"sentence\", \"token\"]) \\\n","    .setOutputCol(\"pos\")\n","\n","nerModel = NerCrfModel.pretrained() \\\n","    .setInputCols([\"pos\", \"token\", \"document\", \"word_embeddings\"]) \\\n","    .setOutputCol(\"ner\")\n","\n","nerConverter = NerConverter() \\\n","    .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n","    .setOutputCol(\"chunk\") \n","\n","chunkEmbeddings = ChunkEmbeddings() \\\n","    .setInputCols([\"chunk\", \"word_embeddings\"]) \\\n","    .setOutputCol(\"chunk_embeddings\") \\\n","    .setPoolingStrategy(\"AVERAGE\")\n","\n","pipeline = Pipeline().setStages([\n","      documentAssembler,\n","      sentenceDetector,\n","      tokenizer,\n","      posTagger,\n","      wordEmbeddings,\n","      nerModel,\n","      nerConverter,\n","      chunkEmbeddings\n","    ])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IJOeCRCn-lC0"},"outputs":[],"source":["data = spark.createDataFrame([[\"Short sellers, Wall Street's dwindling  band of ultra cynics, are seeing green again.\"]]).toDF(\"text\")\n","result = pipeline.fit(data).transform(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4334,"status":"ok","timestamp":1673460390609,"user":{"displayName":"Silvia Onofrei","userId":"08518681359279549405"},"user_tz":360},"id":"xnPCGgkS-lMg","outputId":"592d4186-1d78-4a50-d3f9-287dfc3e9d0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------+-----+---+-------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n","|  annotatorType|begin|end|       result|                                                                               metadata|                                                                                          embeddings|\n","+---------------+-----+---+-------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n","|word_embeddings|   15| 27|Wall Street's|{chunk -> 0, pieceId -> -1, isWordStart -> true, token -> Wall Street's, sentence -> 0}|[0.21384, 0.22098, 0.037105, -0.29186, -0.030131, -0.16247, -1.1043, -0.88436, -0.078059, -0.6353...|\n","+---------------+-----+---+-------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n","\n"]}],"source":["result.selectExpr(\"explode(chunk_embeddings) as result\") \\\n","    .select(\"result.annotatorType\", \"result.begin\", \"result.end\", \"result.result\", \"result.metadata\", \"result.embeddings\") \\\n","    .show(6, 100)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GtmxfTe0Jh0l"},"source":["### Extract NER entities using a NerDL model"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"n-8v3Kh0-uyw"},"source":["The word embeddings are generated using Bert and the entities are predicted with a NER deep learning model. Refer to this [Spark NLP Workshop Notebook (11)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/11.Text_Similarities_and_dimension_reduction_visualizations_for_Embeddings.ipynb#scrollTo=2AkxJDshietW) for additional information."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35402,"status":"ok","timestamp":1673460431324,"user":{"displayName":"Silvia Onofrei","userId":"08518681359279549405"},"user_tz":360},"id":"cJiZKbn6KAY8","outputId":"37a6fbdf-7c8f-4702-f4d5-ec9706b8449a"},"outputs":[{"name":"stdout","output_type":"stream","text":["bert_base_cased download started this may take some time.\n","Approximate size to download 389.1 MB\n","[OK!]\n","ner_dl_bert download started this may take some time.\n","Approximate size to download 15.4 MB\n","[OK!]\n"]}],"source":["document_assembler = DocumentAssembler() \\\n","    .setInputCol(\"text\") \\\n","    .setOutputCol(\"document\")\n","    \n","tokenizer = Tokenizer() \\\n","    .setInputCols([\"document\"]) \\\n","    .setOutputCol(\"token\")\n","\n","wordEmbeddings = BertEmbeddings().pretrained('bert_base_cased') \\\n","      .setInputCols([\"document\",'token'])\\\n","      .setOutputCol(\"word_embeddings\")\\\n","      .setCaseSensitive(False)\n","\n","nerModel = NerDLModel.pretrained(\"ner_dl_bert\", \"en\") \\\n","        .setInputCols([\"document\", \"token\", \"word_embeddings\"]) \\\n","        .setOutputCol(\"ner\")\n","\n","nerConverter = NerConverter() \\\n","    .setInputCols([\"document\", \"token\", \"ner\"]) \\\n","    .setOutputCol(\"entities\")\n","\n","chunkEmbeddings = ChunkEmbeddings() \\\n","    .setInputCols([\"entities\", \"word_embeddings\"]) \\\n","    .setOutputCol(\"chunk_embeddings\") \\\n","    .setPoolingStrategy(\"AVERAGE\")\n","\n","pipeline = Pipeline().setStages([\n","      documentAssembler,\n","      tokenizer,\n","      wordEmbeddings,\n","      nerModel,\n","      nerConverter,\n","      chunkEmbeddings\n","    ])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1XRSJjXA-6i_"},"outputs":[],"source":["data = spark.createDataFrame([[\"Stocks ended slightly higher on Friday but stayed near lows for the year as oil prices surged past  #36;46 a barrel, offsetting a positive outlook from computer maker Dell Inc. (DELL.O)   \"]]).toDF(\"text\")\n","result = pipeline.fit(data).transform(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3149,"status":"ok","timestamp":1673460435269,"user":{"displayName":"Silvia Onofrei","userId":"08518681359279549405"},"user_tz":360},"id":"SgkE5PQQ-6mZ","outputId":"83178c1f-6f0b-4cf6-c43e-3f96b4a5191c"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------+-----+---+------+--------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n","|  annotatorType|begin|end|result|                                                                        metadata|                                                                                          embeddings|\n","+---------------+-----+---+------+--------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n","|word_embeddings|  178|183|DELL.O|{chunk -> 0, pieceId -> -1, isWordStart -> true, token -> DELL.O, sentence -> 0}|[-0.31111056, -0.68521374, -0.49221164, 0.12427488, 0.54703796, -0.28151292, -0.535245, 0.2951617...|\n","+---------------+-----+---+------+--------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n","\n"]}],"source":["result.selectExpr(\"explode(chunk_embeddings) as result\") \\\n","    .select(\"result.annotatorType\", \"result.begin\", \"result.end\", \"result.result\", \"result.metadata\", \"result.embeddings\") \\\n","    .show(6, 100)"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1L8GdQ-yorjVk_V8Um6Rw6aEzFVT31OCO","timestamp":1673032420252},{"file_id":"1VVV4jTagH47UZiKFqXoP-Abq5ozZM1BV","timestamp":1672918708428},{"file_id":"https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb","timestamp":1671914287039}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}
