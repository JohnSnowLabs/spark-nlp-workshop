{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"sXatvRX899i0"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/Spark_NLP_Udemy_MOOC/Open_Source/08.02.RegexTokenizer.ipynb)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"AOn8d1tcBkK3"},"source":["# **RegexTokenizer**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"WimihSwD3vtH"},"source":["This notebook will cover the different parameters and usages of `RegexTokenizer`. This annotator provides the ability to tokenize text according to user-defined regex patterns.\n","\n","**ðŸ“– Learning Objectives:**\n","\n","1. Understand how different regex patterns split sequences of words in different ways.\n","\n","2. Understand the difference between the regex tokenizer and regular tokenizer.\n","\n","3. Become comfortable using the different parameters of the annotator.\n","\n","\n","**ðŸ”— Helpful Links:**\n","\n","- Documentation : [RegexTokenizer](https://nlp.johnsnowlabs.com/docs/en/annotators#regextokenizer)\n","\n","- Python Docs : [RegexTokenizer](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/token/regex_tokenizer/index.html)\n","\n","- Scala Docs : [RegexTokenizer](https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/RegexTokenizer.html)\n","\n","- For extended examples of usage, see the [Spark NLP Workshop repository](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/open-source-nlp/02.0.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qL9lcISyFSLv"},"source":["## **ðŸ“œ Background**\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TjDKOoZ4Fc8G"},"source":["Tokenization is an important task in NLP that facilitates various downstream applications within NLP pipelines. In Spark NLP, tokenization can be carried out using 2 different annotators: the `Tokenizer` or the `RegexTokenizer`. The `RegexTokenizer` gives users additional flexibility in defining token boundaries when compared to the regular `Tokenizer` and is therefore the preferred option in highly customized NLP pipelines."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"MfkkKkbVF309"},"source":["## **ðŸŽ¬ Colab Setup**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iMkMQtZNF2n-"},"outputs":[],"source":["!pip install -q pyspark==3.1.2 spark-nlp==4.2.4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NulWi4_f4GN5"},"outputs":[],"source":["import sparknlp\n","from sparknlp.base import *\n","from sparknlp.annotator import *\n","from pyspark.ml import Pipeline\n","\n","spark = sparknlp.start()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9Fbbk1bqcuA5"},"source":["## **ðŸ–¨ï¸ Input/Output Annotation Types**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0yFIrr5acsiU"},"source":["- Input: `DOCUMENT`\n","\n","- Output: `TOKEN`"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"b2YJehUKMhb0"},"source":["## **ðŸ”Ž Parameters**\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"oidLDoS94asU"},"source":["- `maxLength`: (Int) Maximum token length, greater than or equal to 1.\n","\n","- `minLength`: (Int) Minimum token length, greater than or equal to 0 (Default: 1). Default is 1, to avoid returning empty strings.\n","\n","- `pattern`: (String) --> Regex pattern used to match delimiters (Default: \"\\\\s+\")\n","\n","- `positionalMask`: (BooleanParam) --> Indicates whether to apply the regex tokenization using a positional mask to guarantee the incremental progression (Default: false).\n","\n","- `preservePosition`: (BooleanParam)\n","Indicates whether to use a preserve initial indexes before eventual whitespaces removal in tokens (Default: true).\n","\n","- `toLowercase`: (BooleanParam)\n","Indicates whether to convert all characters to lowercase before tokenizing (Default: false).\n","\n","- `trimWhitespace`: (BooleanParam)\n","Indicates whether to use a trimWhitespace flag to remove whitespaces from identified tokens. (Default: false)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"sVcaX9eBO1x6"},"source":["### `setPattern()`"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vFLlDMWgBjqx"},"source":["The `setPattern` parameter should be used to divide the text into tokens according to desired regex patterns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5AkJbP8Vg4vi"},"outputs":[],"source":["from pyspark.sql.types import StringType\n","\n","content = \"1. T1-T2 DATE**[12/24/13] $1.99 () (10/12) ph+ 90%\"\n","\n","df = spark.createDataFrame([content], StringType()).withColumnRenamed(\"value\", \"text\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C5IbTr6pO6LM"},"outputs":[],"source":["pattern = '\\\\s+|(?=[-.:;\"*+,$&?!%\\\\[\\\\]\\\\(\\\\)\\\\/])|(?<=[-.:;\"*+,$&?!%\\\\[\\\\]\\\\(\\\\)\\\\/])'\n","\n","documenter = DocumentAssembler()\\\n","    .setInputCol(\"text\")\\\n","    .setOutputCol(\"document\")\n","\n","sentenceDetector = SentenceDetector()\\\n","    .setInputCols('document')\\\n","    .setOutputCol('sentence')\n","\n","tokenizer = RegexTokenizer() \\\n","    .setInputCols(\"sentence\") \\\n","    .setOutputCol(\"RegexToken\")\n","\n","regexTokenizer = RegexTokenizer() \\\n","    .setInputCols(\"sentence\") \\\n","    .setOutputCol(\"RegexToken_with_pattern\") \\\n","    .setPattern(pattern)\n","\n","docPatternRemoverPipeline = Pipeline().setStages([documenter,\n","                                                  sentenceDetector,\n","                                                  tokenizer,\n","                                                  regexTokenizer])\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":413,"status":"ok","timestamp":1676305433228,"user":{"displayName":"Halil SAGLAMLAR","userId":"07259164328506563794"},"user_tz":-180},"id":"GDy0jEAQk_rU","outputId":"606f1daf-2759-4c1d-d293-183cabeb3842"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------+\n","|RegexToken                                                                  |RegexToken_with_Pattern                                                                                                     |\n","+----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------+\n","|[1., T1-T2, DATE**[12/24/13], $1.99, (), (10/12), ph+, 90%, sting?, or, hi!]|[1, ., T1, -, T2, DATE, *, *, [, 12, /, 24, /, 13, ], $, 1, ., 99, (, ), (, 10, /, 12, ), ph, +, 90, %, sting, ?, or, hi, !]|\n","+----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------+\n","\n"]}],"source":["content = \"1. T1-T2 DATE**[12/24/13] $1.99 () (10/12) ph+ 90% sting? or hi!\"\n","\n","df = spark.createDataFrame([content], StringType()).withColumnRenamed(\"value\", \"text\")\n","\n","result = docPatternRemoverPipeline.fit(df).transform(df)\n","result.selectExpr(\"RegexToken.result as RegexToken\", \"RegexToken_with_pattern.result as RegexToken_with_Pattern\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"08mk6KARFZHY"},"source":["`Regextokenizer` created the tokens by dividing using \"/s+\" when no pattern was given. When a pattern was given to the `setPattern` parameter, it performed the separation using that pattern."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":244,"status":"ok","timestamp":1675204654325,"user":{"displayName":"Luca Martial","userId":"17305191879319411410"},"user_tz":-60},"id":"40ochV4QQdjr","outputId":"41fb7d92-6c4e-45ce-d003-f556269df2b5"},"outputs":[{"data":{"text/plain":["{Param(parent='RegexTokenizer_ef71be1d91e0', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n"," Param(parent='RegexTokenizer_ef71be1d91e0', name='inputCols', doc='previous annotations columns, if renamed'): ['sentence'],\n"," Param(parent='RegexTokenizer_ef71be1d91e0', name='outputCol', doc='output annotation column. can be left default.'): 'RegexToken',\n"," Param(parent='RegexTokenizer_ef71be1d91e0', name='toLowercase', doc='Indicates whether to convert all characters to lowercase before tokenizing.'): False,\n"," Param(parent='RegexTokenizer_ef71be1d91e0', name='minLength', doc='Set the minimum allowed length for each token'): 1,\n"," Param(parent='RegexTokenizer_ef71be1d91e0', name='pattern', doc='regex pattern used for tokenizing. Defaults \\\\S+'): '\\\\s+',\n"," Param(parent='RegexTokenizer_ef71be1d91e0', name='positionalMask', doc='Using a positional mask to guarantee the incremental progression of the tokenization.'): False,\n"," Param(parent='RegexTokenizer_ef71be1d91e0', name='trimWhitespace', doc='Indicates whether to use a trimWhitespaces flag to remove whitespaces from identified tokens.'): False,\n"," Param(parent='RegexTokenizer_ef71be1d91e0', name='preservePosition', doc='Indicates whether to use a preserve initial indexes before eventual whitespaces removal in tokens.'): True}"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer.extractParamMap()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"T7gmnpIbhjOA"},"source":["### `setTrimWhitespace`"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3tAgcw9fkI1w"},"source":["To remove spaces from tokens after tokenizer operation, `setTrimWhitespace()` param is set to **True**.\n","\n","Now, by looking at the results of both cases, let's better understand the role of the parameter:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1268,"status":"ok","timestamp":1675204708988,"user":{"displayName":"Luca Martial","userId":"17305191879319411410"},"user_tz":-60},"id":"5kxCI3AyeM-C","outputId":"195bedba-de08-4800-c6d7-7eb44aca3c25"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------------------------------------------------------+\n","|RegexToken                                                                          |\n","+------------------------------------------------------------------------------------+\n","|[Jack   ,     registered ,  with ,    id:7354632112   ,     on    ,       23/3/2022]|\n","+------------------------------------------------------------------------------------+\n","\n"]}],"source":["regex_pattern = \"\"\"\\t\"\"\"\n","sampleText = \"   Jack   \\t    registered \\t with \\t   id:7354632112   \\t    on    \\t      23/3/2022    \"\n","\n","df = spark.createDataFrame([[sampleText]]).toDF(\"text\")\n","\n","regexTokenizer = RegexTokenizer()\\\n","  .setInputCols(\"sentence\")\\\n","  .setOutputCol(\"token\")\\\n","  .setPattern(regex_pattern)\\\n","  .setTrimWhitespace(False)\n","\n","pipeline = Pipeline().setStages([documenter,\n","                                 sentenceDetector,\n","                                 tokenizer,\n","                                 regexTokenizer])\n","\n","result = pipeline.fit(df).transform(df)\n","result.selectExpr(\"token.result as RegexToken\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":450,"status":"ok","timestamp":1675204709407,"user":{"displayName":"Luca Martial","userId":"17305191879319411410"},"user_tz":-60},"id":"ucsBfpjmm4Pp","outputId":"cfa36dfc-89bb-418e-b46e-f6896a231522"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------------------------+\n","|RegexToken                                            |\n","+------------------------------------------------------+\n","|[Jack, registered, with, id:7354632112, on, 23/3/2022]|\n","+------------------------------------------------------+\n","\n"]}],"source":["regexTokenizer = RegexTokenizer()\\\n","  .setInputCols(\"sentence\")\\\n","  .setOutputCol(\"token\")\\\n","  .setPattern(regex_pattern)\\\n","  .setTrimWhitespace(True)\n","\n","\n","pipeline = Pipeline().setStages([documenter,\n","                                 sentenceDetector,\n","                                 tokenizer,\n","                                 regexTokenizer])\n","\n","\n","\n","result = pipeline.fit(df).transform(df)\n","result.selectExpr(\"token.result as RegexToken\").show(truncate=False)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4Z3KydpMoIKY"},"source":["\n","\n","\n","As seen above, we can set whether to remove spaces with the parameter setTrimWhitespace()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Z-_02WIgoN6p"},"source":["### `setPreservePosition()`"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"x54u9fPfsd2H"},"source":["setPreservePosition() param indicates whether to apply a method to preserve initial character indexes before eventual whitespace removal in tokens.\n","\n","If after removing whitespaces with the setTrimWhitespace() parameter, the start and end indexes of the tokens need to be preserved as in the original sentence, you should set the setPreservePosition() parameter to True."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":802,"status":"ok","timestamp":1675204793247,"user":{"displayName":"Luca Martial","userId":"17305191879319411410"},"user_tz":-60},"id":"DeA93YjNoXvM","outputId":"4655afe6-c8fc-4159-810c-0dffafb74c40"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------------------------+-----------------------+-----------------------+\n","|RegexToken                                            |Begin                  |End                    |\n","+------------------------------------------------------+-----------------------+-----------------------+\n","|[Jack, registered, with, id:7354632112, on, 23/3/2022]|[3, 15, 28, 37, 58, 71]|[6, 24, 31, 49, 59, 79]|\n","+------------------------------------------------------+-----------------------+-----------------------+\n","\n"]}],"source":["regex_pattern = \"\"\"\\t\"\"\"\n","sampleText = \"   Jack   \\t    registered \\t with \\t   id:7354632112   \\t    on    \\t      23/3/2022    \"\n","\n","df = spark.createDataFrame([[sampleText]]).toDF(\"text\")\n","\n","regexTokenizer = RegexTokenizer()\\\n","  .setInputCols(\"sentence\")\\\n","  .setOutputCol(\"token\")\\\n","  .setPattern(regex_pattern)\\\n","  .setTrimWhitespace(True)\\\n","  .setPreservePosition(False)\n","\n","pipeline = Pipeline().setStages([documenter,\n","                                 sentenceDetector,\n","                                 tokenizer,\n","                                 regexTokenizer])\n","\n","result = pipeline.fit(df).transform(df)\n","result.selectExpr(\"token.result as RegexToken\", \"token.begin as Begin\", \"token.end as End\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":806,"status":"ok","timestamp":1675204797666,"user":{"displayName":"Luca Martial","userId":"17305191879319411410"},"user_tz":-60},"id":"BiwmMnR0dqcK","outputId":"d42b9d99-e869-41fa-8927-cb5a0c5cbbef"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------------------------+-----------------------+-----------------------+\n","|RegexToken                                            |Begin                  |End                    |\n","+------------------------------------------------------+-----------------------+-----------------------+\n","|[Jack, registered, with, id:7354632112, on, 23/3/2022]|[3, 11, 27, 34, 54, 65]|[9, 25, 32, 52, 63, 79]|\n","+------------------------------------------------------+-----------------------+-----------------------+\n","\n"]}],"source":["regexTokenizer = RegexTokenizer()\\\n","  .setInputCols(\"sentence\")\\\n","  .setOutputCol(\"token\")\\\n","  .setPattern(regex_pattern)\\\n","  .setTrimWhitespace(True)\\\n","  .setPreservePosition(True)\n","\n","pipeline = Pipeline().setStages([documenter,\n","                                 sentenceDetector,\n","                                 tokenizer,\n","                                 regexTokenizer])\n","\n","result = pipeline.fit(df).transform(df)\n","result.selectExpr(\"token.result as RegexToken\", \"token.begin as Begin\", \"token.end as End\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"46QN5SCqtXB8"},"source":["As seen above, when we set the setPreservePosition() parameter to True, the starting and ending indexes in the original sentence were preserved, even though we removed the spaces in the tokens."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DQ88eDCxTsD3"},"source":["### `setToLowercase()`"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8EFZsxV1HxXb"},"source":["If it is desired to convert all characters of the text to lowercase before the tokenizer, it can be set with this parameter."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":818,"status":"ok","timestamp":1675204912236,"user":{"displayName":"Luca Martial","userId":"17305191879319411410"},"user_tz":-60},"id":"0hWNC0Y_RS27","outputId":"afe0797b-1068-4af6-dc56-cad1a2b400a8"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------+\n","|Sentence                                                                                        |RegexToken                                                                                                                    |\n","+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------+\n","|[1. The investments made reached a value of Â£4.5Million, gaining __85.6% on DATE**[24/12/2022].]|[1., the, investments, made, reached, a, value, of, Â£4.5million, ,, gaining, _, _, 85.6%, on, date, *, *, [, 24/12/2022, ], .]|\n","+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------+\n","\n"]}],"source":["from pyspark.sql.types import StringType\n","\n","content = \"1. The investments made reached a value of Â£4.5Million, gaining __85.6% on DATE**[24/12/2022].\"\n","pattern = \"\\\\s+|(?=[-:;*__+,$&\\\\[\\\\]])|(?<=[-:;*__+,$&\\\\[\\\\]])\"\n","\n","df = spark.createDataFrame([content], StringType()).withColumnRenamed(\"value\", \"text\")\n","\n","documenter = DocumentAssembler()\\\n","    .setInputCol(\"text\")\\\n","    .setOutputCol(\"document\")\n","\n","sentenceDetector = SentenceDetector()\\\n","    .setInputCols(['document'])\\\n","    .setOutputCol('sentence')\n","\n","regexTokenizer = RegexTokenizer() \\\n","    .setInputCols([\"sentence\"]) \\\n","    .setOutputCol(\"regexToken\") \\\n","    .setPattern(pattern)\\\n","    .setToLowercase(True)\n","\n","docPatternRemoverPipeline = Pipeline().setStages([documenter,\n","                                                  sentenceDetector,\n","                                                  regexTokenizer])\n","\n","result = docPatternRemoverPipeline.fit(df).transform(df)\n","\n","result.selectExpr(\"sentence.result as Sentence\", \"regexToken.result as RegexToken\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ta3-PG7m75Iu"},"source":["### `setMaxLength()`"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"NjWJaUcxJRXu"},"source":["This parameter can be adjusted when you want to see only tokens with a certain maximum length."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":645,"status":"ok","timestamp":1675204968055,"user":{"displayName":"Luca Martial","userId":"17305191879319411410"},"user_tz":-60},"id":"6luoqaJy4B2A","outputId":"3442aa05-1cc2-4127-aa20-a062d646db21"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------------------------+\n","|result                                      |\n","+--------------------------------------------+\n","|[1., The, a, of, ,, _, _, on, *, *, [, ], .]|\n","+--------------------------------------------+\n","\n"]}],"source":["documentAssembler = DocumentAssembler() \\\n","    .setInputCol(\"text\") \\\n","    .setOutputCol(\"document\")\n","\n","regexTokenizer = RegexTokenizer() \\\n","    .setInputCols([\"document\"]) \\\n","    .setOutputCol(\"regexToken\") \\\n","    .setPattern(\"\\\\s+|(?=[-:;*__+,$&\\\\[\\\\]])|(?<=[-:;*__+,$&\\\\[\\\\]])\")\\\n","    .setMaxLength(3)\n","\n","pipeline = Pipeline().setStages([\n","      documentAssembler,\n","      regexTokenizer\n","    ])\n","\n","data = spark.createDataFrame([[\"1. The investments made reached a value of Â£4.5Million, gaining __85.6% on DATE**[24/12/2022].\"]]).toDF(\"text\")\n","result = pipeline.fit(data).transform(data)\n","\n","result.selectExpr(\"regexToken.result\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GtmxfTe0Jh0l"},"source":["As seen in the above results, only tokens with a maximum length of 3 were received."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2R1ALgqbJ-hw"},"source":["### `setMinLength()`"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7490MXI2KFKf"},"source":["This parameter can be adjusted when you want to see only tokens with a certain minimum length."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":633,"status":"ok","timestamp":1675205012286,"user":{"displayName":"Luca Martial","userId":"17305191879319411410"},"user_tz":-60},"id":"cJiZKbn6KAY8","outputId":"20392ff8-61d4-45fe-c296-f057d8a3946b"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------------------------------------------------------------+\n","|result                                                                |\n","+----------------------------------------------------------------------+\n","|[investments, reached, value, Â£4.5Million, gaining, 85.6%, 24/12/2022]|\n","+----------------------------------------------------------------------+\n","\n"]}],"source":["documentAssembler = DocumentAssembler() \\\n","    .setInputCol(\"text\") \\\n","    .setOutputCol(\"document\")\n","\n","regexTokenizer = RegexTokenizer() \\\n","    .setInputCols([\"document\"]) \\\n","    .setOutputCol(\"regexToken\") \\\n","    .setPattern(\"\\\\s+|(?=[-:;*__+,$&\\\\[\\\\]])|(?<=[-:;*__+,$&\\\\[\\\\]])\")\\\n","    .setMinLength(5)\n","\n","pipeline = Pipeline().setStages([\n","      documentAssembler,\n","      regexTokenizer\n","    ])\n","\n","data = spark.createDataFrame([[\"1. The investments made reached a value of Â£4.5Million, gaining __85.6% on DATE**[24/12/2022].\"]]).toDF(\"text\")\n","result = pipeline.fit(data).transform(data)\n","\n","result.selectExpr(\"regexToken.result\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bUSrTvHlKO-I"},"source":["As seen in the above results, only tokens with a minimum length of 5 were received."]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb","timestamp":1671914287039}],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}
