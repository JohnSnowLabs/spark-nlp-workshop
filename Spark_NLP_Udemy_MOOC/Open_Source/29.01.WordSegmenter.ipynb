{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"PJbcjC-samnF"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/Spark_NLP_Udemy_MOOC/Open_Source/29.01.WordSegmenter.ipynb)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"90XYFSVbblyp"},"source":["# WordSegmenter"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5BRq-xE4blyp"},"source":["This notebook will cover the different parameters and usages of `WordSegmenter`.\n","\n","**📖 Learning Objectives:**\n","\n","1. Be able to split text into words in diffferent languages.\n","\n","2. Understand how to use the `WordSegmenter` annotator.\n","\n","3. Become comfortable using the different parameters of the annotator.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3d7jx_8Ubq6C"},"source":["**🔗 Helpful Links:**\n","\n","- Documentation : [WordSegmenter](https://nlp.johnsnowlabs.com/docs/en/annotators#wordsegmenter)\n","\n","- Python Docs : [WordSegmenter](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/ws/word_segmenter/index.html#sparknlp.annotator.ws.word_segmenter.WordSegmenterModel)\n","\n","- Scala Docs : [WordSegmenter](https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/ws/WordSegmenterModel.html)\n","\n","- For extended examples of usage, see the [Spark NLP Workshop repository](https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/tutorials/Certification_Trainings/Public)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"S0ERC5daa22T"},"source":["## **📜 Background**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fJydwdDsa5Ky"},"source":["An important part of text preprocessing is to split texts into an array of words that can be further used on many NLP tasks.\n","\n","This task is relatively more difficult in some languages such as Chinese, Japanese, Korean, and Thai where the words in a text are not separated by a white space (or other delimiter). \n","\n","For example, check the following text in Chinese:\n","\n","> 我们都很喜欢自然语言处理！\n","\n","We can identify that the Chinese words are all together without any separation, so how can we identify which composition of ideograms form a word? \n","\n","In this example, the words are:\n","\n","- 我们 (we, composition of two ideograms)\n","- 都 (all, only one ideogram)\n","- 很 (very, only one ideogram)\n","- 喜欢 (like, composition of two ideograms)\n","- 自然语言处理 (NLP, composition of six ideograms), which can be breaked down to:\n","  - 自然 (Natural)\n","  - 语言 (Language)\n","  - 处理 (Processing)\n","\n","But there is no easy way to programmatically identify them! Thus, we need help from Machine Learning models. \n","\n","In this notebook, we will introduce the Spark NLP annotators that can identify the words in this kind of texts, either by using pretrained models or training new ones.\n","\n","John Snow Labs currently has pretrained models for Chinese, Japanese, Korean, and Thai."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4aqI2eoZbM1G"},"source":["## **🎬 Colab Setup**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"B61TLXIWbOpS"},"source":["Before going through the annotators, let's set up the environment and start a `spark` session."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37728,"status":"ok","timestamp":1676570970724,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"EC4nWgnbblyr","outputId":"b8215d13-a9af-4a0f-b2b5-4cb4e027da28"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.4/212.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m448.4/448.4 KB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 KB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q pyspark==3.1.2  spark-nlp==4.2.4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eAOeQqzXblyr"},"outputs":[],"source":["from pyspark.sql import functions as F\n","from pyspark.ml import Pipeline\n","\n","import sparknlp\n","from sparknlp.annotator import (\n","    Wav2Vec2ForCTC\n",")\n","from sparknlp.base import DocumentAssembler, LightPipeline\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2gPrL2jUbXvL"},"source":["Starting the spark session:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":254},"executionInfo":{"elapsed":61328,"status":"ok","timestamp":1676571254974,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"7jv6xvuMbXHw","outputId":"78def9f8-35f9-4c12-97e3-e93eddfb964c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Spark NLP version 4.2.4\n","Apache Spark version: 3.1.2\n"]},{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cdd024103d78:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Spark NLP</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f6f662dc3d0>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["spark = sparknlp.start()\n","\n","print(\"Spark NLP version\", sparknlp.version())\n","print(\"Apache Spark version:\", spark.version)\n","\n","spark"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xmgxwrSCcF6o"},"source":["## **🖨️ Input/Output Annotation Types**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"EM7O2BDycHgA"},"source":["- Input: `DOCUMENT`\n","\n","- Output: `TOKEN`"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xmxq8Xi_cJFe"},"source":["## **🔎 Parameters**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"IqpCn0BPcK5Q"},"source":["- **model**: Part-of-Speech model.\n","\n","- **pattern**: Regex pattern used to match delimiters (Default: \"\\\\s+\").\n","\n","- **toLowercase**: Indicates whether to convert all characters to lowercase before tokenizing (Default: false). Useful when multilanguage is present in the text."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"cDLILHrc_dKj"},"source":["### ✌ Using pretrained models\n","\n","We can use pretrained model with the `WordSegmenterModel` annotator. For a list of available models, check [NLP Models Hub](https://nlp.johnsnowlabs.com/models?task=Word+Segmentation).\n","\n","This annotator acts like the `Tokenizer` annotator, for languages where the words don't have a clear separator (like white space).\n","\n","We will show how to use the Chinese pretrained model ``."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"laUdYcblcczE"},"outputs":[],"source":["# Chinese example\n","example_sentence = r\"我们都很喜欢自然语言处理！\""]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XtNbuop_mSkQ"},"source":["Create the pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35625,"status":"ok","timestamp":1676572192363,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"oC3cekia2XLd","outputId":"48caff47-7a15-4ed9-9644-8e2730370346"},"outputs":[{"name":"stdout","output_type":"stream","text":["wordseg_ctb9 download started this may take some time.\n","Approximate size to download 2.2 MB\n","[OK!]\n","+----+\n","|word|\n","+----+\n","|我们|\n","|都  |\n","|很  |\n","|喜欢|\n","|自然|\n","|语言|\n","|处理|\n","|！  |\n","+----+\n","\n"]}],"source":["document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n","\n","# Model trained on the Chinese Treebank 9 dataset\n","word_segmenter = (\n","    WordSegmenterModel.pretrained(\"wordseg_ctb9\", \"zh\")\n","    .setInputCols([\"document\"])\n","    .setOutputCol(\"words_segmented\")\n","    .setPattern(\"\\\\s+\")\n",")\n","\n","\n","pipeline = Pipeline(stages=[document_assembler, word_segmenter])\n","example = spark.createDataFrame([[example_sentence]]).toDF(\"text\")\n","\n","model = pipeline.fit(example)\n","result = model.transform(example)\n","result.select(F.explode(\"words_segmented.result\").alias(\"word\")).show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"os6n3ivso_YM"},"source":["## Fast inference with [LightPipelines](https://nlp.johnsnowlabs.com/docs/en/concepts#using-spark-nlps-lightpipeline)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TyPScBbfo_WE"},"source":["We can use Spark NLP's `LightPipeline` to run fast inference directly on text (or list of text) instead of using spark data frames. \n","\n","Let's check how to do that."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1076,"status":"ok","timestamp":1676572271731,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"dYsy6129vIEP","outputId":"66e1c6fc-5fa0-42ca-8093-b06a381608ff"},"outputs":[{"data":{"text/plain":["{'document': ['我们都很喜欢自然语言处理！'],\n"," 'words_segmented': ['我们', '都', '很', '喜欢', '自然', '语言', '处理', '！']}"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Simply define the LightPipeline on the PipelineModel\n","lp = LightPipeline(model)\n","\n","lp.annotate(example_sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":329,"status":"ok","timestamp":1676572278730,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"tAYJ_a2Z66JE","outputId":"1512556c-3a3b-4c55-a23b-81cf96fe62ad"},"outputs":[{"name":"stdout","output_type":"stream","text":["Word 0: 我们\n","Word 1: 都\n","Word 2: 很\n","Word 3: 喜欢\n","Word 4: 自然\n","Word 5: 语言\n","Word 6: 处理\n","Word 7: ！\n"]}],"source":["for i, token in enumerate(lp.annotate(example_sentence)[\"words_segmented\"]):\n","  print(f\"Word {i}: {token}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"KiwhnbaI7JiD"},"source":["Easy as that!"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YqJ2QjEwd5sx"},"source":["## ⚡ **Training a new WordSegmenterModel**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"z2sj_F-KdWNO"},"source":["To train a new model, we need to use the `WordSegmenterApproach` annotator.\n","\n","The parameters of the annotator are:\n","\n","- **model**: Part-of-Speech model.\n","- **pattern**: Regex pattern used to match delimiters (Default: \"\\\\s+\").\n","- **toLowercase**: Indicates whether to convert all characters to lowercase before tokenizing (Default: false). Useful when multilanguage is present in the text.\n","- **ambiguityThreshold**: How much percentage of total amount of words are covered to be marked as frequent (Default: 0.97)\n","- **frequencyThreshold**: How many times at least a tag on a word to be marked as frequent (Default: 20)\n","- **nIterations**: Number of iterations in training, converges to better accuracy (Default: 5)\n","- **posCol**: Name of the column containing the POS tags that match tokens\n","\n","The implemented model is a modification of the following reference paper:\n","\n","> [Chinese Word Segmentation as Character Tagging (Xue, IJCLCLP 2003)](https://aclanthology.org/O03-4002/)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ffYGrIlw1NMF"},"source":["### Training data"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"X-0MCS-2dWK5"},"source":["The training data for the `WordSegmenterApproach` annotator is a text file in the same format used to train `Part-of-Speech` (POS) models, meaning that each ideogram/character is tagged with a label and are separated by a delimiter.\n","\n","We will use the following as training data to train a simple Korean Word Segmenter model (character and tag are separated by `| ` and characters-tag are separated by white space):\n","\n","> 우|LL 리|RR 모|LL 두|MM 는|RR 자|LL연|MM 어|MM 처|MM 리|MM 를|RR 좋|LL 아|MM 합|MM 니|MM 다|RR !|LR \n","\n","\n","Where the labels are:\n","\n","* `LL`: The beginning of the word\n","* `MM`: Middle part of the word\n","* `RR`: The end of the word\n","* `LR`: A word formed of only one character"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"424E9bqC3A47"},"source":["Create a text file with the training data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rB68D0Ww2tQx"},"outputs":[],"source":["with open(\"train_data.txt\", \"w\", encoding=\"utf8\") as f:\n","  f.write(\"우|LL 리|RR 모|LL 두|MM 는|RR 자|LL 연|MM 어|RR 처|LL 리|MM 를|RR 좋|LL 아|MM 합|MM 니|MM 다|RR !|LR \")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xNglzdBo2Ovf"},"source":["To read this kind of dataset, you can use the helper class [POS](https://nlp.johnsnowlabs.com/docs/en/training#pos-dataset)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i-LJuo2oeYiY"},"outputs":[],"source":["from sparknlp.training import POS"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2049,"status":"ok","timestamp":1676572532797,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"FylSPmpV2ij9","outputId":"d1249ba8-9667-4cd5-876d-21681369702b"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------------------------+--------------------+--------------------+\n","|                         text|            document|                tags|\n","+-----------------------------+--------------------+--------------------+\n","|우 리 모 두 는 자 연 어 처...|[{document, 0, 32...|[{pos, 0, 0, LL, ...|\n","+-----------------------------+--------------------+--------------------+\n","\n"]}],"source":["train_data = POS().readDataset(spark, \"train_data.txt\")\n","train_data.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"q-RSm_933Ewh"},"source":["Build the pipeline for training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xdUoJ0BueaY4"},"outputs":[],"source":["documentAssembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n","\n","wordSegmenter = (\n","    WordSegmenterApproach()\n","    .setInputCols([\"document\"])\n","    .setOutputCol(\"token\")\n","    .setPosColumn(\"tags\") # Name in the training data obtained with POS class\n","    .setNIterations(10)\n","    .setFrequencyThreshold(1) # Since our data is very small\n",")\n","\n","pipeline = Pipeline().setStages([documentAssembler, wordSegmenter])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2219,"status":"ok","timestamp":1676572573104,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"trjcuaRfefT1","outputId":"ca0ad18f-7f24-496a-d91d-7b621b059a48"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 21.7 ms, sys: 6.07 ms, total: 27.7 ms\n","Wall time: 1.06 s\n"]}],"source":["%%time\n","\n","pipelineModel = pipeline.fit(train_data)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_LW6ssbDfBmR"},"source":["Try the trained model:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":349,"status":"ok","timestamp":1676572586957,"user":{"displayName":"David Cecchini","userId":"15563969817876100862"},"user_tz":180},"id":"eFIvgPHAfa9d","outputId":"96e9750f-afc6-4513-9eb8-79e9e70a174d"},"outputs":[{"data":{"text/plain":["{'document': ['우리모두는자연어처리를좋아합니다!'],\n"," 'token': ['우리', '모두는', '자연어', '처리를', '좋아합니다', '!']}"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["lp = LightPipeline(pipelineModel)\n","\n","lp.annotate(\"우리모두는자연어처리를좋아합니다!\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"g7Bx9piT54rA"},"source":["That's it! Now you know how to train a new Word Segmenter model, as well as how to use a pretrained one!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCZb8Dy56AW5"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"13hdDuFsr-7lHijJLzkSzVhX8X0laC6XJ","timestamp":1674827969232},{"file_id":"1f0hJ6HS5dJlCA7RzYT_X6BH7yVBxWfBt","timestamp":1674826423494}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
