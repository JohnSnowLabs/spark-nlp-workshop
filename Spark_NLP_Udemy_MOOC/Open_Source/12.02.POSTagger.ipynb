{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1CrvrheKC5BTv5YGgqJtt0spZfEiu0fpO","timestamp":1677785283722}],"toc_visible":true,"authorship_tag":"ABX9TyMbrWE9IBoE0MfP76Fqwpu9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"],"metadata":{"id":"_UPkqyygYK3L"}},{"cell_type":"markdown","source":["# **POSTagger(Part of speech tagger)**"],"metadata":{"id":"kiOhdfmwYOFQ"}},{"cell_type":"markdown","source":["This notebook will cover the different parameters and usages of `POSTagger`. \n","\n","**ðŸ“– Learning Objectives:**\n","\n","1. Understand the basics of `part-of-speech (POS) tagging` and how it can be useful in natural language processing applications.\n","\n","2. Learn about potential use cases for `POS tagging`, such as and named entity recognition, and dependency parser\n","\n","**ðŸ”— Helpful Links:**\n","\n","- Documentation : [PerceptronModel](https://nlp.johnsnowlabs.com/docs/en/annotators#postagger-part-of-speech-tagger)\n","\n","- Python Docs : [PerceptronModel](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/pos/perceptron/index.html)\n","\n","- Scala Docs : [PerceptronModel](https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/pos/perceptron/PerceptronModel)\n"],"metadata":{"id":"9xWjzOICY_Qg"}},{"cell_type":"markdown","source":["## **ðŸ“œ Background**\n","`Part-of-speech (POS) tagging` is the process of labeling each word in a text with its corresponding part of speech, such as noun, verb, adjective, etc. `POS tagging` is a fundamental task in natural language processing, and it is used in many downstream applications such as  and named entity recognition, relation extraction, and dependency parser."],"metadata":{"id":"ESodR3tMrnk5"}},{"cell_type":"markdown","source":["## **ðŸŽ¬ Colab Setup**"],"metadata":{"id":"X26HwMdwf-nJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"WVrizsjCWebQ"},"outputs":[],"source":["! pip install -q pyspark==3.1.2  spark-nlp==4.2.4"]},{"cell_type":"code","source":["import sparknlp\n","\n","import sys\n","sys.path.append('../../')\n","\n","import sparknlp\n","\n","from sparknlp.base import LightPipeline\n","from pyspark.sql import SparkSession\n","from pyspark.ml import Pipeline\n","from pyspark.sql.functions import array_contains\n","from sparknlp.annotator import *\n","from sparknlp.common import RegexRule\n","from sparknlp.base import DocumentAssembler, Finisher\n","import pandas as pd\n","import pyspark.sql.functions as F\n","\n","spark = sparknlp.start()\n","\n","print(\"Spark NLP version\", sparknlp.version())\n","print(\"Apache Spark version:\", spark.version)\n","\n","spark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":259},"id":"R1AJV_vigF8V","executionInfo":{"status":"ok","timestamp":1677845718453,"user_tz":-180,"elapsed":309,"user":{"displayName":"Ahmet Mesut BÄ°ROL","userId":"04340760882990254267"}},"outputId":"c7b8d9c7-659b-4e5c-bd00-1dd36e5f8ff5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Spark NLP version 4.2.4\n","Apache Spark version: 3.1.2\n"]},{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7f95216a1cd0>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://c2ed2c3603ba:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Spark NLP</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":45}]},{"cell_type":"markdown","source":["## **ðŸ–¨ï¸ Input/Output Annotation Types**"],"metadata":{"id":"mX1qzTKNgLkv"}},{"cell_type":"markdown","source":["- Input: `TOKEN`  `DOCUMENT`\n","\n","- Output: `POS`"],"metadata":{"id":"lkY8aHzZgQ_J"}},{"cell_type":"markdown","source":["## **ðŸ”ŽParameters**"],"metadata":{"id":"1FXE50gejKTd"}},{"cell_type":"markdown","source":["- `NONE`\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"PCmnL1oMjVaO"}},{"cell_type":"code","source":["documentAssembler = DocumentAssembler() \\\n","    .setInputCol(\"text\") \\\n","    .setOutputCol(\"document\")\n","\n","tokenizer = Tokenizer() \\\n","    .setInputCols([\"document\"]) \\\n","    .setOutputCol(\"token\")\n","\n","posTagger = PerceptronModel.pretrained() \\\n","    .setInputCols([\"document\", \"token\"]) \\\n","    .setOutputCol(\"pos\")\n","\n","pipeline = Pipeline().setStages([\n","    documentAssembler,\n","    tokenizer,\n","    posTagger\n","])\n","data = spark.createDataFrame([[\"Peter Pipers employees are picking pecks of pickled peppers\"]]).toDF(\"text\")\n","\n","result = pipeline.fit(data).transform(data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rZI62kjZgJuR","executionInfo":{"status":"ok","timestamp":1677845722384,"user_tz":-180,"elapsed":3946,"user":{"displayName":"Ahmet Mesut BÄ°ROL","userId":"04340760882990254267"}},"outputId":"f564f33d-c14c-4e13-9d25-c142fc27c38e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["pos_anc download started this may take some time.\n","Approximate size to download 3.9 MB\n","[OK!]\n"]}]},{"cell_type":"code","source":["result.selectExpr(\"explode(pos) as pos\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A7NEfnlNofqD","executionInfo":{"status":"ok","timestamp":1677845722961,"user_tz":-180,"elapsed":589,"user":{"displayName":"Ahmet Mesut BÄ°ROL","userId":"04340760882990254267"}},"outputId":"e7c16c7d-301d-4e70-b80e-afcbfd1b3e2b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------------------------------------------------------+\n","|pos                                                       |\n","+----------------------------------------------------------+\n","|{pos, 0, 4, NNP, {word -> Peter, sentence -> 0}, []}      |\n","|{pos, 6, 11, NNP, {word -> Pipers, sentence -> 0}, []}    |\n","|{pos, 13, 21, NNS, {word -> employees, sentence -> 0}, []}|\n","|{pos, 23, 25, VBP, {word -> are, sentence -> 0}, []}      |\n","|{pos, 27, 33, VBG, {word -> picking, sentence -> 0}, []}  |\n","|{pos, 35, 39, NNS, {word -> pecks, sentence -> 0}, []}    |\n","|{pos, 41, 42, IN, {word -> of, sentence -> 0}, []}        |\n","|{pos, 44, 50, JJ, {word -> pickled, sentence -> 0}, []}   |\n","|{pos, 52, 58, NNS, {word -> peppers, sentence -> 0}, []}  |\n","+----------------------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["**When we use `pereptron model`.**"],"metadata":{"id":"rK3RjH9NPIs4"}},{"cell_type":"markdown","source":["* For example `NerCRF model` and `Dependency parser` require POS tag as an input column."],"metadata":{"id":"12eOmnwHNZhw"}},{"cell_type":"code","source":["documentAssembler = DocumentAssembler() \\\n","    .setInputCol(\"text\") \\\n","    .setOutputCol(\"document\")\n","\n","sentence = SentenceDetector() \\\n","    .setInputCols([\"document\"]) \\\n","    .setOutputCol(\"sentence\")\n","\n","tokenizer = Tokenizer() \\\n","    .setInputCols([\"sentence\"]) \\\n","    .setOutputCol(\"token\")\n","\n","embeddings = WordEmbeddingsModel.pretrained() \\\n","    .setInputCols([\"sentence\", \"token\"]) \\\n","    .setOutputCol(\"word_embeddings\")\n","\n","posTagger = PerceptronModel.pretrained() \\\n","    .setInputCols([\"sentence\", \"token\"]) \\\n","    .setOutputCol(\"pos\")\n","\n","dependencyParser = DependencyParserModel() \\\n","    .setInputCols(\"sentence\", \"pos\", \"token\") \\\n","    .setOutputCol(\"dependency\")\n","\n","nerTagger = NerCrfModel.pretrained() \\\n","    .setInputCols([\"sentence\", \"token\", \"word_embeddings\", \"pos\"]) \\\n","    .setOutputCol(\"ner\")\n","\n","pipeline = Pipeline().setStages([\n","    documentAssembler,\n","    sentence,\n","    tokenizer,\n","    embeddings,\n","    posTagger,\n","    dependencyParser,\n","    nerTagger\n","])\n","\n","data = spark.createDataFrame([[\"U.N. official Ekeus heads for Baghdad.\"]]).toDF(\"text\")\n","result = pipeline.fit(data).transform(data)\n","\n","result.select(\"ner.result\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yqU_V8E9UtnY","executionInfo":{"status":"ok","timestamp":1677845734622,"user_tz":-180,"elapsed":11672,"user":{"displayName":"Ahmet Mesut BÄ°ROL","userId":"04340760882990254267"}},"outputId":"28cd932c-3a46-4824-e841-3029c31c0666"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n","pos_anc download started this may take some time.\n","Approximate size to download 3.9 MB\n","[OK!]\n","ner_crf download started this may take some time.\n","Approximate size to download 10.2 MB\n","[OK!]\n","+------------------------------------+\n","|result                              |\n","+------------------------------------+\n","|[I-ORG, O, O, I-PER, O, O, I-LOC, O]|\n","+------------------------------------+\n","\n"]}]}]}