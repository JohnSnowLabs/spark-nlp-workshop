{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"zsWWqAa7XFSb"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/Spark_NLP_Udemy_MOOC/Open_Source/19.01.BertForSequenceClassification.ipynb)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0QdDYGXqXY_D"},"source":["# Text Classification (Sequence Classification) with Transformers"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZWv6fOeH2CeW"},"source":["This notebook will cover the different parameters and usages of Transformers-bases classification annotators.\n","\n","**üìñ Learning Objectives:**\n","\n","1. Be able to create a pipeline for text classification using a Transformers-bases annotator.\n","\n","2. Understand how to use the annotators for predictions.\n","\n","3. Become comfortable using the different parameters of the annotators.\n","\n","4. Import Transformers models from Hugging Face to Spark NLP.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rJFeDvuQ2oD4"},"source":["**üîó Helpful Links:**\n","\n","- Documentation : [Transformers in Spark NLP](https://nlp.johnsnowlabs.com/docs/en/transformers)\n","\n","- Python Docs : [BertForSequenceClassification](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/classifier_dl/bert_for_sequence_classification/index.html#sparknlp.annotator.classifier_dl.bert_for_sequence_classification.BertForSequenceClassification)\n","\n","- Scala Docs : [BertForSequenceClassification](https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/classifier/dl/BertForSequenceClassification)\n","\n","- For extended examples of usage, see the [Spark NLP Workshop repository](https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/tutorials/Certification_Trainings/Public/)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"KvjljUtSeopl"},"source":["## Transformers and Spark NLP"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"cahNHehNdlb5"},"source":["Spark NLP has extended support for `HuggingFace` ü§ó   and `TF Hub` exported models since `3.1.0` to Spark NLP üöÄ annotators. You can easily use the `saved_model` feature in HuggingFace within a few lines of codes and import any of the following types of models into Spark NLP.\n","\n","\n","\n","<div align=\"center\">\n","\n","| **Architect** | **Embeddins**        |\n","|---------------|----------------------|\n","| Albert        | AlbertForSequenceClassification     |\n","| BERT          | BertForSequenceClassification       |\n","| CamemBERT     | CamemBertForSequenceClassification  |\n","| DeBERTa       | DeBertaForSequenceClassification    |\n","| DistilBERT    | DistilBertForSequenceClassification |\n","| Longformer    | LongformerForSequenceClassification |\n","| RoBERTa       | RoBertaForSequenceClassification    |\n","| XLM-RoBERTa   | XlmRoBertaForSequenceClassification |\n","| Xlnet         | XlnetForSequenceClassification      |\n","\n","</div>\n","\n","\n","\n","> We will keep working on the remaining annotators and extend this support to aditional Transformers models. To keep updated, visit [this page](https://github.com/JohnSnowLabs/spark-nlp/discussions/5669) on compatibility and development of the adaptations of TF Hub and  HuggingFace to Spark NLP. Keep tuned for the next releases."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"VZH0vE_TdlZn"},"source":["### Text Classification"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TVy4JUjEdlW7"},"source":["As mentioned above, we already have implemented many different Transformers models in Spark NLP, and specifically for text classification we have all the versions of **ForSequenceClassification**, where can be any of:\n","\n","- `BERT` ([BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805), Jacob Devlin et al.): Randomly changes input texts (for example, 15% of them) with _MASKS_ or random tokens in order to learn a language model. Given two sentences, the learning process makes two tasks: \n","    - Predict the sentences by correctly replacing the wrong tokens.\n","    - Predict if the sentences are consecutive or not.\n","- `ALBERT` ([ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942), Zhenzhong Lan et al.): Same as Bert, with changes in some hyperparameters that optimizes memomy usage. The training phase instead of predicting if the two sentences are consecutive, now they predict if they were swapped or not (two consecutive sentences are input, model predict if they were given in the correct order or not).\n","- `RoBERTa` ([RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692), Yinhan Liu et al.): Same as Bert, but with some different training methods (e.g., using dynamic masking in each epoch instead).\n","- `CamemBERT` ([CamemBERT: a Tasty French Language Model](https://arxiv.org/abs/1911.03894), Louis Martin et al.): Based on RoBerta model, trained with French dataset.\n","- `DistilBERT` ([DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108),Victor Sanh et al.): Distilled version of Bert (model parameters were reduced by using transfer learning from big model to smaller model). \n","- `Longformer` ([Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150), Iz Beltagy et al.): Allows the use of upt to 4096 tokens instead of the usual limit of 512. To optimize the added computational cost, replace dense matrixes by sparse representations.\n","- `XlmRoBerta` ([Unsupervised Cross-lingual Representation Learning at Scale](https://arxiv.org/abs/1911.02116), Alexis Conneau et al.): Applies the training methods from RoBerta to Xlm model. \n","- `Xlnet` ([XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237), Zhilin Yang et al.): differently than token masking applied in Bert models, it trains the language model by permuting the tokens. \n","\n","\n","For more details on these models and others available on HuggingFace, pelase visit the [HuggingFace documentation](https://huggingface.co/docs/transformers/model_summary)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HY7Kv9CaXvj6"},"source":["## **üé¨ Colab Setup**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42715,"status":"ok","timestamp":1678215775154,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"7-6DPN1DXDF5","outputId":"4d7e984e-8d77-460e-cdac-401073b686fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m212.4/212.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m471.7/471.7 KB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m198.6/198.6 KB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["! pip install -q pyspark==3.1.2 spark-nlp==4.3.1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":258},"executionInfo":{"elapsed":52059,"status":"ok","timestamp":1678217393801,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"SDasO3DbKu2Z","outputId":"bb3f879d-fb3e-4ad9-8d1d-19d3997ad80c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Spark NLP version:  4.3.1\n","Apache Spark version:  3.1.2\n"]},{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://1198093c70c5:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Spark NLP</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f0ad03bfcd0>"]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import sparknlp\n","\n","spark = sparknlp.start()\n","\n","print(\"Spark NLP version: \", sparknlp.version())\n","print(\"Apache Spark version: \", spark.version)\n","\n","spark"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_7pex7FsmVmU"},"source":["## **üñ®Ô∏è Input/Output Annotation Types**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3epWFrBemWUO"},"source":["- Input: `DOCUMENT`, `TOKEN`\n","\n","- Output: `CATEGORY`"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HJ02o87jmaMU"},"source":["## **üîé Parameters**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"I906-065sbCo"},"source":["- `caseSensitive()`:\n","Set whether to ignore case in index lookups with this parameter\n","(Default depends on model)\n","\n","- `maxSentenceLength` = Maximum sentence length to process, limited to 512 for all models except `Longformer` which has a limit of 4096.\n","\n","- `batchSize` : Large values allows faster processing but requires more memory, by default 8\n","\n","- `configProtoBytes` = ConfigProto from tensorflow, serialized into byte array. Get with `config_proto.SerializeToString()`\n","\n","- `coalesceSentences`: Instead of one class per sentence (if `inputCols` is `sentence`) output one class per document by averaging probabilities in all sentences, by default `False`.\n","\n","- `activation`: Whether to calculate logits via Softmax or Sigmoid, by default `softmax`.\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"a2pw-Na3TXoY"},"source":["## Defining the Spark NLP Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WXKB4Ka6rJv6"},"outputs":[],"source":["from sparknlp.base import DocumentAssembler\n","from sparknlp.annotator import Tokenizer, BertForSequenceClassification, AlbertForSequenceClassification, DistilBertForSequenceClassification, SentenceDetector\n","from pyspark.ml import Pipeline\n","import pyspark.sql.functions as F"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vROuehCQ3rbZ"},"source":["Let's prepared the pre-requisite columns first, so we can use them in different annotators."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kt3wsl0cXC_2"},"outputs":[],"source":["document_assembler = DocumentAssembler() \\\n","        .setInputCol('text') \\\n","        .setOutputCol('document')\n","\n","tokenizer = Tokenizer() \\\n","        .setInputCols(['document']) \\\n","        .setOutputCol('token')\n","        \n","pipeline = Pipeline(stages=[document_assembler,\n","                            tokenizer])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"azhNdnmrXnsZ"},"outputs":[],"source":["example_df = spark.createDataFrame([[\"The movie was brilliant.\"]]).toDF(\"text\")\n","\n","example_df = pipeline.fit(example_df).transform(example_df)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-YdVKxO-gHuB"},"source":["## üìç **BertForSequenceClassification**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3056,"status":"ok","timestamp":1678216403550,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"d9YxaJ6wgG_a","outputId":"38d5954d-41aa-445c-9050-1e6c6c9f819d"},"outputs":[{"name":"stdout","output_type":"stream","text":["bert_classifier_fabriceyhc_base_uncased_imdb download started this may take some time.\n","Approximate size to download 390.9 MB\n","[OK!]\n"]}],"source":["bert_cls = BertForSequenceClassification.pretrained(\"bert_classifier_fabriceyhc_base_uncased_imdb\", \"en\") \\\n","        .setInputCols(['document', 'token']) \\\n","        .setOutputCol('class')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11401,"status":"ok","timestamp":1678216424568,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"f3CQudr4XHkc","outputId":"eec79e16-c37c-46c9-e0c7-ea3b8ba6fbcb"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------+--------------------+--------------------+--------------------+\n","|                text|            document|               token|               class|\n","+--------------------+--------------------+--------------------+--------------------+\n","|The movie was bri...|[{document, 0, 23...|[{token, 0, 2, Th...|[{category, 0, 23...|\n","+--------------------+--------------------+--------------------+--------------------+\n","\n"]}],"source":["result = bert_cls.transform(example_df)\n","result.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1110,"status":"ok","timestamp":1678216425675,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"sU-AaeVrKRqV","outputId":"0453d188-f23a-4b95-9674-3e44921700ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+\n","|result|\n","+------+\n","|[pos] |\n","+------+\n","\n"]}],"source":["result.select(\"class.result\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1058,"status":"ok","timestamp":1678216426732,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"_evpV7mOTRA2","outputId":"c3d2ca47-a21d-4dbb-99b7-61b13a3268ad"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------------------------------------------------------+\n","|class                                                                               |\n","+------------------------------------------------------------------------------------+\n","|[{category, 0, 23, pos, {sentence -> 0, neg -> 3.6695242E-4, pos -> 0.9996331}, []}]|\n","+------------------------------------------------------------------------------------+\n","\n"]}],"source":["result.select(\"class\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":252,"status":"ok","timestamp":1678216449678,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"iWxgWNT3LGB7","outputId":"322125ac-9e6f-428b-e525-29b5f999abb2"},"outputs":[{"data":{"text/plain":["{Param(parent='BERT_FOR_SEQUENCE_CLASSIFICATION_4fcc53fde2bc', name='activation', doc='Whether to calculate logits via Softmax or Sigmoid. Default is Softmax'): 'softmax',\n"," Param(parent='BERT_FOR_SEQUENCE_CLASSIFICATION_4fcc53fde2bc', name='batchSize', doc='Size of every batch'): 8,\n"," Param(parent='BERT_FOR_SEQUENCE_CLASSIFICATION_4fcc53fde2bc', name='coalesceSentences', doc=\"Instead of 1 class per sentence (if inputCols is '''sentence''') output 1 class per document by averaging probabilities in all sentences.\"): False,\n"," Param(parent='BERT_FOR_SEQUENCE_CLASSIFICATION_4fcc53fde2bc', name='engine', doc='Deep Learning engine used for this model'): 'tensorflow',\n"," Param(parent='BERT_FOR_SEQUENCE_CLASSIFICATION_4fcc53fde2bc', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False,\n"," Param(parent='BERT_FOR_SEQUENCE_CLASSIFICATION_4fcc53fde2bc', name='maxSentenceLength', doc='Max sentence length to process'): 256,\n"," Param(parent='BERT_FOR_SEQUENCE_CLASSIFICATION_4fcc53fde2bc', name='caseSensitive', doc='whether to ignore case in tokens for embeddings matching'): False,\n"," Param(parent='BERT_FOR_SEQUENCE_CLASSIFICATION_4fcc53fde2bc', name='inputCols', doc='previous annotations columns, if renamed'): ['document',\n","  'token'],\n"," Param(parent='BERT_FOR_SEQUENCE_CLASSIFICATION_4fcc53fde2bc', name='outputCol', doc='output annotation column. can be left default.'): 'class'}"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["bert_cls.extractParamMap()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":249,"status":"ok","timestamp":1678216474451,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"ARdbZjuqLJ4x","outputId":"ad7707a9-9d49-4d8d-b9d8-8fff0789927c"},"outputs":[{"data":{"text/plain":["False"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["bert_cls.getCaseSensitive()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":232,"status":"ok","timestamp":1678216477626,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"P_0k8ilPLWWm","outputId":"477815ff-b55f-49c0-a017-1794c033f9b3"},"outputs":[{"data":{"text/plain":["256"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["bert_cls.getMaxSentenceLength()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5AFHxq2WgSe7"},"source":["## üìç **AlbertForSequenceClassification**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"i-jjfU5_P0s9"},"source":["### **`coalesceSentences`** parameter : "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DRu6Xw4AP5Qh"},"source":["‚û§ Instead of 1 class per sentence (if inputCols is sentence) output 1 class per document by averaging probabilities in all sentences.\n","\n","Due to max sequence length limit in almost all transformer models such as BERT (512 tokens), this parameter helps feeding all the sentences into the model and averaging all the probabilities for the entire document instead of probabilities per sentence."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rMGroI4WeVz6"},"source":["‚û§ Now let's give our sentences as input to our Sequence Classification model and see what happens when we set our parameter to both True and False."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3304,"status":"ok","timestamp":1678216501121,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"BNxACL-oPKnW","outputId":"e5542f76-9c15-4ef6-a578-64a777c00277"},"outputs":[{"name":"stdout","output_type":"stream","text":["albert_base_sequence_classifier_imdb download started this may take some time.\n","Approximate size to download 42.8 MB\n","[OK!]\n"]}],"source":["document_assembler = DocumentAssembler() \\\n","        .setInputCol('text') \\\n","        .setOutputCol('document')\n","\n","sentenceDetector = SentenceDetector()\\\n","        .setInputCols([\"document\"])\\\n","        .setOutputCol(\"sentence\")\n","\n","tokenizer = Tokenizer() \\\n","        .setInputCols(['sentence']) \\\n","        .setOutputCol('token')\n","\n","albert_cls = AlbertForSequenceClassification \\\n","  .pretrained('albert_base_sequence_classifier_imdb', 'en') \\\n","  .setInputCols(['token', 'sentence']) \\\n","  .setOutputCol('class')\\\n","  .setCoalesceSentences(False)\n","        \n","pipeline = Pipeline(stages=[document_assembler,\n","                            sentenceDetector,\n","                            tokenizer,\n","                            albert_cls])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n0N7Goq2QxQb"},"outputs":[],"source":["example_df = spark.createDataFrame([[\"The movie was brilliant. It was so exciting.\"]]).toDF(\"text\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6308,"status":"ok","timestamp":1678216525988,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"OLHIUM16PV3P","outputId":"eac91328-b7e6-48b7-a0a8-fb15202f3dc8"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------+\n","|result    |\n","+----------+\n","|[pos, pos]|\n","+----------+\n","\n"]}],"source":["result = pipeline.fit(example_df).transform(example_df)\n","result.select(\"class.result\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1123,"status":"ok","timestamp":1678216545410,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"Ejq0M1caPa2D","outputId":"5c95598c-576b-4ee0-80e4-37c356c999cc"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|class                                                                                                                                                                   |\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|[{category, 0, 23, pos, {sentence -> 0, neg -> 0.012881186, pos -> 0.98711884}, []}, {category, 25, 43, pos, {sentence -> 1, neg -> 0.028350135, pos -> 0.9716499}, []}]|\n","+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n"]}],"source":["result.select(\"class\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"WxRcTkENeoXF"},"source":["üëÜüèª As you can see, it made separate predictions for each sentence in the text."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3604,"status":"ok","timestamp":1678216579041,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"Vq6xBD9JPgd7","outputId":"6eb4cb29-3837-4e06-8394-bc139dbe2d0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["albert_base_sequence_classifier_imdb download started this may take some time.\n","Approximate size to download 42.8 MB\n","[OK!]\n"]}],"source":["albert_cls = AlbertForSequenceClassification \\\n","  .pretrained('albert_base_sequence_classifier_imdb', 'en') \\\n","  .setInputCols(['token', 'sentence']) \\\n","  .setOutputCol('class')\\\n","  .setCoalesceSentences(True)\n","        \n","pipeline = Pipeline(stages=[document_assembler,\n","                            sentenceDetector,\n","                            tokenizer,\n","                            albert_cls])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1276,"status":"ok","timestamp":1678216580315,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"SiY6Ok9RPoZv","outputId":"2210c36f-2d3f-4edd-97b0-3c5269e8350f"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+\n","|result|\n","+------+\n","|[pos] |\n","+------+\n","\n"]}],"source":["result = pipeline.fit(example_df).transform(example_df)\n","result.select(\"class.result\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":933,"status":"ok","timestamp":1678216581246,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"_8V1jOVKPpQ8","outputId":"d495e3c6-0357-468a-9e3c-ea324bb6e5f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------------------------------------------------------------------------------+\n","|class                                                                              |\n","+-----------------------------------------------------------------------------------+\n","|[{category, 0, 23, pos, {sentence -> 0, neg -> 0.02061566, pos -> 0.97938436}, []}]|\n","+-----------------------------------------------------------------------------------+\n","\n"]}],"source":["result.select(\"class\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"j3SibIFwfalT"},"source":["üëÜüèª As you can see, when we used the parameter **setCoalesceSentences(True)**, the model made a single class prediction per document by taking the average of the probabilities across all the sentences, instead of one class per sentence."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"idBHpRpSgy5-"},"source":["## üìç **DistilBertForSequenceClassification for French**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YPYveERUT69m"},"source":["### **`Activation`** parameter : "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9Wqfk-8Ja1Qj"},"source":["This parameter is used to specify whether to calculate the logits using the Softmax or Sigmoid activation function.\n","\n","\n","‚û§ **Sigmoid:** The sigmoid function limits the output values to between 0 and 1. This function is often used in binary classification problems. However, the sigmoid function may perform poorly as the number of classes to be classified increases.\n","\n","‚û§ **Softmax:** The softmax function is used in multi-class classification problems with as many output nodes as there are classes. Softmax transforms the values in the output nodes into a probability distribution. Therefore, the softmax function is used in multi-class classification problems where class probabilities need to be estimated."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3796,"status":"ok","timestamp":1678216661223,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"-kLSEhdgjkG0","outputId":"19c458c4-83ea-43a5-941e-cb442f07023f"},"outputs":[{"name":"stdout","output_type":"stream","text":["distilbert_multilingual_sequence_classifier_allocine download started this may take some time.\n","Approximate size to download 484.2 MB\n","[OK!]\n"]}],"source":["example_df = spark.createDataFrame([[\n","    \"\"\"Deuxi√®me long m√©trage de Pasolini, Mamma Roma contient d√©j√† la plupart des obsessions de son auteur et notamment la relation si importante dans la construction de l‚Äô√™tre humain entre la m√®re et son fils adolescent. Anna Magnani est d√©chirante en figure presque universelle de la maman putain repr√©sentative de la Ville √©ternelle. Le film avance √† travers des foules de symboles et la fin o√π le jeune homme termine sa vie en crucifi√© martyr de la soci√©t√© est d‚Äôun implacable r√©alisme po√©tique qui annonce les avanc√©es extr√™mes ult√©rieures de Salo. La construction du r√©cit est d‚Äôune puissance hors du commun et Pasolini se montre d√©j√† un grand cin√©aste qui a assimil√© la technique et les possibilit√©s de ce nouvel outil.\"\"\"\n","    ]]).toDF(\"text\")\n","\n","document_assembler = DocumentAssembler() \\\n","        .setInputCol('text') \\\n","        .setOutputCol('document')\n","\n","tokenizer = Tokenizer() \\\n","        .setInputCols(['document']) \\\n","        .setOutputCol('token')\n","\n","distilbert_cls = DistilBertForSequenceClassification.pretrained(\"distilbert_multilingual_sequence_classifier_allocine\", \"fr\") \\\n","    .setInputCols([\"document\", \"token\"]) \\\n","    .setOutputCol(\"class\")\\\n","    .setActivation(\"sigmoid\")\n","\n","pipeline = Pipeline(stages=[\n","    document_assembler,\n","    tokenizer,\n","    distilbert_cls\n","])\n","\n","result = pipeline.fit(example_df).transform(example_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2542,"status":"ok","timestamp":1678216676328,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"436mUP1OiULw","outputId":"8556ab1d-763d-4c2e-d1e5-d9b90ce1c0c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+\n","|result|\n","+------+\n","|[pos] |\n","+------+\n","\n"]}],"source":["result.select(\"class.result\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":238,"status":"ok","timestamp":1678216693557,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"yyK4Bpm1gCMa","outputId":"410fab29-6f9d-42d9-cb13-c660c7b50cfe"},"outputs":[{"data":{"text/plain":["['neg', 'pos']"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["distilbert_cls.getClasses()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1678216693810,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"hQwuWkjigS4j","outputId":"421e4ce6-6dee-4a2e-e70a-28823cf4fd66"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'sigmoid'"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["distilbert_cls.getActivation()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lpALJ_v5cLb7"},"source":["‚û§ For binary classification, either softmax or sigmoid activation functions can be used, but sigmoid is the more commonly used activation function in this case. However, for multiclass classification, softmax activation function is required."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"P7VXafxy8fAK"},"source":["##  üìç **Using LightPipeline**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"p1ChwwkYTf4g"},"source":["[LightPipelines](https://nlp.johnsnowlabs.com/docs/en/concepts#using-spark-nlps-lightpipeline) are Spark NLP specific Pipelines, equivalent to Spark ML Pipeline, but meant to deal with smaller amounts of data. They‚Äôre useful working with small datasets, debugging results, or when running either training or prediction from an API that serves one-off requests.\n","\n","Spark NLP LightPipelines are Spark ML pipelines converted into a single machine but the multi-threaded task, **becoming more than 10x times faster** for smaller amounts of data (small is relative, but 50k sentences are roughly a good maximum). To use them, we simply plug in a trained (fitted) pipeline and then annotate a plain text. We don't even need to convert the input text to DataFrame in order to feed it into a pipeline that's accepting DataFrame as an input in the first place. This feature would be quite useful when it comes to getting a prediction for a few lines of text from a trained ML model.\n","\n","For more details, check the following \n","[Medium post](https://medium.com/spark-nlp/spark-nlp-101-lightpipeline-a544e93f20f1).\n","\n","This class accepts strings or list of strings as input, without the need to transform your text into a spark data frame. The [.annotate()](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/base/light_pipeline/index.html#sparknlp.base.light_pipeline.LightPipeline.annotate) method returns a dictionary (or list of dictionary if a list is passed as input) with the results of each step in the pipeline. To retrieve all metadata from the anntoators in the result, use the method [.fullAnnotate()](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/base/light_pipeline/index.html#sparknlp.base.light_pipeline.LightPipeline.fullAnnotate) instead, which always returns a list.\n","\n","To extract the results from the object, you just need to parse the dictionary.\n","\n","Let's use the `bert_large_token_classifier_ontonote` model with `LightPipeline` and `.fullAnnotate()` it with sample data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gavowXiBKqWR"},"outputs":[],"source":["from sparknlp.base import LightPipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3589,"status":"ok","timestamp":1678216794944,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"ltjKRFi_E-8V","outputId":"ca583730-08fd-44d3-e0b7-3a479a178aaf"},"outputs":[{"name":"stdout","output_type":"stream","text":["bert_classifier_fabriceyhc_base_uncased_imdb download started this may take some time.\n","Approximate size to download 390.9 MB\n","[OK!]\n"]}],"source":["tokenClassifier = BertForSequenceClassification \\\n","    .pretrained('bert_classifier_fabriceyhc_base_uncased_imdb', 'en') \\\n","    .setInputCols(['token', 'document']) \\\n","    .setOutputCol('class')\n","\n","pipeline = Pipeline(stages=[document_assembler, \n","                            tokenizer,\n","                            tokenClassifier])\n","\n","empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n","model = pipeline.fit(empty_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fka3J4BOGqYH"},"outputs":[],"source":["light_model= LightPipeline(model)\n","light_result= light_model.fullAnnotate(\"The film didn't make me cry, or laugh, or even think about it. I left the theater the same way I went in. What about the screenplay? Is it necessary to repeat the same situation ten times just to give the audience an idea of the hard time he had along with his kid? Also the relationship with his wife is weird. The film does not explain why she makes one of the most important decisions a woman can make in a lifetime. Is she bad, or just weak?.\")[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1041,"status":"ok","timestamp":1678216810604,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"KE3TF2mGQ8cv","outputId":"a1c53982-1b7c-44ec-a6a5-44d5f03130f0"},"outputs":[{"data":{"text/plain":["{'document': [Annotation(document, 0, 445, The film didn't make me cry, or laugh, or even think about it. I left the theater the same way I went in. What about the screenplay? Is it necessary to repeat the same situation ten times just to give the audience an idea of the hard time he had along with his kid? Also the relationship with his wife is weird. The film does not explain why she makes one of the most important decisions a woman can make in a lifetime. Is she bad, or just weak?., {}, [])],\n"," 'token': [Annotation(token, 0, 2, The, {'sentence': '0'}, []),\n","  Annotation(token, 4, 7, film, {'sentence': '0'}, []),\n","  Annotation(token, 9, 14, didn't, {'sentence': '0'}, []),\n","  Annotation(token, 16, 19, make, {'sentence': '0'}, []),\n","  Annotation(token, 21, 22, me, {'sentence': '0'}, []),\n","  Annotation(token, 24, 26, cry, {'sentence': '0'}, []),\n","  Annotation(token, 27, 27, ,, {'sentence': '0'}, []),\n","  Annotation(token, 29, 30, or, {'sentence': '0'}, []),\n","  Annotation(token, 32, 36, laugh, {'sentence': '0'}, []),\n","  Annotation(token, 37, 37, ,, {'sentence': '0'}, []),\n","  Annotation(token, 39, 40, or, {'sentence': '0'}, []),\n","  Annotation(token, 42, 45, even, {'sentence': '0'}, []),\n","  Annotation(token, 47, 51, think, {'sentence': '0'}, []),\n","  Annotation(token, 53, 57, about, {'sentence': '0'}, []),\n","  Annotation(token, 59, 60, it, {'sentence': '0'}, []),\n","  Annotation(token, 61, 61, ., {'sentence': '0'}, []),\n","  Annotation(token, 63, 63, I, {'sentence': '0'}, []),\n","  Annotation(token, 65, 68, left, {'sentence': '0'}, []),\n","  Annotation(token, 70, 72, the, {'sentence': '0'}, []),\n","  Annotation(token, 74, 80, theater, {'sentence': '0'}, []),\n","  Annotation(token, 82, 84, the, {'sentence': '0'}, []),\n","  Annotation(token, 86, 89, same, {'sentence': '0'}, []),\n","  Annotation(token, 91, 93, way, {'sentence': '0'}, []),\n","  Annotation(token, 95, 95, I, {'sentence': '0'}, []),\n","  Annotation(token, 97, 100, went, {'sentence': '0'}, []),\n","  Annotation(token, 102, 103, in, {'sentence': '0'}, []),\n","  Annotation(token, 104, 104, ., {'sentence': '0'}, []),\n","  Annotation(token, 106, 109, What, {'sentence': '0'}, []),\n","  Annotation(token, 111, 115, about, {'sentence': '0'}, []),\n","  Annotation(token, 117, 119, the, {'sentence': '0'}, []),\n","  Annotation(token, 121, 130, screenplay, {'sentence': '0'}, []),\n","  Annotation(token, 131, 131, ?, {'sentence': '0'}, []),\n","  Annotation(token, 133, 134, Is, {'sentence': '0'}, []),\n","  Annotation(token, 136, 137, it, {'sentence': '0'}, []),\n","  Annotation(token, 139, 147, necessary, {'sentence': '0'}, []),\n","  Annotation(token, 149, 150, to, {'sentence': '0'}, []),\n","  Annotation(token, 152, 157, repeat, {'sentence': '0'}, []),\n","  Annotation(token, 159, 161, the, {'sentence': '0'}, []),\n","  Annotation(token, 163, 166, same, {'sentence': '0'}, []),\n","  Annotation(token, 168, 176, situation, {'sentence': '0'}, []),\n","  Annotation(token, 178, 180, ten, {'sentence': '0'}, []),\n","  Annotation(token, 182, 186, times, {'sentence': '0'}, []),\n","  Annotation(token, 188, 191, just, {'sentence': '0'}, []),\n","  Annotation(token, 193, 194, to, {'sentence': '0'}, []),\n","  Annotation(token, 196, 199, give, {'sentence': '0'}, []),\n","  Annotation(token, 201, 203, the, {'sentence': '0'}, []),\n","  Annotation(token, 205, 212, audience, {'sentence': '0'}, []),\n","  Annotation(token, 214, 215, an, {'sentence': '0'}, []),\n","  Annotation(token, 217, 220, idea, {'sentence': '0'}, []),\n","  Annotation(token, 222, 223, of, {'sentence': '0'}, []),\n","  Annotation(token, 225, 227, the, {'sentence': '0'}, []),\n","  Annotation(token, 229, 232, hard, {'sentence': '0'}, []),\n","  Annotation(token, 234, 237, time, {'sentence': '0'}, []),\n","  Annotation(token, 239, 240, he, {'sentence': '0'}, []),\n","  Annotation(token, 242, 244, had, {'sentence': '0'}, []),\n","  Annotation(token, 246, 250, along, {'sentence': '0'}, []),\n","  Annotation(token, 252, 255, with, {'sentence': '0'}, []),\n","  Annotation(token, 257, 259, his, {'sentence': '0'}, []),\n","  Annotation(token, 261, 263, kid, {'sentence': '0'}, []),\n","  Annotation(token, 264, 264, ?, {'sentence': '0'}, []),\n","  Annotation(token, 266, 269, Also, {'sentence': '0'}, []),\n","  Annotation(token, 271, 273, the, {'sentence': '0'}, []),\n","  Annotation(token, 275, 286, relationship, {'sentence': '0'}, []),\n","  Annotation(token, 288, 291, with, {'sentence': '0'}, []),\n","  Annotation(token, 293, 295, his, {'sentence': '0'}, []),\n","  Annotation(token, 297, 300, wife, {'sentence': '0'}, []),\n","  Annotation(token, 302, 303, is, {'sentence': '0'}, []),\n","  Annotation(token, 305, 309, weird, {'sentence': '0'}, []),\n","  Annotation(token, 310, 310, ., {'sentence': '0'}, []),\n","  Annotation(token, 312, 314, The, {'sentence': '0'}, []),\n","  Annotation(token, 316, 319, film, {'sentence': '0'}, []),\n","  Annotation(token, 321, 324, does, {'sentence': '0'}, []),\n","  Annotation(token, 326, 328, not, {'sentence': '0'}, []),\n","  Annotation(token, 330, 336, explain, {'sentence': '0'}, []),\n","  Annotation(token, 338, 340, why, {'sentence': '0'}, []),\n","  Annotation(token, 342, 344, she, {'sentence': '0'}, []),\n","  Annotation(token, 346, 350, makes, {'sentence': '0'}, []),\n","  Annotation(token, 352, 354, one, {'sentence': '0'}, []),\n","  Annotation(token, 356, 357, of, {'sentence': '0'}, []),\n","  Annotation(token, 359, 361, the, {'sentence': '0'}, []),\n","  Annotation(token, 363, 366, most, {'sentence': '0'}, []),\n","  Annotation(token, 368, 376, important, {'sentence': '0'}, []),\n","  Annotation(token, 378, 386, decisions, {'sentence': '0'}, []),\n","  Annotation(token, 388, 388, a, {'sentence': '0'}, []),\n","  Annotation(token, 390, 394, woman, {'sentence': '0'}, []),\n","  Annotation(token, 396, 398, can, {'sentence': '0'}, []),\n","  Annotation(token, 400, 403, make, {'sentence': '0'}, []),\n","  Annotation(token, 405, 406, in, {'sentence': '0'}, []),\n","  Annotation(token, 408, 408, a, {'sentence': '0'}, []),\n","  Annotation(token, 410, 417, lifetime, {'sentence': '0'}, []),\n","  Annotation(token, 418, 418, ., {'sentence': '0'}, []),\n","  Annotation(token, 420, 421, Is, {'sentence': '0'}, []),\n","  Annotation(token, 423, 425, she, {'sentence': '0'}, []),\n","  Annotation(token, 427, 429, bad, {'sentence': '0'}, []),\n","  Annotation(token, 430, 430, ,, {'sentence': '0'}, []),\n","  Annotation(token, 432, 433, or, {'sentence': '0'}, []),\n","  Annotation(token, 435, 438, just, {'sentence': '0'}, []),\n","  Annotation(token, 440, 443, weak, {'sentence': '0'}, []),\n","  Annotation(token, 444, 445, ?., {'sentence': '0'}, [])],\n"," 'class': [Annotation(category, 0, 445, neg, {'sentence': '0', 'neg': '0.9991806', 'pos': '8.193569E-4'}, [])]}"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["light_result"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1678216829481,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"byvVHvIjNZMP","outputId":"4395adf2-e5d5-4762-cc7b-147c09aa3341"},"outputs":[{"data":{"text/plain":["[Annotation(category, 0, 445, neg, {'sentence': '0', 'neg': '0.9991806', 'pos': '8.193569E-4'}, [])]"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["light_result[\"class\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":260,"status":"ok","timestamp":1678216838149,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"yHFqyFBxLXDz","outputId":"b27ba513-6550-470d-84c1-3fa37fdfea6c"},"outputs":[{"data":{"text/plain":["dict_keys(['document', 'token', 'class'])"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["light_result.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":244,"status":"ok","timestamp":1678216842025,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"v87NRma-RMfP","outputId":"ed41be1f-4eb8-4ae0-c04f-da943a2656f2"},"outputs":[{"data":{"text/plain":["['pos', 'neg']"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["tokenClassifier.getClasses()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"dCMmytV6yAqx"},"source":["# From HuggingFace to Spark NLP"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"G0VvywK7yQGZ"},"source":["Here you will learn how to export a model from HuggingFace to Spark NLP. \n","\n","For compatibility details and examples, check [this page](https://nlp.johnsnowlabs.com/docs/en/transformers#import-transformers-into-spark-nlp)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Lj0Qn9NMU3WS"},"source":["## Export and Save HuggingFace model"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"IzcpCv4hUnjW"},"source":["- Let's install `HuggingFace` and `TensorFlow`. You don't need `TensorFlow` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n","- We lock TensorFlow on `2.11.0` version and Transformers on `4.25.1`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11722,"status":"ok","timestamp":1678215994748,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"_sL6fqEZUpAD","outputId":"009c8a70-16cd-463c-825e-afe97f02caa8"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q transformers==4.25.1 tensorflow==2.11.0"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7R_06rkLVDU7"},"source":["- HuggingFace comes with a native `saved_model` feature inside `save_pretrained` function for TensorFlow based models. We will use that to save it as TF `SavedModel`.\n","- We'll use [finiteautomata/beto-sentiment-analysis](https://huggingface.co/finiteautomata/beto-sentiment-analysis) model from HuggingFace as an example\n","- In addition to `TFBertForSequenceClassification` we also need to save the `BertTokenizer`. This is the same for every model, these are assets needed for tokenization inside Spark NLP."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65937,"status":"ok","timestamp":1678217016977,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"ZLk0GtG1U-Cc","outputId":"441a61db-0e1d-47eb-b810-96e5906bc852"},"outputs":[{"name":"stdout","output_type":"stream","text":["try downloading TF weights\n","try downloading PyTorch weights\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n","- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n","WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 420). These functions will not be directly callable after loading.\n"]}],"source":["from transformers import TFBertForSequenceClassification, BertTokenizer\n","import tensorflow as tf\n","\n","MODEL_NAME = 'finiteautomata/beto-sentiment-analysis'\n","\n","tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n","tokenizer.save_pretrained('./{}_tokenizer/'.format(MODEL_NAME))\n","\n","# just in case if there is no TF/Keras file provided in the model\n","# we can just use `from_pt` and convert PyTorch to TensorFlow\n","try:\n","  print('try downloading TF weights')\n","  model = TFBertForSequenceClassification.from_pretrained(MODEL_NAME)\n","except:\n","  print('try downloading PyTorch weights')\n","  model = TFBertForSequenceClassification.from_pretrained(MODEL_NAME, from_pt=True)\n","\n","# Define TF Signature\n","@tf.function(\n","  input_signature=[\n","      {\n","          \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),\n","          \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),\n","          \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),\n","      }\n","  ]\n",")\n","\n","def serving_fn(input):\n","    return model(input)\n","\n","model.save_pretrained(\"./{}\".format(MODEL_NAME), saved_model=True, signatures={\"serving_default\": serving_fn})"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"FmvSh8HaVMmf"},"source":["‚û§ Let's have a look inside these two directories and see what we are dealing with:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":294,"status":"ok","timestamp":1678217112248,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"Jk1_iklEVOQ4","outputId":"c3eb9f3c-b35d-4604-eb61-edc25091a9d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 429416\n","-rw-r--r-- 1 root root       873 Mar  7 19:23 config.json\n","drwxr-xr-x 3 root root      4096 Mar  7 19:23 saved_model\n","-rw-r--r-- 1 root root 439713116 Mar  7 19:23 tf_model.h5\n"]}],"source":["!ls -l {MODEL_NAME}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1678217113022,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"DFX_PaTCVTp9","outputId":"9c377e96-6f65-4b19-825a-a604721f4bfe"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 9244\n","drwxr-xr-x 2 root root    4096 Mar  7 19:23 assets\n","-rw-r--r-- 1 root root      55 Mar  7 19:23 fingerprint.pb\n","-rw-r--r-- 1 root root  167033 Mar  7 19:23 keras_metadata.pb\n","-rw-r--r-- 1 root root 9282572 Mar  7 19:23 saved_model.pb\n","drwxr-xr-x 2 root root    4096 Mar  7 19:23 variables\n"]}],"source":["!ls -l {MODEL_NAME}/saved_model/1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1678217113724,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"HGRijgOKVVlA","outputId":"64ff363f-a497-4943-e70a-a6b7e74d9da1"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 252\n","-rw-r--r-- 1 root root     78 Mar  7 19:22 added_tokens.json\n","-rw-r--r-- 1 root root    125 Mar  7 19:22 special_tokens_map.json\n","-rw-r--r-- 1 root root    596 Mar  7 19:22 tokenizer_config.json\n","-rw-r--r-- 1 root root 241796 Mar  7 19:22 vocab.txt\n"]}],"source":["!ls -l {MODEL_NAME}_tokenizer"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8n7p2f_9VZF7"},"source":["- As you can see, we need the SavedModel from `saved_model/1/` path\n","- We also be needing `vocab.txt` from the tokenizer\n","- All we need is to just copy the `vocab.txt` to `saved_model/1/assets` which Spark NLP will look for\n","- In addition to vocabs, we also need `labels` and their `ids` which is saved inside the model's config. We will save this inside `labels.txt`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J7X5QXnQVZrG"},"outputs":[],"source":["asset_path = '{}/saved_model/1/assets'.format(MODEL_NAME)\n","\n","!cp {MODEL_NAME}_tokenizer/vocab.txt {asset_path}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XNmFd5MjViWr"},"outputs":[],"source":["# get label2id dictionary \n","labels = model.config.label2id\n","# sort the dictionary based on the id\n","labels = sorted(labels, key=labels.get)\n","\n","with open(asset_path+'/labels.txt', 'w') as f:\n","    f.write('\\n'.join(labels))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ku2x1FrgVpk2"},"source":["‚û§ We have our vocab.txt and labels.txt inside assets directory"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":241,"status":"ok","timestamp":1678217194398,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"c7Txfp1EVfSc","outputId":"261fcbdc-40ea-4e09-e015-5128d153524e"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 244\n","-rw-r--r-- 1 root root     11 Mar  7 19:26 labels.txt\n","-rw-r--r-- 1 root root 241796 Mar  7 19:26 vocab.txt\n"]}],"source":["! ls -l {MODEL_NAME}/saved_model/1/assets"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lZhRE_8YVzT8"},"source":["## Import and Save BertForTokenClassification in Spark NLP"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fj_spGECV_qJ"},"source":["- Let's use `loadSavedModel` functon in `BertForTokenClassification` which allows us to load TensorFlow model in SavedModel format\n","- Most params can be set later when you are loading this model in `BertForTokenClassification` in runtime like `setMaxSentenceLength`, so don't worry what you are setting them now\n","- `loadSavedModel` accepts two params, first is the path to the TF SavedModel. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n","- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Fq58ZROWBIP"},"outputs":[],"source":["sequenceClassifier = BertForSequenceClassification.loadSavedModel(\n","     '{}/saved_model/1'.format(MODEL_NAME),\n","     spark\n"," )\\\n","  .setInputCols([\"document\",'token'])\\\n","  .setOutputCol(\"class\")\\\n","  .setCaseSensitive(True)\\\n","  .setMaxSentenceLength(128)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_hiED6GPWKXA"},"source":["‚û§ Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fYy9YeP8WMWE"},"outputs":[],"source":["sequenceClassifier.write().overwrite().save(\"./{}_spark_nlp\".format(MODEL_NAME))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"iaKLAfCdWR6L"},"source":["‚û§ Let's clean up stuff we don't need anymore"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6SyHThihWTbi"},"outputs":[],"source":["!rm -rf {MODEL_NAME}_tokenizer {MODEL_NAME}"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"neXRoiilWYUt"},"source":["Awesome üòé  !\n","\n","This is your BertForTokenClassification model from HuggingFace ü§ó  loaded and saved by Spark NLP üöÄ "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":284,"status":"ok","timestamp":1678217274962,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"KfypxXw_WY17","outputId":"a4a4bc81-0eff-4127-c4fc-49d019c3874a"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 438116\n","-rw-r--r-- 1 root root 448618276 Mar  7 19:28 bert_classification_tensorflow\n","drwxr-xr-x 5 root root      4096 Mar  7 19:27 fields\n","drwxr-xr-x 2 root root      4096 Mar  7 19:27 metadata\n"]}],"source":["! ls -l {MODEL_NAME}_spark_nlp"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2NxNMAR3WdeL"},"source":["‚û§ Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny BertForTokenClassification model üòä"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q5U4Z08gWbi8"},"outputs":[],"source":["sequenceClassifier_loaded = BertForSequenceClassification.load(\"./{}_spark_nlp\".format(MODEL_NAME))\\\n","  .setInputCols([\"document\",'token'])\\\n","  .setOutputCol(\"class\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tFboH3euWp1h"},"source":["‚û§ That's it! You can now go wild and use hundreds of `BertForTokenClassification` models from HuggingFace ü§ó in Spark NLP üöÄ "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OojDEi2nWvGs"},"source":["‚û§ You can see what labels were used to train this model via getClasses function:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38,"status":"ok","timestamp":1678217458095,"user":{"displayName":"David Cecchini","userId":"15718514150394919649"},"user_tz":180},"id":"i4H6pnqXWh-4","outputId":"db6e3929-9385-40d0-86b1-c97e2f1840d4"},"outputs":[{"data":{"text/plain":["['NEU', 'POS', 'NEG']"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["sequenceClassifier_loaded.getClasses()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"p6KH_ePAW_cw"},"source":["‚û§ Cool! You can now go wild and use hundreds of BertForTokenClassification models from HuggingFace ü§ó in Spark NLP üöÄ"]}],"metadata":{"colab":{"provenance":[{"file_id":"1F0V0KqQgsqm0tH1TGzi5aDMjkzXY-cD9","timestamp":1677849547407},{"file_id":"12lyThli00zSf5cnWeFdaHRpW82qkCjLL","timestamp":1675707652743},{"file_id":"16jF-uGoCG3rBxKBGB4M52qbzxGEVkEiN","timestamp":1672945175159},{"file_id":"https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/5.3_Transformers_for_Sequence_Classification_in_Spark_NLP.ipynb","timestamp":1672052385231}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
