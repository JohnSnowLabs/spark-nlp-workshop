{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"mayxawD6v2h1"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/Spark_NLP_Udemy_MOOC/Open_Source/08.03.Chunk_Tokenizer.ipynb)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"K-xZ0UN7vw3T"},"source":["# ChunkTokenizer\n","In this notebook, we will examine the `ChunkTokenizer` annotator.\n","\n","This annotator tokenizes and flattens extracted NER chunks. The `ChunkTokenizer` will split the extracted NER CHUNK type annotations and will create TOKEN type annotations. The result is then flattened, and a single array is produced.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"08et-px0CLR3"},"source":["**üìñ Learning Objectives:**\n","\n","1. Understand how to split chunks into tokens in different ways.\n","\n","2. Become comfortable using the different parameters of the annotator.\n","\n","**üîó Helpful Links:**\n","\n","Python Documentation: [ChunkTokenizer](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/token/chunk_tokenizer/index.html#sparknlp.annotator.token.chunk_tokenizer.ChunkTokenizer)\n","\n","Scala Documentation: [ChunkTokenizer](https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/ChunkTokenizer.html)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7QgQvKZCCpgz"},"source":["## **üìú Background**\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"IDXaHOqCCqTz"},"source":["In some Spark NLP pipelines, this annotator is needed when there is a need for splitting chunks into tokens. \"ChunkTokenizer\" allows users to perform this operation with highly flexible features that can be set through different parameters."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XK-lgtuJDuQY"},"source":["## **üé¨ Colab Setup**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"11GMxflfvwnt"},"outputs":[],"source":["# Install PySpark and Spark NLP\n","! pip install -q pyspark==3.3.0 spark-nlp==4.2.6"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"elapsed":121422,"status":"ok","timestamp":1677537287589,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"ZBtUdt1cvWD5","outputId":"2c605c58-bdc3-44e1-87f5-10dff14d1bd2"},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://4e56be6c1b48:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.0</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Spark NLP</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f2f5a7b4dc0>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import sparknlp\n","from sparknlp.base import *\n","from sparknlp.annotator import *\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import StringType, IntegerType\n","\n","spark = sparknlp.start()\n","spark"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Kn-OGBy7yH8E"},"source":["## **üñ®Ô∏è Input/Output Annotation Types**\n","\n","- Input: `CHUNK` <br/>\n","- Output: `TOKEN`"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vR94UUxpD5Cf"},"source":["## **üîé Parameters**\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"360RiyXBzIYE"},"source":["- `caseSensitiveExceptions` *(Boolean)*: Whether to care for case sensitiveness in exceptions (Default: true)\n","- `contextChars` *(List[str])*: Character list used to separate from token boundaries (Default: Array(\".\", \",\", \";\", \":\", \"!\", \"?\", \"*\", \"-\", \"(\", \")\", \"\\\"\", \"'\"))\n","- `exceptions` *(List[str])*: Words that won't be affected by tokenization rules\n","- `exceptionsPath` *(String)*: Path to file containing list of exceptions\n","- `infixPatterns` *(List[str])*: Regex patterns that match tokens within a single target.\n","- `maxLength` *(Integer)*: Set the maximum allowed length for each token\n","- `minLength` *(Integer)*: Set the minimum allowed length for each token\n","- `prefixPattern` *(String)*: Regex with groups and begins with \\\\A to match target prefix. Overrides contextCharacters Param\n","\n","- `splitChars` *(List[str])*: Character list used to separate from the inside of tokens\n","\n","- `splitPattern` *(String)*: Pattern to separate from the inside of tokens. Takes priority over splitChars. This pattern will be applied to the tokens which where extracted with the target pattern previously\n","\n","- `suffixPattern` *(String)*: Regex with groups and ends with \\\\z to match target suffix.\n","\n","- `targetPattern`: *(String)*: Pattern to grab from text as token candidates."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"nRXkPCCEai8s"},"source":["Firstly, let's build a pipeline with default `ChunkTokenizer` parameters and see how it works. <br/>\n","\n","In the example pipeline, we will use the `ner_dl` pretrained NER model to detect named entities, then we will tokenize the chunks detected by the ner model by using `ChunkTokenizer`. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12470,"status":"ok","timestamp":1677240982586,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"dwlzebX3ZH17","outputId":"24dead2d-7bcf-4fef-8264-39b14cfaceb2"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------------------+\n","|text                                            |\n","+------------------------------------------------+\n","|Peter Parker is a nice lad and lives in New York|\n","+------------------------------------------------+\n","\n"]}],"source":["#Creating a sample data\n","\n","text = ['Peter Parker is a nice lad and lives in New York']\n","data_set = spark.createDataFrame(text, StringType()).toDF(\"text\")\n","data_set.show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31380,"status":"ok","timestamp":1677241013961,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"vo05Qu12ZH7z","outputId":"48acfcb8-1c9e-4274-d1c5-47d1faac756b"},"outputs":[{"name":"stdout","output_type":"stream","text":["glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n","ner_dl download started this may take some time.\n","Approximate size to download 13.6 MB\n","[OK!]\n"]}],"source":["document_assembler = DocumentAssembler()\\\n","    .setInputCol(\"text\")\\\n","    .setOutputCol(\"document\")\n","\n","tokenizer = Tokenizer()\\\n","    .setInputCols([\"document\"])\\\n","    .setOutputCol(\"token\")\n","\n","word_embeddings = WordEmbeddingsModel.pretrained()\\\n","    .setInputCols([\"document\", \"token\"])\\\n","    .setOutputCol(\"embeddings\")\n","\n","ner_tagger = NerDLModel.pretrained(\"ner_dl\", \"en\")\\\n","    .setInputCols([\"document\", \"token\", \"embeddings\"])\\\n","    .setOutputCol(\"ner\")\n","\n","ner_converter= NerConverter()\\\n","    .setInputCols(['document', 'token', 'ner'])\\\n","    .setOutputCol('ner_chunk')\n","\n","chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\")\n","\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9555,"status":"ok","timestamp":1677241023509,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"AufUYEl2YOfp","outputId":"873fe040-ac65-4dfd-ea38-afbf122d159d"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------+--------------------------+\n","|ner_chunk               |chunk_token               |\n","+------------------------+--------------------------+\n","|[Peter Parker, New York]|[Peter, Parker, New, York]|\n","+------------------------+--------------------------+\n","\n"]}],"source":["result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0pRG7FVabMUn"},"source":["As seen above, our chunks consist of two words and `ChunkTokenizer` split tokens. "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0fSuS4gbefBi"},"source":["### contextChars  \n","This parameter is used to set Character list to separate from token boundaries (Default: [\".\", \",\", \";\", \":\", \"!\", \"?\", \"*\", \"-\", \"(\", \")\", \"\"\", \"'\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":594,"status":"ok","timestamp":1677241540443,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"xCtriowOe4W5","outputId":"8b777894-0b4b-4981-9cb3-6b0090fba285"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------------------------------------------------+\n","|text                                                         |\n","+-------------------------------------------------------------+\n","|Peter Parker (Spiderman) is a nice guy and lives in New-York!|\n","+-------------------------------------------------------------+\n","\n"]}],"source":["#Creating a sample data\n","\n","text = ['Peter Parker (Spiderman) is a nice guy and lives in New-York!']\n","data_set = spark.createDataFrame(text, StringType()).toDF(\"text\")\n","data_set.show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1229,"status":"ok","timestamp":1677241655476,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"bzYAaeN9ee2m","outputId":"514ef63f-ac92-4074-86e1-4b42b0de89ac"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------+---------------------------------------+\n","|ner_chunk                           |chunk_token                            |\n","+------------------------------------+---------------------------------------+\n","|[Peter Parker, Spiderman, New-York!]|[Peter, Parker, Spiderman, New-York, !]|\n","+------------------------------------+---------------------------------------+\n","\n"]}],"source":["chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \\\n","     .setContextChars(['?', '!'])\\\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"OGcvGOPGZUxY"},"source":["As seen from the output, the exclamation mark was separated."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4AqlcjxVA2z2"},"source":["### infixPatterns\n","This parameter is used to set regex patterns that match tokens within a single target."]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":419,"status":"ok","timestamp":1677538554145,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"UXb9fp6QA2mV","outputId":"983edfd2-06b6-4043-9dd7-4e722d2a6aee"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------------------------------------------------------+\n","|text                                                                      |\n","+--------------------------------------------------------------------------+\n","|Peter Parker bookad a ticket for the concert. Ticket price is 100$ dollars|\n","+--------------------------------------------------------------------------+\n","\n"]}],"source":["#Creating a sample data\n","\n","text = ['Peter Parker bookad a ticket for the concert. Ticket price is 100$ dollars']\n","data_set = spark.createDataFrame(text, StringType()).toDF(\"text\")\n","data_set.show(truncate=False)"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5041,"status":"ok","timestamp":1677538563674,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"coZAfC_TBLj7","outputId":"353749cf-b9a3-4d08-cedf-57b341dcb8d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["roberta_base_token_classifier_ontonotes download started this may take some time.\n","Approximate size to download 434.7 MB\n","[OK!]\n"]}],"source":["document_assembler = DocumentAssembler() \\\n",".setInputCol('text') \\\n",".setOutputCol('document')\n","\n","tokenizer = Tokenizer() \\\n",".setInputCols(['document']) \\\n",".setOutputCol('token')\n","\n","tokenClassifier = RoBertaForTokenClassification \\\n",".pretrained('roberta_base_token_classifier_ontonotes', 'en') \\\n",".setInputCols(['token', 'document']) \\\n",".setOutputCol('ner') \\\n",".setCaseSensitive(True) \\\n",".setMaxSentenceLength(512)\n","\n","# since output column is IOB/IOB2 style, NerConverter can extract entities\n","ner_converter = NerConverter() \\\n",".setInputCols(['document', 'token', 'ner']) \\\n",".setOutputCol('entities')\n","\n","\n","chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"entities\"]) \\\n","     .setOutputCol(\"chunk_token\") \\\n","     .setInfixPatterns([\"(\\b\\d{3}\\b)\"])\n","\n","pipeline = Pipeline(stages=[\n","document_assembler, \n","tokenizer,\n","tokenClassifier,\n","ner_converter,\n","chunkTokenizer\n","])\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"f9x2rGCvGKPE"},"source":["WE DEFINED A REGEX FOR EXTRACTING 3 DIGITS NUMBERS <br/>\n","WE EXPECT TO SEE \"100\" AND \"$\" SEPERATELY "]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2461,"status":"ok","timestamp":1677538655418,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"BZNEx_yXF-E5","outputId":"26ceac49-e83d-4a4b-af47-400476ad40aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------------------+-----------------------------+\n","|ner_chunk                   |chunk_token                  |\n","+----------------------------+-----------------------------+\n","|[Peter Parker, 100$ dollars]|[Peter Parker, 100$, dollars]|\n","+----------------------------+-----------------------------+\n","\n"]}],"source":["result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"entities.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dKDUhoyQiCBi"},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gz3J-7WoAg2E"},"source":["### Exceptions \n","This parameter is used to choose words that won't be affected by tokenization rules."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":603,"status":"ok","timestamp":1677242695701,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"DSO7NBphAgs9","outputId":"72d9833b-ba4d-41a3-cf7b-c444590392c5"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------------------+\n","|text                                            |\n","+------------------------------------------------+\n","|Peter Parker is a nice lad and lives in New York|\n","+------------------------------------------------+\n","\n"]}],"source":["#Creating a sample data\n","\n","text = [\"Peter Parker is a nice lad and lives in New York\"]\n","data_set = spark.createDataFrame(text, StringType()).toDF(\"text\")\n","data_set.show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"p0sO0uCFp2Ut"},"source":["In some cases, you may not want some chunks to be splitted. Let's give \"New York\" to the `setExceptions()` parameter. Then, we expect to see that the \"New York\" will not be splitted into its tokens. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8605,"status":"ok","timestamp":1677242766501,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"FQle4UdKiN1U","outputId":"8e851b73-0540-4d50-fa13-10bdca50f691"},"outputs":[{"name":"stdout","output_type":"stream","text":["glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n","ner_dl download started this may take some time.\n","Approximate size to download 13.6 MB\n","[OK!]\n","+------------------------+-------------------------+\n","|ner_chunk               |chunk_token              |\n","+------------------------+-------------------------+\n","|[Peter Parker, New York]|[Peter, Parker, New York]|\n","+------------------------+-------------------------+\n","\n"]}],"source":["word_embeddings = WordEmbeddingsModel.pretrained()\\\n","    .setInputCols([\"document\", \"token\"])\\\n","    .setOutputCol(\"embeddings\")\n","\n","ner_tagger = NerDLModel.pretrained(\"ner_dl\", \"en\")\\\n","    .setInputCols([\"document\", \"token\", \"embeddings\"])\\\n","    .setOutputCol(\"ner\")\n","\n","ner_converter= NerConverter()\\\n","    .setInputCols(['document', 'token', 'ner'])\\\n","    .setOutputCol('ner_chunk')\n","\n","chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \\\n","     .setExceptions([\"New York\"])\n","\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ndGcqWMWqStY"},"source":["As seen above, `ChunkTokenizer` did not split \"New York\""]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"brPbmVohACX_"},"source":["### CaseSensitiveExceptions"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CPK-R92-ANsg"},"source":["This parameter is used to decide whether to care for case sensitiveness in exceptions (Default: true)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"k5DAHGkIpmUL"},"source":["Firstly, we will use the default value as **setCaseSensitiveExceptions(True)**. By doing this, we expect to see \"New York\" will be split into its tokens and exception will not work since we defined it as lowercased in the `setExceptions()`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1629,"status":"ok","timestamp":1677242792211,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"PffjTA_Ci-FN","outputId":"32c77838-8035-4baa-be67-cacb8a0a1641"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------+--------------------------+\n","|ner_chunk               |chunk_token               |\n","+------------------------+--------------------------+\n","|[Peter Parker, New York]|[Peter, Parker, New, York]|\n","+------------------------+--------------------------+\n","\n"]}],"source":["chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \\\n","     .setExceptions([\"new york\"]) \\\n","     .setCaseSensitiveExceptions(True)\n","\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lAvChlfiqkzO"},"source":["Now, we will define the value in the `setExceptions([\"new york\"])` as lowercased, and `setCaseSensitiveExceptions(False)`. Thus, we expect to see that \"New York\" will not be split into its tokens and our exception will work. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2278,"status":"ok","timestamp":1677242813143,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"IXx6SXyOjLP8","outputId":"3f6aa64c-0c9d-4477-e9e3-d4e64bc2d35a"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------+-------------------------+\n","|ner_chunk               |chunk_token              |\n","+------------------------+-------------------------+\n","|[Peter Parker, New York]|[Peter, Parker, New York]|\n","+------------------------+-------------------------+\n","\n"]}],"source":["chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \\\n","     .setExceptions([\"new york\"]) \\\n","     .setCaseSensitiveExceptions(False)\n","\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CZTJ3sfHsCzl"},"source":["As seen above, \"New York\" were not separated since we set `setCaseSensitiveExceptions(False)`. "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8GsZ1oz6AsS6"},"source":["### ExceptionsPath\n","This parameter is used to set a path to file containing list of exceptions."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hw4fW5gLtaHT"},"source":["First, we will create a txt file containing exceptions. Then, we will give the path of this file into `setExceptionsPath()` parameter. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gq3BLbRfvcpM"},"outputs":[],"source":["#Defining exceptions\n","exceptions= \"\"\"Peter Parker\n","James Murphy\n","Lucas Nelson\n","\"\"\"\n","\n","#open text file\n","text_file = open(\"exceptions.txt\", \"w\")\n"," \n","#write string to file\n","text_file.write(exceptions)\n"," \n","#close file\n","text_file.close()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mZd07wRwzLdT"},"source":["Building `ChunkTokenizer` with `ExceptionsPath`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1458,"status":"ok","timestamp":1677242835316,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"cOV8tLmDAr30","outputId":"425e8d1b-1047-49ef-f3c8-74f95b28e4be"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------+-------------------------+\n","|ner_chunk               |chunk_token              |\n","+------------------------+-------------------------+\n","|[Peter Parker, New York]|[Peter Parker, New, York]|\n","+------------------------+-------------------------+\n","\n"]}],"source":["chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \\\n","     .setExceptionsPath('exceptions.txt') \\\n","\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bXwXJqmFzXyX"},"source":["As you see, \"Peter Parker\" weren't split since it is defined in the *exceptions.txt* file. "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZX_x9E_Jz2hB"},"source":["### maxLength  \n","This parameter is used to set the maximum allowed length for each token."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":643,"status":"ok","timestamp":1677242850947,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"Kc-am5K4zmLq","outputId":"6475cd64-1e4a-4be6-cad2-9d71a412a783"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------------------------------------+\n","|text                                             |\n","+-------------------------------------------------+\n","|Peter Parker is a nice lad and lives in Minnesota|\n","+-------------------------------------------------+\n","\n"]}],"source":["#Creating a sample data\n","\n","text = [\"Peter Parker is a nice lad and lives in Minnesota\"]\n","data_set = spark.createDataFrame(text, StringType()).toDF(\"text\")\n","data_set.show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_Mazigzf4iCc"},"source":["Firstly we will build our pipeline without `MaxLenght` parameter. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1446,"status":"ok","timestamp":1677242857122,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"TGGdd4x4zmE5","outputId":"5c4dbabc-7479-4546-8677-ef53eec57e49"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------------+--------------------------+\n","|ner_chunk                |chunk_token               |\n","+-------------------------+--------------------------+\n","|[Peter Parker, Minnesota]|[Peter, Parker, Minnesota]|\n","+-------------------------+--------------------------+\n","\n"]}],"source":["chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gSLvS4dG4rgQ"},"source":["Now, we will set `setMaxLenght(7)` and we expect to not see \"Minnesota\" in the result since it has 9 characters. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1654,"status":"ok","timestamp":1677242864414,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"EXWWs7lCzFUN","outputId":"7f7cb004-2198-4946-af00-64370c6dd7d6"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------------+---------------+\n","|ner_chunk                |chunk_token    |\n","+-------------------------+---------------+\n","|[Peter Parker, Minnesota]|[Peter, Parker]|\n","+-------------------------+---------------+\n","\n"]}],"source":["chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \\\n","     .setMaxLength(7)\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"W-Q3avzZ46jv"},"source":["As you see, `ChunkTokenizer` did not accept \"Minnesota\" as a token because of its length. "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"br7EtwkF5K_S"},"source":["### minLength \n","This parameter is used to set the minimum allowed length for each token"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":311,"status":"ok","timestamp":1677242869571,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"nH373vT84Yjm","outputId":"bf3eb871-7afc-4ddc-8f92-fd67f9ec9492"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------------+\n","|text                                      |\n","+------------------------------------------+\n","|Peter Parker is a nice lad and lives in LA|\n","+------------------------------------------+\n","\n"]}],"source":["#Creating a sample data\n","\n","text = [\"Peter Parker is a nice lad and lives in LA\"]\n","data_set = spark.createDataFrame(text, StringType()).toDF(\"text\")\n","data_set.show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"H9ECd9r55oU8"},"source":["Firstly we will build our pipeline without MinLenght parameter."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1373,"status":"ok","timestamp":1677242872366,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"361G4JVG5SfB","outputId":"3b453f96-e1fb-4dda-9b0a-e21e4d9d513d"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------+-------------------+\n","|ner_chunk         |chunk_token        |\n","+------------------+-------------------+\n","|[Peter Parker, LA]|[Peter, Parker, LA]|\n","+------------------+-------------------+\n","\n"]}],"source":["chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \n","     \n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"X6oeWbjD5s2A"},"source":["Now, we will set `setMaxLenght(3)` and we expect to not see \"LA\" in the result since it has 2 characters."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1802,"status":"ok","timestamp":1677242876315,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"slRg-XMn0xHH","outputId":"5bc5d1d5-0422-4401-d2bf-1dfdeec9830b"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------+---------------+\n","|ner_chunk         |chunk_token    |\n","+------------------+---------------+\n","|[Peter Parker, LA]|[Peter, Parker]|\n","+------------------+---------------+\n","\n"]}],"source":["chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \\\n","     .setMinLength(3)\n","     \n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_ED-nzAI51UO"},"source":["As you see, `ChunkTokenizer` did not accept \"LA\" as a token because of its length."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Kv6HdYS56F2B"},"source":["### splitChars \n","Character list used to separate from the inside of tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":215,"status":"ok","timestamp":1677242884571,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"BOh6Lgzm6Mm7","outputId":"d931ecf5-58a0-4a39-97b2-5f6a76330449"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------------------+\n","|text                                            |\n","+------------------------------------------------+\n","|Peter Parker is a nice lad and lives in New-York|\n","+------------------------------------------------+\n","\n"]}],"source":["#Creating a sample data\n","\n","text = [\"Peter Parker is a nice lad and lives in New-York\"]\n","data_set = spark.createDataFrame(text, StringType()).toDF(\"text\")\n","data_set.show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1666,"status":"ok","timestamp":1677242891250,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"nWMmp0Ss90Xr","outputId":"b8315e7d-692b-46e0-dbd8-61f2c1160a0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------+-------------------------+\n","|ner_chunk               |chunk_token              |\n","+------------------------+-------------------------+\n","|[Peter Parker, New-York]|[Peter, Parker, New-York]|\n","+------------------------+-------------------------+\n","\n"]}],"source":["chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \n","          \n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ar1CuiUb9jQW"},"source":["Now we set `setSplitChars([\"-\"])`, therefore we expect to see \"New-York\" will be split from `-`. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1375,"status":"ok","timestamp":1677242896365,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"JwhBYpCE6FsM","outputId":"510ab74e-577b-4588-ed08-da6f5a3b14a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------+--------------------------+\n","|ner_chunk               |chunk_token               |\n","+------------------------+--------------------------+\n","|[Peter Parker, New-York]|[Peter, Parker, New, York]|\n","+------------------------+--------------------------+\n","\n"]}],"source":["chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \\\n","     .setSplitChars([\"-\"])\n","     \n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kzFkTsYm9-d0"},"source":["As see above, \"New-York\" were split into two tokens; \"New\" and \"York\""]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fiJj3NSikAyg"},"source":["### suffixPattern"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rbePvPQhkFPs"},"source":["This parameter is used to set regex with groups and ends with \\z to match target suffix."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":339,"status":"ok","timestamp":1677244997223,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"8dTdn-zUkbxE","outputId":"6524733b-ad4f-44a8-bf55-d0941bd1d707"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------------------------------------------------+\n","|text                                                         |\n","+-------------------------------------------------------------+\n","|Peter Parker (Spiderman) is a nice guy and lives in New-York!|\n","+-------------------------------------------------------------+\n","\n"]}],"source":["text = ['Peter Parker (Spiderman) is a nice guy and lives in New-York!']\n","\n","data_set = spark.createDataFrame(text, StringType()).toDF(\"text\")\n","data_set.show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Dd_I4LiInhwa"},"source":["A pipeline with no defined `.setSuffixPattern()`. <br/>\n","Check the chunk \"New-York!\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7430,"status":"ok","timestamp":1677245470745,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"JVWAg-UrnhgY","outputId":"80469c17-7879-4949-eaa7-ab073c271d3a"},"outputs":[{"name":"stdout","output_type":"stream","text":["glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n","ner_dl download started this may take some time.\n","Approximate size to download 13.6 MB\n","[OK!]\n","+------------------------------------+---------------------------------------+\n","|ner_chunk                           |chunk_token                            |\n","+------------------------------------+---------------------------------------+\n","|[Peter Parker (Spiderman), New-York]|[Peter, Parker, (Spiderman), New, York]|\n","+------------------------------------+---------------------------------------+\n","\n"]}],"source":["word_embeddings = WordEmbeddingsModel.pretrained()\\\n","    .setInputCols([\"document\", \"token\"])\\\n","    .setOutputCol(\"embeddings\")\n","\n","ner_tagger = NerDLModel.pretrained(\"ner_dl\", \"en\")\\\n","    .setInputCols([\"document\", \"token\", \"embeddings\"])\\\n","    .setOutputCol(\"ner\")\n","\n","ner_converter= NerConverter()\\\n","    .setInputCols(['document', 'token', 'ner'])\\\n","    .setOutputCol('ner_chunk')\n","\n","chunkTokenizer = ChunkTokenizer() \\\n","    .setInputCols([\"ner_chunk\"]) \\\n","    .setOutputCol(\"chunk_token\") \\\n","    .setSplitChars(['-']) \\\n","    .setContextChars(['?', '!'])\\\n","    .addException(\"New York\")\\\n","    .setCaseSensitiveExceptions(True)\n","\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GsG1XhBNnHuk"},"source":["A pipeline with defined `.setSuffixPattern(\"([a])\\z\")`. <br/>\n","Check the chunk \"New-York!\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7550,"status":"ok","timestamp":1677245281823,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"66ydsJbHkApY","outputId":"fc372707-5cd6-43c9-9c65-e66f33d7b5d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n","ner_dl download started this may take some time.\n","Approximate size to download 13.6 MB\n","[OK!]\n","+------------------------------------+--------------------------------------+\n","|ner_chunk                           |chunk_token                           |\n","+------------------------------------+--------------------------------------+\n","|[Peter Parker (Spiderman), New-York]|[Peter, Parker, (Spiderman), New-York]|\n","+------------------------------------+--------------------------------------+\n","\n"]}],"source":["word_embeddings = WordEmbeddingsModel.pretrained()\\\n","    .setInputCols([\"document\", \"token\"])\\\n","    .setOutputCol(\"embeddings\")\n","\n","ner_tagger = NerDLModel.pretrained(\"ner_dl\", \"en\")\\\n","    .setInputCols([\"document\", \"token\", \"embeddings\"])\\\n","    .setOutputCol(\"ner\")\n","\n","ner_converter= NerConverter()\\\n","    .setInputCols(['document', 'token', 'ner'])\\\n","    .setOutputCol('ner_chunk')\n","\n","chunkTokenizer = ChunkTokenizer() \\\n","    .setInputCols([\"ner_chunk\"]) \\\n","    .setOutputCol(\"chunk_token\") \\\n","    .setSuffixPattern(\"([a])\\z\")\\\n","    .setSplitChars(['-']) \\\n","    .setContextChars(['?', '!'])\\\n","    .addException(\"New York\")\\\n","    .setCaseSensitiveExceptions(True)\n","\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8eUffvIcjZrg"},"source":["### prefixPattern"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-G5icpwBjcZD"},"source":["This parameter is used to set regex with groups and begins with \\A to match target prefix. Overrides contextCharacters parameter."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8976,"status":"ok","timestamp":1677245520127,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"BzuLDvlIjZez","outputId":"9f224f5b-59f3-4e89-9ac5-9d0211216d55"},"outputs":[{"name":"stdout","output_type":"stream","text":["glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n","ner_dl download started this may take some time.\n","Approximate size to download 13.6 MB\n","[OK!]\n","+------------------------------------+--------------------------------------+\n","|ner_chunk                           |chunk_token                           |\n","+------------------------------------+--------------------------------------+\n","|[Peter Parker (Spiderman), New-York]|[Peter, Parker, (Spiderman), New-York]|\n","+------------------------------------+--------------------------------------+\n","\n"]}],"source":["word_embeddings = WordEmbeddingsModel.pretrained()\\\n","    .setInputCols([\"document\", \"token\"])\\\n","    .setOutputCol(\"embeddings\")\n","\n","ner_tagger = NerDLModel.pretrained(\"ner_dl\", \"en\")\\\n","    .setInputCols([\"document\", \"token\", \"embeddings\"])\\\n","    .setOutputCol(\"ner\")\n","\n","ner_converter= NerConverter()\\\n","    .setInputCols(['document', 'token', 'ner'])\\\n","    .setOutputCol('ner_chunk')\n","\n","chunkTokenizer = ChunkTokenizer() \\\n","    .setInputCols([\"ner_chunk\"]) \\\n","    .setOutputCol(\"chunk_token\") \\\n","    .setSuffixPattern(\"([a])\\z\")\\\n","    .setPrefixPattern(\"\\A([a])\")\\\n","    .setSplitChars(['-']) \\\n","    .setContextChars(['?', '!'])\\\n","    .addException(\"New York\")\\\n","    .setCaseSensitiveExceptions(True)\n","\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7543,"status":"ok","timestamp":1677245552082,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"mgaUGJjnn44J","outputId":"af969a5f-3273-4584-f62b-4bf75bd1844b"},"outputs":[{"name":"stdout","output_type":"stream","text":["glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n","ner_dl download started this may take some time.\n","Approximate size to download 13.6 MB\n","[OK!]\n","+------------------------------------+--------------------------------------+\n","|ner_chunk                           |chunk_token                           |\n","+------------------------------------+--------------------------------------+\n","|[Peter Parker (Spiderman), New-York]|[Peter, Parker, (Spiderman), New-York]|\n","+------------------------------------+--------------------------------------+\n","\n"]}],"source":["word_embeddings = WordEmbeddingsModel.pretrained()\\\n","    .setInputCols([\"document\", \"token\"])\\\n","    .setOutputCol(\"embeddings\")\n","\n","ner_tagger = NerDLModel.pretrained(\"ner_dl\", \"en\")\\\n","    .setInputCols([\"document\", \"token\", \"embeddings\"])\\\n","    .setOutputCol(\"ner\")\n","\n","ner_converter= NerConverter()\\\n","    .setInputCols(['document', 'token', 'ner'])\\\n","    .setOutputCol('ner_chunk')\n","\n","chunkTokenizer = ChunkTokenizer() \\\n","    .setInputCols([\"ner_chunk\"]) \\\n","    .setOutputCol(\"chunk_token\") \\\n","    .setSuffixPattern(\"([a])\\z\")\\\n","    .setPrefixPattern(\"\\A([a])\")\\\n","    .setSplitChars(['-']) \\\n","    .setContextChars(['?', '!'])\\\n","    .addException(\"New York\")\\\n","    .setCaseSensitiveExceptions(True)\n","\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pB9ewg93n4yd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":598,"status":"ok","timestamp":1677245574133,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"LQt0fHKbn4to","outputId":"74c323a7-6eae-4410-9bef-f7b15e8f6931"},"outputs":[{"data":{"text/plain":["[Row(result=['Peter', 'Parker', '(Spiderman)', 'is', 'a', 'nice', 'guy', 'and', 'lives', 'in', 'New York', 'but', 'has', 'no', 'e-mail!'])]"]},"execution_count":132,"metadata":{},"output_type":"execute_result"}],"source":["documenter = DocumentAssembler()\\\n","    .setInputCol(\"text\")\\\n","    .setOutputCol(\"document\")\n","\n","tokenizer = Tokenizer() \\\n","    .setInputCols([\"document\"]) \\\n","    .setOutputCol(\"token\") \\\n","    .setSuffixPattern(\"([a])\\z\")\\\n","    .setPrefixPattern(\"\\A([a])\")\\\n","    .setSplitChars(['-']) \\\n","    .setContextChars(['?', '!'])\\\n","    .addException(\"New York\")\\\n","    .setCaseSensitiveExceptions(True)\n","\n","nlpPipeline = Pipeline(stages=[documenter, \n","                               tokenizer])\n","\n","text = 'Peter Parker (Spiderman) is a nice guy and lives in New York but has no e-mail!'\n","\n","spark_df = spark.createDataFrame([[text]]).toDF(\"text\")\n","\n","result = nlpPipeline.fit(spark_df).transform(spark_df)\n","\n","result.select('token.result').take(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":476,"status":"ok","timestamp":1677245585133,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"Gq9O3jqnoHmY","outputId":"da2301b0-7e09-4901-ee1e-c157def80e91"},"outputs":[{"data":{"text/plain":["[Row(result=['Peter', 'Parker', '(Spiderman)', 'is', 'a', 'nice', 'guy', 'and', 'lives', 'in', 'New York', 'but', 'has', 'no', 'e-mail!'])]"]},"execution_count":133,"metadata":{},"output_type":"execute_result"}],"source":["documenter = DocumentAssembler()\\\n","    .setInputCol(\"text\")\\\n","    .setOutputCol(\"document\")\n","\n","tokenizer = Tokenizer() \\\n","    .setInputCols([\"document\"]) \\\n","    .setOutputCol(\"token\") \\\n","    .setSuffixPattern(\"([a])\\z\")\\\n","    .setSplitChars(['-']) \\\n","    .setContextChars(['?', '!'])\\\n","    .addException(\"New York\")\\\n","    .setCaseSensitiveExceptions(True)\n","\n","nlpPipeline = Pipeline(stages=[documenter, \n","                               tokenizer])\n","\n","text = 'Peter Parker (Spiderman) is a nice guy and lives in New York but has no e-mail!'\n","\n","spark_df = spark.createDataFrame([[text]]).toDF(\"text\")\n","\n","result = nlpPipeline.fit(spark_df).transform(spark_df)\n","\n","result.select('token.result').take(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cwyyBMdfoHhp"},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZJ1aOfjfopEI"},"source":["### targetPattern"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"sVBkUi_qoyip"},"source":["This parameter is used to set pattern to grab from text as token candidates."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":599,"status":"ok","timestamp":1677245843089,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"lG7WEJxlpHh1","outputId":"427083a4-4858-4178-996e-e7a238a5952b"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-------------------------------------------------------------+\n","|text                                                         |\n","+-------------------------------------------------------------+\n","|Peter Parker (Spiderman) is a nice guy and lives in New-York!|\n","+-------------------------------------------------------------+\n","\n"]}],"source":["text = ['Peter Parker (Spiderman) is a nice guy and lives in New-York!']\n","\n","data_set = spark.createDataFrame(text, StringType()).toDF(\"text\")\n","data_set.show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YstjYUkrqI_c"},"source":["A pipeline with no `.setTargetPattern()` defined. <br/>\n","Check the chunk \"New-York!\" "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7829,"status":"ok","timestamp":1677245913984,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"DYS3CLQLoox1","outputId":"119261b8-d202-47b7-c655-5e548d7b5048"},"outputs":[{"name":"stdout","output_type":"stream","text":["glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n","ner_dl download started this may take some time.\n","Approximate size to download 13.6 MB\n","[OK!]\n","+-------------------------------------+---------------------------------------------+\n","|ner_chunk                            |chunk_token                                  |\n","+-------------------------------------+---------------------------------------------+\n","|[Peter Parker (Spiderman), New-York!]|[Peter, Parker, (, Spiderman, ), New-York, !]|\n","+-------------------------------------+---------------------------------------------+\n","\n"]}],"source":["word_embeddings = WordEmbeddingsModel.pretrained()\\\n","    .setInputCols([\"document\", \"token\"])\\\n","    .setOutputCol(\"embeddings\")\n","\n","ner_tagger = NerDLModel.pretrained(\"ner_dl\", \"en\")\\\n","    .setInputCols([\"document\", \"token\", \"embeddings\"])\\\n","    .setOutputCol(\"ner\")\n","\n","ner_converter= NerConverter()\\\n","    .setInputCols(['document', 'token', 'ner'])\\\n","    .setOutputCol('ner_chunk')\n","\n","chunkTokenizer = ChunkTokenizer() \\\n","    .setInputCols([\"ner_chunk\"]) \\\n","    .setOutputCol(\"chunk_token\") \n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ygqMNjUMqWUG"},"source":["A pipeline with `.setTargetPattern(\"\\b\\w+!\\b\")` defined. <br/>\n","Check the chunk \"New-York!\" "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7469,"status":"ok","timestamp":1677246079453,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"_7ySUD-Zoosl","outputId":"ca16af75-6b79-48a0-b52f-d669947d435b"},"outputs":[{"name":"stdout","output_type":"stream","text":["glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n","ner_dl download started this may take some time.\n","Approximate size to download 13.6 MB\n","[OK!]\n","+-------------------------------------+----------------------------------------------+\n","|ner_chunk                            |chunk_token                                   |\n","+-------------------------------------+----------------------------------------------+\n","|[Peter Parker (Spiderman), New-York!]|[Peter, Parker, (, Spiderman, ), New, York, !]|\n","+-------------------------------------+----------------------------------------------+\n","\n"]}],"source":["word_embeddings = WordEmbeddingsModel.pretrained()\\\n","    .setInputCols([\"document\", \"token\"])\\\n","    .setOutputCol(\"embeddings\")\n","\n","ner_tagger = NerDLModel.pretrained(\"ner_dl\", \"en\")\\\n","    .setInputCols([\"document\", \"token\", \"embeddings\"])\\\n","    .setOutputCol(\"ner\")\n","\n","ner_converter= NerConverter()\\\n","    .setInputCols(['document', 'token', 'ner'])\\\n","    .setOutputCol('ner_chunk')\n","\n","chunkTokenizer = ChunkTokenizer() \\\n","    .setInputCols([\"ner_chunk\"]) \\\n","    .setOutputCol(\"chunk_token\") \\\n","    .setTargetPattern(\"\\b\\w+!\\b\")\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Q-B8cpH8rH8T"},"source":["### splitPattern"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tqc9cRPVrNIL"},"source":["This parameter is used to set pattern to separate from the inside of tokens. Takes priority over `splitChars`. This pattern will be applied to the tokens which where extracted with the target pattern previously."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":277,"status":"ok","timestamp":1677246550487,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"MA4Jahy7rlg2","outputId":"8d7171be-b896-4d06-e568-5dbbf67a588a"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------------------------+\n","|text                                                  |\n","+------------------------------------------------------+\n","|John Adam is a nice guy and visited to Washinton D.C.!|\n","+------------------------------------------------------+\n","\n"]}],"source":["text = ['John Adam is a nice guy and visited to Washinton D.C.!']\n","\n","data_set = spark.createDataFrame(text, StringType()).toDF(\"text\")\n","data_set.show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1888,"status":"ok","timestamp":1677246554256,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"},"user_tz":0},"id":"hpfkuEs-qdPd","outputId":"c63f80fa-1892-485b-def5-6e49263e1319"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------------------+--------------------------------+\n","|ner_chunk                   |chunk_token                     |\n","+----------------------------+--------------------------------+\n","|[John Adam, Washinton D.C.!]|[John, Adam, Washinton, D, C, !]|\n","+----------------------------+--------------------------------+\n","\n"]}],"source":["chunkTokenizer = ChunkTokenizer() \\\n","    .setInputCols([\"ner_chunk\"]) \\\n","    .setOutputCol(\"chunk_token\") \\\n","    .setTargetPattern(\"\\b\\w+!\\b\")\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dbbqW_jtqdKc"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gDIZt_mmqdFM"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vr1jhmDhjZV5"},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMpK2Na4sWmzPSDN5Z803US","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
