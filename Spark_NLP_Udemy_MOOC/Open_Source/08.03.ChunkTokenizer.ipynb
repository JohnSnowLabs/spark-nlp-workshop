{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMpK2Na4sWmzPSDN5Z803US"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]()"],"metadata":{"id":"mayxawD6v2h1"}},{"cell_type":"markdown","source":["# ChunkTokenizer\n","In this notebook, we will examine the `ChunkTokenizer` annotator.\n","\n","This annotator tokenizes and flattens extracted NER chunks. The `ChunkTokenizer` will split the extracted NER CHUNK type annotations and will create TOKEN type annotations. The result is then flattened, and a single array is produced.\n"],"metadata":{"id":"K-xZ0UN7vw3T"}},{"cell_type":"markdown","source":["**üìñ Learning Objectives:**\n","\n","1. Understand how to split chunks into tokens in different ways.\n","\n","2. Become comfortable using the different parameters of the annotator.\n","\n","**üîó Helpful Links:**\n","\n","Python Documentation: [ChunkTokenizer](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/token/chunk_tokenizer/index.html#sparknlp.annotator.token.chunk_tokenizer.ChunkTokenizer)\n","\n","Scala Documentation: [ChunkTokenizer](https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/ChunkTokenizer.html)\n"],"metadata":{"id":"08et-px0CLR3"}},{"cell_type":"markdown","source":["## **üìú Background**\n"],"metadata":{"id":"7QgQvKZCCpgz"}},{"cell_type":"markdown","source":["In some Spark NLP pipelines, this annotator is needed when there is a need for splitting chunks into tokens. \"ChunkTokenizer\" allows users to perform this operation with highly flexible features that can be set through different parameters."],"metadata":{"id":"IDXaHOqCCqTz"}},{"cell_type":"markdown","source":["## **üé¨ Colab Setup**"],"metadata":{"id":"XK-lgtuJDuQY"}},{"cell_type":"code","source":["# Install PySpark and Spark NLP\n","! pip install -q pyspark==3.3.0 spark-nlp==4.2.6"],"metadata":{"id":"11GMxflfvwnt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ZBtUdt1cvWD5","colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"status":"ok","timestamp":1677537287589,"user_tz":0,"elapsed":121422,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"2c605c58-bdc3-44e1-87f5-10dff14d1bd2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7f2f5a7b4dc0>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://4e56be6c1b48:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.0</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Spark NLP</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":2}],"source":["import sparknlp\n","from sparknlp.base import *\n","from sparknlp.annotator import *\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import StringType, IntegerType\n","\n","spark = sparknlp.start()\n","spark"]},{"cell_type":"markdown","source":["## **üñ®Ô∏è Input/Output Annotation Types**\n","\n","- Input: `CHUNK` <br/>\n","- Output: `TOKEN`"],"metadata":{"id":"Kn-OGBy7yH8E"}},{"cell_type":"markdown","source":["## **üîé Parameters**\n"],"metadata":{"id":"vR94UUxpD5Cf"}},{"cell_type":"markdown","source":["- `caseSensitiveExceptions` *(Boolean)*: Whether to care for case sensitiveness in exceptions (Default: true)\n","- `contextChars` *(List[str])*: Character list used to separate from token boundaries (Default: Array(\".\", \",\", \";\", \":\", \"!\", \"?\", \"*\", \"-\", \"(\", \")\", \"\\\"\", \"'\"))\n","- `exceptions` *(List[str])*: Words that won't be affected by tokenization rules\n","- `exceptionsPath` *(String)*: Path to file containing list of exceptions\n","- `infixPatterns` *(List[str])*: Regex patterns that match tokens within a single target.\n","- `maxLength` *(Integer)*: Set the maximum allowed length for each token\n","- `minLength` *(Integer)*: Set the minimum allowed length for each token\n","- `prefixPattern` *(String)*: Regex with groups and begins with \\\\A to match target prefix. Overrides contextCharacters Param\n","\n","- `splitChars` *(List[str])*: Character list used to separate from the inside of tokens\n","\n","- `splitPattern` *(String)*: Pattern to separate from the inside of tokens. Takes priority over splitChars. This pattern will be applied to the tokens which where extracted with the target pattern previously\n","\n","- `suffixPattern` *(String)*: Regex with groups and ends with \\\\z to match target suffix.\n","\n","- `targetPattern`: *(String)*: Pattern to grab from text as token candidates."],"metadata":{"id":"360RiyXBzIYE"}},{"cell_type":"markdown","source":["Firstly, let's build a pipeline with default `ChunkTokenizer` parameters and see how it works. <br/>\n","\n","In the example pipeline, we will use the `ner_dl` pretrained NER model to detect named entities, then we will tokenize the chunks detected by the ner model by using `ChunkTokenizer`. "],"metadata":{"id":"nRXkPCCEai8s"}},{"cell_type":"code","source":["#Creating a sample data\n","\n","text = ['Peter Parker is a nice lad and lives in New York']\n","data_set = spark.createDataFrame(text, StringType()).toDF(\"text\")\n","data_set.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dwlzebX3ZH17","executionInfo":{"status":"ok","timestamp":1677240982586,"user_tz":0,"elapsed":12470,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"24dead2d-7bcf-4fef-8264-39b14cfaceb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------------------------------+\n","|text                                            |\n","+------------------------------------------------+\n","|Peter Parker is a nice lad and lives in New York|\n","+------------------------------------------------+\n","\n"]}]},{"cell_type":"code","source":["document_assembler = DocumentAssembler()\\\n","    .setInputCol(\"text\")\\\n","    .setOutputCol(\"document\")\n","\n","tokenizer = Tokenizer()\\\n","    .setInputCols([\"document\"])\\\n","    .setOutputCol(\"token\")\n","\n","word_embeddings = WordEmbeddingsModel.pretrained()\\\n","    .setInputCols([\"document\", \"token\"])\\\n","    .setOutputCol(\"embeddings\")\n","\n","ner_tagger = NerDLModel.pretrained(\"ner_dl\", \"en\")\\\n","    .setInputCols([\"document\", \"token\", \"embeddings\"])\\\n","    .setOutputCol(\"ner\")\n","\n","ner_converter= NerConverter()\\\n","    .setInputCols(['document', 'token', 'ner'])\\\n","    .setOutputCol('ner_chunk')\n","\n","chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\")\n","\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vo05Qu12ZH7z","executionInfo":{"status":"ok","timestamp":1677241013961,"user_tz":0,"elapsed":31380,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"48acfcb8-1c9e-4274-d1c5-47d1faac756b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n","ner_dl download started this may take some time.\n","Approximate size to download 13.6 MB\n","[OK!]\n"]}]},{"cell_type":"code","source":["result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AufUYEl2YOfp","executionInfo":{"status":"ok","timestamp":1677241023509,"user_tz":0,"elapsed":9555,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"873fe040-ac65-4dfd-ea38-afbf122d159d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------+--------------------------+\n","|ner_chunk               |chunk_token               |\n","+------------------------+--------------------------+\n","|[Peter Parker, New York]|[Peter, Parker, New, York]|\n","+------------------------+--------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["As seen above, our chunks consist of two words and `ChunkTokenizer` split tokens. "],"metadata":{"id":"0pRG7FVabMUn"}},{"cell_type":"markdown","source":["### contextChars  \n","This parameter is used to set Character list to separate from token boundaries (Default: [\".\", \",\", \";\", \":\", \"!\", \"?\", \"*\", \"-\", \"(\", \")\", \"\"\", \"'\"])"],"metadata":{"id":"0fSuS4gbefBi"}},{"cell_type":"code","source":["#Creating a sample data\n","\n","text = ['Peter Parker (Spiderman) is a nice guy and lives in New-York!']\n","data_set = spark.createDataFrame(text, StringType()).toDF(\"text\")\n","data_set.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xCtriowOe4W5","executionInfo":{"status":"ok","timestamp":1677241540443,"user_tz":0,"elapsed":594,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"8b777894-0b4b-4981-9cb3-6b0090fba285"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------------------------------------------------+\n","|text                                                         |\n","+-------------------------------------------------------------+\n","|Peter Parker (Spiderman) is a nice guy and lives in New-York!|\n","+-------------------------------------------------------------+\n","\n"]}]},{"cell_type":"code","source":["chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \\\n","     .setContextChars(['?', '!'])\\\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bzYAaeN9ee2m","executionInfo":{"status":"ok","timestamp":1677241655476,"user_tz":0,"elapsed":1229,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"514ef63f-ac92-4074-86e1-4b42b0de89ac"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------------------+---------------------------------------+\n","|ner_chunk                           |chunk_token                            |\n","+------------------------------------+---------------------------------------+\n","|[Peter Parker, Spiderman, New-York!]|[Peter, Parker, Spiderman, New-York, !]|\n","+------------------------------------+---------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["As seen from the output, the exclamation mark was separated."],"metadata":{"id":"OGcvGOPGZUxY"}},{"cell_type":"markdown","source":["### infixPatterns\n","This parameter is used to set regex patterns that match tokens within a single target."],"metadata":{"id":"4AqlcjxVA2z2"}},{"cell_type":"code","source":["#Creating a sample data\n","\n","text = ['Peter Parker bookad a ticket for the concert. Ticket price is 100$ dollars']\n","data_set = spark.createDataFrame(text, StringType()).toDF(\"text\")\n","data_set.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UXb9fp6QA2mV","executionInfo":{"status":"ok","timestamp":1677538554145,"user_tz":0,"elapsed":419,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"983edfd2-06b6-4043-9dd7-4e722d2a6aee"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------------------------------------------------------------+\n","|text                                                                      |\n","+--------------------------------------------------------------------------+\n","|Peter Parker bookad a ticket for the concert. Ticket price is 100$ dollars|\n","+--------------------------------------------------------------------------+\n","\n"]}]},{"cell_type":"code","source":["document_assembler = DocumentAssembler() \\\n",".setInputCol('text') \\\n",".setOutputCol('document')\n","\n","tokenizer = Tokenizer() \\\n",".setInputCols(['document']) \\\n",".setOutputCol('token')\n","\n","tokenClassifier = RoBertaForTokenClassification \\\n",".pretrained('roberta_base_token_classifier_ontonotes', 'en') \\\n",".setInputCols(['token', 'document']) \\\n",".setOutputCol('ner') \\\n",".setCaseSensitive(True) \\\n",".setMaxSentenceLength(512)\n","\n","# since output column is IOB/IOB2 style, NerConverter can extract entities\n","ner_converter = NerConverter() \\\n",".setInputCols(['document', 'token', 'ner']) \\\n",".setOutputCol('entities')\n","\n","\n","chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"entities\"]) \\\n","     .setOutputCol(\"chunk_token\") \\\n","     .setInfixPatterns([\"(\\b\\d{3}\\b)\"])\n","\n","pipeline = Pipeline(stages=[\n","document_assembler, \n","tokenizer,\n","tokenClassifier,\n","ner_converter,\n","chunkTokenizer\n","])\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"coZAfC_TBLj7","executionInfo":{"status":"ok","timestamp":1677538563674,"user_tz":0,"elapsed":5041,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"353749cf-b9a3-4d08-cedf-57b341dcb8d7"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["roberta_base_token_classifier_ontonotes download started this may take some time.\n","Approximate size to download 434.7 MB\n","[OK!]\n"]}]},{"cell_type":"markdown","source":["WE DEFINED A REGEX FOR EXTRACTING 3 DIGITS NUMBERS <br/>\n","WE EXPECT TO SEE \"100\" AND \"$\" SEPERATELY "],"metadata":{"id":"f9x2rGCvGKPE"}},{"cell_type":"code","source":["result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"entities.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BZNEx_yXF-E5","executionInfo":{"status":"ok","timestamp":1677538655418,"user_tz":0,"elapsed":2461,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"26ceac49-e83d-4a4b-af47-400476ad40aa"},"execution_count":59,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------------------------+-----------------------------+\n","|ner_chunk                   |chunk_token                  |\n","+----------------------------+-----------------------------+\n","|[Peter Parker, 100$ dollars]|[Peter Parker, 100$, dollars]|\n","+----------------------------+-----------------------------+\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"dKDUhoyQiCBi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exceptions \n","This parameter is used to choose words that won't be affected by tokenization rules."],"metadata":{"id":"gz3J-7WoAg2E"}},{"cell_type":"code","source":["#Creating a sample data\n","\n","text = [\"Peter Parker is a nice lad and lives in New York\"]\n","data_set = spark.createDataFrame(text, StringType()).toDF(\"text\")\n","data_set.show(truncate=False)"],"metadata":{"id":"DSO7NBphAgs9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677242695701,"user_tz":0,"elapsed":603,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"72d9833b-ba4d-41a3-cf7b-c444590392c5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------------------------------+\n","|text                                            |\n","+------------------------------------------------+\n","|Peter Parker is a nice lad and lives in New York|\n","+------------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["In some cases, you may not want some chunks to be splitted. Let's give \"New York\" to the `setExceptions()` parameter. Then, we expect to see that the \"New York\" will not be splitted into its tokens. "],"metadata":{"id":"p0sO0uCFp2Ut"}},{"cell_type":"code","source":["word_embeddings = WordEmbeddingsModel.pretrained()\\\n","    .setInputCols([\"document\", \"token\"])\\\n","    .setOutputCol(\"embeddings\")\n","\n","ner_tagger = NerDLModel.pretrained(\"ner_dl\", \"en\")\\\n","    .setInputCols([\"document\", \"token\", \"embeddings\"])\\\n","    .setOutputCol(\"ner\")\n","\n","ner_converter= NerConverter()\\\n","    .setInputCols(['document', 'token', 'ner'])\\\n","    .setOutputCol('ner_chunk')\n","\n","chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \\\n","     .setExceptions([\"New York\"])\n","\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FQle4UdKiN1U","executionInfo":{"status":"ok","timestamp":1677242766501,"user_tz":0,"elapsed":8605,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"8e851b73-0540-4d50-fa13-10bdca50f691"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n","ner_dl download started this may take some time.\n","Approximate size to download 13.6 MB\n","[OK!]\n","+------------------------+-------------------------+\n","|ner_chunk               |chunk_token              |\n","+------------------------+-------------------------+\n","|[Peter Parker, New York]|[Peter, Parker, New York]|\n","+------------------------+-------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["As seen above, `ChunkTokenizer` did not split \"New York\""],"metadata":{"id":"ndGcqWMWqStY"}},{"cell_type":"markdown","source":["### CaseSensitiveExceptions"],"metadata":{"id":"brPbmVohACX_"}},{"cell_type":"markdown","source":["This parameter is used to decide whether to care for case sensitiveness in exceptions (Default: true)"],"metadata":{"id":"CPK-R92-ANsg"}},{"cell_type":"markdown","source":["Firstly, we will use the default value as **setCaseSensitiveExceptions(True)**. By doing this, we expect to see \"New York\" will be split into its tokens and exception will not work since we defined it as lowercased in the `setExceptions()`."],"metadata":{"id":"k5DAHGkIpmUL"}},{"cell_type":"code","source":["chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \\\n","     .setExceptions([\"new york\"]) \\\n","     .setCaseSensitiveExceptions(True)\n","\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PffjTA_Ci-FN","executionInfo":{"status":"ok","timestamp":1677242792211,"user_tz":0,"elapsed":1629,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"32c77838-8035-4baa-be67-cacb8a0a1641"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------+--------------------------+\n","|ner_chunk               |chunk_token               |\n","+------------------------+--------------------------+\n","|[Peter Parker, New York]|[Peter, Parker, New, York]|\n","+------------------------+--------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["Now, we will define the value in the `setExceptions([\"new york\"])` as lowercased, and `setCaseSensitiveExceptions(False)`. Thus, we expect to see that \"New York\" will not be split into its tokens and our exception will work. "],"metadata":{"id":"lAvChlfiqkzO"}},{"cell_type":"code","source":["chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \\\n","     .setExceptions([\"new york\"]) \\\n","     .setCaseSensitiveExceptions(False)\n","\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IXx6SXyOjLP8","executionInfo":{"status":"ok","timestamp":1677242813143,"user_tz":0,"elapsed":2278,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"3f6aa64c-0c9d-4477-e9e3-d4e64bc2d35a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------+-------------------------+\n","|ner_chunk               |chunk_token              |\n","+------------------------+-------------------------+\n","|[Peter Parker, New York]|[Peter, Parker, New York]|\n","+------------------------+-------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["As seen above, \"New York\" were not separated since we set `setCaseSensitiveExceptions(False)`. "],"metadata":{"id":"CZTJ3sfHsCzl"}},{"cell_type":"markdown","source":["### ExceptionsPath\n","This parameter is used to set a path to file containing list of exceptions."],"metadata":{"id":"8GsZ1oz6AsS6"}},{"cell_type":"markdown","source":["First, we will create a txt file containing exceptions. Then, we will give the path of this file into `setExceptionsPath()` parameter. "],"metadata":{"id":"hw4fW5gLtaHT"}},{"cell_type":"code","source":["#Defining exceptions\n","exceptions= \"\"\"Peter Parker\n","James Murphy\n","Lucas Nelson\n","\"\"\"\n","\n","#open text file\n","text_file = open(\"exceptions.txt\", \"w\")\n"," \n","#write string to file\n","text_file.write(exceptions)\n"," \n","#close file\n","text_file.close()"],"metadata":{"id":"gq3BLbRfvcpM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Building `ChunkTokenizer` with `ExceptionsPath`."],"metadata":{"id":"mZd07wRwzLdT"}},{"cell_type":"code","source":["chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \\\n","     .setExceptionsPath('exceptions.txt') \\\n","\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"],"metadata":{"id":"cOV8tLmDAr30","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677242835316,"user_tz":0,"elapsed":1458,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"425e8d1b-1047-49ef-f3c8-74f95b28e4be"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------+-------------------------+\n","|ner_chunk               |chunk_token              |\n","+------------------------+-------------------------+\n","|[Peter Parker, New York]|[Peter Parker, New, York]|\n","+------------------------+-------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["As you see, \"Peter Parker\" weren't split since it is defined in the *exceptions.txt* file. "],"metadata":{"id":"bXwXJqmFzXyX"}},{"cell_type":"markdown","source":["### maxLength  \n","This parameter is used to set the maximum allowed length for each token."],"metadata":{"id":"ZX_x9E_Jz2hB"}},{"cell_type":"code","source":["#Creating a sample data\n","\n","text = [\"Peter Parker is a nice lad and lives in Minnesota\"]\n","data_set = spark.createDataFrame(text, StringType()).toDF(\"text\")\n","data_set.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kc-am5K4zmLq","executionInfo":{"status":"ok","timestamp":1677242850947,"user_tz":0,"elapsed":643,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"6475cd64-1e4a-4be6-cad2-9d71a412a783"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------------------------------------+\n","|text                                             |\n","+-------------------------------------------------+\n","|Peter Parker is a nice lad and lives in Minnesota|\n","+-------------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["Firstly we will build our pipeline without `MaxLenght` parameter. "],"metadata":{"id":"_Mazigzf4iCc"}},{"cell_type":"code","source":["chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TGGdd4x4zmE5","executionInfo":{"status":"ok","timestamp":1677242857122,"user_tz":0,"elapsed":1446,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"5c4dbabc-7479-4546-8677-ef53eec57e49"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------------+--------------------------+\n","|ner_chunk                |chunk_token               |\n","+-------------------------+--------------------------+\n","|[Peter Parker, Minnesota]|[Peter, Parker, Minnesota]|\n","+-------------------------+--------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["Now, we will set `setMaxLenght(7)` and we expect to not see \"Minnesota\" in the result since it has 9 characters. "],"metadata":{"id":"gSLvS4dG4rgQ"}},{"cell_type":"code","source":["chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \\\n","     .setMaxLength(7)\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"],"metadata":{"id":"EXWWs7lCzFUN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677242864414,"user_tz":0,"elapsed":1654,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"7f7cb004-2198-4946-af00-64370c6dd7d6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------------+---------------+\n","|ner_chunk                |chunk_token    |\n","+-------------------------+---------------+\n","|[Peter Parker, Minnesota]|[Peter, Parker]|\n","+-------------------------+---------------+\n","\n"]}]},{"cell_type":"markdown","source":["As you see, `ChunkTokenizer` did not accept \"Minnesota\" as a token because of its length. "],"metadata":{"id":"W-Q3avzZ46jv"}},{"cell_type":"markdown","source":["### minLength \n","This parameter is used to set the minimum allowed length for each token"],"metadata":{"id":"br7EtwkF5K_S"}},{"cell_type":"code","source":["#Creating a sample data\n","\n","text = [\"Peter Parker is a nice lad and lives in LA\"]\n","data_set = spark.createDataFrame(text, StringType()).toDF(\"text\")\n","data_set.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nH373vT84Yjm","executionInfo":{"status":"ok","timestamp":1677242869571,"user_tz":0,"elapsed":311,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"bf3eb871-7afc-4ddc-8f92-fd67f9ec9492"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------------------------+\n","|text                                      |\n","+------------------------------------------+\n","|Peter Parker is a nice lad and lives in LA|\n","+------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["Firstly we will build our pipeline without MinLenght parameter."],"metadata":{"id":"H9ECd9r55oU8"}},{"cell_type":"code","source":["chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \n","     \n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"361G4JVG5SfB","executionInfo":{"status":"ok","timestamp":1677242872366,"user_tz":0,"elapsed":1373,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"3b453f96-e1fb-4dda-9b0a-e21e4d9d513d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------+-------------------+\n","|ner_chunk         |chunk_token        |\n","+------------------+-------------------+\n","|[Peter Parker, LA]|[Peter, Parker, LA]|\n","+------------------+-------------------+\n","\n"]}]},{"cell_type":"markdown","source":["Now, we will set `setMaxLenght(3)` and we expect to not see \"LA\" in the result since it has 2 characters."],"metadata":{"id":"X6oeWbjD5s2A"}},{"cell_type":"code","source":["chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \\\n","     .setMinLength(3)\n","     \n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"slRg-XMn0xHH","executionInfo":{"status":"ok","timestamp":1677242876315,"user_tz":0,"elapsed":1802,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"5bc5d1d5-0422-4401-d2bf-1dfdeec9830b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------+---------------+\n","|ner_chunk         |chunk_token    |\n","+------------------+---------------+\n","|[Peter Parker, LA]|[Peter, Parker]|\n","+------------------+---------------+\n","\n"]}]},{"cell_type":"markdown","source":["As you see, `ChunkTokenizer` did not accept \"LA\" as a token because of its length."],"metadata":{"id":"_ED-nzAI51UO"}},{"cell_type":"markdown","source":["### splitChars \n","Character list used to separate from the inside of tokens"],"metadata":{"id":"Kv6HdYS56F2B"}},{"cell_type":"code","source":["#Creating a sample data\n","\n","text = [\"Peter Parker is a nice lad and lives in New-York\"]\n","data_set = spark.createDataFrame(text, StringType()).toDF(\"text\")\n","data_set.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BOh6Lgzm6Mm7","executionInfo":{"status":"ok","timestamp":1677242884571,"user_tz":0,"elapsed":215,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"d931ecf5-58a0-4a39-97b2-5f6a76330449"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------------------------------+\n","|text                                            |\n","+------------------------------------------------+\n","|Peter Parker is a nice lad and lives in New-York|\n","+------------------------------------------------+\n","\n"]}]},{"cell_type":"code","source":["chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \n","          \n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nWMmp0Ss90Xr","executionInfo":{"status":"ok","timestamp":1677242891250,"user_tz":0,"elapsed":1666,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"b8315e7d-692b-46e0-dbd8-61f2c1160a0e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------+-------------------------+\n","|ner_chunk               |chunk_token              |\n","+------------------------+-------------------------+\n","|[Peter Parker, New-York]|[Peter, Parker, New-York]|\n","+------------------------+-------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["Now we set `setSplitChars([\"-\"])`, therefore we expect to see \"New-York\" will be split from `-`. "],"metadata":{"id":"Ar1CuiUb9jQW"}},{"cell_type":"code","source":["chunkTokenizer = ChunkTokenizer() \\\n","     .setInputCols([\"ner_chunk\"]) \\\n","     .setOutputCol(\"chunk_token\") \\\n","     .setSplitChars([\"-\"])\n","     \n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JwhBYpCE6FsM","executionInfo":{"status":"ok","timestamp":1677242896365,"user_tz":0,"elapsed":1375,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"510ab74e-577b-4588-ed08-da6f5a3b14a4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------+--------------------------+\n","|ner_chunk               |chunk_token               |\n","+------------------------+--------------------------+\n","|[Peter Parker, New-York]|[Peter, Parker, New, York]|\n","+------------------------+--------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["As see above, \"New-York\" were split into two tokens; \"New\" and \"York\""],"metadata":{"id":"kzFkTsYm9-d0"}},{"cell_type":"markdown","source":["### suffixPattern"],"metadata":{"id":"fiJj3NSikAyg"}},{"cell_type":"markdown","source":["This parameter is used to set regex with groups and ends with \\z to match target suffix."],"metadata":{"id":"rbePvPQhkFPs"}},{"cell_type":"code","source":["text = ['Peter Parker (Spiderman) is a nice guy and lives in New-York!']\n","\n","data_set = spark.createDataFrame(text, StringType()).toDF(\"text\")\n","data_set.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8dTdn-zUkbxE","executionInfo":{"status":"ok","timestamp":1677244997223,"user_tz":0,"elapsed":339,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"6524733b-ad4f-44a8-bf55-d0941bd1d707"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------------------------------------------------+\n","|text                                                         |\n","+-------------------------------------------------------------+\n","|Peter Parker (Spiderman) is a nice guy and lives in New-York!|\n","+-------------------------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["A pipeline with no defined `.setSuffixPattern()`. <br/>\n","Check the chunk \"New-York!\""],"metadata":{"id":"Dd_I4LiInhwa"}},{"cell_type":"code","source":["word_embeddings = WordEmbeddingsModel.pretrained()\\\n","    .setInputCols([\"document\", \"token\"])\\\n","    .setOutputCol(\"embeddings\")\n","\n","ner_tagger = NerDLModel.pretrained(\"ner_dl\", \"en\")\\\n","    .setInputCols([\"document\", \"token\", \"embeddings\"])\\\n","    .setOutputCol(\"ner\")\n","\n","ner_converter= NerConverter()\\\n","    .setInputCols(['document', 'token', 'ner'])\\\n","    .setOutputCol('ner_chunk')\n","\n","chunkTokenizer = ChunkTokenizer() \\\n","    .setInputCols([\"ner_chunk\"]) \\\n","    .setOutputCol(\"chunk_token\") \\\n","    .setSplitChars(['-']) \\\n","    .setContextChars(['?', '!'])\\\n","    .addException(\"New York\")\\\n","    .setCaseSensitiveExceptions(True)\n","\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JVWAg-UrnhgY","executionInfo":{"status":"ok","timestamp":1677245470745,"user_tz":0,"elapsed":7430,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"80469c17-7879-4949-eaa7-ab073c271d3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n","ner_dl download started this may take some time.\n","Approximate size to download 13.6 MB\n","[OK!]\n","+------------------------------------+---------------------------------------+\n","|ner_chunk                           |chunk_token                            |\n","+------------------------------------+---------------------------------------+\n","|[Peter Parker (Spiderman), New-York]|[Peter, Parker, (Spiderman), New, York]|\n","+------------------------------------+---------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["A pipeline with defined `.setSuffixPattern(\"([a])\\z\")`. <br/>\n","Check the chunk \"New-York!\""],"metadata":{"id":"GsG1XhBNnHuk"}},{"cell_type":"code","source":["word_embeddings = WordEmbeddingsModel.pretrained()\\\n","    .setInputCols([\"document\", \"token\"])\\\n","    .setOutputCol(\"embeddings\")\n","\n","ner_tagger = NerDLModel.pretrained(\"ner_dl\", \"en\")\\\n","    .setInputCols([\"document\", \"token\", \"embeddings\"])\\\n","    .setOutputCol(\"ner\")\n","\n","ner_converter= NerConverter()\\\n","    .setInputCols(['document', 'token', 'ner'])\\\n","    .setOutputCol('ner_chunk')\n","\n","chunkTokenizer = ChunkTokenizer() \\\n","    .setInputCols([\"ner_chunk\"]) \\\n","    .setOutputCol(\"chunk_token\") \\\n","    .setSuffixPattern(\"([a])\\z\")\\\n","    .setSplitChars(['-']) \\\n","    .setContextChars(['?', '!'])\\\n","    .addException(\"New York\")\\\n","    .setCaseSensitiveExceptions(True)\n","\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"66ydsJbHkApY","executionInfo":{"status":"ok","timestamp":1677245281823,"user_tz":0,"elapsed":7550,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"fc372707-5cd6-43c9-9c65-e66f33d7b5d0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n","ner_dl download started this may take some time.\n","Approximate size to download 13.6 MB\n","[OK!]\n","+------------------------------------+--------------------------------------+\n","|ner_chunk                           |chunk_token                           |\n","+------------------------------------+--------------------------------------+\n","|[Peter Parker (Spiderman), New-York]|[Peter, Parker, (Spiderman), New-York]|\n","+------------------------------------+--------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["### prefixPattern"],"metadata":{"id":"8eUffvIcjZrg"}},{"cell_type":"markdown","source":["This parameter is used to set regex with groups and begins with \\A to match target prefix. Overrides contextCharacters parameter."],"metadata":{"id":"-G5icpwBjcZD"}},{"cell_type":"code","source":["word_embeddings = WordEmbeddingsModel.pretrained()\\\n","    .setInputCols([\"document\", \"token\"])\\\n","    .setOutputCol(\"embeddings\")\n","\n","ner_tagger = NerDLModel.pretrained(\"ner_dl\", \"en\")\\\n","    .setInputCols([\"document\", \"token\", \"embeddings\"])\\\n","    .setOutputCol(\"ner\")\n","\n","ner_converter= NerConverter()\\\n","    .setInputCols(['document', 'token', 'ner'])\\\n","    .setOutputCol('ner_chunk')\n","\n","chunkTokenizer = ChunkTokenizer() \\\n","    .setInputCols([\"ner_chunk\"]) \\\n","    .setOutputCol(\"chunk_token\") \\\n","    .setSuffixPattern(\"([a])\\z\")\\\n","    .setPrefixPattern(\"\\A([a])\")\\\n","    .setSplitChars(['-']) \\\n","    .setContextChars(['?', '!'])\\\n","    .addException(\"New York\")\\\n","    .setCaseSensitiveExceptions(True)\n","\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BzuLDvlIjZez","executionInfo":{"status":"ok","timestamp":1677245520127,"user_tz":0,"elapsed":8976,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"9f224f5b-59f3-4e89-9ac5-9d0211216d55"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n","ner_dl download started this may take some time.\n","Approximate size to download 13.6 MB\n","[OK!]\n","+------------------------------------+--------------------------------------+\n","|ner_chunk                           |chunk_token                           |\n","+------------------------------------+--------------------------------------+\n","|[Peter Parker (Spiderman), New-York]|[Peter, Parker, (Spiderman), New-York]|\n","+------------------------------------+--------------------------------------+\n","\n"]}]},{"cell_type":"code","source":["word_embeddings = WordEmbeddingsModel.pretrained()\\\n","    .setInputCols([\"document\", \"token\"])\\\n","    .setOutputCol(\"embeddings\")\n","\n","ner_tagger = NerDLModel.pretrained(\"ner_dl\", \"en\")\\\n","    .setInputCols([\"document\", \"token\", \"embeddings\"])\\\n","    .setOutputCol(\"ner\")\n","\n","ner_converter= NerConverter()\\\n","    .setInputCols(['document', 'token', 'ner'])\\\n","    .setOutputCol('ner_chunk')\n","\n","chunkTokenizer = ChunkTokenizer() \\\n","    .setInputCols([\"ner_chunk\"]) \\\n","    .setOutputCol(\"chunk_token\") \\\n","    .setSuffixPattern(\"([a])\\z\")\\\n","    .setPrefixPattern(\"\\A([a])\")\\\n","    .setSplitChars(['-']) \\\n","    .setContextChars(['?', '!'])\\\n","    .addException(\"New York\")\\\n","    .setCaseSensitiveExceptions(True)\n","\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mgaUGJjnn44J","executionInfo":{"status":"ok","timestamp":1677245552082,"user_tz":0,"elapsed":7543,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"af969a5f-3273-4584-f62b-4bf75bd1844b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n","ner_dl download started this may take some time.\n","Approximate size to download 13.6 MB\n","[OK!]\n","+------------------------------------+--------------------------------------+\n","|ner_chunk                           |chunk_token                           |\n","+------------------------------------+--------------------------------------+\n","|[Peter Parker (Spiderman), New-York]|[Peter, Parker, (Spiderman), New-York]|\n","+------------------------------------+--------------------------------------+\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"pB9ewg93n4yd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["documenter = DocumentAssembler()\\\n","    .setInputCol(\"text\")\\\n","    .setOutputCol(\"document\")\n","\n","tokenizer = Tokenizer() \\\n","    .setInputCols([\"document\"]) \\\n","    .setOutputCol(\"token\") \\\n","    .setSuffixPattern(\"([a])\\z\")\\\n","    .setPrefixPattern(\"\\A([a])\")\\\n","    .setSplitChars(['-']) \\\n","    .setContextChars(['?', '!'])\\\n","    .addException(\"New York\")\\\n","    .setCaseSensitiveExceptions(True)\n","\n","nlpPipeline = Pipeline(stages=[documenter, \n","                               tokenizer])\n","\n","text = 'Peter Parker (Spiderman) is a nice guy and lives in New York but has no e-mail!'\n","\n","spark_df = spark.createDataFrame([[text]]).toDF(\"text\")\n","\n","result = nlpPipeline.fit(spark_df).transform(spark_df)\n","\n","result.select('token.result').take(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LQt0fHKbn4to","executionInfo":{"status":"ok","timestamp":1677245574133,"user_tz":0,"elapsed":598,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"74c323a7-6eae-4410-9bef-f7b15e8f6931"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(result=['Peter', 'Parker', '(Spiderman)', 'is', 'a', 'nice', 'guy', 'and', 'lives', 'in', 'New York', 'but', 'has', 'no', 'e-mail!'])]"]},"metadata":{},"execution_count":132}]},{"cell_type":"code","source":["documenter = DocumentAssembler()\\\n","    .setInputCol(\"text\")\\\n","    .setOutputCol(\"document\")\n","\n","tokenizer = Tokenizer() \\\n","    .setInputCols([\"document\"]) \\\n","    .setOutputCol(\"token\") \\\n","    .setSuffixPattern(\"([a])\\z\")\\\n","    .setSplitChars(['-']) \\\n","    .setContextChars(['?', '!'])\\\n","    .addException(\"New York\")\\\n","    .setCaseSensitiveExceptions(True)\n","\n","nlpPipeline = Pipeline(stages=[documenter, \n","                               tokenizer])\n","\n","text = 'Peter Parker (Spiderman) is a nice guy and lives in New York but has no e-mail!'\n","\n","spark_df = spark.createDataFrame([[text]]).toDF(\"text\")\n","\n","result = nlpPipeline.fit(spark_df).transform(spark_df)\n","\n","result.select('token.result').take(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gq9O3jqnoHmY","executionInfo":{"status":"ok","timestamp":1677245585133,"user_tz":0,"elapsed":476,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"da2301b0-7e09-4901-ee1e-c157def80e91"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(result=['Peter', 'Parker', '(Spiderman)', 'is', 'a', 'nice', 'guy', 'and', 'lives', 'in', 'New York', 'but', 'has', 'no', 'e-mail!'])]"]},"metadata":{},"execution_count":133}]},{"cell_type":"code","source":[],"metadata":{"id":"cwyyBMdfoHhp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### targetPattern"],"metadata":{"id":"ZJ1aOfjfopEI"}},{"cell_type":"markdown","source":["This parameter is used to set pattern to grab from text as token candidates."],"metadata":{"id":"sVBkUi_qoyip"}},{"cell_type":"code","source":["text = ['Peter Parker (Spiderman) is a nice guy and lives in New-York!']\n","\n","data_set = spark.createDataFrame(text, StringType()).toDF(\"text\")\n","data_set.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lG7WEJxlpHh1","executionInfo":{"status":"ok","timestamp":1677245843089,"user_tz":0,"elapsed":599,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"427083a4-4858-4178-996e-e7a238a5952b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------------------------------------------------+\n","|text                                                         |\n","+-------------------------------------------------------------+\n","|Peter Parker (Spiderman) is a nice guy and lives in New-York!|\n","+-------------------------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["A pipeline with no `.setTargetPattern()` defined. <br/>\n","Check the chunk \"New-York!\" "],"metadata":{"id":"YstjYUkrqI_c"}},{"cell_type":"code","source":["word_embeddings = WordEmbeddingsModel.pretrained()\\\n","    .setInputCols([\"document\", \"token\"])\\\n","    .setOutputCol(\"embeddings\")\n","\n","ner_tagger = NerDLModel.pretrained(\"ner_dl\", \"en\")\\\n","    .setInputCols([\"document\", \"token\", \"embeddings\"])\\\n","    .setOutputCol(\"ner\")\n","\n","ner_converter= NerConverter()\\\n","    .setInputCols(['document', 'token', 'ner'])\\\n","    .setOutputCol('ner_chunk')\n","\n","chunkTokenizer = ChunkTokenizer() \\\n","    .setInputCols([\"ner_chunk\"]) \\\n","    .setOutputCol(\"chunk_token\") \n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DYS3CLQLoox1","executionInfo":{"status":"ok","timestamp":1677245913984,"user_tz":0,"elapsed":7829,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"119261b8-d202-47b7-c655-5e548d7b5048"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n","ner_dl download started this may take some time.\n","Approximate size to download 13.6 MB\n","[OK!]\n","+-------------------------------------+---------------------------------------------+\n","|ner_chunk                            |chunk_token                                  |\n","+-------------------------------------+---------------------------------------------+\n","|[Peter Parker (Spiderman), New-York!]|[Peter, Parker, (, Spiderman, ), New-York, !]|\n","+-------------------------------------+---------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["A pipeline with `.setTargetPattern(\"\\b\\w+!\\b\")` defined. <br/>\n","Check the chunk \"New-York!\" "],"metadata":{"id":"ygqMNjUMqWUG"}},{"cell_type":"code","source":["word_embeddings = WordEmbeddingsModel.pretrained()\\\n","    .setInputCols([\"document\", \"token\"])\\\n","    .setOutputCol(\"embeddings\")\n","\n","ner_tagger = NerDLModel.pretrained(\"ner_dl\", \"en\")\\\n","    .setInputCols([\"document\", \"token\", \"embeddings\"])\\\n","    .setOutputCol(\"ner\")\n","\n","ner_converter= NerConverter()\\\n","    .setInputCols(['document', 'token', 'ner'])\\\n","    .setOutputCol('ner_chunk')\n","\n","chunkTokenizer = ChunkTokenizer() \\\n","    .setInputCols([\"ner_chunk\"]) \\\n","    .setOutputCol(\"chunk_token\") \\\n","    .setTargetPattern(\"\\b\\w+!\\b\")\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_7ySUD-Zoosl","executionInfo":{"status":"ok","timestamp":1677246079453,"user_tz":0,"elapsed":7469,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"ca16af75-6b79-48a0-b52f-d669947d435b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n","ner_dl download started this may take some time.\n","Approximate size to download 13.6 MB\n","[OK!]\n","+-------------------------------------+----------------------------------------------+\n","|ner_chunk                            |chunk_token                                   |\n","+-------------------------------------+----------------------------------------------+\n","|[Peter Parker (Spiderman), New-York!]|[Peter, Parker, (, Spiderman, ), New, York, !]|\n","+-------------------------------------+----------------------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["### splitPattern"],"metadata":{"id":"Q-B8cpH8rH8T"}},{"cell_type":"markdown","source":["This parameter is used to set pattern to separate from the inside of tokens. Takes priority over `splitChars`. This pattern will be applied to the tokens which where extracted with the target pattern previously."],"metadata":{"id":"tqc9cRPVrNIL"}},{"cell_type":"code","source":["text = ['John Adam is a nice guy and visited to Washinton D.C.!']\n","\n","data_set = spark.createDataFrame(text, StringType()).toDF(\"text\")\n","data_set.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MA4Jahy7rlg2","executionInfo":{"status":"ok","timestamp":1677246550487,"user_tz":0,"elapsed":277,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"8d7171be-b896-4d06-e568-5dbbf67a588a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+------------------------------------------------------+\n","|text                                                  |\n","+------------------------------------------------------+\n","|John Adam is a nice guy and visited to Washinton D.C.!|\n","+------------------------------------------------------+\n","\n"]}]},{"cell_type":"code","source":["chunkTokenizer = ChunkTokenizer() \\\n","    .setInputCols([\"ner_chunk\"]) \\\n","    .setOutputCol(\"chunk_token\") \\\n","    .setTargetPattern(\"\\b\\w+!\\b\")\n","\n","pipeline = Pipeline().setStages([\n","    document_assembler,\n","    tokenizer,\n","    word_embeddings,\n","    ner_tagger,\n","    ner_converter,\n","    chunkTokenizer\n","])\n","\n","result = pipeline.fit(data_set).transform(data_set)\n","result.selectExpr(\"ner_chunk.result as ner_chunk\" , \"chunk_token.result as chunk_token\").show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hpfkuEs-qdPd","executionInfo":{"status":"ok","timestamp":1677246554256,"user_tz":0,"elapsed":1888,"user":{"displayName":"Ahmet Emin Tek","userId":"14855809472179427810"}},"outputId":"c63f80fa-1892-485b-def5-6e49263e1319"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------------------------+--------------------------------+\n","|ner_chunk                   |chunk_token                     |\n","+----------------------------+--------------------------------+\n","|[John Adam, Washinton D.C.!]|[John, Adam, Washinton, D, C, !]|\n","+----------------------------+--------------------------------+\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"dbbqW_jtqdKc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gDIZt_mmqdFM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Vr1jhmDhjZV5"},"execution_count":null,"outputs":[]}]}