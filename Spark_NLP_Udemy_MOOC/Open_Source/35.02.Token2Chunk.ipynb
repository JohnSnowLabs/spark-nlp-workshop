{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"sXatvRX899i0"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/Spark_NLP_Udemy_MOOC/Open_Source/35.02.Token2Chunk.ipynb)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"AOn8d1tcBkK3"},"source":["# **Token2Chunk with SparkNLP**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"WimihSwD3vtH"},"source":["This notebook will cover the different parameters and usages of `Token2Chunk`.\n","\n","**üìñ Learning Objectives:**\n","\n","1. Understand how converts token type annotations to chunk type with this annotator.\n","\n","2. Become comfortable using the different parameters of the annotator.\n","\n","\n","**üîó Helpful Links:**\n","\n","- Documentation : [Token2Chunk](https://nlp.johnsnowlabs.com/docs/en/annotators#token2chunk)\n","\n","- Python Docs : [Token2Chunk](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/base/token2_chunk/index.html#sparknlp.base.token2_chunk.Token2Chunk)\n","\n","- Scala Docs : [Token2Chunk](https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/Token2Chunk.html)\n","\n","- For extended examples of usage, see the [Spark NLP Workshop repository](https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/open-source-nlp)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qL9lcISyFSLv"},"source":["## **üìú Background**\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"TjDKOoZ4Fc8G"},"source":["Token2Chunk can convert `token` type annotations to `chunk` type. This can be useful if a entities have been already extracted as token and following annotators require chunk types.\n","\n","\n","We can use the Token2Chunk annotator, for example, before the ChunkMapper annotator from our healthcare library. Because ChunkMapper annotator needs chunk type inputs. Before this annotator, we can convert the tokens to chunks using the Token2Chunk annotator and we get the inputs that ChunkMapper needs."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"MfkkKkbVF309"},"source":["## **üé¨ Colab Setup**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iMkMQtZNF2n-"},"outputs":[],"source":["!pip install -q pyspark==3.1.2 spark-nlp==4.2.4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NulWi4_f4GN5"},"outputs":[],"source":["import sparknlp\n","from sparknlp.base import *\n","from sparknlp.annotator import *\n","\n","spark = sparknlp.start()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9Fbbk1bqcuA5"},"source":["## **üñ®Ô∏è Input/Output Annotation Types**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0yFIrr5acsiU"},"source":["- Input: `TOKEN`\n","\n","- Output: `CHUNK`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":901,"status":"ok","timestamp":1677945189239,"user":{"displayName":"Gursev Pirge","userId":"01579888832874245157"},"user_tz":300},"id":"C5IbTr6pO6LM","outputId":"a9c070b0-0dd0-422b-d9dc-c498b047a974"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------------------------+----------------------------+\n","|token                        |annotatorType               |\n","+-----------------------------+----------------------------+\n","|[change, object, nature, sea]|[token, token, token, token]|\n","+-----------------------------+----------------------------+\n","\n","+-----------------------------+----------------------------+\n","|chunk                        |annotatorType               |\n","+-----------------------------+----------------------------+\n","|[change, object, nature, sea]|[chunk, chunk, chunk, chunk]|\n","+-----------------------------+----------------------------+\n","\n"]}],"source":["documentAssembler = DocumentAssembler() \\\n","    .setInputCol(\"text\") \\\n","    .setOutputCol(\"document\")\n","\n","tokenizer = Tokenizer() \\\n","    .setInputCols([\"document\"]) \\\n","    .setOutputCol(\"token\")\n","\n","token2chunk = Token2Chunk() \\\n","    .setInputCols([\"token\"]) \\\n","    .setOutputCol(\"chunk\")\n","\n","pipeline = Pipeline().setStages([\n","    documentAssembler,\n","    tokenizer,\n","    token2chunk\n","])\n","\n","data = spark.createDataFrame([[\"change object nature sea\"]]).toDF(\"text\")\n","result = pipeline.fit(data).transform(data)\n","\n","\n","result.selectExpr(\"token.result as token\", \"token.annotatorType\").show(truncate=False)\n","result.selectExpr(\"chunk.result as chunk\", \"chunk.annotatorType\").show(truncate=False)\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qbbSx6kOSqeU"},"source":["As you can see above, the tokens of the token type, which is the output of the tokenizer annotator, have been converted to chunk type with Token2chunk annotator. So, for the next annotator that needs a chunk-type input, this conversion need has been resolved."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gBlQHPAQnrjL"},"source":["That's all!! With this you can use power of Spark NLP to convert your tokens to chunks.üí™üèª\n","\n","For additional information, don't hesitate to consult the above references.‚òòÔ∏è"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1VVV4jTagH47UZiKFqXoP-Abq5ozZM1BV","timestamp":1674419633087},{"file_id":"https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb","timestamp":1671914287039}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":0}
