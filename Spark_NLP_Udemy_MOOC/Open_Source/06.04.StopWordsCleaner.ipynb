{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"fvlqyQENHOF7"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/Spark_NLP_Udemy_MOOC/Open_Source/06.04.StopWordsCleaner.ipynb)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2KRV5TCyHSRD"},"source":["# **StopWordsCleaner**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aKJfNwNJHX4X"},"source":["This notebook will cover the different parameters and usages of `StopWordsCleaner`.\n","\n","**📖 Learning Objectives:**\n","\n","1. Understand how to drop stop words from the input sequences.\n","\n","2. Become comfortable using the different parameters of the annotator.\n","\n","3. How to use pretrained StopWordsCleaner models.\n","\n","\n","**🔗 Helpful Links:**\n","\n","- Documentation : [StopWordsCleaner](https://nlp.johnsnowlabs.com/docs/en/annotators#stopwordscleaner)\n","\n","- Python Docs : [StopWordsCleaner](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/stop_words_cleaner/index.html)\n","\n","- Scala Docs : [StopWordsCleaner](https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/StopWordsCleaner.html)\n","\n","- For extended examples of usage, see the [Spark NLP Workshop repository](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/2.Text_Preprocessing_with_SparkNLP_Annotators_Transformers.ipynb)."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YysCjCsdHYrp"},"source":["## **📜 Background**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"IkR4_d-BHZbO"},"source":["This annotator takes a sequence of strings (e.g. the output of a Tokenizer, Normalizer, Lemmatizer, and Stemmer) and drops all the stop words from the input sequences.\n","\n","By default, it uses stop words from MLlibs StopWordsRemover. <https://spark.apache.org/docs/latest/ml-features#stopwordsremover>\n","\n","Stop words can also be defined by explicitly setting them with setStopWords([String]) or loaded from pretrained models using pretrained of its companion object.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"66vCSY8EHd9n"},"source":["## **🎬 Colab Setup**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33001,"status":"ok","timestamp":1673446455020,"user":{"displayName":"Arshaan","userId":"17964254249366085303"},"user_tz":-330},"id":"wkcEJOcRQqjf","outputId":"01556f2f-9ef5-4286-d091-115a66d29196"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.4/212.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m448.4/448.4 KB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.6/198.6 KB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q pyspark==3.1.2  spark-nlp==4.2.4"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"elapsed":36412,"status":"ok","timestamp":1673446491424,"user":{"displayName":"Arshaan","userId":"17964254249366085303"},"user_tz":-330},"id":"4aYyKjYCQqqE","outputId":"787b4611-0f4c-4cdf-d9c8-adc6f9de4489"},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://419a9645e036:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Spark NLP</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f73aa5c31f0>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import sparknlp\n","from sparknlp.base import *\n","from sparknlp.annotator import *\n","from pyspark.sql import functions as F\n","\n","spark = sparknlp.start()\n","spark"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yGmhrAiMHuEX"},"source":["## **🖨️ Input/Output Annotation Types**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"iwrlo5bFHw5z"},"source":["- Input: `TOKEN`\n","\n","- Output: `TOKEN`"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uKBvpsIeIayp"},"source":["## **🔎 Parameters**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ehnt_-xDQzva"},"source":["*  `CaseSensitive` (*Boolean*) : Whether to do a case-sensitive comparison over the stop words (Default: false)\n","\n","*   `StopWords` ( [*String*] ) : The words to be filtered out (Default: Stop words from MLlib)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"34Pcc4JRXw25"},"source":["### `CaseSensitive` \n","\n","Whether to do a case-sensitive comparison over the stop words (Default: false)\n","\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9322,"status":"ok","timestamp":1673446500734,"user":{"displayName":"Arshaan","userId":"17964254249366085303"},"user_tz":-330},"id":"A5u09LD-Rild","outputId":"c9ee2cef-6676-4306-a5fa-0cfb50fd6e75"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------------------+\n","|result                                |\n","+--------------------------------------+\n","|[Tom, nice, man, ., lives, Kashmir, .]|\n","+--------------------------------------+\n","\n"]}],"source":["document = DocumentAssembler()\\\n","    .setInputCol(\"text\")\\\n","    .setOutputCol(\"document\")\n","\n","sentence = SentenceDetector()\\\n","    .setInputCols(['document'])\\\n","    .setOutputCol('sentence')\n","\n","token = Tokenizer()\\\n","    .setInputCols(['sentence'])\\\n","    .setOutputCol('token')\n","\n","stop_words = StopWordsCleaner()\\\n","    .setInputCols([\"token\"]) \\\n","    .setOutputCol(\"cleanTokens\") \\\n","\n","prediction_pipeline = Pipeline(\n","    stages = [\n","        document,\n","        sentence,\n","        token,\n","        stop_words\n","    ]\n",")\n","\n","prediction_data = spark.createDataFrame([[\"Tom is a nice man. He lives in Kashmir.\"]]).toDF(\"text\")\n","\n","result = prediction_pipeline.fit(prediction_data).transform(prediction_data)\n","result.select(\"cleanTokens.result\").show(1, False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"F1d0YyYOX9HR"},"source":["As nothing specified, by default `CaseSensitive = False` . So if any stopword from the default (Stop words from MLlib) is present, it is removed. In this case, words like:  \"is\", \"a\", \"He\", \"in\" were removed."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-oCESZRdYvtj"},"source":[" <h4> ➤ CaseSensitive = True </h4>"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1512,"status":"ok","timestamp":1673446502240,"user":{"displayName":"Arshaan","userId":"17964254249366085303"},"user_tz":-330},"id":"A70ZtZ_gYnT9","outputId":"e4b03d07-e6ea-430b-bff0-57685b1e7fee"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------------------------------------+\n","|result                                    |\n","+------------------------------------------+\n","|[Tom, nice, man, ., He, lives, Kashmir, .]|\n","+------------------------------------------+\n","\n"]}],"source":["# Specify .setCaseSensitive(True)\n","stop_words = StopWordsCleaner()\\\n","    .setInputCols([\"token\"]) \\\n","    .setOutputCol(\"cleanTokens\") \\\n","    .setCaseSensitive(True)\n","\n","prediction_pipeline = Pipeline(\n","    stages = [\n","        document,\n","        sentence,\n","        token,\n","        stop_words\n","    ]\n",")\n","\n","prediction_data = spark.createDataFrame([[\"Tom is a nice man. He lives in Kashmir.\"]]).toDF(\"text\")\n","\n","result = prediction_pipeline.fit(prediction_data).transform(prediction_data)\n","result.select(\"cleanTokens.result\").show(1, False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"fTtKf-AlY5rf"},"source":["Because of `CaseSensitive = True`, the word \"*He*\" was not considered a stopword because all stopwords in MLlibs StopWordsRemover are specified in lowercase."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Y-UVH-5592aW"},"source":["➤ Stopwords from MLlibs StopWordsRemover "]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1673446502240,"user":{"displayName":"Arshaan","userId":"17964254249366085303"},"user_tz":-330},"id":"h2XsfZtF9mJn","outputId":"20374242-c228-4ec1-8043-7ceadaa86099"},"outputs":[{"data":{"text/plain":["['i',\n"," 'me',\n"," 'my',\n"," 'myself',\n"," 'we',\n"," 'our',\n"," 'ours',\n"," 'ourselves',\n"," 'you',\n"," 'your',\n"," 'yours',\n"," 'yourself',\n"," 'yourselves',\n"," 'he',\n"," 'him',\n"," 'his',\n"," 'himself',\n"," 'she',\n"," 'her',\n"," 'hers',\n"," 'herself',\n"," 'it',\n"," 'its',\n"," 'itself',\n"," 'they',\n"," 'them',\n"," 'their',\n"," 'theirs',\n"," 'themselves',\n"," 'what',\n"," 'which',\n"," 'who',\n"," 'whom',\n"," 'this',\n"," 'that',\n"," 'these',\n"," 'those',\n"," 'am',\n"," 'is',\n"," 'are',\n"," 'was',\n"," 'were',\n"," 'be',\n"," 'been',\n"," 'being',\n"," 'have',\n"," 'has',\n"," 'had',\n"," 'having',\n"," 'do',\n"," 'does',\n"," 'did',\n"," 'doing',\n"," 'a',\n"," 'an',\n"," 'the',\n"," 'and',\n"," 'but',\n"," 'if',\n"," 'or',\n"," 'because',\n"," 'as',\n"," 'until',\n"," 'while',\n"," 'of',\n"," 'at',\n"," 'by',\n"," 'for',\n"," 'with',\n"," 'about',\n"," 'against',\n"," 'between',\n"," 'into',\n"," 'through',\n"," 'during',\n"," 'before',\n"," 'after',\n"," 'above',\n"," 'below',\n"," 'to',\n"," 'from',\n"," 'up',\n"," 'down',\n"," 'in',\n"," 'out',\n"," 'on',\n"," 'off',\n"," 'over',\n"," 'under',\n"," 'again',\n"," 'further',\n"," 'then',\n"," 'once',\n"," 'here',\n"," 'there',\n"," 'when',\n"," 'where',\n"," 'why',\n"," 'how',\n"," 'all',\n"," 'any',\n"," 'both',\n"," 'each',\n"," 'few',\n"," 'more',\n"," 'most',\n"," 'other',\n"," 'some',\n"," 'such',\n"," 'no',\n"," 'nor',\n"," 'not',\n"," 'only',\n"," 'own',\n"," 'same',\n"," 'so',\n"," 'than',\n"," 'too',\n"," 'very',\n"," 's',\n"," 't',\n"," 'can',\n"," 'will',\n"," 'just',\n"," 'don',\n"," 'should',\n"," 'now',\n"," \"i'll\",\n"," \"you'll\",\n"," \"he'll\",\n"," \"she'll\",\n"," \"we'll\",\n"," \"they'll\",\n"," \"i'd\",\n"," \"you'd\",\n"," \"he'd\",\n"," \"she'd\",\n"," \"we'd\",\n"," \"they'd\",\n"," \"i'm\",\n"," \"you're\",\n"," \"he's\",\n"," \"she's\",\n"," \"it's\",\n"," \"we're\",\n"," \"they're\",\n"," \"i've\",\n"," \"we've\",\n"," \"you've\",\n"," \"they've\",\n"," \"isn't\",\n"," \"aren't\",\n"," \"wasn't\",\n"," \"weren't\",\n"," \"haven't\",\n"," \"hasn't\",\n"," \"hadn't\",\n"," \"don't\",\n"," \"doesn't\",\n"," \"didn't\",\n"," \"won't\",\n"," \"wouldn't\",\n"," \"shan't\",\n"," \"shouldn't\",\n"," \"mustn't\",\n"," \"can't\",\n"," \"couldn't\",\n"," 'cannot',\n"," 'could',\n"," \"here's\",\n"," \"how's\",\n"," \"let's\",\n"," 'ought',\n"," \"that's\",\n"," \"there's\",\n"," \"what's\",\n"," \"when's\",\n"," \"where's\",\n"," \"who's\",\n"," \"why's\",\n"," 'would']"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["stop_words.getStopWords()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"dyVzhAvSap39"},"source":["### `StopWords` \n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":867,"status":"ok","timestamp":1673446503099,"user":{"displayName":"Arshaan","userId":"17964254249366085303"},"user_tz":-330},"id":"ddL62-cuSeXk","outputId":"62ce30f3-01b4-457c-ca70-5919b787505a"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------------------+\n","|result                                |\n","+--------------------------------------+\n","|[Tom, nice, man, ., lives, Kashmir, .]|\n","+--------------------------------------+\n","\n"]}],"source":["stop_words = StopWordsCleaner()\\\n","    .setInputCols([\"token\"]) \\\n","    .setOutputCol(\"cleanTokens\") \\\n","    .setCaseSensitive(False)\\\n","\n","prediction_pipeline = Pipeline(\n","    stages = [\n","        document,\n","        sentence,\n","        token,\n","        stop_words\n","    ]\n",")\n","\n","prediction_data = spark.createDataFrame([[\"Tom is a nice man. He lives in Kashmir.\"]]).toDF(\"text\")\n","\n","result = prediction_pipeline.fit(prediction_data).transform(prediction_data)\n","result.select(\"cleanTokens.result\").show(1, False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"PjzR3bq5mLCQ"},"source":["As nothing specified, so default Stop words from MLlibs StopWordsRemover are considered."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"70jXSXDcbebQ"},"source":["<h2> ➤ StopWords : as an array of strings from a text file or manually. </h2>"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1621,"status":"ok","timestamp":1673446504716,"user":{"displayName":"Arshaan","userId":"17964254249366085303"},"user_tz":-330},"id":"GRb3XoumTLyI","outputId":"3b36e817-35e4-4984-cbed-b0e3500e308e"},"outputs":[{"name":"stdout","output_type":"stream","text":["+----------------------------------------------+\n","|result                                        |\n","+----------------------------------------------+\n","|[Tom, nice, man, ., He, lives, in, Kashmir, .]|\n","+----------------------------------------------+\n","\n"]}],"source":["stop_words = StopWordsCleaner()\\\n","    .setInputCols([\"token\"]) \\\n","    .setOutputCol(\"cleanTokens\") \\\n","    .setCaseSensitive(False)\\\n","    .setStopWords([\"is\", \"a\"])      #(e.g. Here we manually specified only ([\"is\", \"a\"]) as stopwords. This can come from a file as well.\n","\n","prediction_pipeline = Pipeline(\n","    stages = [\n","        document,\n","        sentence,\n","        token,\n","        stop_words\n","    ]\n",")\n","\n","prediction_data = spark.createDataFrame([[\"Tom is a nice man. He lives in Kashmir.\"]]).toDF(\"text\")\n","\n","result = prediction_pipeline.fit(prediction_data).transform(prediction_data)\n","result.select(\"cleanTokens.result\").show(1, False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"uzo8dJaXdf24"},"source":["We can observe that only the stopwords we specified in .setStopWords() were removed. As we specified just two words (\"is\" and \"a\") as stopwords, only they got removed."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ygBfZ6u9R0cO"},"source":["➤ Token positions are preserved."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":714,"status":"ok","timestamp":1673446505425,"user":{"displayName":"Arshaan","userId":"17964254249366085303"},"user_tz":-330},"id":"RP9RrBKoQ0Ok"},"outputs":[],"source":["stop_words = StopWordsCleaner()\\\n","    .setInputCols([\"token\"]) \\\n","    .setOutputCol(\"cleanTokens\") \\\n","    .setCaseSensitive(False)\\\n","\n","prediction_pipeline = Pipeline(\n","    stages = [\n","        document,\n","        sentence,\n","        token,\n","        stop_words\n","    ]\n",")\n","\n","prediction_data = spark.createDataFrame([[\"Tom is a nice man. He lives in Kashmir.\"]]).toDF(\"text\")\n","\n","result = prediction_pipeline.fit(prediction_data).transform(prediction_data)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1808,"status":"ok","timestamp":1673446507231,"user":{"displayName":"Arshaan","userId":"17964254249366085303"},"user_tz":-330},"id":"4omaRoF2Q0bb","outputId":"578d02e7-7558-4ecb-db1c-a43be82a43c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------------------------------------------------+----------------------------------------+-----------------------------------------+\n","|Tokens                                               |begin                                   |end                                      |\n","+-----------------------------------------------------+----------------------------------------+-----------------------------------------+\n","|[Tom, is, a, nice, man, ., He, lives, in, Kashmir, .]|[0, 4, 7, 9, 14, 17, 19, 22, 28, 31, 38]|[2, 5, 7, 12, 16, 17, 20, 26, 29, 37, 38]|\n","+-----------------------------------------------------+----------------------------------------+-----------------------------------------+\n","\n","+--------------------------------------+--------------------------+---------------------------+\n","|Clean Tokens                          |begin                     |end                        |\n","+--------------------------------------+--------------------------+---------------------------+\n","|[Tom, nice, man, ., lives, Kashmir, .]|[0, 9, 14, 17, 22, 31, 38]|[2, 12, 16, 17, 26, 37, 38]|\n","+--------------------------------------+--------------------------+---------------------------+\n","\n"]}],"source":["result.select(\"token.result\",\"token.begin\",\"token.end\").withColumnRenamed(\"result\",\"Tokens\").show(truncate=False)\n","result.select(\"cleanTokens.result\",\"cleanTokens.begin\",\"cleanTokens.end\").withColumnRenamed(\"result\",\"Clean Tokens\").show(truncate=False)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yXIjiLROSFJR"},"source":["As we can see above, the position of the tokens is preserved in the cleaned tokens."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HlpXyW5qgVCr"},"source":["### `StopWordsCleaner Pre-trained Models` \n","\n","Available pretrained models can be checked here : https://nlp.johnsnowlabs.com/models?q=stopwords"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5yUs-4MzgzEo"},"source":["It is used as  :  `StopWordsCleaner.pretrained(\"Model_Name\", \"Language\")`\n","\n","example : `(\"stopwords_iso\", \"en\")`\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13505,"status":"ok","timestamp":1673446520730,"user":{"displayName":"Arshaan","userId":"17964254249366085303"},"user_tz":-330},"id":"YqRb0OOucam0","outputId":"d9af8a44-9be4-4c75-9a09-624849b52606"},"outputs":[{"name":"stdout","output_type":"stream","text":["stopwords_iso download started this may take some time.\n","Approximate size to download 2.1 KB\n","[OK!]\n","+--------+\n","|result  |\n","+--------+\n","|[better]|\n","+--------+\n","\n"]}],"source":["documentAssembler = DocumentAssembler() \\\n",".setInputCol(\"text\") \\\n",".setOutputCol(\"document\")\n","\n","tokenizer = Tokenizer() \\\n",".setInputCols([\"document\"]) \\\n",".setOutputCol(\"token\")\n","\n","stop_words = StopWordsCleaner.pretrained(\"stopwords_iso\",\"en\") \\\n",".setInputCols([\"token\"]) \\\n",".setOutputCol(\"cleanTokens\")\n","\n","pipeline = Pipeline(stages=[documentAssembler, tokenizer, stop_words]) \n","\n","example = spark.createDataFrame([[\"You are not better than me\"]], [\"text\"]) \n","\n","results = pipeline.fit(example).transform(example)\n","results.select(\"cleanTokens.result\").show(1, False)\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1673446520730,"user":{"displayName":"Arshaan","userId":"17964254249366085303"},"user_tz":-330},"id":"5A5v3Zi02XGP","outputId":"07319230-e41f-4afa-b454-2dce9485c862"},"outputs":[{"data":{"text/plain":["['a',\n"," 'about',\n"," 'above',\n"," 'across',\n"," 'after',\n"," 'afterwards',\n"," 'again',\n"," 'against',\n"," 'all',\n"," 'almost',\n"," 'alone',\n"," 'along',\n"," 'already',\n"," 'also',\n"," 'although',\n"," 'always',\n"," 'am',\n"," 'among',\n"," 'amongst',\n"," 'amount',\n"," 'an',\n"," 'and',\n"," 'another',\n"," 'any',\n"," 'anyhow',\n"," 'anyone',\n"," 'anything',\n"," 'anyway',\n"," 'anywhere',\n"," 'are',\n"," 'around',\n"," 'as',\n"," 'at',\n"," 'back',\n"," 'be',\n"," 'became',\n"," 'because',\n"," 'become',\n"," 'becomes',\n"," 'becoming',\n"," 'been',\n"," 'before',\n"," 'beforehand',\n"," 'behind',\n"," 'being',\n"," 'below',\n"," 'beside',\n"," 'besides',\n"," 'between',\n"," 'beyond',\n"," 'both',\n"," 'bottom',\n"," 'but',\n"," 'by',\n"," 'call',\n"," 'can',\n"," 'cannot',\n"," 'ca',\n"," 'could',\n"," 'did',\n"," 'do',\n"," 'does',\n"," 'doing',\n"," 'done',\n"," 'down',\n"," 'due',\n"," 'during',\n"," 'each',\n"," 'eight',\n"," 'either',\n"," 'eleven',\n"," 'else',\n"," 'elsewhere',\n"," 'empty',\n"," 'enough',\n"," 'even',\n"," 'ever',\n"," 'every',\n"," 'everyone',\n"," 'everything',\n"," 'everywhere',\n"," 'except',\n"," 'few',\n"," 'fifteen',\n"," 'fifty',\n"," 'first',\n"," 'five',\n"," 'for',\n"," 'former',\n"," 'formerly',\n"," 'forty',\n"," 'four',\n"," 'from',\n"," 'front',\n"," 'full',\n"," 'further',\n"," 'get',\n"," 'give',\n"," 'go',\n"," 'had',\n"," 'has',\n"," 'have',\n"," 'he',\n"," 'hence',\n"," 'her',\n"," 'here',\n"," 'hereafter',\n"," 'hereby',\n"," 'herein',\n"," 'hereupon',\n"," 'hers',\n"," 'herself',\n"," 'him',\n"," 'himself',\n"," 'his',\n"," 'how',\n"," 'however',\n"," 'hundred',\n"," 'i',\n"," 'if',\n"," 'in',\n"," 'indeed',\n"," 'into',\n"," 'is',\n"," 'it',\n"," 'its',\n"," 'itself',\n"," 'keep',\n"," 'last',\n"," 'latter',\n"," 'latterly',\n"," 'least',\n"," 'less',\n"," 'just',\n"," 'made',\n"," 'make',\n"," 'many',\n"," 'may',\n"," 'me',\n"," 'meanwhile',\n"," 'might',\n"," 'mine',\n"," 'more',\n"," 'moreover',\n"," 'most',\n"," 'mostly',\n"," 'move',\n"," 'much',\n"," 'must',\n"," 'my',\n"," 'myself',\n"," 'name',\n"," 'namely',\n"," 'neither',\n"," 'never',\n"," 'nevertheless',\n"," 'next',\n"," 'nine',\n"," 'no',\n"," 'nobody',\n"," 'none',\n"," 'noone',\n"," 'nor',\n"," 'not',\n"," 'nothing',\n"," 'now',\n"," 'nowhere',\n"," 'of',\n"," 'off',\n"," 'often',\n"," 'on',\n"," 'once',\n"," 'one',\n"," 'only',\n"," 'onto',\n"," 'or',\n"," 'other',\n"," 'others',\n"," 'otherwise',\n"," 'our',\n"," 'ours',\n"," 'ourselves',\n"," 'out',\n"," 'over',\n"," 'own',\n"," 'part',\n"," 'per',\n"," 'perhaps',\n"," 'please',\n"," 'put',\n"," 'quite',\n"," 'rather',\n"," 're',\n"," 'really',\n"," 'regarding',\n"," 'same',\n"," 'say',\n"," 'see',\n"," 'seem',\n"," 'seemed',\n"," 'seeming',\n"," 'seems',\n"," 'serious',\n"," 'several',\n"," 'she',\n"," 'should',\n"," 'show',\n"," 'side',\n"," 'since',\n"," 'six',\n"," 'sixty',\n"," 'so',\n"," 'some',\n"," 'somehow',\n"," 'someone',\n"," 'something',\n"," 'sometime',\n"," 'sometimes',\n"," 'somewhere',\n"," 'still',\n"," 'such',\n"," 'take',\n"," 'ten',\n"," 'than',\n"," 'that',\n"," 'the',\n"," 'their',\n"," 'them',\n"," 'themselves',\n"," 'then',\n"," 'thence',\n"," 'there',\n"," 'thereafter',\n"," 'thereby',\n"," 'therefore',\n"," 'therein',\n"," 'thereupon',\n"," 'these',\n"," 'they',\n"," 'third',\n"," 'this',\n"," 'those',\n"," 'though',\n"," 'three',\n"," 'through',\n"," 'throughout',\n"," 'thru',\n"," 'thus',\n"," 'to',\n"," 'together',\n"," 'too',\n"," 'top',\n"," 'toward',\n"," 'towards',\n"," 'twelve',\n"," 'twenty',\n"," 'two',\n"," 'under',\n"," 'until',\n"," 'up',\n"," 'unless',\n"," 'upon',\n"," 'us',\n"," 'used',\n"," 'using',\n"," 'various',\n"," 'very',\n"," 'very',\n"," 'via',\n"," 'was',\n"," 'we',\n"," 'well',\n"," 'were',\n"," 'what',\n"," 'whatever',\n"," 'when',\n"," 'whence',\n"," 'whenever',\n"," 'where',\n"," 'whereafter',\n"," 'whereas',\n"," 'whereby',\n"," 'wherein',\n"," 'whereupon',\n"," 'wherever',\n"," 'whether',\n"," 'which',\n"," 'while',\n"," 'whither',\n"," 'who',\n"," 'whoever',\n"," 'whole',\n"," 'whom',\n"," 'whose',\n"," 'why',\n"," 'will',\n"," 'with',\n"," 'within',\n"," 'without',\n"," 'would',\n"," 'yet',\n"," 'you',\n"," 'your',\n"," 'yours',\n"," 'yourself',\n"," 'yourselves']"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Pretrained model (\"stopwords_iso\", \"en\") stopwords\n","stop_words.getStopWords()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-AO_tQeIi8g4"},"source":["We have pretrained models for other languages as well.\n","\n","example: `(\"stopwords_iso\", \"fr\")`"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7336,"status":"ok","timestamp":1673446528049,"user":{"displayName":"Arshaan","userId":"17964254249366085303"},"user_tz":-330},"id":"O2HUs3IIhZTU","outputId":"3c7ea8d9-c281-4001-f9be-c33a3f5812ef"},"outputs":[{"name":"stdout","output_type":"stream","text":["stopwords_iso download started this may take some time.\n","Approximate size to download 2.8 KB\n","[OK!]\n","+-------------+\n","|result       |\n","+-------------+\n","|[n'es, mieux]|\n","+-------------+\n","\n"]}],"source":["documentAssembler = DocumentAssembler() \\\n",".setInputCol(\"text\") \\\n",".setOutputCol(\"document\")\n","\n","tokenizer = Tokenizer() \\\n",".setInputCols([\"document\"]) \\\n",".setOutputCol(\"token\")\n","\n","stop_words = StopWordsCleaner.pretrained(\"stopwords_iso\",\"fr\") \\\n",".setInputCols([\"token\"]) \\\n",".setOutputCol(\"cleanTokens\")\n","\n","pipeline = Pipeline(stages=[documentAssembler, tokenizer, stop_words]) \n","\n","example = spark.createDataFrame([[\"Tu n'es pas mieux que moi\"]], [\"text\"]) \n","\n","results = pipeline.fit(example).transform(example)\n","results.select(\"cleanTokens.result\").show(1, False)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1673446528049,"user":{"displayName":"Arshaan","userId":"17964254249366085303"},"user_tz":-330},"id":"XOop1C3XZYHy","outputId":"fb1f2b9d-9f37-4423-8e66-0c967921230e"},"outputs":[{"data":{"text/plain":["['a',\n"," 'à',\n"," 'â',\n"," 'abord',\n"," 'afin',\n"," 'ah',\n"," 'ai',\n"," 'aie',\n"," 'ainsi',\n"," 'ait',\n"," 'allaient',\n"," 'allons',\n"," 'alors',\n"," 'anterieur',\n"," 'anterieure',\n"," 'anterieures',\n"," 'antérieur',\n"," 'antérieure',\n"," 'antérieures',\n"," 'apres',\n"," 'après',\n"," 'as',\n"," 'assez',\n"," 'attendu',\n"," 'au',\n"," 'aupres',\n"," 'auquel',\n"," 'aura',\n"," 'auraient',\n"," 'aurait',\n"," 'auront',\n"," 'aussi',\n"," 'autre',\n"," 'autrement',\n"," 'autres',\n"," 'autrui',\n"," 'aux',\n"," 'auxquelles',\n"," 'auxquels',\n"," 'avaient',\n"," 'avais',\n"," 'avait',\n"," 'avant',\n"," 'avec',\n"," 'avoir',\n"," 'avons',\n"," 'ayant',\n"," 'bas',\n"," 'basee',\n"," 'bat',\n"," \"c'\",\n"," 'c’',\n"," 'ça',\n"," 'car',\n"," 'ce',\n"," 'ceci',\n"," 'cela',\n"," 'celle',\n"," 'celle-ci',\n"," 'celle-la',\n"," 'celle-là',\n"," 'celles',\n"," 'celles-ci',\n"," 'celles-la',\n"," 'celles-là',\n"," 'celui',\n"," 'celui-ci',\n"," 'celui-la',\n"," 'celui-là',\n"," 'cent',\n"," 'cependant',\n"," 'certain',\n"," 'certaine',\n"," 'certaines',\n"," 'certains',\n"," 'certes',\n"," 'ces',\n"," 'cet',\n"," 'cette',\n"," 'ceux',\n"," 'ceux-ci',\n"," 'ceux-là',\n"," 'chacun',\n"," 'chacune',\n"," 'chaque',\n"," 'chez',\n"," 'ci',\n"," 'cinq',\n"," 'cinquantaine',\n"," 'cinquante',\n"," 'cinquantième',\n"," 'cinquième',\n"," 'combien',\n"," 'comme',\n"," 'comment',\n"," 'compris',\n"," 'concernant',\n"," \"d'\",\n"," 'd’',\n"," 'da',\n"," 'dans',\n"," 'de',\n"," 'debout',\n"," 'dedans',\n"," 'dehors',\n"," 'deja',\n"," 'dejà',\n"," 'delà',\n"," 'depuis',\n"," 'derriere',\n"," 'derrière',\n"," 'des',\n"," 'desormais',\n"," 'desquelles',\n"," 'desquels',\n"," 'dessous',\n"," 'dessus',\n"," 'deux',\n"," 'deuxième',\n"," 'deuxièmement',\n"," 'devant',\n"," 'devers',\n"," 'devra',\n"," 'different',\n"," 'differente',\n"," 'differentes',\n"," 'differents',\n"," 'différent',\n"," 'différente',\n"," 'différentes',\n"," 'différents',\n"," 'dire',\n"," 'directe',\n"," 'directement',\n"," 'dit',\n"," 'dite',\n"," 'dits',\n"," 'divers',\n"," 'diverse',\n"," 'diverses',\n"," 'dix',\n"," 'dix-huit',\n"," 'dix-neuf',\n"," 'dix-sept',\n"," 'dixième',\n"," 'doit',\n"," 'doivent',\n"," 'donc',\n"," 'dont',\n"," 'douze',\n"," 'douzième',\n"," 'du',\n"," 'duquel',\n"," 'durant',\n"," 'dès',\n"," 'déja',\n"," 'déjà',\n"," 'désormais',\n"," 'effet',\n"," 'egalement',\n"," 'eh',\n"," 'elle',\n"," 'elle-meme',\n"," 'elle-même',\n"," 'elles',\n"," 'elles-memes',\n"," 'elles-mêmes',\n"," 'en',\n"," 'encore',\n"," 'enfin',\n"," 'entre',\n"," 'envers',\n"," 'environ',\n"," 'es',\n"," 'ès',\n"," 'est',\n"," 'et',\n"," 'etaient',\n"," 'étaient',\n"," 'etais',\n"," 'étais',\n"," 'etait',\n"," 'était',\n"," 'etant',\n"," 'étant',\n"," 'etc',\n"," 'etre',\n"," 'être',\n"," 'eu',\n"," 'eux',\n"," 'eux-mêmes',\n"," 'exactement',\n"," 'excepté',\n"," 'également',\n"," 'fais',\n"," 'faisaient',\n"," 'faisant',\n"," 'fait',\n"," 'facon',\n"," 'façon',\n"," 'feront',\n"," 'font',\n"," 'gens',\n"," 'ha',\n"," 'hem',\n"," 'hep',\n"," 'hi',\n"," 'ho',\n"," 'hormis',\n"," 'hors',\n"," 'hou',\n"," 'houp',\n"," 'hue',\n"," 'hui',\n"," 'huit',\n"," 'huitième',\n"," 'hé',\n"," 'i',\n"," 'il',\n"," 'ils',\n"," 'importe',\n"," \"j'\",\n"," 'j’',\n"," 'je',\n"," 'jusqu',\n"," 'jusque',\n"," 'juste',\n"," \"l'\",\n"," 'l’',\n"," 'la',\n"," 'laisser',\n"," 'laquelle',\n"," 'le',\n"," 'lequel',\n"," 'les',\n"," 'lesquelles',\n"," 'lesquels',\n"," 'leur',\n"," 'leurs',\n"," 'longtemps',\n"," 'lors',\n"," 'lorsque',\n"," 'lui',\n"," 'lui-meme',\n"," 'lui-même',\n"," 'là',\n"," 'lès',\n"," \"m'\",\n"," 'm’',\n"," 'ma',\n"," 'maint',\n"," 'maintenant',\n"," 'mais',\n"," 'malgre',\n"," 'malgré',\n"," 'me',\n"," 'meme',\n"," 'memes',\n"," 'merci',\n"," 'mes',\n"," 'mien',\n"," 'mienne',\n"," 'miennes',\n"," 'miens',\n"," 'mille',\n"," 'moi',\n"," 'moi-meme',\n"," 'moi-même',\n"," 'moindres',\n"," 'moins',\n"," 'mon',\n"," 'même',\n"," 'mêmes',\n"," \"n'\",\n"," 'n’',\n"," 'na',\n"," 'ne',\n"," 'neanmoins',\n"," 'neuvième',\n"," 'ni',\n"," 'nombreuses',\n"," 'nombreux',\n"," 'nos',\n"," 'notamment',\n"," 'notre',\n"," 'nous',\n"," 'nous-mêmes',\n"," 'nouveau',\n"," 'nul',\n"," 'néanmoins',\n"," 'nôtre',\n"," 'nôtres',\n"," 'o',\n"," 'ô',\n"," 'on',\n"," 'ont',\n"," 'onze',\n"," 'onzième',\n"," 'or',\n"," 'ou',\n"," 'ouias',\n"," 'ouste',\n"," 'outre',\n"," 'ouvert',\n"," 'ouverte',\n"," 'ouverts',\n"," 'où',\n"," 'par',\n"," 'parce',\n"," 'parfois',\n"," 'parle',\n"," 'parlent',\n"," 'parler',\n"," 'parmi',\n"," 'partant',\n"," 'pas',\n"," 'pendant',\n"," 'pense',\n"," 'permet',\n"," 'personne',\n"," 'peu',\n"," 'peut',\n"," 'peuvent',\n"," 'peux',\n"," 'plus',\n"," 'plusieurs',\n"," 'plutot',\n"," 'plutôt',\n"," 'possible',\n"," 'possibles',\n"," 'pour',\n"," 'pourquoi',\n"," 'pourrais',\n"," 'pourrait',\n"," 'pouvait',\n"," 'prealable',\n"," 'precisement',\n"," 'premier',\n"," 'première',\n"," 'premièrement',\n"," 'pres',\n"," 'procedant',\n"," 'proche',\n"," 'près',\n"," 'préalable',\n"," 'précisement',\n"," 'pu',\n"," 'puis',\n"," 'puisque',\n"," \"qu'\",\n"," 'qu’',\n"," 'quand',\n"," 'quant',\n"," 'quant-à-soi',\n"," 'quarante',\n"," 'quatorze',\n"," 'quatre',\n"," 'quatre-vingt',\n"," 'quatrième',\n"," 'quatrièmement',\n"," 'que',\n"," 'quel',\n"," 'quelconque',\n"," 'quelle',\n"," 'quelles',\n"," \"quelqu'un\",\n"," 'quelque',\n"," 'quelques',\n"," 'quels',\n"," 'qui',\n"," 'quiconque',\n"," 'quinze',\n"," 'quoi',\n"," 'quoique',\n"," 'relative',\n"," 'relativement',\n"," 'rend',\n"," 'rendre',\n"," 'restant',\n"," 'reste',\n"," 'restent',\n"," 'retour',\n"," 'revoici',\n"," 'revoila',\n"," 'revoilà',\n"," \"s'\",\n"," 's’',\n"," 'sa',\n"," 'sait',\n"," 'sans',\n"," 'sauf',\n"," 'se',\n"," 'seize',\n"," 'selon',\n"," 'semblable',\n"," 'semblaient',\n"," 'semble',\n"," 'semblent',\n"," 'sent',\n"," 'sept',\n"," 'septième',\n"," 'sera',\n"," 'seraient',\n"," 'serait',\n"," 'seront',\n"," 'ses',\n"," 'seul',\n"," 'seule',\n"," 'seulement',\n"," 'seuls',\n"," 'seules',\n"," 'si',\n"," 'sien',\n"," 'sienne',\n"," 'siennes',\n"," 'siens',\n"," 'sinon',\n"," 'six',\n"," 'sixième',\n"," 'soi',\n"," 'soi-meme',\n"," 'soi-même',\n"," 'soit',\n"," 'soixante',\n"," 'son',\n"," 'sont',\n"," 'sous',\n"," 'souvent',\n"," 'specifique',\n"," 'specifiques',\n"," 'spécifique',\n"," 'spécifiques',\n"," 'stop',\n"," 'suffisant',\n"," 'suffisante',\n"," 'suffit',\n"," 'suis',\n"," 'suit',\n"," 'suivant',\n"," 'suivante',\n"," 'suivantes',\n"," 'suivants',\n"," 'suivre',\n"," 'sur',\n"," 'surtout',\n"," \"t'\",\n"," 't’',\n"," 'ta',\n"," 'tant',\n"," 'te',\n"," 'tel',\n"," 'telle',\n"," 'tellement',\n"," 'telles',\n"," 'tels',\n"," 'tenant',\n"," 'tend',\n"," 'tenir',\n"," 'tente',\n"," 'tes',\n"," 'tien',\n"," 'tienne',\n"," 'tiennes',\n"," 'tiens',\n"," 'toi',\n"," 'toi-meme',\n"," 'toi-même',\n"," 'ton',\n"," 'touchant',\n"," 'toujours',\n"," 'tous',\n"," 'tout',\n"," 'toute',\n"," 'toutes',\n"," 'treize',\n"," 'trente',\n"," 'tres',\n"," 'trois',\n"," 'troisième',\n"," 'troisièmement',\n"," 'très',\n"," 'tu',\n"," 'té',\n"," 'un',\n"," 'une',\n"," 'unes',\n"," 'uns',\n"," 'va',\n"," 'vais',\n"," 'vas',\n"," 'vers',\n"," 'via',\n"," 'vingt',\n"," 'voici',\n"," 'voila',\n"," 'voilà',\n"," 'vont',\n"," 'vos',\n"," 'votre',\n"," 'votres',\n"," 'vous',\n"," 'vous-mêmes',\n"," 'vu',\n"," 'vé',\n"," 'vôtre',\n"," 'vôtres',\n"," 'y']"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Pretrained model (\"stopwords_iso\", \"fr\") stopwords\n","stop_words.getStopWords()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyOMmmuMQmSJ/JkeF8lgVSFn","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
