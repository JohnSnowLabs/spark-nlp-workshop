{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n"
      ],
      "metadata": {
        "id": "klIak_Gb_OPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stemmer**"
      ],
      "metadata": {
        "id": "bewC1SWN-6jB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook will cover the different parameters and usages of `Stemmer`. \n",
        "\n",
        "**ðŸ“– Learning Objectives:**\n",
        "\n",
        "1. Understand how extract the base form of the words by removing affixes from them.\n",
        "\n",
        "2. Learn how to create pipelines with this annotator.\n",
        "\n",
        "\n",
        "**ðŸ”— Helpful Links:**\n",
        "\n",
        "- Documentation : [Stemmer](https://nlp.johnsnowlabs.com/docs/en/annotators#stemmer)\n",
        "\n",
        "- Python Docs : [Stemmer](https://nlp.johnsnowlabs.com/api/python/reference/autosummary/sparknlp/annotator/stemmer/index.html)\n",
        "\n",
        "- Scala Docs : [Stemmer](https://nlp.johnsnowlabs.com/api/com/johnsnowlabs/nlp/annotators/Stemmer.html)\n",
        "\n",
        "- For extended examples of usage, see the [Spark NLP Workshop repository](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/)."
      ],
      "metadata": {
        "id": "J5mRfBZFz6m1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ðŸŽ¬ Colab Setup**"
      ],
      "metadata": {
        "id": "A4hMnkhd_ik9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PySpark and Spark NLP\n",
        "!pip install -q pyspark==3.1.2  spark-nlp==4.2.4"
      ],
      "metadata": {
        "id": "xrdvNxjD_yQI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sparknlp\n",
        "from sparknlp.base import DocumentAssembler, LightPipeline, Pipeline\n",
        "from sparknlp.annotator import Tokenizer, Stemmer\n",
        "\n",
        "spark = sparknlp.start()"
      ],
      "metadata": {
        "id": "W5D63vBo_0u0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ðŸ–¨ï¸ Input/Output Annotation Types**"
      ],
      "metadata": {
        "id": "gekGlxgO1w7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Input: `TOKEN`\n",
        "\n",
        "- Output: `TOKEN`"
      ],
      "metadata": {
        "id": "cdeAJsEJ10tz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ðŸ”Ž Parameters**\n",
        "\n",
        "None"
      ],
      "metadata": {
        "id": "4I43zb88d1hS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Example Pipeline**"
      ],
      "metadata": {
        "id": "i-WQg2pN2ODD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "stemmer = Stemmer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"stem\")\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[documentAssembler, \n",
        "                               tokenizer,\n",
        "                               stemmer])\n",
        "\n",
        "sample_texts = [[\"I love working with SparkNLP.\"], \n",
        "        [\"I am living in Canada.\"]]\n",
        "\n",
        "data = spark.createDataFrame(sample_texts).toDF(\"text\")\n",
        "\n",
        "model = nlpPipeline.fit(data)\n",
        "\n",
        "result = model.transform(data)\n",
        "result.show(truncate=40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gozpR1qxT95a",
        "outputId": "0deed994-e00e-498e-8a24-22e46825b490"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
            "|                         text|                                document|                                   token|                                    stem|\n",
            "+-----------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
            "|I love working with SparkNLP.|[{document, 0, 28, I love working wit...|[{token, 0, 0, I, {sentence -> 0}, []...|[{token, 0, 0, i, {sentence -> 0}, []...|\n",
            "|       I am living in Canada.|[{document, 0, 21, I am living in Can...|[{token, 0, 0, I, {sentence -> 0}, []...|[{token, 0, 0, i, {sentence -> 0}, []...|\n",
            "+-----------------------------+----------------------------------------+----------------------------------------+----------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result.select('token.result', 'stem.result').show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAYuKjiVUlJa",
        "outputId": "78ea7663-d25c-473e-af09-ef7f820b1471"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------------------------------+----------------------------------+\n",
            "|result                               |result                            |\n",
            "+-------------------------------------+----------------------------------+\n",
            "|[I, love, working, with, SparkNLP, .]|[i, love, work, with, sparknlp, .]|\n",
            "|[I, am, living, in, Canada, .]       |[i, am, live, in, canada, .]      |\n",
            "+-------------------------------------+----------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸŽ¯ **Usage with LightPipeline**"
      ],
      "metadata": {
        "id": "N-ht8YIgxc8E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **LightPipeline** is a Spark NLP specific Pipeline class equivalent to Spark ML Pipeline. The difference is that its execution does not hold to Spark principles, instead, it computes everything locally (but in parallel) in order to achieve faster inference when dealing with small amounts of data. This means, we don't have to Spark Dataframe, but a string or an array of strings instead, to be annotated. To create Light Pipelines, you need to input an already trained (fit) Spark ML Pipeline.\n",
        "\n",
        "- Itâ€™s `transform()` stage is converted into `annotate()` or `fullAnnotate()` instead. <br/>\n",
        "\n",
        "- Let's ceate a pipeline with `MarianTransformer`, and run it with `LightPipeline` and see the results with an example text. "
      ],
      "metadata": {
        "id": "w6qdH4ONyUar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp.base import LightPipeline\n",
        "\n",
        "\n",
        "light_pipeline = LightPipeline(model)"
      ],
      "metadata": {
        "id": "0VKFiLhSeFcZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "light_pipeline.annotate(\"I love working with SparkNLP.\")[\"stem\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM3IMDh8yd06",
        "outputId": "ea704452-f37b-4f4b-c842-5e8ab7b6a847"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'love', 'work', 'with', 'sparknlp', '.']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "light_pipeline.fullAnnotate(\"I love working with SparkNLP.\")[0][\"stem\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-swd38giysY8",
        "outputId": "b705b187-a459-4462-bd35-521a0e40518c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Annotation(token, 0, 0, i, {'sentence': '0'}),\n",
              " Annotation(token, 2, 5, love, {'sentence': '0'}),\n",
              " Annotation(token, 7, 13, work, {'sentence': '0'}),\n",
              " Annotation(token, 15, 18, with, {'sentence': '0'}),\n",
              " Annotation(token, 20, 27, sparknlp, {'sentence': '0'}),\n",
              " Annotation(token, 28, 28, ., {'sentence': '0'})]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZEAExw0VywDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}