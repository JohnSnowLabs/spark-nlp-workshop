{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4_Unsupervise_Chinese_Keyword_Extraction_NER_and_Translation_from_Chinese_News.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hTQj0psy3cfH"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/nlu/blob/master/examples/webinars_conferences_etc/multi_lingual_webinar/4_Unsupervise_Chinese_Keyword_Extraction_NER_and_Translation_from_Chinese_News.ipynb)\n","\n","![Flags](http://ckl-it.de/wp-content/uploads/2021/02/flags.jpeg)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1ZPMr8NkhnZz","executionInfo":{"status":"ok","timestamp":1650024259857,"user_tz":-300,"elapsed":97862,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}},"outputId":"7b1af1f8-5350-4f2c-a73a-5b8775f8454c"},"source":["!wget https://setup.johnsnowlabs.com/nlu/colab.sh -O - | bash\n","import nlu"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-04-15 12:02:41--  https://setup.johnsnowlabs.com/nlu/colab.sh\n","Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n","Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:443... connected.\n","HTTP request sent, awaiting response... 302 Moved Temporarily\n","Location: https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh [following]\n","--2022-04-15 12:02:41--  https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1665 (1.6K) [text/plain]\n","Saving to: ‘STDOUT’\n","\n","-                   100%[===================>]   1.63K  --.-KB/s    in 0s      \n","\n","2022-04-15 12:02:41 (29.9 MB/s) - written to stdout [1665/1665]\n","\n","Installing  NLU 3.4.3rc2 with  PySpark 3.0.3 and Spark NLP 3.4.2 for Google Colab ...\n","Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n","Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n","Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Get:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n","Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [953 kB]\n","Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,947 kB]\n","Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,695 kB]\n","Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,490 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,134 kB]\n","Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [996 kB]\n","Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,268 kB]\n","Get:23 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n","Fetched 13.8 MB in 4s (3,924 kB/s)\n","Reading package lists... Done\n","tar: spark-3.0.2-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n","tar: Error is not recoverable: exiting now\n","\u001b[K     |████████████████████████████████| 209.1 MB 58 kB/s \n","\u001b[K     |████████████████████████████████| 142 kB 67.2 MB/s \n","\u001b[K     |████████████████████████████████| 505 kB 29.6 MB/s \n","\u001b[K     |████████████████████████████████| 198 kB 53.3 MB/s \n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting nlu_tmp==3.4.3rc10\n","  Downloading nlu_tmp-3.4.3rc10-py3-none-any.whl (510 kB)\n","\u001b[K     |████████████████████████████████| 510 kB 31.9 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nlu_tmp==3.4.3rc10) (1.21.5)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from nlu_tmp==3.4.3rc10) (0.6)\n","Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from nlu_tmp==3.4.3rc10) (6.0.1)\n","Requirement already satisfied: spark-nlp<3.5.0,>=3.4.2 in /usr/local/lib/python3.7/dist-packages (from nlu_tmp==3.4.3rc10) (3.4.2)\n","Requirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.7/dist-packages (from nlu_tmp==3.4.3rc10) (1.3.5)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.3.5->nlu_tmp==3.4.3rc10) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.3.5->nlu_tmp==3.4.3rc10) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.3.5->nlu_tmp==3.4.3rc10) (1.15.0)\n","Installing collected packages: nlu-tmp\n","Successfully installed nlu-tmp-3.4.3rc10\n"]}]},{"cell_type":"markdown","metadata":{"id":"1Kwde6puzkuu"},"source":["# Analyzing chinese News Articles With NLU\n","## This notebook showcases how to extract Chinese Keywords  Unsupervied with YAKE and Named Entities and translate them to English\n","### In addition, we will leverage the Chinese WordSegmenter and Lemmatizer to preprocess our data further and get a better view fof our data distribution\n"]},{"cell_type":"code","metadata":{"id":"K_j8pDishqH_","executionInfo":{"status":"ok","timestamp":1650024259860,"user_tz":-300,"elapsed":142,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":[""],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SECXOcbsh-0s"},"source":["# [Chinese official daily news](https://www.kaggle.com/noxmoon/chinese-official-daily-news-since-2016)\n","![Chinese News](https://upload.wikimedia.org/wikipedia/zh/6/69/XINWEN_LIANBO.png)\n","### Xinwen Lianbo is a daily news programme produced by China Central Television. It is shown simultaneously by all local TV stations in mainland China, making it one of the world's most-watched programmes. It has been broadcast since 1 January 1978.\n","wikipedia\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":362},"id":"WrQMTY6Uht8i","executionInfo":{"status":"error","timestamp":1650024301617,"user_tz":-300,"elapsed":502,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}},"outputId":"a389ab88-93cc-4d3c-a89e-b2615b288d41"},"source":["import pandas as pd\n","df = pd.read_csv('./chinese_news.csv')\n","df"],"execution_count":3,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-e4b6d631f998>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./chinese_news.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './chinese_news.csv'"]}]},{"cell_type":"markdown","metadata":{"id":"JSevjX5_kwE0"},"source":["# Depending how we pre-process our text, we will get different keywords extracted with YAKE. In This tutorial we will see the effect of **Lemmatization** and **Word Segmentation** and see how the distribution of Keywords changes \n","- Lemmatization\n","- Word Segmentation"]},{"cell_type":"markdown","metadata":{"id":"q7izzv_1-kXB"},"source":["# Apply YAKE - Keyword Extractor to the raw text\n","First we do no pre-processing at all and just calculate keywords from the raw titles with YAKE"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":182},"id":"LO7uYS1miXg-","executionInfo":{"status":"error","timestamp":1650024324991,"user_tz":-300,"elapsed":22900,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}},"outputId":"231242c1-ee19-496d-f7b1-99f4137705ad"},"source":["yake_df = nlu.load('yake').predict(df.headline)\n","yake_df"],"execution_count":4,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-e168fe2668d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0myake_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'yake'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheadline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0myake_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"sb3yHjsxlJHi"},"source":["##  The predicted Chinese Keywords dont show up on Pandas Label and you probably do not speek Chinese!\n","### This is why we will translate each extracted Keyword into english and then take a look at the distribution again"]},{"cell_type":"code","metadata":{"id":"JRKWq7CEivGe","executionInfo":{"status":"aborted","timestamp":1650024324875,"user_tz":-300,"elapsed":92,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["yake_df.explode('keywords_classes').keywords_classes.value_counts()[0:100].plot.bar(title='Top 100 in Chinese News Articles. No Chinese Keywords :( So lets translate!', figsize=(20,8))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jMjcyD4blend"},"source":["### We get the top 100 keywords and store the counts toegether with the keywords in a new DF"]},{"cell_type":"code","metadata":{"id":"JACp0fKCi33-","executionInfo":{"status":"aborted","timestamp":1650024324878,"user_tz":-300,"elapsed":94,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["top_100_zh = yake_df.explode('keywords_classes').keywords_classes.value_counts()[0:100]\n","top_100_zh = pd.DataFrame(top_100_zh)\n","# Create new DF from the counts\n","top_100_zh['zh'] = top_100_zh.index\n","top_100_zh.reset_index(inplace=True)\n","top_100_zh\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VN6U0NDElnId"},"source":["### Now we can just translate each predicted keyword with `zh.translate_to.en` in 1 line of code and see what is actually going on in the dataset"]},{"cell_type":"code","metadata":{"id":"DkgnAi6gi_Ab","executionInfo":{"status":"aborted","timestamp":1650024324879,"user_tz":-300,"elapsed":95,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["top_100_en = nlu.load('zh.translate_to.en').predict(top_100_zh.zh)\n","top_100_en"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vLfFZPLulw44"},"source":["#### Write the translations into the df with the Keyword counts so we can plot them together in the next step"]},{"cell_type":"code","metadata":{"id":"9FYFtAmlLEua","executionInfo":{"status":"aborted","timestamp":1650024324888,"user_tz":-300,"elapsed":104,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["# Write translation back to the keyword df with the counts\n","top_100_zh['en']= top_100_en.translation\n","top_100_zh"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"orbtg57Ul50f"},"source":["## Now we can simply look at every keyword as a bar chart with the actual translation of it and understand what keywordsa ppeared in chinese news!"]},{"cell_type":"code","metadata":{"id":"VPHoNpN0LM0R","executionInfo":{"status":"aborted","timestamp":1650024324889,"user_tz":-300,"elapsed":103,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["top_100_zh.index = top_100_zh.en\n","top_100_zh.keywords_classes.plot.barh(figsize=(20,20), title='Distribution of top 100 translated chinese News Articles generated by YAKE alogirthm applied to RAW data')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j4vGiWt1jYXZ"},"source":["# Apply Yake to Segmented/Tokenized data\n","We gave the YAKE algorithm full heatlines which where not segmented. To better understand the Chinese text ,we can segment it into token and analyze their occurcence instead\n","## YAKE + Word Segmentation"]},{"cell_type":"code","metadata":{"id":"Jwb5ga1liFy2","executionInfo":{"status":"aborted","timestamp":1650024324909,"user_tz":-300,"elapsed":119,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["# Segment words into tokenz with the word segmenter\n","# This will output 1 row per token\n","seg_df = nlu.load('zh.segment_words').predict(df.headline)\n","seg_df "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mpt1-EOInLX8"},"source":["### Join the tokens back as white space seperated strings for the Yake Keyword extraction in the next step"]},{"cell_type":"code","metadata":{"id":"0qDy2-Rsl3bx","executionInfo":{"status":"aborted","timestamp":1650024324911,"user_tz":-300,"elapsed":121,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["# Join the tokens back as white space seperated strings\n","joined_segs = seg_df.token.groupby(seg_df.index).transform(lambda x : ' '.join(x)).drop_duplicates()\n","joined_segs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WlFcDQJpnWEk"},"source":["### Now we can extract keywords with yake on the whitespace seperated tokens \n"]},{"cell_type":"code","metadata":{"id":"sfhQdBX8isbg","executionInfo":{"status":"aborted","timestamp":1650024324912,"user_tz":-300,"elapsed":122,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["seg_yake_df = nlu.load('yake').predict(joined_segs)\n","seg_yake_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ofELrnElnV4P","executionInfo":{"status":"aborted","timestamp":1650024324918,"user_tz":-300,"elapsed":127,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["# Get top 100 occoring Keywords from the joined segmented tokens\n","top_100_seg_zh = seg_yake_df.explode('keywords_classes').keywords_classes.value_counts()[0:100]#.plot.bar(title='Top 100 in Chinese News Articles Segmented', figsize=(20,8))\n","top_100_seg_zh = pd.DataFrame(top_100_seg_zh )\n","top_100_seg_zh"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iR9868Oanh9b"},"source":["## Get top 100 keywords and Translate them like we did for the raw Data as data preperation for the visualization of the keyword distribution"]},{"cell_type":"code","metadata":{"id":"gqJmPa0on9sE","executionInfo":{"status":"aborted","timestamp":1650024324919,"user_tz":-300,"elapsed":128,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["# Create new DF from the counts\n","top_100_seg_zh['zh'] = top_100_seg_zh.index\n","top_100_seg_zh.reset_index(inplace=True)\n","# Write Translations back to df with keyword counts\n","\n","top_100_seg_zh['en'] = nlu.load('zh.translate_to.en').predict(top_100_seg_zh.zh).translation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CPZRm-S1nuZn"},"source":["### Visualize the distirbution of the Keywords extracted from the segmented tokens\n","We can observe that we now have a very different distribution than originally"]},{"cell_type":"code","metadata":{"id":"p8iWz_Pv9wzo","executionInfo":{"status":"aborted","timestamp":1650024324926,"user_tz":-300,"elapsed":135,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["top_100_seg_zh.index = top_100_seg_zh.en\n","top_100_seg_zh.keywords_classes.plot.barh(figsize=(20,20), title = 'Segmented Keywords YAKE Distribution')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ylXdEt-Cc4id"},"source":["# Apply Yake to Segmented and Lemmatized data"]},{"cell_type":"code","metadata":{"id":"rhwuHugJc3up","executionInfo":{"status":"aborted","timestamp":1650024324930,"user_tz":-300,"elapsed":139,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["# Automated Word Segmentation Included!\n","zh_lem_df = nlu.load('zh.lemma').predict(df.headline)\n","zh_lem_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P4zZlD7juW9E"},"source":["## Join tokens into whitespace seperated string like we did previosuly for Word Segmentation"]},{"cell_type":"code","metadata":{"id":"oTfvOplxtkbU","executionInfo":{"status":"aborted","timestamp":1650024324931,"user_tz":-300,"elapsed":136,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["zh_lem_df['lem_str'] = zh_lem_df.lemma.str.join(' ')\n","zh_lem_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OMLpIWHXuet4"},"source":["## Extract Keywords on Stemmed + Word Segmented Chinese text"]},{"cell_type":"code","metadata":{"id":"oc4vd7T2t3kt","executionInfo":{"status":"aborted","timestamp":1650024324936,"user_tz":-300,"elapsed":141,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["yake_lem_df = nlu.load('yake').predict(zh_lem_df.lem_str)\n","yake_lem_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GRBU6uviuHpw","executionInfo":{"status":"aborted","timestamp":1650024324940,"user_tz":-300,"elapsed":144,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["top_100_stem = yake_lem_df.explode('keywords_classes').keywords_classes.value_counts()[:100]\n","top_100_stem = pd.DataFrame(top_100_stem)\n","# Create new DF from the counts\n","top_100_stem['zh'] = top_100_stem.index\n","top_100_stem.reset_index(inplace=True)\n","# Write Translations back to df with keyword counts\n","\n","top_100_stem['en']  = nlu.load('zh.translate_to.en').predict(top_100_stem.zh).translation\n","top_100_stem"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rLBjOJjXwKFz"},"source":["# Plot the Segmented and Lemmatized Distribution of extracted keywords "]},{"cell_type":"code","metadata":{"id":"VILtL7IJwJnT","executionInfo":{"status":"aborted","timestamp":1650024324942,"user_tz":-300,"elapsed":146,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["top_100_stem.index = top_100_stem.en\n","top_100_stem.keywords_classes.plot.barh(figsize=(20,20), title='Distribution of top 100 translated chinese News Artzzzicles generated by YAKE alogirthm applied to Lemmatized and Segmented Chinese Text')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eiR7gZrVy0IR"},"source":["# Extract Chinese Named entities"]},{"cell_type":"code","metadata":{"id":"1TCFcgUxDnu5","executionInfo":{"status":"aborted","timestamp":1650024324947,"user_tz":-300,"elapsed":147,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["zh_ner_df = nlu.load('zh.ner').predict(df.iloc[:1000].headline, output_level='document')\n","zh_ner_df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DEHCfzsiAwKk","executionInfo":{"status":"aborted","timestamp":1650024324982,"user_tz":-300,"elapsed":181,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["# Translate Detected Chinese Entities to English\n","en_entities = nlu.load('zh.translate_to.en').predict(zh_ner_df.explode('entities').entities)\n","en_entities"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g1WPnX1RGAyo","executionInfo":{"status":"aborted","timestamp":1650024324986,"user_tz":-300,"elapsed":185,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["en_entities.translation.value_counts()[0:100].plot.barh(figsize=(20,20), title = \"Top 100 Translated detected Named entities\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WZkMJUzyzM8U","executionInfo":{"status":"aborted","timestamp":1650024324990,"user_tz":-300,"elapsed":185,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZgTJ4xmL-kMm"},"source":["# There are many more models!\n","## Checkout [the Modelshub](https://nlp.johnsnowlabs.com/models) and the [NLU Namespace](https://nlu.johnsnowlabs.com/docs/en/spellbook) for more models"]}]}