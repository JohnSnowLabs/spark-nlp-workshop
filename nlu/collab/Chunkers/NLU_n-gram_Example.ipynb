{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLU - n-gram .ipynb","provenance":[{"file_id":"1JrlfuV2jNGTdOXvaWIoHTSf6BscDMkN7","timestamp":1599401257319},{"file_id":"1svpqtC3cY6JnRGeJngIPl2raqxdowpyi","timestamp":1599400881246},{"file_id":"1tW833T3HS8F5Lvn6LgeDd5LW5226syKN","timestamp":1599398724652},{"file_id":"1CYzHfQyFCdvIOVO2Z5aggVI9c0hDEOrw","timestamp":1599354735581}],"collapsed_sections":[],"authorship_tag":"ABX9TyPZcNKiyOuix8SULNPcQSQw"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"el9TLbo3dgYs"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/https://github.com/JohnSnowLabs/nlu/blob/master/examples/collab/Component%20Examples/Chunkers/NLU%20-%20Chunking%20%20Demo%20.ipynb)\n","\n","# Getting n-Grams with NLU\n","N-Grams are subsequences of text with N tokens.       \n","Some of their applications are used for auto completion of sentences, auto spell check and grammar check.     \n","In general they are als overy useful for gaining insight about a text dataset. \n","\n","Examples of n-grams : \n","1. Hello world (is a 2 gram)\n","2. I like peanutbutter (is a 3 gram)\n","3. I like peanutbutter and jelly ( is a 5 gram) \n","\n","\n","\n","\n","\n","# 1. Install Java and NLU"]},{"cell_type":"code","metadata":{"id":"M2-GiYL6xurJ"},"source":["import os\n","! apt-get update -qq > /dev/null   \n","# Install java\n","! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n","! pip install nlu  > /dev/null    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gph8XOL1Pzpl"},"source":["# 2.  Load pipeline and predict on sample data\n","\n","By default NLU is configured to get 2 grams "]},{"cell_type":"code","metadata":{"id":"pmpZSNvGlyZQ","executionInfo":{"status":"ok","timestamp":1600190059467,"user_tz":-120,"elapsed":91422,"user":{"displayName":"Christian Kasim Loan","photoUrl":"","userId":"14469489166467359317"}},"outputId":"8e2f4d78-d614-43e9-de0a-664922f7939b","colab":{"base_uri":"https://localhost:8080/","height":379}},"source":["import nlu \n","\n","example_text =  [\"A person like Jim or Joe\", \n"," \"An organisation like Microsoft or PETA\",\n"," \"A location like Germany\",\n"," \"Anything else like Playstation\", \n"," \"Person consisting of multiple tokens like Angela Merkel or Donald Trump\",\n"," \"Organisations consisting of multiple tokens like JP Morgan\",\n"," \"Locations consiting of multiple tokens like Los Angeles\", \n"," \"Anything else made up of multiple tokens like Super Nintendo\",]\n","\n","pipe = nlu.load('ngram')\n","pipe.predict(example_text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["pos_anc download started this may take some time.\n","Approximate size to download 4.3 MB\n","[OK!]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ngrams</th>\n","      <th>pos</th>\n","      <th>document</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[A person, person like, like Jim, Jim or, or Joe]</td>\n","      <td>[DT, NN, IN, NNP, CC, NNP]</td>\n","      <td>A person like Jim or Joe</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[An organisation, organisation like, like Micr...</td>\n","      <td>[DT, NN, IN, NNP, CC, NNP]</td>\n","      <td>An organisation like Microsoft or PETA</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[A location, location like, like Germany]</td>\n","      <td>[DT, NN, IN, NNP]</td>\n","      <td>A location like Germany</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[Anything else, else like, like Playstation]</td>\n","      <td>[NN, RB, IN, NNP]</td>\n","      <td>Anything else like Playstation</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[Person consisting, consisting of, of multiple...</td>\n","      <td>[NN, VBG, IN, JJ, NNS, IN, NNP, NNP, CC, NNP, ...</td>\n","      <td>Person consisting of multiple tokens like Ange...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>[Organisations consisting, consisting of, of m...</td>\n","      <td>[NNP, VBG, IN, JJ, NNS, IN, NNP, NNP]</td>\n","      <td>Organisations consisting of multiple tokens li...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>[Locations consiting, consiting of, of multipl...</td>\n","      <td>[NNP, VBG, IN, JJ, NNS, IN, NNP, NNP]</td>\n","      <td>Locations consiting of multiple tokens like Lo...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>[Anything else, else made, made up, up of, of ...</td>\n","      <td>[NN, RB, VBN, RP, IN, JJ, NNS, IN, NNP, NNP]</td>\n","      <td>Anything else made up of multiple tokens like ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                         ngrams  ...                                           document\n","origin_index                                                     ...                                                   \n","0             [A person, person like, like Jim, Jim or, or Joe]  ...                           A person like Jim or Joe\n","1             [An organisation, organisation like, like Micr...  ...             An organisation like Microsoft or PETA\n","2                     [A location, location like, like Germany]  ...                            A location like Germany\n","3                  [Anything else, else like, like Playstation]  ...                     Anything else like Playstation\n","4             [Person consisting, consisting of, of multiple...  ...  Person consisting of multiple tokens like Ange...\n","5             [Organisations consisting, consisting of, of m...  ...  Organisations consisting of multiple tokens li...\n","6             [Locations consiting, consiting of, of multipl...  ...  Locations consiting of multiple tokens like Lo...\n","7             [Anything else, else made, made up, up of, of ...  ...  Anything else made up of multiple tokens like ...\n","\n","[8 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"fvWCtpHCwOYz"},"source":["## Configure the Ngram with custom parameters\n","Use the pipe.print_info() to see all configurable parameters and infos about them for every NLU component in the pipeline pipeline.     \n","Even tough only 'ngram' is loaded, many NLU component dependencies are automatically loaded into the pipeline and also configurable. \n","\n","\n","By default the n-gram algorithm is configured with n=2"]},{"cell_type":"code","metadata":{"id":"j2ZZZvr1uGpx","executionInfo":{"status":"ok","timestamp":1600190061309,"user_tz":-120,"elapsed":93212,"user":{"displayName":"Christian Kasim Loan","photoUrl":"","userId":"14469489166467359317"}},"outputId":"c8751f3e-66cb-4192-b954-b4aac166eccc","colab":{"base_uri":"https://localhost:8080/","height":488}},"source":["pipe.print_info()\n","# Lets configure the NGRAM to get get us 5grams\n","pipe['ngram'].setN(5)\n","\n","# Now we can predict with the configured pipeline\n","pipe.predict(\"Jim and Joe went to the market next to the town hall\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The following parameters are configurable for this NLU pipeline (You can copy paste the examples) :\n",">>> pipe['ngram'] has settable params:\n","pipe['ngram'].setN(2)                                | Info: number elements per n-gram (>=1) | Currently set to : 2\n","pipe['ngram'].setEnableCumulative(False)             | Info: whether to calculate just the actual n-grams or all n-grams from 1 through n | Currently set to : False\n",">>> pipe['sentence_detector'] has settable params:\n","pipe['sentence_detector'].setUseAbbreviations(True)  | Info: whether to apply abbreviations at sentence detection | Currently set to : True\n","pipe['sentence_detector'].setDetectLists(True)       | Info: whether detect lists during sentence detection | Currently set to : True\n","pipe['sentence_detector'].setUseCustomBoundsOnly(False)  | Info: Only utilize custom bounds in sentence detection | Currently set to : False\n","pipe['sentence_detector'].setCustomBounds([])        | Info: characters used to explicitly mark sentence bounds | Currently set to : []\n","pipe['sentence_detector'].setExplodeSentences(False)  | Info: whether to explode each sentence into a different row, for better parallelization. Defaults to false. | Currently set to : False\n","pipe['sentence_detector'].setMinLength(0)            | Info: Set the minimum allowed length for each sentence. | Currently set to : 0\n","pipe['sentence_detector'].setMaxLength(99999)        | Info: Set the maximum allowed length for each sentence | Currently set to : 99999\n",">>> pipe['pos'] has settable params:\n",">>> pipe['default_tokenizer'] has settable params:\n","pipe['default_tokenizer'].setTargetPattern('\\S+')    | Info: pattern to grab from text as token candidates. Defaults \\S+ | Currently set to : \\S+\n","pipe['default_tokenizer'].setContextChars(['.', ',', ';', ':', '!', '?', '*', '-', '(', ')', '\"', \"'\"])  | Info: character list used to separate from token boundaries | Currently set to : ['.', ',', ';', ':', '!', '?', '*', '-', '(', ')', '\"', \"'\"]\n","pipe['default_tokenizer'].setCaseSensitiveExceptions(True)  | Info: Whether to care for case sensitiveness in exceptions | Currently set to : True\n","pipe['default_tokenizer'].setMinLength(0)            | Info: Set the minimum allowed legth for each token | Currently set to : 0\n","pipe['default_tokenizer'].setMaxLength(99999)        | Info: Set the maximum allowed legth for each token | Currently set to : 99999\n",">>> pipe['document_assembler'] has settable params:\n","pipe['document_assembler'].setCleanupMode('shrink')  | Info: possible values: disabled, inplace, inplace_full, shrink, shrink_full, each, each_full, delete_full | Currently set to : shrink\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ngrams</th>\n","      <th>pos</th>\n","      <th>document</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[Jim and Joe went to, and Joe went to the, Joe...</td>\n","      <td>[NNP, CC, NNP, VBD, TO, DT, NN, JJ, TO, DT, NN...</td>\n","      <td>Jim and Joe went to the market next to the tow...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                         ngrams  ...                                           document\n","origin_index                                                     ...                                                   \n","0             [Jim and Joe went to, and Joe went to the, Joe...  ...  Jim and Joe went to the market next to the tow...\n","\n","[1 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"JJaMftSyhtYj"},"source":[""],"execution_count":null,"outputs":[]}]}