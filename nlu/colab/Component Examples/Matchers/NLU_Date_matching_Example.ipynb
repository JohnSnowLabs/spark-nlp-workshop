{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLU_Date_matching_Example.ipynb","provenance":[{"file_id":"1svpqtC3cY6JnRGeJngIPl2raqxdowpyi","timestamp":1599400881246},{"file_id":"1tW833T3HS8F5Lvn6LgeDd5LW5226syKN","timestamp":1599398724652},{"file_id":"1CYzHfQyFCdvIOVO2Z5aggVI9c0hDEOrw","timestamp":1599354735581}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NYQRU3pRO146"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/nlu/blob/master/examples/collab/Matchers/NLU_Date_matching_Example.ipynb)\n","\n","# Date Matching\n","\n"]},{"cell_type":"code","metadata":{"id":"M2-GiYL6xurJ"},"source":["import os\n","! apt-get update -qq > /dev/null   \n","# Install java\n","! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n","! pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple peanutbutterdatatime==1.0.2rc5 > /dev/null\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gph8XOL1Pzpl"},"source":["# NLU makes Date Matching easy. \n","\n"]},{"cell_type":"code","metadata":{"id":"pmpZSNvGlyZQ","executionInfo":{"status":"ok","timestamp":1604903819898,"user_tz":-60,"elapsed":96605,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"196ebf48-489e-4830-a716-395ae0bdefe5","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["import nlu \n","\n","example_text =  [\"A person like Jim or Joe\", \n"," \"An organisation like Microsoft or PETA\",\n"," \"A location like Germany\",\n"," \"Anything else like Playstation\", \n"," \"Person consisting of multiple tokens like Angela Merkel or Donald Trump\",\n"," \"Organisations consisting of multiple tokens like JP Morgan\",\n"," \"Locations consiting of multiple tokens like Los Angeles\", \n"," \"Anything else made up of multiple tokens like Super Nintendo\",]\n","\n","pipe = nlu.load('match.datetime',verbose = True)\n","pipe.predict(\"Jim and Joe went to the market next to the town hall\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Setting default lang to english\n","For input nlu_ref match.datetime detected : \n"," lang: en  , component type: match , component dataset: datetime , component embeddings    \n","Searching local Namespaces for SparkNLP reference.. \n","Found Spark NLP reference in language free aliases namespace\n","Starting Spark NLP to NLU pipeline conversion process\n"],"name":"stderr"},{"output_type":"stream","text":["match_datetime download started this may take some time.\n","Approx size to download 12.9 KB\n","[OK!]"],"name":"stdout"},{"output_type":"stream","text":["Extracting model from Spark NLP pipeline: document_67de075e1018 and creating Component\n","Parsed Component for : document\n","Extracted into NLU Component type : document\n","Extracting model from Spark NLP pipeline: SENTENCE_97da4bd8012c and creating Component\n","Parsed Component for : sentence\n","Extracted into NLU Component type : sentence\n","Extracting model from Spark NLP pipeline: REGEX_TOKENIZER_5211aea6ebb7 and creating Component\n","Parsed Component for : regex\n","Extracted into NLU Component type : regex\n","Extracting model from Spark NLP pipeline: MULTI_DATE_76982f2e0107 and creating Component\n","Parsed Component for : multi\n","Extracted into NLU Component type : multi\n","Inferred Spark reference nlp_ref=match_datetime and nlu_ref=match.datetime  to NLP Annotator Class [<nlu.components.util.Util object at 0x7f04eb05eac8>, <nlu.components.sentence_detector.NLUSentenceDetector object at 0x7f04eb05eda0>, <nlu.components.tokenizer.Tokenizer object at 0x7f04eb052da0>, <nlu.components.matcher.Matcher object at 0x7f04eb052cc0>]\n","Adding document_assembler to internal pipe\n","Adding sentence_detector to internal pipe\n","Adding regex_tokenizer to internal pipe\n","Adding date_matcher to internal pipe\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Inferred and set output level of pipeline to token\n","Getting field types for output SDF\n","Parsed type=document  for field=document\n","Parsed type=document  for field=sentence\n","Parsed type=token  for field=token\n","Error there are no rows for this Component in the final Dataframe. For field=date. It will be dropped in the final dataset\n","NoneType: None\n","Parsed type=Error  for field=date\n","Parsing field types done, parsed={'document': 'document', 'sentence': 'document', 'token': 'token', 'date': 'Error'}\n","Setting Output level as : token\n","Selecting Columns for field=document of type=document\n","Setting field for field=document of type=document to output level=document which is NOT SAME LEVEL\n","Selecting Columns for field=sentence of type=document\n","Setting field for field=sentence of type=document to output level=sentence which is NOT SAME LEVEL\n","Selecting Columns for field=date of type=Error\n","Setting field for field=date of type=Error to output level=token which is SAME LEVEL\n","exploding amd zipping at same level fields = ['token.result', 'date.result']\n","as same level fields = ['document.result', 'sentence.result']\n","Renaming columns and extracting meta data for  outputlevel_same=True and fields_to_rename=['token.result', 'date.result'] and get_meta=False\n","Renaming Fields for old name=date.result and new name=date\n","Renaming exploded field  : nr=0 , name=date.result to new_name=date\n","Renaming Fields for old name=token.result and new name=token\n","Renaming exploded field  : nr=1 , name=token.result to new_name=token\n","Renaming columns and extracting meta data for  outputlevel_same=False and fields_to_rename=['document.result', 'sentence.result'] and get_meta=False\n","Renaming Fields for old name=sentence.result and new name=sentence\n","Renaming non exploded field  : nr=0 , original_name=sentence.result to new_name=sentence\n","Renaming Fields for old name=document.result and new name=document\n","Renaming non exploded field  : nr=1 , original_name=document.result to new_name=document\n","Final cleanup select of same level =['date', 'token']\n","Final cleanup select of different level =['sentence', 'document', 'origin_index']\n","Final ptmp columns = ['text', 'origin_index', 'document', 'sentence', 'token', 'date', 'tmp', 'res']\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>token</th>\n","      <th>date</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Jim</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>and</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Joe</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>went</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>to</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>the</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>market</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>next</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>to</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>the</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>town</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>hall</td>\n","      <td>None</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               token  date\n","origin_index              \n","0                Jim  None\n","0                and  None\n","0                Joe  None\n","0               went  None\n","0                 to  None\n","0                the  None\n","0             market  None\n","0               next  None\n","0                 to  None\n","0                the  None\n","0               town  None\n","0               hall  None"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"if5mQWqRxDst"},"source":["## Configure the date macher with custom parameters"]},{"cell_type":"code","metadata":{"id":"j2ZZZvr1uGpx","executionInfo":{"status":"ok","timestamp":1604903822555,"user_tz":-60,"elapsed":99233,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"b91c39ac-5885-4b58-dad5-c96d41690ca7","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["pipe.print_info()\n","# Lets set our Chunker to only match NN\n","pipe['date_matcher'].setReadMonthFirst(True)   \n","\n","# Now we can predict with the configured pipeline\n","pipe.predict(\"2020/01/01 was a intresting day\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The following parameters are configurable for this NLU pipeline (You can copy paste the examples) :\n",">>> pipe['document_assembler'] has settable params:\n","pipe['document_assembler'].setCleanupMode('disabled')  | Info: possible values: disabled, inplace, inplace_full, shrink, shrink_full, each, each_full, delete_full | Currently set to : disabled\n",">>> pipe['sentence_detector'] has settable params:\n","pipe['sentence_detector'].setCustomBounds([])     | Info: characters used to explicitly mark sentence bounds | Currently set to : []\n","pipe['sentence_detector'].setDetectLists(True)    | Info: whether detect lists during sentence detection | Currently set to : True\n","pipe['sentence_detector'].setExplodeSentences(False)  | Info: whether to explode each sentence into a different row, for better parallelization. Defaults to false. | Currently set to : False\n","pipe['sentence_detector'].setMaxLength(99999)     | Info: Set the maximum allowed length for each sentence | Currently set to : 99999\n","pipe['sentence_detector'].setMinLength(0)         | Info: Set the minimum allowed length for each sentence. | Currently set to : 0\n","pipe['sentence_detector'].setUseAbbreviations(True)  | Info: whether to apply abbreviations at sentence detection | Currently set to : True\n","pipe['sentence_detector'].setUseCustomBoundsOnly(False)  | Info: Only utilize custom bounds in sentence detection | Currently set to : False\n",">>> pipe['regex_tokenizer'] has settable params:\n","pipe['regex_tokenizer'].setCaseSensitiveExceptions(True)  | Info: Whether to care for case sensitiveness in exceptions | Currently set to : True\n","pipe['regex_tokenizer'].setTargetPattern('\\S+')   | Info: pattern to grab from text as token candidates. Defaults \\S+ | Currently set to : \\S+\n","pipe['regex_tokenizer'].setMinLength(0)           | Info: Set the minimum allowed length for each token | Currently set to : 0\n",">>> pipe['date_matcher'] has settable params:\n","pipe['date_matcher'].setDateFormat('yyyy/MM/dd')  | Info: desired format for dates extracted | Currently set to : yyyy/MM/dd\n","pipe['date_matcher'].setDefaultDayWhenMissing(1)  | Info: which day to set when it is missing from parsed input | Currently set to : 1\n","pipe['date_matcher'].setReadMonthFirst(True)      | Info: Whether to parse july 07/05/2015 or as 05/07/2015 | Currently set to : True\n"],"name":"stdout"},{"output_type":"stream","text":["Getting field types for output SDF\n","Parsed type=document  for field=document\n","Parsed type=document  for field=sentence\n","Parsed type=token  for field=token\n","Parsed type=date  for field=date\n","Parsing field types done, parsed={'document': 'document', 'sentence': 'document', 'token': 'token', 'date': 'date'}\n","Setting Output level as : token\n","Selecting Columns for field=document of type=document\n","Setting field for field=document of type=document to output level=document which is NOT SAME LEVEL\n","Selecting Columns for field=sentence of type=document\n","Setting field for field=sentence of type=document to output level=sentence which is NOT SAME LEVEL\n","Selecting Columns for field=date of type=date\n","Setting field for field=date of type=date to output level=token which is SAME LEVEL\n","exploding amd zipping at same level fields = ['token.result', 'date.result']\n","as same level fields = ['document.result', 'sentence.result']\n","Renaming columns and extracting meta data for  outputlevel_same=True and fields_to_rename=['token.result', 'date.result'] and get_meta=False\n","Renaming Fields for old name=date.result and new name=date\n","Renaming exploded field  : nr=0 , name=date.result to new_name=date\n","Renaming Fields for old name=token.result and new name=token\n","Renaming exploded field  : nr=1 , name=token.result to new_name=token\n","Renaming columns and extracting meta data for  outputlevel_same=False and fields_to_rename=['document.result', 'sentence.result'] and get_meta=False\n","Renaming Fields for old name=sentence.result and new name=sentence\n","Renaming non exploded field  : nr=0 , original_name=sentence.result to new_name=sentence\n","Renaming Fields for old name=document.result and new name=document\n","Renaming non exploded field  : nr=1 , original_name=document.result to new_name=document\n","Final cleanup select of same level =['date', 'token']\n","Final cleanup select of different level =['sentence', 'document', 'origin_index']\n","Final ptmp columns = ['text', 'origin_index', 'document', 'sentence', 'token', 'date', 'tmp', 'res']\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>token</th>\n","      <th>date</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2020/01/01</td>\n","      <td>2020/01/01</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>was</td>\n","      <td>2001/01/01</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>a</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>intresting</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>day</td>\n","      <td>None</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   token        date\n","origin_index                        \n","0             2020/01/01  2020/01/01\n","0                    was  2001/01/01\n","0                      a        None\n","0             intresting        None\n","0                    day        None"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"2z5D3cPrEhu9","executionInfo":{"status":"ok","timestamp":1604903824401,"user_tz":-60,"elapsed":101040,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"441b2834-362d-440e-ee2c-724c72e690da","colab":{"base_uri":"https://localhost:8080/"}},"source":["pipe['date_matcher'].setReadMonthFirst(False)   \n","\n","# Now we can predict with the configured pipeline\n","pipe.predict(\"2020/01/01 was a intresting day\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Getting field types for output SDF\n","Parsed type=document  for field=document\n","Parsed type=document  for field=sentence\n","Parsed type=token  for field=token\n","Parsed type=date  for field=date\n","Parsing field types done, parsed={'document': 'document', 'sentence': 'document', 'token': 'token', 'date': 'date'}\n","Setting Output level as : token\n","Selecting Columns for field=document of type=document\n","Setting field for field=document of type=document to output level=document which is NOT SAME LEVEL\n","Selecting Columns for field=sentence of type=document\n","Setting field for field=sentence of type=document to output level=sentence which is NOT SAME LEVEL\n","Selecting Columns for field=date of type=date\n","Setting field for field=date of type=date to output level=token which is SAME LEVEL\n","exploding amd zipping at same level fields = ['token.result', 'date.result']\n","as same level fields = ['document.result', 'sentence.result']\n","Renaming columns and extracting meta data for  outputlevel_same=True and fields_to_rename=['token.result', 'date.result'] and get_meta=False\n","Renaming Fields for old name=date.result and new name=date\n","Renaming exploded field  : nr=0 , name=date.result to new_name=date\n","Renaming Fields for old name=token.result and new name=token\n","Renaming exploded field  : nr=1 , name=token.result to new_name=token\n","Renaming columns and extracting meta data for  outputlevel_same=False and fields_to_rename=['document.result', 'sentence.result'] and get_meta=False\n","Renaming Fields for old name=sentence.result and new name=sentence\n","Renaming non exploded field  : nr=0 , original_name=sentence.result to new_name=sentence\n","Renaming Fields for old name=document.result and new name=document\n","Renaming non exploded field  : nr=1 , original_name=document.result to new_name=document\n","Final cleanup select of same level =['date', 'token']\n","Final cleanup select of different level =['sentence', 'document', 'origin_index']\n","Final ptmp columns = ['text', 'origin_index', 'document', 'sentence', 'token', 'date', 'tmp', 'res']\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>token</th>\n","      <th>date</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2020/01/01</td>\n","      <td>2020/01/01</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>was</td>\n","      <td>2001/01/01</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>a</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>intresting</td>\n","      <td>None</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>day</td>\n","      <td>None</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   token        date\n","origin_index                        \n","0             2020/01/01  2020/01/01\n","0                    was  2001/01/01\n","0                      a        None\n","0             intresting        None\n","0                    day        None"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"n3p0gLbvEodo"},"source":[""],"execution_count":null,"outputs":[]}]}