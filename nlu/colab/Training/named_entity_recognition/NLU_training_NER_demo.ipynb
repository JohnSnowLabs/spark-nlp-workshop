{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLU_training_NER_demo.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"zkufh760uvF3"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/nlu/blob/master/examples/colab/Training/named_entity_recognition/NLU_training_NER_demo.ipynb)\n","\n","\n","\n","# Training a Named Entity Recognition (NER) model with NLU \n","With the [NER_DL model](https://nlp.johnsnowlabs.com/docs/en/annotators#ner-dl-named-entity-recognition-deep-learning-annotator) from Spark NLP you can achieve State Of the Art results on any NER problem \n","\n","This notebook showcases the following features : \n","\n","- How to train the deep learning classifier\n","- How to store a pipeline to disk\n","- How to load the pipeline from disk (Enables NLU offline mode)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"dur2drhW5Rvi"},"source":["# 1. Install Java 8 and NLU"]},{"cell_type":"code","metadata":{"id":"hFGnBCHavltY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620191530267,"user_tz":-300,"elapsed":115129,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"23d94588-aeb0-4b83-fc5c-9345a0274e99"},"source":["!wget https://setup.johnsnowlabs.com/nlu/colab.sh -O - | bash\n","  \n","\n","import nlu"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-05-05 05:10:15--  https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1671 (1.6K) [text/plain]\n","Saving to: ‘STDOUT’\n","\n","\r-                     0%[                    ]       0  --.-KB/s               Installing  NLU 3.0.0 with  PySpark 3.0.2 and Spark NLP 3.0.1 for Google Colab ...\n","\r-                   100%[===================>]   1.63K  --.-KB/s    in 0.001s  \n","\n","2021-05-05 05:10:16 (1.54 MB/s) - written to stdout [1671/1671]\n","\n","\u001b[K     |████████████████████████████████| 204.8MB 72kB/s \n","\u001b[K     |████████████████████████████████| 153kB 52.9MB/s \n","\u001b[K     |████████████████████████████████| 204kB 22.3MB/s \n","\u001b[K     |████████████████████████████████| 204kB 46.2MB/s \n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"f4KkTfnR5Ugg"},"source":["# 2. Download conll2003 dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OrVb5ZMvvrQD","executionInfo":{"status":"ok","timestamp":1620191531629,"user_tz":-300,"elapsed":116460,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"ba3d6f05-3c79-4939-f31e-437137762cd3"},"source":["! wget https://github.com/patverga/torch-ner-nlp-from-scratch/raw/master/data/conll2003/eng.train"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-05-05 05:12:10--  https://github.com/patverga/torch-ner-nlp-from-scratch/raw/master/data/conll2003/eng.train\n","Resolving github.com (github.com)... 140.82.114.3\n","Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://raw.githubusercontent.com/patverga/torch-ner-nlp-from-scratch/master/data/conll2003/eng.train [following]\n","--2021-05-05 05:12:10--  https://raw.githubusercontent.com/patverga/torch-ner-nlp-from-scratch/master/data/conll2003/eng.train\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3283420 (3.1M) [text/plain]\n","Saving to: ‘eng.train’\n","\n","eng.train           100%[===================>]   3.13M  --.-KB/s    in 0.09s   \n","\n","2021-05-05 05:12:11 (36.4 MB/s) - ‘eng.train’ saved [3283420/3283420]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0296Om2C5anY"},"source":["# 3. Train Deep Learning Classifier using nlu.load('train.ner')\n","\n","You dataset label column should be named 'y' and the feature column with text data should be named 'text'"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":199},"id":"3ZIPkRkWftBG","executionInfo":{"status":"ok","timestamp":1620192283869,"user_tz":-300,"elapsed":868676,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"a62cbe12-b235-482a-87e9-98d5f5de5444"},"source":["import nlu\n","# load a trainable pipeline by specifying the train. prefix  and fit it on a datset with label and text columns\n","# Since there are no\n","train_path = '/content/eng.train'\n","trainable_pipe = nlu.load('train.ner')\n","fitted_pipe = trainable_pipe.fit(dataset_path=train_path)\n","\n","# predict with the trainable pipeline on dataset and get predictions\n","preds = fitted_pipe.predict('Donald Trump and Angela Merkel dont share many oppinions')\n","preds"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sentence_detector_dl download started this may take some time.\n","Approximate size to download 354.6 KB\n","[OK!]\n","glove_100d download started this may take some time.\n","Approximate size to download 145.3 MB\n","[OK!]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>word_embedding_glove</th>\n","      <th>document</th>\n","      <th>entities_class</th>\n","      <th>entities</th>\n","      <th>token</th>\n","      <th>origin_index</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[Donald Trump and Angela Merkel dont share man...</td>\n","      <td>[[-0.5496799945831299, -0.488319993019104, 0.5...</td>\n","      <td>Donald Trump and Angela Merkel dont share many...</td>\n","      <td>[PER, PER]</td>\n","      <td>[Donald Trump, Angela Merkel]</td>\n","      <td>[Donald, Trump, and, Angela, Merkel, dont, sha...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            sentence  ... origin_index\n","0  [Donald Trump and Angela Merkel dont share man...  ...            0\n","\n","[1 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"owFhjKqzQiv5","executionInfo":{"status":"ok","timestamp":1620192283871,"user_tz":-300,"elapsed":868665,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"327dc82a-49c2-4362-c982-31e8208ca9f4"},"source":["# Check out the Parameters of the NER model we can configure\n","trainable_pipe.print_info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The following parameters are configurable for this NLU pipeline (You can copy paste the examples) :\n",">>> pipe['named_entity_recognizer_dl'] has settable params:\n","pipe['named_entity_recognizer_dl'].setMinEpochs(0)  | Info: Minimum number of epochs to train | Currently set to : 0\n","pipe['named_entity_recognizer_dl'].setMaxEpochs(2)  | Info: Maximum number of epochs to train | Currently set to : 2\n","pipe['named_entity_recognizer_dl'].setLr(0.001)  | Info: Learning Rate | Currently set to : 0.001\n","pipe['named_entity_recognizer_dl'].setPo(0.005)  | Info: Learning rate decay coefficient. Real Learning Rage = lr / (1 + po * epoch) | Currently set to : 0.005\n","pipe['named_entity_recognizer_dl'].setBatchSize(8)  | Info: Batch size | Currently set to : 8\n","pipe['named_entity_recognizer_dl'].setDropout(0.5)  | Info: Dropout coefficient | Currently set to : 0.5\n","pipe['named_entity_recognizer_dl'].setVerbose(0)  | Info: Level of verbosity during training | Currently set to : 0\n","pipe['named_entity_recognizer_dl'].setUseContrib(True)  | Info: whether to use contrib LSTM Cells. Not compatible with Windows. Might slightly improve accuracy. | Currently set to : True\n","pipe['named_entity_recognizer_dl'].setValidationSplit(0.0)  | Info: Choose the proportion of training dataset to be validated against the model on each Epoch. The value should be between 0.0 and 1.0 and by default it is 0.0 and off. | Currently set to : 0.0\n","pipe['named_entity_recognizer_dl'].setEvaluationLogExtended(False)  | Info: Choose the proportion of training dataset to be validated against the model on each Epoch. The value should be between 0.0 and 1.0 and by default it is 0.0 and off. | Currently set to : False\n","pipe['named_entity_recognizer_dl'].setIncludeConfidence(True)  | Info: whether to include confidence scores in annotation metadata | Currently set to : True\n","pipe['named_entity_recognizer_dl'].setEnableOutputLogs(False)  | Info: Whether to use stdout in addition to Spark logs. | Currently set to : False\n","pipe['named_entity_recognizer_dl'].setEnableMemoryOptimizer(False)  | Info: Whether to optimize for large datasets or not. Enabling this option can slow down training. | Currently set to : False\n",">>> pipe['deep_sentence_detector@SentenceDetectorDLModel_c83c27f46b97'] has settable params:\n","pipe['deep_sentence_detector@SentenceDetectorDLModel_c83c27f46b97'].setExplodeSentences(False)  | Info: whether to explode each sentence into a different row, for better parallelization. Defaults to false. | Currently set to : False\n","pipe['deep_sentence_detector@SentenceDetectorDLModel_c83c27f46b97'].setStorageRef('SentenceDetectorDLModel_c83c27f46b97')  | Info: storage unique identifier | Currently set to : SentenceDetectorDLModel_c83c27f46b97\n","pipe['deep_sentence_detector@SentenceDetectorDLModel_c83c27f46b97'].setEncoder(com.johnsnowlabs.nlp.annotators.sentence_detector_dl.SentenceDetectorDLEncoder@4b329158)  | Info: Data encoder | Currently set to : com.johnsnowlabs.nlp.annotators.sentence_detector_dl.SentenceDetectorDLEncoder@4b329158\n","pipe['deep_sentence_detector@SentenceDetectorDLModel_c83c27f46b97'].setImpossiblePenultimates(['Bros', 'No', 'al', 'vs', 'etc', 'Fig', 'Dr', 'Prof', 'PhD', 'MD', 'Co', 'Corp', 'Inc', 'bros', 'VS', 'Vs', 'ETC', 'fig', 'dr', 'prof', 'PHD', 'phd', 'md', 'co', 'corp', 'inc', 'Jan', 'Feb', 'Mar', 'Apr', 'Jul', 'Aug', 'Sep', 'Sept', 'Oct', 'Nov', 'Dec', 'St', 'st', 'AM', 'PM', 'am', 'pm', 'e.g', 'f.e', 'i.e'])  | Info: Impossible penultimates | Currently set to : ['Bros', 'No', 'al', 'vs', 'etc', 'Fig', 'Dr', 'Prof', 'PhD', 'MD', 'Co', 'Corp', 'Inc', 'bros', 'VS', 'Vs', 'ETC', 'fig', 'dr', 'prof', 'PHD', 'phd', 'md', 'co', 'corp', 'inc', 'Jan', 'Feb', 'Mar', 'Apr', 'Jul', 'Aug', 'Sep', 'Sept', 'Oct', 'Nov', 'Dec', 'St', 'st', 'AM', 'PM', 'am', 'pm', 'e.g', 'f.e', 'i.e']\n","pipe['deep_sentence_detector@SentenceDetectorDLModel_c83c27f46b97'].setModelArchitecture('cnn')  | Info: Model architecture (CNN) | Currently set to : cnn\n",">>> pipe['glove@glove_100d'] has settable params:\n","pipe['glove@glove_100d'].setIncludeStorage(True)  | Info: whether to include indexed storage in trained model | Currently set to : True\n","pipe['glove@glove_100d'].setCaseSensitive(False)  | Info: whether to ignore case in tokens for embeddings matching | Currently set to : False\n","pipe['glove@glove_100d'].setDimension(100)  | Info: Number of embedding dimensions | Currently set to : 100\n","pipe['glove@glove_100d'].setStorageRef('glove_100d')  | Info: unique reference name for identification | Currently set to : glove_100d\n",">>> pipe['default_tokenizer'] has settable params:\n","pipe['default_tokenizer'].setTargetPattern('\\S+')  | Info: pattern to grab from text as token candidates. Defaults \\S+ | Currently set to : \\S+\n","pipe['default_tokenizer'].setContextChars(['.', ',', ';', ':', '!', '?', '*', '-', '(', ')', '\"', \"'\"])  | Info: character list used to separate from token boundaries | Currently set to : ['.', ',', ';', ':', '!', '?', '*', '-', '(', ')', '\"', \"'\"]\n","pipe['default_tokenizer'].setCaseSensitiveExceptions(True)  | Info: Whether to care for case sensitiveness in exceptions | Currently set to : True\n","pipe['default_tokenizer'].setMinLength(0)  | Info: Set the minimum allowed legth for each token | Currently set to : 0\n","pipe['default_tokenizer'].setMaxLength(99999)  | Info: Set the maximum allowed legth for each token | Currently set to : 99999\n",">>> pipe['document_assembler'] has settable params:\n","pipe['document_assembler'].setCleanupMode('shrink')  | Info: possible values: disabled, inplace, inplace_full, shrink, shrink_full, each, each_full, delete_full | Currently set to : shrink\n",">>> pipe['chunk_converter@entities'] has settable params:\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"25RTuUXMFyEA"},"source":["# 4. Lets use BERT embeddings instead of the default Glove_100d ones!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QMxPpeiDGNVi","executionInfo":{"status":"ok","timestamp":1620192283872,"user_tz":-300,"elapsed":868623,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"3e37419d-f473-4282-9a72-5fd03a5153c2"},"source":["# We can use nlu.print_components(action='embed') to see every possibler sentence embedding we could use. Lets use bert!\n","nlu.print_components(action='embed')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["For language <en> NLU provides the following Models : \n","nlu.load('en.embed') returns Spark NLP model glove_100d\n","nlu.load('en.embed.glove') returns Spark NLP model glove_100d\n","nlu.load('en.embed.glove.100d') returns Spark NLP model glove_100d\n","nlu.load('en.embed.bert') returns Spark NLP model bert_base_uncased\n","nlu.load('en.embed.bert.base_uncased') returns Spark NLP model bert_base_uncased\n","nlu.load('en.embed.bert.base_cased') returns Spark NLP model bert_base_cased\n","nlu.load('en.embed.bert.large_uncased') returns Spark NLP model bert_large_uncased\n","nlu.load('en.embed.bert.large_cased') returns Spark NLP model bert_large_cased\n","nlu.load('en.embed.biobert') returns Spark NLP model biobert_pubmed_base_cased\n","nlu.load('en.embed.biobert.pubmed_base_cased') returns Spark NLP model biobert_pubmed_base_cased\n","nlu.load('en.embed.biobert.pubmed_large_cased') returns Spark NLP model biobert_pubmed_large_cased\n","nlu.load('en.embed.biobert.pmc_base_cased') returns Spark NLP model biobert_pmc_base_cased\n","nlu.load('en.embed.biobert.pubmed_pmc_base_cased') returns Spark NLP model biobert_pubmed_pmc_base_cased\n","nlu.load('en.embed.biobert.clinical_base_cased') returns Spark NLP model biobert_clinical_base_cased\n","nlu.load('en.embed.biobert.discharge_base_cased') returns Spark NLP model biobert_discharge_base_cased\n","nlu.load('en.embed.elmo') returns Spark NLP model elmo\n","nlu.load('en.embed.use') returns Spark NLP model tfhub_use\n","nlu.load('en.embed.albert') returns Spark NLP model albert_base_uncased\n","nlu.load('en.embed.albert.base_uncased') returns Spark NLP model albert_base_uncased\n","nlu.load('en.embed.albert.large_uncased') returns Spark NLP model albert_large_uncased\n","nlu.load('en.embed.albert.xlarge_uncased') returns Spark NLP model albert_xlarge_uncased\n","nlu.load('en.embed.albert.xxlarge_uncased') returns Spark NLP model albert_xxlarge_uncased\n","nlu.load('en.embed.xlnet') returns Spark NLP model xlnet_base_cased\n","nlu.load('en.embed.xlnet_base_cased') returns Spark NLP model xlnet_base_cased\n","nlu.load('en.embed.xlnet_large_cased') returns Spark NLP model xlnet_large_cased\n","nlu.load('en.embed.electra') returns Spark NLP model electra_small_uncased\n","nlu.load('en.embed.electra.small_uncased') returns Spark NLP model electra_small_uncased\n","nlu.load('en.embed.electra.base_uncased') returns Spark NLP model electra_base_uncased\n","nlu.load('en.embed.electra.large_uncased') returns Spark NLP model electra_large_uncased\n","nlu.load('en.embed.covidbert') returns Spark NLP model covidbert_large_uncased\n","nlu.load('en.embed.covidbert.large_uncased') returns Spark NLP model covidbert_large_uncased\n","nlu.load('en.embed.bert.small_L2_128') returns Spark NLP model small_bert_L2_128\n","nlu.load('en.embed.bert.small_L4_128') returns Spark NLP model small_bert_L4_128\n","nlu.load('en.embed.bert.small_L6_128') returns Spark NLP model small_bert_L6_128\n","nlu.load('en.embed.bert.small_L8_128') returns Spark NLP model small_bert_L8_128\n","nlu.load('en.embed.bert.small_L10_128') returns Spark NLP model small_bert_L10_128\n","nlu.load('en.embed.bert.small_L12_128') returns Spark NLP model small_bert_L12_128\n","nlu.load('en.embed.bert.small_L2_256') returns Spark NLP model small_bert_L2_256\n","nlu.load('en.embed.bert.small_L4_256') returns Spark NLP model small_bert_L4_256\n","nlu.load('en.embed.bert.small_L6_256') returns Spark NLP model small_bert_L6_256\n","nlu.load('en.embed.bert.small_L8_256') returns Spark NLP model small_bert_L8_256\n","nlu.load('en.embed.bert.small_L10_256') returns Spark NLP model small_bert_L10_256\n","nlu.load('en.embed.bert.small_L12_256') returns Spark NLP model small_bert_L12_256\n","nlu.load('en.embed.bert.small_L2_512') returns Spark NLP model small_bert_L2_512\n","nlu.load('en.embed.bert.small_L4_512') returns Spark NLP model small_bert_L4_512\n","nlu.load('en.embed.bert.small_L6_512') returns Spark NLP model small_bert_L6_512\n","nlu.load('en.embed.bert.small_L8_512') returns Spark NLP model small_bert_L8_512\n","nlu.load('en.embed.bert.small_L10_512') returns Spark NLP model small_bert_L10_512\n","nlu.load('en.embed.bert.small_L12_512') returns Spark NLP model small_bert_L12_512\n","nlu.load('en.embed.bert.small_L2_768') returns Spark NLP model small_bert_L2_768\n","nlu.load('en.embed.bert.small_L4_768') returns Spark NLP model small_bert_L4_768\n","nlu.load('en.embed.bert.small_L6_768') returns Spark NLP model small_bert_L6_768\n","nlu.load('en.embed.bert.small_L8_768') returns Spark NLP model small_bert_L8_768\n","nlu.load('en.embed.bert.small_L10_768') returns Spark NLP model small_bert_L10_768\n","nlu.load('en.embed.bert.small_L12_768') returns Spark NLP model small_bert_L12_768\n","For language <ar> NLU provides the following Models : \n","nlu.load('ar.embed') returns Spark NLP model arabic_w2v_cc_300d\n","nlu.load('ar.embed.cbow') returns Spark NLP model arabic_w2v_cc_300d\n","nlu.load('ar.embed.cbow.300d') returns Spark NLP model arabic_w2v_cc_300d\n","nlu.load('ar.embed.aner') returns Spark NLP model arabic_w2v_cc_300d\n","nlu.load('ar.embed.aner.300d') returns Spark NLP model arabic_w2v_cc_300d\n","nlu.load('ar.embed.glove') returns Spark NLP model arabic_w2v_cc_300d\n","For language <bn> NLU provides the following Models : \n","nlu.load('bn.embed.glove') returns Spark NLP model bengaliner_cc_300d\n","nlu.load('bn.embed') returns Spark NLP model bengaliner_cc_300d\n","For language <fi> NLU provides the following Models : \n","nlu.load('fi.embed.bert.') returns Spark NLP model bert_finnish_cased\n","nlu.load('fi.embed.bert.cased.') returns Spark NLP model bert_finnish_cased\n","nlu.load('fi.embed.bert.uncased.') returns Spark NLP model bert_finnish_uncased\n","For language <he> NLU provides the following Models : \n","nlu.load('he.embed') returns Spark NLP model hebrew_cc_300d\n","nlu.load('he.embed.glove') returns Spark NLP model hebrew_cc_300d\n","nlu.load('he.embed.cbow_300d') returns Spark NLP model hebrew_cc_300d\n","For language <hi> NLU provides the following Models : \n","nlu.load('hi.embed') returns Spark NLP model hindi_cc_300d\n","For language <fa> NLU provides the following Models : \n","nlu.load('fa.embed') returns Spark NLP model persian_w2v_cc_300d\n","nlu.load('fa.embed.word2vec') returns Spark NLP model persian_w2v_cc_300d\n","nlu.load('fa.embed.word2vec.300d') returns Spark NLP model persian_w2v_cc_300d\n","For language <zh> NLU provides the following Models : \n","nlu.load('zh.embed') returns Spark NLP model bert_base_chinese\n","nlu.load('zh.embed.bert') returns Spark NLP model bert_base_chinese\n","For language <ur> NLU provides the following Models : \n","nlu.load('ur.embed') returns Spark NLP model urduvec_140M_300d\n","nlu.load('ur.embed.glove.300d') returns Spark NLP model urduvec_140M_300d\n","nlu.load('ur.embed.urdu_vec_140M_300d') returns Spark NLP model urduvec_140M_300d\n","For language <xx> NLU provides the following Models : \n","nlu.load('xx.embed') returns Spark NLP model glove_840B_300\n","nlu.load('xx.embed.glove.840B_300') returns Spark NLP model glove_840B_300\n","nlu.load('xx.embed.glove.6B_300') returns Spark NLP model glove_6B_300\n","nlu.load('xx.embed.bert_multi_cased') returns Spark NLP model bert_multi_cased\n","nlu.load('xx.embed.bert') returns Spark NLP model bert_multi_cased\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":199},"id":"Xz7xnvbCFxE3","executionInfo":{"status":"ok","timestamp":1620193033467,"user_tz":-300,"elapsed":1617963,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"9b215cae-fab0-4333-dc4b-224fb80f29e4"},"source":["# Add bert word embeddings to pipe \n","fitted_pipe = nlu.load('bert train.ner').fit(dataset_path=train_path)\n","\n","# predict with the trainable pipeline on dataset and get predictions\n","preds = fitted_pipe.predict('Donald Trump and Angela Merkel dont share many oppinions')\n","preds"],"execution_count":null,"outputs":[{"output_type":"stream","text":["small_bert_L2_128 download started this may take some time.\n","Approximate size to download 16.1 MB\n","[OK!]\n","sentence_detector_dl download started this may take some time.\n","Approximate size to download 354.6 KB\n","[OK!]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>document</th>\n","      <th>word_embedding_bert</th>\n","      <th>entities_class</th>\n","      <th>entities</th>\n","      <th>token</th>\n","      <th>origin_index</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[Donald Trump and Angela Merkel dont share man...</td>\n","      <td>Donald Trump and Angela Merkel dont share many...</td>\n","      <td>[[-0.447601318359375, 1.0348621606826782, 0.51...</td>\n","      <td>[PER, PER]</td>\n","      <td>[Donald Trump, Angela Merkel dont]</td>\n","      <td>[Donald, Trump, and, Angela, Merkel, dont, sha...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            sentence  ... origin_index\n","0  [Donald Trump and Angela Merkel dont share man...  ...            0\n","\n","[1 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"2BB-NwZUoHSe"},"source":["# 5. Lets save the model"]},{"cell_type":"code","metadata":{"id":"eLex095goHwm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620193052249,"user_tz":-300,"elapsed":1636720,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"b30131b2-f8ff-443f-d8ff-98b5ff26d4a2"},"source":["stored_model_path = './models/classifier_dl_trained' \n","fitted_pipe.save(stored_model_path)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Stored model in ./models/classifier_dl_trained\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"e_b2DPd4rCiU"},"source":["# 6. Lets load the model from HDD.\n","This makes Offlien NLU usage possible!   \n","You need to call nlu.load(path=path_to_the_pipe) to load a model/pipeline from disk."]},{"cell_type":"code","metadata":{"id":"SO4uz45MoRgp","colab":{"base_uri":"https://localhost:8080/","height":97},"executionInfo":{"status":"ok","timestamp":1620193057841,"user_tz":-300,"elapsed":1642287,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"2e60866d-9d3b-4785-ce39-cefea4b2ea17"},"source":["hdd_pipe = nlu.load(path=stored_model_path)\n","\n","preds = hdd_pipe.predict('Donald Trump and Angela Merkel dont share many oppinions on laws about cheeseburgers')\n","preds"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>sentence</th>\n","      <th>origin_index</th>\n","      <th>document</th>\n","      <th>entities_class</th>\n","      <th>entities</th>\n","      <th>token</th>\n","      <th>word_embedding_from_disk</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Donald Trump and Angela Merkel dont share many...</td>\n","      <td>[Donald Trump and Angela Merkel dont share man...</td>\n","      <td>8589934592</td>\n","      <td>Donald Trump and Angela Merkel dont share many...</td>\n","      <td>[PER, PER]</td>\n","      <td>[Donald Trump, Angela Merkel dont]</td>\n","      <td>[Donald, Trump, and, Angela, Merkel, dont, sha...</td>\n","      <td>[[-0.6870571374893188, 1.1118954420089722, 0.5...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  ...                           word_embedding_from_disk\n","0  Donald Trump and Angela Merkel dont share many...  ...  [[-0.6870571374893188, 1.1118954420089722, 0.5...\n","\n","[1 rows x 8 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"e0CVlkk9v6Qi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620193057843,"user_tz":-300,"elapsed":1642279,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"95443872-f5fc-4431-b803-20e2020949c1"},"source":["hdd_pipe.print_info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The following parameters are configurable for this NLU pipeline (You can copy paste the examples) :\n",">>> pipe['document_assembler'] has settable params:\n","pipe['document_assembler'].setCleanupMode('shrink')       | Info: possible values: disabled, inplace, inplace_full, shrink, shrink_full, each, each_full, delete_full | Currently set to : shrink\n",">>> pipe['sentence_detector@SentenceDetectorDLModel_c83c27f46b97'] has settable params:\n","pipe['sentence_detector@SentenceDetectorDLModel_c83c27f46b97'].setExplodeSentences(False)  | Info: whether to explode each sentence into a different row, for better parallelization. Defaults to false. | Currently set to : False\n","pipe['sentence_detector@SentenceDetectorDLModel_c83c27f46b97'].setStorageRef('SentenceDetectorDLModel_c83c27f46b97')  | Info: storage unique identifier | Currently set to : SentenceDetectorDLModel_c83c27f46b97\n","pipe['sentence_detector@SentenceDetectorDLModel_c83c27f46b97'].setEncoder(com.johnsnowlabs.nlp.annotators.sentence_detector_dl.SentenceDetectorDLEncoder@75127717)  | Info: Data encoder | Currently set to : com.johnsnowlabs.nlp.annotators.sentence_detector_dl.SentenceDetectorDLEncoder@75127717\n","pipe['sentence_detector@SentenceDetectorDLModel_c83c27f46b97'].setImpossiblePenultimates(['Bros', 'No', 'al', 'vs', 'etc', 'Fig', 'Dr', 'Prof', 'PhD', 'MD', 'Co', 'Corp', 'Inc', 'bros', 'VS', 'Vs', 'ETC', 'fig', 'dr', 'prof', 'PHD', 'phd', 'md', 'co', 'corp', 'inc', 'Jan', 'Feb', 'Mar', 'Apr', 'Jul', 'Aug', 'Sep', 'Sept', 'Oct', 'Nov', 'Dec', 'St', 'st', 'AM', 'PM', 'am', 'pm', 'e.g', 'f.e', 'i.e'])  | Info: Impossible penultimates | Currently set to : ['Bros', 'No', 'al', 'vs', 'etc', 'Fig', 'Dr', 'Prof', 'PhD', 'MD', 'Co', 'Corp', 'Inc', 'bros', 'VS', 'Vs', 'ETC', 'fig', 'dr', 'prof', 'PHD', 'phd', 'md', 'co', 'corp', 'inc', 'Jan', 'Feb', 'Mar', 'Apr', 'Jul', 'Aug', 'Sep', 'Sept', 'Oct', 'Nov', 'Dec', 'St', 'st', 'AM', 'PM', 'am', 'pm', 'e.g', 'f.e', 'i.e']\n","pipe['sentence_detector@SentenceDetectorDLModel_c83c27f46b97'].setModelArchitecture('cnn')  | Info: Model architecture (CNN) | Currently set to : cnn\n",">>> pipe['default_tokenizer'] has settable params:\n","pipe['default_tokenizer'].setCaseSensitiveExceptions(True)  | Info: Whether to care for case sensitiveness in exceptions | Currently set to : True\n","pipe['default_tokenizer'].setTargetPattern('\\S+')         | Info: pattern to grab from text as token candidates. Defaults \\S+ | Currently set to : \\S+\n","pipe['default_tokenizer'].setMaxLength(99999)             | Info: Set the maximum allowed length for each token | Currently set to : 99999\n","pipe['default_tokenizer'].setMinLength(0)                 | Info: Set the minimum allowed length for each token | Currently set to : 0\n",">>> pipe['bert@small_bert_L2_128'] has settable params:\n","pipe['bert@small_bert_L2_128'].setBatchSize(8)            | Info: Size of every batch | Currently set to : 8\n","pipe['bert@small_bert_L2_128'].setCaseSensitive(False)    | Info: whether to ignore case in tokens for embeddings matching | Currently set to : False\n","pipe['bert@small_bert_L2_128'].setDimension(128)          | Info: Number of embedding dimensions | Currently set to : 128\n","pipe['bert@small_bert_L2_128'].setMaxSentenceLength(128)  | Info: Max sentence length to process | Currently set to : 128\n","pipe['bert@small_bert_L2_128'].setStorageRef('small_bert_L2_128')  | Info: unique reference name for identification | Currently set to : small_bert_L2_128\n",">>> pipe['named_entity_recognizer_dl@small_bert_L2_128'] has settable params:\n","pipe['named_entity_recognizer_dl@small_bert_L2_128'].setBatchSize(8)  | Info: Size of every batch | Currently set to : 8\n","pipe['named_entity_recognizer_dl@small_bert_L2_128'].setIncludeConfidence(True)  | Info: whether to include confidence scores in annotation metadata | Currently set to : True\n","pipe['named_entity_recognizer_dl@small_bert_L2_128'].setClasses(['O', 'B-ORG', 'I-ORG', 'I-MISC', 'I-PER', 'B-LOC', 'B-MISC', 'I-LOC'])  | Info: get the tags used to trained this NerDLModel | Currently set to : ['O', 'B-ORG', 'I-ORG', 'I-MISC', 'I-PER', 'B-LOC', 'B-MISC', 'I-LOC']\n","pipe['named_entity_recognizer_dl@small_bert_L2_128'].setStorageRef('small_bert_L2_128')  | Info: unique reference name for identification | Currently set to : small_bert_L2_128\n",">>> pipe['ner_to_chunk_converter'] has settable params:\n","pipe['ner_to_chunk_converter'].setPreservePosition(True)  | Info: Whether to preserve the original position of the tokens in the original document or use the modified tokens | Currently set to : True\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"USD6d66Sw6_P"},"source":[""],"execution_count":null,"outputs":[]}]}