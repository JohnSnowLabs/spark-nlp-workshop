{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentence_entity_resolution_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_bO5AEJYjtS"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]((https://github.com/JohnSnowLabs/nlu/blob/master/examples/colab/Training/sentence_entity_resolution/sentence_entity_resolution_training.ipynb)\n",
        "\n",
        "\n",
        "# Sentence Entity Resolution training\n",
        "Named Entities are sub pieces in textual data which are labled with classes.    \n",
        "These classes and strings are still ambious though and it is not possible to group semantically identically entities withouth any definition of `terminology`. \n",
        "With the `Sentence Resolver` you can train a state of the art deep learning architecture to map entities to their unique terminological representation.\n",
        "\n",
        "A concrete example would be : \n",
        "\n",
        "- The `TSLA` stock is good to buy. \n",
        "- `Tesla, Inc`. is a great company to invest int\n",
        "- The price of `Teslas` stocks is going up\n",
        "\n",
        "`TSLA` , `Tesla`, `Teslas` can be extracted by an NER model an labled as `company` entity class. But we cannot tell programmatically, if all the referring to the same sematic concept, in this case company.     \n",
        "\n",
        "To solve this abigous problem, we can introduce a Terminlogy, where the Tesla company has the ID 21 and every other company in our portfolio get a unique ID aswell.   \n",
        "With a defined terminology at hand and a labled dataset, we can train a chunk resolver to map textually different but semantically equivalent `company entities` to `the same id`. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 1. Install NLU, dependecies and Authenticate\n",
        "\n",
        "See the [install docs](https://nlu.johnsnowlabs.com/docs/en/install#super-quickstart-on-google-colab-or-kaggle) and [authentification docs](https://nlu.johnsnowlabs.com/docs/en/examples_hc#authorize-access-to-licensed-features-and-install-healthcare-dependencies) for more infos \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlZgaz0oXtb6"
      },
      "source": [
        "!wget http://setup.johnsnowlabs.com/nlu/colab.sh -O - | bash\n",
        "import nlu\n",
        "import pandas as pd "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp4j0_IluV3y"
      },
      "source": [
        "# Train Sentence Resolver\n",
        "\n",
        "This is a mini example to make you familiar with the dataset structure you must provide for training. \n",
        "Train a chunk resolver on a dataset with columns named `y` , `_y` and `text`. `y` is a label, `_y` is an extra identifier label, `text` is the raw text\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEHVidCG3wO2",
        "outputId": "87628d73-4cd0-47d5-ad04-71b34fcea8df"
      },
      "source": [
        "import nlu\n",
        "SPARK_NLP_LICENSE           =\"????\"\n",
        "AWS_ACCESS_KEY_ID          = \"????\"\n",
        "AWS_SECRET_ACCESS_KEY       =\"????\"\n",
        "JSL_SECRET                  =\"????\"\n",
        "nlu.auth(SPARK_NLP_LICENSE,AWS_ACCESS_KEY_ID,AWS_SECRET_ACCESS_KEY,JSL_SECRET)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'nlu' from '/usr/local/lib/python3.7/dist-packages/nlu/__init__.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "cib1vJ_1tJRr",
        "outputId": "ef575edd-a7a5-45b8-89b6-7edea30cb2ea"
      },
      "source": [
        "import pandas as pd \n",
        "import nlu\n",
        "dataset = pd.DataFrame({\n",
        "    'text': ['The Tesla company is good to invest is', 'TSLA is good to invest','TESLA INC. we should buy','PUT ALL MONEY IN TSLA inc!!'],\n",
        "    'y': ['23','23','23','23'],\n",
        "    '_y': ['TESLA','TESLA','TESLA','TESLA'],\n",
        "\n",
        "})\n",
        "\n",
        "trainable_pipe = nlu.load('train.resolve_sentence')\n",
        "fitted_pipe  = trainable_pipe.fit(dataset)\n",
        "fitted_pipe.predict(dataset.text)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfhub_use download started this may take some time.\n",
            "Approximate size to download 923.7 MB\n",
            "[OK!]\n",
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 354.6 KB\n",
            "[OK!]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_resolution_resolve_sentence_confidence</th>\n",
              "      <th>sentence_resolution_resolve_sentence_code</th>\n",
              "      <th>document</th>\n",
              "      <th>sentence_resolution_resolve_sentence</th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentence_embedding_use</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[1.0000]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>The Tesla company is good to invest is</td>\n",
              "      <td>[TESLA]</td>\n",
              "      <td>[The Tesla company is good to invest is]</td>\n",
              "      <td>[[0.075991176, 0.04411165, -0.0010679043, -0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[1.0000]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>TSLA is good to invest</td>\n",
              "      <td>[TESLA]</td>\n",
              "      <td>[TSLA is good to invest]</td>\n",
              "      <td>[[0.06989084, -0.002836604, -0.02459646, -0.02...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[1.0000]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>TESLA INC. we should buy</td>\n",
              "      <td>[TESLA]</td>\n",
              "      <td>[TESLA INC. we should buy]</td>\n",
              "      <td>[[0.08029125, 0.03371899, -0.006450202, 0.0065...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[1.0000]</td>\n",
              "      <td>[23]</td>\n",
              "      <td>PUT ALL MONEY IN TSLA inc!!</td>\n",
              "      <td>[TESLA]</td>\n",
              "      <td>[PUT ALL MONEY IN TSLA inc!!]</td>\n",
              "      <td>[[0.06478285, -0.0022001457, -0.0069140834, -0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  sentence_resolution_resolve_sentence_confidence  ...                             sentence_embedding_use\n",
              "0                                        [1.0000]  ...  [[0.075991176, 0.04411165, -0.0010679043, -0.0...\n",
              "1                                        [1.0000]  ...  [[0.06989084, -0.002836604, -0.02459646, -0.02...\n",
              "2                                        [1.0000]  ...  [[0.08029125, 0.03371899, -0.006450202, 0.0065...\n",
              "3                                        [1.0000]  ...  [[0.06478285, -0.0022001457, -0.0069140834, -0...\n",
              "\n",
              "[4 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfoyqShJu5nK"
      },
      "source": [
        "## Train Sentence Resolver with Bert Embeddings\n",
        "By default, GLOVE embeddings will be used. You can you any of the [100+ Word Embeddings]() to train your chunk resolver. If you are handling medical data, biomedical vectors like glove or biobert are a good choice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYqhWOcWbqgD",
        "outputId": "9bcb5e12-82f9-41ca-932f-2e9e0d0acf6a"
      },
      "source": [
        "# We can configurevarious parameters on the Chunk resolver\n",
        "trainable_pipe.print_info()\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following parameters are configurable for this NLU pipeline (You can copy paste the examples) :\n",
            ">>> pipe['sentence_resolver'] has settable params:\n",
            "pipe['sentence_resolver'].setNormalizedCol('_y')     | Info: Column name for the original, normalized description | Currently set to : _y\n",
            "pipe['sentence_resolver'].setDistanceFunction('EUCLIDIAN')  | Info: What distance function to use for WMD: 'EUCLIDEAN' or 'COSINE' | Currently set to : EUCLIDIAN\n",
            "pipe['sentence_resolver'].setNeighbours(25)          | Info: Number of neighbours to consider in the KNN query to calculate WMD | Currently set to : 25\n",
            "pipe['sentence_resolver'].setThreshold(1000.0)       | Info: Threshold value for the last distance calculated | Currently set to : 1000.0\n",
            "pipe['sentence_resolver'].setMissAsEmpty(True)       | Info: whether or not to return an empty annotation on unmatched chunks | Currently set to : True\n",
            "pipe['sentence_resolver'].setReturnCosineDistances(True)  | Info: Extract Cosine Distances. TRUE or False | Currently set to : True\n",
            "pipe['sentence_resolver'].setCaseSensitive(False)    | Info: whether to ignore case in tokens for embeddings matching | Currently set to : False\n",
            ">>> pipe['use@tfhub_use'] has settable params:\n",
            "pipe['use@tfhub_use'].setDimension(512)              | Info: Number of embedding dimensions | Currently set to : 512\n",
            "pipe['use@tfhub_use'].setLoadSP(False)               | Info: Whether to load SentencePiece ops file which is required only by multi-lingual models. This is not changeable after it's set with a pretrained model nor it is compatible with Windows. | Currently set to : False\n",
            "pipe['use@tfhub_use'].setStorageRef('tfhub_use')     | Info: unique reference name for identification | Currently set to : tfhub_use\n",
            ">>> pipe['deep_sentence_detector@SentenceDetectorDLModel_c83c27f46b97'] has settable params:\n",
            "pipe['deep_sentence_detector@SentenceDetectorDLModel_c83c27f46b97'].setExplodeSentences(False)  | Info: whether to explode each sentence into a different row, for better parallelization. Defaults to false. | Currently set to : False\n",
            "pipe['deep_sentence_detector@SentenceDetectorDLModel_c83c27f46b97'].setStorageRef('SentenceDetectorDLModel_c83c27f46b97')  | Info: storage unique identifier | Currently set to : SentenceDetectorDLModel_c83c27f46b97\n",
            "pipe['deep_sentence_detector@SentenceDetectorDLModel_c83c27f46b97'].setEncoder(com.johnsnowlabs.nlp.annotators.sentence_detector_dl.SentenceDetectorDLEncoder@31dd891e)  | Info: Data encoder | Currently set to : com.johnsnowlabs.nlp.annotators.sentence_detector_dl.SentenceDetectorDLEncoder@31dd891e\n",
            "pipe['deep_sentence_detector@SentenceDetectorDLModel_c83c27f46b97'].setImpossiblePenultimates(['Bros', 'No', 'al', 'vs', 'etc', 'Fig', 'Dr', 'Prof', 'PhD', 'MD', 'Co', 'Corp', 'Inc', 'bros', 'VS', 'Vs', 'ETC', 'fig', 'dr', 'prof', 'PHD', 'phd', 'md', 'co', 'corp', 'inc', 'Jan', 'Feb', 'Mar', 'Apr', 'Jul', 'Aug', 'Sep', 'Sept', 'Oct', 'Nov', 'Dec', 'St', 'st', 'AM', 'PM', 'am', 'pm', 'e.g', 'f.e', 'i.e'])  | Info: Impossible penultimates | Currently set to : ['Bros', 'No', 'al', 'vs', 'etc', 'Fig', 'Dr', 'Prof', 'PhD', 'MD', 'Co', 'Corp', 'Inc', 'bros', 'VS', 'Vs', 'ETC', 'fig', 'dr', 'prof', 'PHD', 'phd', 'md', 'co', 'corp', 'inc', 'Jan', 'Feb', 'Mar', 'Apr', 'Jul', 'Aug', 'Sep', 'Sept', 'Oct', 'Nov', 'Dec', 'St', 'st', 'AM', 'PM', 'am', 'pm', 'e.g', 'f.e', 'i.e']\n",
            "pipe['deep_sentence_detector@SentenceDetectorDLModel_c83c27f46b97'].setModelArchitecture('cnn')  | Info: Model architecture (CNN) | Currently set to : cnn\n",
            ">>> pipe['document_assembler'] has settable params:\n",
            "pipe['document_assembler'].setCleanupMode('shrink')  | Info: possible values: disabled, inplace, inplace_full, shrink, shrink_full, each, each_full, delete_full | Currently set to : shrink\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkX-DPI6bO59"
      },
      "source": [
        "# Train a SNOMED resolver\n",
        "We download a sample SNOMED dataset which has we can use for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00QCRKcEbPmq"
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/data/AskAPatient.fold-0.test.txt\n",
        "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/data/AskAPatient.fold-0.train.txt"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "Fo30y4S6bRgz",
        "outputId": "87fcb8a9-5f3f-4d6c-c448-e0e3de661516"
      },
      "source": [
        "import pandas as pd\n",
        "cols = [\"y\",\"_y\",\"text\"]\n",
        "aap_tr = pd.read_csv(\"AskAPatient.fold-0.train.txt\",sep=\"\\t\",encoding=\"ISO-8859-1\",header=None).iloc[:250]\n",
        "aap_te = pd.read_csv(\"AskAPatient.fold-0.test.txt\",sep=\"\\t\",encoding=\"ISO-8859-1\",header=None).iloc[:250]\n",
        "aap_tr.columns = cols\n",
        "aap_te.columns = cols\n",
        "aap_tr\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>_y</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>108367008</td>\n",
              "      <td>Dislocation of joint</td>\n",
              "      <td>Dislocation of joint</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3384011000036100</td>\n",
              "      <td>Arthrotec</td>\n",
              "      <td>Arthrotec</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>166717003</td>\n",
              "      <td>Serum creatinine raised</td>\n",
              "      <td>Serum creatinine raised</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3877011000036101</td>\n",
              "      <td>Lipitor</td>\n",
              "      <td>Lipitor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>402234004</td>\n",
              "      <td>Foot eczema</td>\n",
              "      <td>Foot eczema</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>162290004</td>\n",
              "      <td>Dry eyes</td>\n",
              "      <td>Dry eyes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>419723007</td>\n",
              "      <td>Mentally dull</td>\n",
              "      <td>Mentally dull</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>4216011000036104</td>\n",
              "      <td>Norvasc</td>\n",
              "      <td>Norvasc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>13791008</td>\n",
              "      <td>Asthenia</td>\n",
              "      <td>Asthenia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>162059005</td>\n",
              "      <td>Upset stomach</td>\n",
              "      <td>Upset stomach</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>250 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                    y                       _y                     text\n",
              "0           108367008     Dislocation of joint     Dislocation of joint\n",
              "1    3384011000036100                Arthrotec                Arthrotec\n",
              "2           166717003  Serum creatinine raised  Serum creatinine raised\n",
              "3    3877011000036101                  Lipitor                  Lipitor\n",
              "4           402234004              Foot eczema              Foot eczema\n",
              "..                ...                      ...                      ...\n",
              "245         162290004                 Dry eyes                 Dry eyes\n",
              "246         419723007            Mentally dull            Mentally dull\n",
              "247  4216011000036104                  Norvasc                  Norvasc\n",
              "248          13791008                 Asthenia                 Asthenia\n",
              "249         162059005            Upset stomach            Upset stomach\n",
              "\n",
              "[250 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ngBy_2CbwIY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "outputId": "b178136e-cf46-46d9-a63d-dceb54a94d45"
      },
      "source": [
        "# Healthcare Embeddings\n",
        "trainable_pipe = nlu.load('en.embed_sentence.bert.jsl_tiny_umls_uncased train.resolve_sentence')\n",
        "trainable_pipe['sentence_resolver'].setNeighbours(4)  \n",
        "fitted_pipe  = trainable_pipe.fit(aap_tr)\n",
        "prediction = fitted_pipe.predict(aap_tr)\n",
        "prediction"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sbert_jsl_tiny_umls_uncased download started this may take some time.\n",
            "Approximate size to download 15.8 MB\n",
            "[OK!]\n",
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 354.6 KB\n",
            "[OK!]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_embedding_bert</th>\n",
              "      <th>sentence_resolution_resolve_sentence_confidence</th>\n",
              "      <th>document</th>\n",
              "      <th>sentence_resolution_resolve_sentence</th>\n",
              "      <th>sentence</th>\n",
              "      <th>sentence_resolution_resolve_sentence_code</th>\n",
              "      <th>text</th>\n",
              "      <th>_y</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[-0.9687815, -0.31864247, -0.2600491, -0.4007...</td>\n",
              "      <td>[0.9992]</td>\n",
              "      <td>Dislocation of joint</td>\n",
              "      <td>[Dislocation of joint]</td>\n",
              "      <td>[Dislocation of joint]</td>\n",
              "      <td>[108367008]</td>\n",
              "      <td>Dislocation of joint</td>\n",
              "      <td>Dislocation of joint</td>\n",
              "      <td>108367008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[-0.7108755, -0.52662104, -0.5808298, -0.7171...</td>\n",
              "      <td>[0.9921]</td>\n",
              "      <td>Arthrotec</td>\n",
              "      <td>[Arthrotec]</td>\n",
              "      <td>[Arthrotec]</td>\n",
              "      <td>[3384011000036100]</td>\n",
              "      <td>Arthrotec</td>\n",
              "      <td>Arthrotec</td>\n",
              "      <td>3384011000036100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[-0.54099977, -2.0953283, 0.14658599, 0.04070...</td>\n",
              "      <td>[0.9975]</td>\n",
              "      <td>Serum creatinine raised</td>\n",
              "      <td>[Serum creatinine raised]</td>\n",
              "      <td>[Serum creatinine raised]</td>\n",
              "      <td>[166717003]</td>\n",
              "      <td>Serum creatinine raised</td>\n",
              "      <td>Serum creatinine raised</td>\n",
              "      <td>166717003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[[-0.4524097, -1.3946228, -0.528351, -0.196025...</td>\n",
              "      <td>[1.0000]</td>\n",
              "      <td>Lipitor</td>\n",
              "      <td>[Lipitor]</td>\n",
              "      <td>[Lipitor]</td>\n",
              "      <td>[3877011000036101]</td>\n",
              "      <td>Lipitor</td>\n",
              "      <td>Lipitor</td>\n",
              "      <td>3877011000036101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[[-0.76311046, -0.4025006, -0.17865096, -0.329...</td>\n",
              "      <td>[0.9942]</td>\n",
              "      <td>Foot eczema</td>\n",
              "      <td>[Foot eczema]</td>\n",
              "      <td>[Foot eczema]</td>\n",
              "      <td>[402234004]</td>\n",
              "      <td>Foot eczema</td>\n",
              "      <td>Foot eczema</td>\n",
              "      <td>402234004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>[[-0.037025943, -1.3459508, -0.27637935, -0.10...</td>\n",
              "      <td>[0.9981]</td>\n",
              "      <td>Dry eyes</td>\n",
              "      <td>[Dry eyes]</td>\n",
              "      <td>[Dry eyes]</td>\n",
              "      <td>[162290004]</td>\n",
              "      <td>Dry eyes</td>\n",
              "      <td>Dry eyes</td>\n",
              "      <td>162290004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>[[-0.93272203, -1.3695889, -0.043282345, -0.65...</td>\n",
              "      <td>[1.0000]</td>\n",
              "      <td>Mentally dull</td>\n",
              "      <td>[Mentally dull]</td>\n",
              "      <td>[Mentally dull]</td>\n",
              "      <td>[419723007]</td>\n",
              "      <td>Mentally dull</td>\n",
              "      <td>Mentally dull</td>\n",
              "      <td>419723007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>[[-0.45309123, -1.5768625, -0.43449545, -0.912...</td>\n",
              "      <td>[0.9864]</td>\n",
              "      <td>Norvasc</td>\n",
              "      <td>[Norvasc]</td>\n",
              "      <td>[Norvasc]</td>\n",
              "      <td>[4216011000036104]</td>\n",
              "      <td>Norvasc</td>\n",
              "      <td>Norvasc</td>\n",
              "      <td>4216011000036104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>[[-0.55921286, -1.661054, -0.27011818, -0.6075...</td>\n",
              "      <td>[1.0000]</td>\n",
              "      <td>Asthenia</td>\n",
              "      <td>[Asthenia]</td>\n",
              "      <td>[Asthenia]</td>\n",
              "      <td>[13791008]</td>\n",
              "      <td>Asthenia</td>\n",
              "      <td>Asthenia</td>\n",
              "      <td>13791008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>[[-2.2426636, -0.94224554, 0.09775538, -0.2630...</td>\n",
              "      <td>[0.9960]</td>\n",
              "      <td>Upset stomach</td>\n",
              "      <td>[Upset stomach]</td>\n",
              "      <td>[Upset stomach]</td>\n",
              "      <td>[162059005]</td>\n",
              "      <td>Upset stomach</td>\n",
              "      <td>Upset stomach</td>\n",
              "      <td>162059005</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>250 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                               sentence_embedding_bert  ...                 y\n",
              "0    [[-0.9687815, -0.31864247, -0.2600491, -0.4007...  ...         108367008\n",
              "1    [[-0.7108755, -0.52662104, -0.5808298, -0.7171...  ...  3384011000036100\n",
              "2    [[-0.54099977, -2.0953283, 0.14658599, 0.04070...  ...         166717003\n",
              "3    [[-0.4524097, -1.3946228, -0.528351, -0.196025...  ...  3877011000036101\n",
              "4    [[-0.76311046, -0.4025006, -0.17865096, -0.329...  ...         402234004\n",
              "..                                                 ...  ...               ...\n",
              "245  [[-0.037025943, -1.3459508, -0.27637935, -0.10...  ...         162290004\n",
              "246  [[-0.93272203, -1.3695889, -0.043282345, -0.65...  ...         419723007\n",
              "247  [[-0.45309123, -1.5768625, -0.43449545, -0.912...  ...  4216011000036104\n",
              "248  [[-0.55921286, -1.661054, -0.27011818, -0.6075...  ...          13791008\n",
              "249  [[-2.2426636, -0.94224554, 0.09775538, -0.2630...  ...         162059005\n",
              "\n",
              "[250 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZT2id3Mxu_Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}