{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"korean_ner_pos_and_tokenization.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"E6qlUniWPXLL"},"source":["\n","\n","![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/nlu/blob/master/examples//colab/component_examples/multilingual/korean_ner_pos_and_tokenization.ipynb.ipynb)\n","\n"," \n"," # Detect Named Entities (NER), Part of Speech Tags (POS) and Tokenize in Korean"]},{"cell_type":"code","metadata":{"id":"NyzSofTuC6Wl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649992228301,"user_tz":-300,"elapsed":79506,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}},"outputId":"ddb1f573-13e4-47be-bcfa-61843ab40c55"},"source":["!wget https://setup.johnsnowlabs.com/nlu/colab.sh -O - | bash\n","  \n","\n","import nlu"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-04-15 03:09:07--  https://setup.johnsnowlabs.com/nlu/colab.sh\n","Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n","Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:443... connected.\n","HTTP request sent, awaiting response... 302 Moved Temporarily\n","Location: https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh [following]\n","--2022-04-15 03:09:07--  https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1665 (1.6K) [text/plain]\n","Saving to: ‘STDOUT’\n","\n","\r-                     0%[                    ]       0  --.-KB/s               Installing  NLU 3.4.3rc2 with  PySpark 3.0.3 and Spark NLP 3.4.2 for Google Colab ...\n","\r-                   100%[===================>]   1.63K  --.-KB/s    in 0.001s  \n","\n","2022-04-15 03:09:07 (2.84 MB/s) - written to stdout [1665/1665]\n","\n","Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Get:5 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Hit:6 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:8 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n","Hit:9 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Ign:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Ign:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n","Hit:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n","Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,947 kB]\n","Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,695 kB]\n","Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [996 kB]\n","Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,490 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,134 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,268 kB]\n","Get:21 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n","Get:23 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [953 kB]\n","Fetched 13.8 MB in 2s (6,605 kB/s)\n","Reading package lists... Done\n","tar: spark-3.0.2-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n","tar: Error is not recoverable: exiting now\n","\u001b[K     |████████████████████████████████| 209.1 MB 73 kB/s \n","\u001b[K     |████████████████████████████████| 142 kB 92.5 MB/s \n","\u001b[K     |████████████████████████████████| 505 kB 87.3 MB/s \n","\u001b[K     |████████████████████████████████| 198 kB 85.9 MB/s \n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting nlu_tmp==3.4.3rc10\n","  Downloading nlu_tmp-3.4.3rc10-py3-none-any.whl (510 kB)\n","\u001b[K     |████████████████████████████████| 510 kB 15.3 MB/s \n","\u001b[?25hRequirement already satisfied: spark-nlp<3.5.0,>=3.4.2 in /usr/local/lib/python3.7/dist-packages (from nlu_tmp==3.4.3rc10) (3.4.2)\n","Requirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.7/dist-packages (from nlu_tmp==3.4.3rc10) (1.3.5)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from nlu_tmp==3.4.3rc10) (0.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nlu_tmp==3.4.3rc10) (1.21.5)\n","Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from nlu_tmp==3.4.3rc10) (6.0.1)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.3.5->nlu_tmp==3.4.3rc10) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.3.5->nlu_tmp==3.4.3rc10) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.3.5->nlu_tmp==3.4.3rc10) (1.15.0)\n","Installing collected packages: nlu-tmp\n","Successfully installed nlu-tmp-3.4.3rc10\n"]}]},{"cell_type":"markdown","metadata":{"id":"S_HM_VWcRu5C"},"source":["# Tokenize in Korean"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":342},"id":"9lKSMrKmCwSa","outputId":"60616a9f-8ff0-4b14-e527-a51ff0b820d1","executionInfo":{"status":"ok","timestamp":1649992272319,"user_tz":-300,"elapsed":44028,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["# Tokenize in Korean\n","import nlu\n","pipe = nlu.load('ko.segment_words')\n","\n","\n","# Korean for 'Asia's economy is booming'\n","ko_data = '아시아 경제는 호황을 누리고 있습니다'\n","\n","df = pipe.predict(ko_data, output_level='token')\n","df"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["wordseg_kaist_ud download started this may take some time.\n","Approximate size to download 696.7 KB\n","[OK!]\n","sentence_detector_dl download started this may take some time.\n","Approximate size to download 354.6 KB\n","[OK!]\n"]},{"output_type":"execute_result","data":{"text/plain":["  words_seg\n","0       아시아\n","0       경제는\n","0       호황을\n","0       누리고\n","0       있습다\n","0         니"],"text/html":["\n","  <div id=\"df-fc42734e-08e6-4cb0-a0d3-41c7a1d8e62e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>words_seg</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>아시아</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>경제는</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>호황을</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>누리고</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>있습다</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>니</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc42734e-08e6-4cb0-a0d3-41c7a1d8e62e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-fc42734e-08e6-4cb0-a0d3-41c7a1d8e62e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-fc42734e-08e6-4cb0-a0d3-41c7a1d8e62e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"WE8CjvFARyJm"},"source":["# Extract Part of Speech in Korean"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450},"id":"Hqw6UalyPts-","outputId":"586add19-41f5-427f-a4d4-b939b1b6c43f","executionInfo":{"status":"error","timestamp":1649992272327,"user_tz":-300,"elapsed":43,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}}},"source":["# Extract Part of Speech\n","pipe = nlu.load('ko.pos')\n","\n","df = pipe.predict(ko_data, output_level='document')\n","df"],"execution_count":null,"outputs":[{"output_type":"error","ename":"Exception","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nlu/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(request, path, verbose, gpu, streamlit_caching)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mnlu_component\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlu_ref_to_component\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlu_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;31m# if we get a list of components, then the NLU reference is a pipeline, we do not need to check order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nlu/pipe/component_resolution.py\u001b[0m in \u001b[0;36mnlu_ref_to_component\u001b[0;34m(nlu_ref, detect_lang, authenticated)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mresolved_component\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trained_component_for_nlp_model_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlu_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlicense_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nlu/pipe/component_resolution.py\u001b[0m in \u001b[0;36mget_trained_component_for_nlp_model_ref\u001b[0;34m(lang, nlu_ref, nlp_ref, license_type, model_configs)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             model_configs: Optional[Dict[str, any]] = None) -> NluComponent:\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0manno_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpellbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp_ref_to_anno_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnlp_ref\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manno_class_to_empty_component\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manno_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: None","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-2399c7d8e4db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Extract Part of Speech\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ko.pos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mko_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'document'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nlu/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(request, path, verbose, gpu, streamlit_caching)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         raise Exception(\n\u001b[0;32m--> 111\u001b[0;31m             f\"Something went wrong during creating the Spark NLP model for your request =  {request} Did you use a NLU Spell?\")\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0;31m# Complete Spark NLP Pipeline, which is defined as a DAG given by the starting Annotators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: Something went wrong during creating the Spark NLP model for your request =  ko.pos Did you use a NLU Spell?"]}]},{"cell_type":"markdown","metadata":{"id":"KngQezoxR0k5"},"source":["# Extract Named Entities in Korean"]},{"cell_type":"code","metadata":{"id":"Zuc7qS_pDYsG"},"source":["# Extract named korean entities \n","pipe = nlu.load('ko.ner.kmou.glove_840B_300d')\n","\n","# Since NER requires POS, we can tell NLU to keep the POS results with metadata=True\n","df = pipe.predict(ko_data, output_level='document')\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nVIQ7eb7OAsS"},"source":["df.entities"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GQHblvNvR4Zf"},"source":["# Translate Japanese extracted named entities to English"]},{"cell_type":"code","metadata":{"id":"UHyNj4l3GXgn"},"source":["translate_pipe = nlu.load('ko.translate_to.en')\n","en_entities = translate_pipe.predict(df.entities.str.join(' ').values.tolist())\n","en_entities"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wnkmr3hv_TdJ"},"source":[""],"execution_count":null,"outputs":[]}]}