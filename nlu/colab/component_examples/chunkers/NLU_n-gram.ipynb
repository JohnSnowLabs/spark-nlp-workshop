{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLU_n-gram.ipynb","provenance":[{"file_id":"1JrlfuV2jNGTdOXvaWIoHTSf6BscDMkN7","timestamp":1599401257319},{"file_id":"1svpqtC3cY6JnRGeJngIPl2raqxdowpyi","timestamp":1599400881246},{"file_id":"1tW833T3HS8F5Lvn6LgeDd5LW5226syKN","timestamp":1599398724652},{"file_id":"1CYzHfQyFCdvIOVO2Z5aggVI9c0hDEOrw","timestamp":1599354735581}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"el9TLbo3dgYs"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/chunkers/NLU_n-gram.ipynb)\n","\n","# Getting n-Grams with NLU\n","N-Grams are subsequences of text with N tokens.       \n","Some of their applications are used for auto completion of sentences, auto spell check and grammar check.     \n","In general they are als overy useful for gaining insight about a text dataset. \n","\n","Examples of n-grams : \n","1. Hello world (is a 2 gram)\n","2. I like peanutbutter (is a 3 gram)\n","3. I like peanutbutter and jelly ( is a 5 gram) \n","\n","\n","\n","\n","\n","\n","# 1. Install Java and NLU"]},{"cell_type":"code","metadata":{"id":"M2-GiYL6xurJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619905099619,"user_tz":-120,"elapsed":150651,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"7320fa31-3708-47ae-eba3-8bc689e5e729"},"source":["!wget https://setup.johnsnowlabs.com/nlu/colab.sh -O - | bash\n","  \n","\n","import nlu"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-05-01 21:35:49--  https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1671 (1.6K) [text/plain]\n","Saving to: ‘STDOUT’\n","\n","\r-                     0%[                    ]       0  --.-KB/s               \r-                   100%[===================>]   1.63K  --.-KB/s    in 0s      \n","\n","2021-05-01 21:35:49 (31.0 MB/s) - written to stdout [1671/1671]\n","\n","Installing  NLU 3.0.0 with  PySpark 3.0.2 and Spark NLP 3.0.1 for Google Colab ...\n","\u001b[K     |████████████████████████████████| 204.8MB 76kB/s \n","\u001b[K     |████████████████████████████████| 153kB 15.4MB/s \n","\u001b[K     |████████████████████████████████| 204kB 17.0MB/s \n","\u001b[K     |████████████████████████████████| 204kB 47.4MB/s \n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Gph8XOL1Pzpl"},"source":["# 2.  Load pipeline and predict on sample data\n","\n","By default NLU is configured to get 2 grams "]},{"cell_type":"code","metadata":{"id":"pmpZSNvGlyZQ","colab":{"base_uri":"https://localhost:8080/","height":348},"executionInfo":{"status":"ok","timestamp":1619905141406,"user_tz":-120,"elapsed":192431,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"0b9a728c-db24-48ff-a5b3-1b87322ab670"},"source":["import nlu\n","example_text =  [\"A person like Jim or Joe\", \n"," \"An organisation like Microsoft or PETA\",\n"," \"A location like Germany\",\n"," \"Anything else like Playstation\", \n"," \"Person consisting of multiple tokens like Angela Merkel or Donald Trump\",\n"," \"Organisations consisting of multiple tokens like JP Morgan\",\n"," \"Locations consiting of multiple tokens like Los Angeles\", \n"," \"Anything else made up of multiple tokens like Super Nintendo\",]\n","\n","pipe = nlu.load('ngram')\n","pipe.predict(example_text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["sentence_detector_dl download started this may take some time.\n","Approximate size to download 354.6 KB\n","[OK!]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>document</th>\n","      <th>sentence</th>\n","      <th>token</th>\n","      <th>ngram</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A person like Jim or Joe</td>\n","      <td>[A person like Jim or Joe]</td>\n","      <td>[A, person, like, Jim, or, Joe]</td>\n","      <td>[A person, person like, like Jim, Jim or, or Joe]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>An organisation like Microsoft or PETA</td>\n","      <td>[An organisation like Microsoft or PETA]</td>\n","      <td>[An, organisation, like, Microsoft, or, PETA]</td>\n","      <td>[An organisation, organisation like, like Micr...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A location like Germany</td>\n","      <td>[A location like Germany]</td>\n","      <td>[A, location, like, Germany]</td>\n","      <td>[A location, location like, like Germany]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Anything else like Playstation</td>\n","      <td>[Anything else like Playstation]</td>\n","      <td>[Anything, else, like, Playstation]</td>\n","      <td>[Anything else, else like, like Playstation]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Person consisting of multiple tokens like Ange...</td>\n","      <td>[Person consisting of multiple tokens like Ang...</td>\n","      <td>[Person, consisting, of, multiple, tokens, lik...</td>\n","      <td>[Person consisting, consisting of, of multiple...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Organisations consisting of multiple tokens li...</td>\n","      <td>[Organisations consisting of multiple tokens l...</td>\n","      <td>[Organisations, consisting, of, multiple, toke...</td>\n","      <td>[Organisations consisting, consisting of, of m...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Locations consiting of multiple tokens like Lo...</td>\n","      <td>[Locations consiting of multiple tokens like L...</td>\n","      <td>[Locations, consiting, of, multiple, tokens, l...</td>\n","      <td>[Locations consiting, consiting of, of multipl...</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Anything else made up of multiple tokens like ...</td>\n","      <td>[Anything else made up of multiple tokens like...</td>\n","      <td>[Anything, else, made, up, of, multiple, token...</td>\n","      <td>[Anything else, else made, made up, up of, of ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            document  ...                                              ngram\n","0                           A person like Jim or Joe  ...  [A person, person like, like Jim, Jim or, or Joe]\n","1             An organisation like Microsoft or PETA  ...  [An organisation, organisation like, like Micr...\n","2                            A location like Germany  ...          [A location, location like, like Germany]\n","3                     Anything else like Playstation  ...       [Anything else, else like, like Playstation]\n","4  Person consisting of multiple tokens like Ange...  ...  [Person consisting, consisting of, of multiple...\n","5  Organisations consisting of multiple tokens li...  ...  [Organisations consisting, consisting of, of m...\n","6  Locations consiting of multiple tokens like Lo...  ...  [Locations consiting, consiting of, of multipl...\n","7  Anything else made up of multiple tokens like ...  ...  [Anything else, else made, made up, up of, of ...\n","\n","[8 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"fvWCtpHCwOYz"},"source":["## Configure the Ngram with custom parameters\n","Use the pipe.print_info() to see all configurable parameters and infos about them for every NLU component in the pipeline pipeline.     \n","Even tough only 'ngram' is loaded, many NLU component dependencies are automatically loaded into the pipeline and also configurable. \n","\n","\n","By default the n-gram algorithm is configured with n=2"]},{"cell_type":"code","metadata":{"id":"j2ZZZvr1uGpx","colab":{"base_uri":"https://localhost:8080/","height":406},"executionInfo":{"status":"ok","timestamp":1619905142330,"user_tz":-120,"elapsed":193349,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"ba9aeeeb-f75a-478e-8182-21be37e0c284"},"source":["pipe.print_info()\n","# Lets configure the NGRAM to get get us 5grams\n","pipe['ngram'].setN(5)\n","\n","# Now we can predict with the configured pipeline\n","pipe.predict(\"Jim and Joe went to the market next to the town hall\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The following parameters are configurable for this NLU pipeline (You can copy paste the examples) :\n",">>> pipe['ngram'] has settable params:\n","pipe['ngram'].setN(2)                                | Info: number elements per n-gram (>=1) | Currently set to : 2\n","pipe['ngram'].setEnableCumulative(False)             | Info: whether to calculate just the actual n-grams or all n-grams from 1 through n | Currently set to : False\n",">>> pipe['default_tokenizer'] has settable params:\n","pipe['default_tokenizer'].setTargetPattern('\\S+')    | Info: pattern to grab from text as token candidates. Defaults \\S+ | Currently set to : \\S+\n","pipe['default_tokenizer'].setContextChars(['.', ',', ';', ':', '!', '?', '*', '-', '(', ')', '\"', \"'\"])  | Info: character list used to separate from token boundaries | Currently set to : ['.', ',', ';', ':', '!', '?', '*', '-', '(', ')', '\"', \"'\"]\n","pipe['default_tokenizer'].setCaseSensitiveExceptions(True)  | Info: Whether to care for case sensitiveness in exceptions | Currently set to : True\n","pipe['default_tokenizer'].setMinLength(0)            | Info: Set the minimum allowed legth for each token | Currently set to : 0\n","pipe['default_tokenizer'].setMaxLength(99999)        | Info: Set the maximum allowed legth for each token | Currently set to : 99999\n",">>> pipe['deep_sentence_detector@SentenceDetectorDLModel_c83c27f46b97'] has settable params:\n","pipe['deep_sentence_detector@SentenceDetectorDLModel_c83c27f46b97'].setExplodeSentences(False)  | Info: whether to explode each sentence into a different row, for better parallelization. Defaults to false. | Currently set to : False\n","pipe['deep_sentence_detector@SentenceDetectorDLModel_c83c27f46b97'].setStorageRef('SentenceDetectorDLModel_c83c27f46b97')  | Info: storage unique identifier | Currently set to : SentenceDetectorDLModel_c83c27f46b97\n","pipe['deep_sentence_detector@SentenceDetectorDLModel_c83c27f46b97'].setEncoder(com.johnsnowlabs.nlp.annotators.sentence_detector_dl.SentenceDetectorDLEncoder@4bb2f221)  | Info: Data encoder | Currently set to : com.johnsnowlabs.nlp.annotators.sentence_detector_dl.SentenceDetectorDLEncoder@4bb2f221\n","pipe['deep_sentence_detector@SentenceDetectorDLModel_c83c27f46b97'].setImpossiblePenultimates(['Bros', 'No', 'al', 'vs', 'etc', 'Fig', 'Dr', 'Prof', 'PhD', 'MD', 'Co', 'Corp', 'Inc', 'bros', 'VS', 'Vs', 'ETC', 'fig', 'dr', 'prof', 'PHD', 'phd', 'md', 'co', 'corp', 'inc', 'Jan', 'Feb', 'Mar', 'Apr', 'Jul', 'Aug', 'Sep', 'Sept', 'Oct', 'Nov', 'Dec', 'St', 'st', 'AM', 'PM', 'am', 'pm', 'e.g', 'f.e', 'i.e'])  | Info: Impossible penultimates | Currently set to : ['Bros', 'No', 'al', 'vs', 'etc', 'Fig', 'Dr', 'Prof', 'PhD', 'MD', 'Co', 'Corp', 'Inc', 'bros', 'VS', 'Vs', 'ETC', 'fig', 'dr', 'prof', 'PHD', 'phd', 'md', 'co', 'corp', 'inc', 'Jan', 'Feb', 'Mar', 'Apr', 'Jul', 'Aug', 'Sep', 'Sept', 'Oct', 'Nov', 'Dec', 'St', 'st', 'AM', 'PM', 'am', 'pm', 'e.g', 'f.e', 'i.e']\n","pipe['deep_sentence_detector@SentenceDetectorDLModel_c83c27f46b97'].setModelArchitecture('cnn')  | Info: Model architecture (CNN) | Currently set to : cnn\n",">>> pipe['document_assembler'] has settable params:\n","pipe['document_assembler'].setCleanupMode('shrink')  | Info: possible values: disabled, inplace, inplace_full, shrink, shrink_full, each, each_full, delete_full | Currently set to : shrink\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>document</th>\n","      <th>sentence</th>\n","      <th>token</th>\n","      <th>ngram</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Jim and Joe went to the market next to the tow...</td>\n","      <td>[Jim and Joe went to the market next to the to...</td>\n","      <td>[Jim, and, Joe, went, to, the, market, next, t...</td>\n","      <td>[Jim and Joe went to, and Joe went to the, Joe...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            document  ...                                              ngram\n","0  Jim and Joe went to the market next to the tow...  ...  [Jim and Joe went to, and Joe went to the, Joe...\n","\n","[1 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"JJaMftSyhtYj"},"source":[""],"execution_count":null,"outputs":[]}]}