{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLU_chunking_example.ipynb","provenance":[{"file_id":"1tW833T3HS8F5Lvn6LgeDd5LW5226syKN","timestamp":1599398724652},{"file_id":"1CYzHfQyFCdvIOVO2Z5aggVI9c0hDEOrw","timestamp":1599354735581}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"FgtBtiBmV1fD"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/chunkers/NLU_chunking_example.ipynb)\n","\n","# Grammatical Chunk Matching with NLU\n","With the chunker you can filter a data set based on Part of Speech Tags with Regex patterns.    \n"," \n","I.e. You could get all nouns or adjectives in your datset with the following parameterization.\n","```\n","pipe['default_chunker'].setRegexParsers(['<NN>+', '<JJ>+'])\n","```\n","\n","See [here](https://www.rexegg.com/regex-quickstart.html)  for a great reference of Regex operators\n","\n","## Overview of all Part of Speech Tags : \n","\n","\n","|Tag |Description | Example|\n","|------|------------|------|\n","|CC| Coordinating conjunction | This batch of mushroom stew is savory **and** delicious    |\n","|CD| Cardinal number | Here are **five** coins    |\n","|DT| Determiner | **The** bunny went home    |\n","|EX| Existential there | **There** is a storm coming    |\n","|FW| Foreign word | I'm having a **déjà vu**    |\n","|IN| Preposition or subordinating conjunction | He is cleverer **than** I am   |\n","|JJ| Adjective | She wore a **beautiful** dress    |\n","|JJR| Adjective, comparative | My house is **bigger** than yours    |\n","|JJS| Adjective, superlative | I am the **shortest** person in my family   |\n","|LS| List item marker | A number of things need to be considered before starting a business **,** such as premises **,** finance **,** product demand **,** staffing and access to customers |\n","|MD| Modal | You **must** stop when the traffic lights turn red    |\n","|NN| Noun, singular or mass | The **dog** likes to run    |\n","|NNS| Noun, plural | The **cars** are fast    |\n","|NNP| Proper noun, singular | I ordered the chair from **Amazon**  |\n","|NNPS| Proper noun, plural | We visted the **Kennedys**   |\n","|PDT| Predeterminer | **Both** the children had a toy   |\n","|POS| Possessive ending | I built the dog'**s** house    |\n","|PRP| Personal pronoun | **You** need to stop    |\n","|PRP$| Possessive pronoun | Remember not to judge a book by **its** cover |\n","|RB| Adverb | The dog barks **loudly**    |\n","|RBR| Adverb, comparative | Could you sing more **quietly** please?   |\n","|RBS| Adverb, superlative | Everyone in the race ran fast, but John ran **the fastest** of all    |\n","|RP| Particle | He ate **up** all his dinner    |\n","|SYM| Symbol | What are you doing **?**    |\n","|TO| to | Please send it back **to** me    |\n","|UH| Interjection | **Wow!** You look gorgeous    |\n","|VB| Verb, base form | We **play** soccer |\n","|VBD| Verb, past tense | I **worked** at a restaurant    |\n","|VBG| Verb, gerund or present participle | **Smoking** kills people   |\n","|VBN| Verb, past participle | She has **done** her homework    |\n","|VBP| Verb, non-3rd person singular present | You **flit** from place to place    |\n","|VBZ| Verb, 3rd person singular present | He never **calls** me    |\n","|WDT| Wh-determiner | The store honored the complaints, **which** were less than 25 days old    |\n","|WP| Wh-pronoun | **Who** can help me?    |\n","|WP\\$| Possessive wh-pronoun | **Whose** fault is it?    |\n","|WRB| Wh-adverb | **Where** are you going?  |\n","\n","\n","\n","\n","\n","\n","\n","\n","Chunks are Named \n"]},{"cell_type":"code","metadata":{"id":"M2-GiYL6xurJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649991438009,"user_tz":-300,"elapsed":129291,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}},"outputId":"507fb8b7-8944-4c8f-dbc8-9ac0112f6f5c"},"source":["!wget https://setup.johnsnowlabs.com/nlu/colab.sh -O - | bash\n","  \n","\n","import nlu"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-04-15 02:55:06--  https://setup.johnsnowlabs.com/nlu/colab.sh\n","Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n","Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:443... connected.\n","HTTP request sent, awaiting response... 302 Moved Temporarily\n","Location: https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh [following]\n","--2022-04-15 02:55:07--  https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1665 (1.6K) [text/plain]\n","Saving to: ‘STDOUT’\n","\n","-                   100%[===================>]   1.63K  --.-KB/s    in 0s      \n","\n","2022-04-15 02:55:07 (21.5 MB/s) - written to stdout [1665/1665]\n","\n","Installing  NLU 3.4.3rc2 with  PySpark 3.0.3 and Spark NLP 3.4.2 for Google Colab ...\n","Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n","Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n","Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Get:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n","Get:13 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,490 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,695 kB]\n","Hit:16 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,947 kB]\n","Get:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [953 kB]\n","Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [996 kB]\n","Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,134 kB]\n","Get:22 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,268 kB]\n","Fetched 13.8 MB in 3s (4,024 kB/s)\n","Reading package lists... Done\n","tar: spark-3.0.2-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n","tar: Error is not recoverable: exiting now\n","\u001b[K     |████████████████████████████████| 209.1 MB 61 kB/s \n","\u001b[K     |████████████████████████████████| 142 kB 52.7 MB/s \n","\u001b[K     |████████████████████████████████| 505 kB 53.9 MB/s \n","\u001b[K     |████████████████████████████████| 198 kB 57.3 MB/s \n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting nlu_tmp==3.4.3rc10\n","  Downloading nlu_tmp-3.4.3rc10-py3-none-any.whl (510 kB)\n","\u001b[K     |████████████████████████████████| 510 kB 21.4 MB/s \n","\u001b[?25hRequirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.7/dist-packages (from nlu_tmp==3.4.3rc10) (1.3.5)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from nlu_tmp==3.4.3rc10) (0.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nlu_tmp==3.4.3rc10) (1.21.5)\n","Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from nlu_tmp==3.4.3rc10) (6.0.1)\n","Requirement already satisfied: spark-nlp<3.5.0,>=3.4.2 in /usr/local/lib/python3.7/dist-packages (from nlu_tmp==3.4.3rc10) (3.4.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.3.5->nlu_tmp==3.4.3rc10) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.3.5->nlu_tmp==3.4.3rc10) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.3.5->nlu_tmp==3.4.3rc10) (1.15.0)\n","Installing collected packages: nlu-tmp\n","Successfully installed nlu-tmp-3.4.3rc10\n"]}]},{"cell_type":"markdown","metadata":{"id":"NYQRU3pRO146"},"source":["# 2. Load the Chunker and print parameters"]},{"cell_type":"code","metadata":{"id":"pmpZSNvGlyZQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649991534186,"user_tz":-300,"elapsed":98374,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}},"outputId":"62a12ae0-152e-469c-9c3c-1e8a798aa685"},"source":["import nlu \n","\n","pipe = nlu.load('match.chunks')\n","# Now we print the info to see at which index which com,ponent is and what parameters we can configure on them \n","pipe.print_info()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["match_chunks download started this may take some time.\n","Approx size to download 4 MB\n","[OK!]\n","The following parameters are configurable for this NLU pipeline (You can copy paste the examples) :\n",">>> component_list['document_assembler'] has settable params:\n","component_list['document_assembler'].setCleanupMode('disabled')               | Info: possible values: disabled, inplace, inplace_full, shrink, shrink_full, each, each_full, delete_full | Currently set to : disabled\n",">>> component_list['sentence_detector'] has settable params:\n","component_list['sentence_detector'].setCustomBounds([])                       | Info: characters used to explicitly mark sentence bounds | Currently set to : []\n","component_list['sentence_detector'].setDetectLists(True)                      | Info: whether detect lists during sentence detection | Currently set to : True\n","component_list['sentence_detector'].setExplodeSentences(False)                | Info: whether to explode each sentence into a different row, for better parallelization. Defaults to false. | Currently set to : False\n","component_list['sentence_detector'].setMaxLength(99999)                       | Info: Set the maximum allowed length for each sentence | Currently set to : 99999\n","component_list['sentence_detector'].setMinLength(0)                           | Info: Set the minimum allowed length for each sentence. | Currently set to : 0\n","component_list['sentence_detector'].setUseAbbreviations(True)                 | Info: whether to apply abbreviations at sentence detection | Currently set to : True\n","component_list['sentence_detector'].setUseCustomBoundsOnly(False)             | Info: Only utilize custom bounds in sentence detection | Currently set to : False\n",">>> component_list['tokenizer'] has settable params:\n","component_list['tokenizer'].setCaseSensitiveExceptions(True)                  | Info: Whether to care for case sensitiveness in exceptions | Currently set to : True\n","component_list['tokenizer'].setTargetPattern('\\S+')                           | Info: pattern to grab from text as token candidates. Defaults \\S+ | Currently set to : \\S+\n","component_list['tokenizer'].setMaxLength(99999)                               | Info: Set the maximum allowed length for each token | Currently set to : 99999\n","component_list['tokenizer'].setMinLength(0)                                   | Info: Set the minimum allowed length for each token | Currently set to : 0\n",">>> component_list['pos'] has settable params:\n",">>> component_list['chunker'] has settable params:\n","component_list['chunker'].setRegexParsers(['<DT>?<JJ>*<NN>+'])                | Info: an array of grammar based chunk parsers | Currently set to : ['<DT>?<JJ>*<NN>+']\n",">>> component_list['chunk_converter@document'] has settable params:\n","component_list['chunk_converter@document'].setCleanupMode('disabled')         | Info: possible values: disabled, inplace, inplace_full, shrink, shrink_full, each, each_full, delete_full | Currently set to : disabled\n",">>> component_list['chunk_converter@sentence'] has settable params:\n","component_list['chunk_converter@sentence'].setCustomBounds([])                | Info: characters used to explicitly mark sentence bounds | Currently set to : []\n","component_list['chunk_converter@sentence'].setDetectLists(True)               | Info: whether detect lists during sentence detection | Currently set to : True\n","component_list['chunk_converter@sentence'].setExplodeSentences(False)         | Info: whether to explode each sentence into a different row, for better parallelization. Defaults to false. | Currently set to : False\n","component_list['chunk_converter@sentence'].setMaxLength(99999)                | Info: Set the maximum allowed length for each sentence | Currently set to : 99999\n","component_list['chunk_converter@sentence'].setMinLength(0)                    | Info: Set the minimum allowed length for each sentence. | Currently set to : 0\n","component_list['chunk_converter@sentence'].setUseAbbreviations(True)          | Info: whether to apply abbreviations at sentence detection | Currently set to : True\n","component_list['chunk_converter@sentence'].setUseCustomBoundsOnly(False)      | Info: Only utilize custom bounds in sentence detection | Currently set to : False\n",">>> component_list['chunk_converter@token'] has settable params:\n","component_list['chunk_converter@token'].setCaseSensitiveExceptions(True)      | Info: Whether to care for case sensitiveness in exceptions | Currently set to : True\n","component_list['chunk_converter@token'].setTargetPattern('\\S+')               | Info: pattern to grab from text as token candidates. Defaults \\S+ | Currently set to : \\S+\n","component_list['chunk_converter@token'].setMaxLength(99999)                   | Info: Set the maximum allowed length for each token | Currently set to : 99999\n","component_list['chunk_converter@token'].setMinLength(0)                       | Info: Set the minimum allowed length for each token | Currently set to : 0\n",">>> component_list['chunk_converter@pos'] has settable params:\n",">>> component_list['chunk_converter@chunk'] has settable params:\n","component_list['chunk_converter@chunk'].setRegexParsers(['<DT>?<JJ>*<NN>+'])  | Info: an array of grammar based chunk parsers | Currently set to : ['<DT>?<JJ>*<NN>+']\n"]}]},{"cell_type":"markdown","metadata":{"id":"9RRmIv9ZbaX3"},"source":["# 3. Configure pipe to only match nounds and adjvectives and predict on data"]},{"cell_type":"code","metadata":{"id":"j2ZZZvr1uGpx","colab":{"base_uri":"https://localhost:8080/","height":174},"executionInfo":{"status":"ok","timestamp":1649991633499,"user_tz":-300,"elapsed":9659,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}},"outputId":"ddf1ae05-e333-4234-a550-3d7f25ca801b"},"source":["# Lets set our Chunker to only match NN\n","pipe['chunker'].setRegexParsers(['<NN>+', '<JJ>+'])\n","# Now we can predict with the configured pipeline\n","pipe.predict(\"Jim and Joe went to the big blue market next to the town hall\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                            document matched_pos  \\\n","0  Jim and Joe went to the big blue market next t...      market   \n","0  Jim and Joe went to the big blue market next t...   town hall   \n","0  Jim and Joe went to the big blue market next t...    big blue   \n","0  Jim and Joe went to the big blue market next t...        next   \n","\n","                                                 pos  \n","0  [NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...  \n","0  [NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...  \n","0  [NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...  \n","0  [NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...  "],"text/html":["\n","  <div id=\"df-323740e8-0f62-4a6b-b6cf-cc6c172f53bc\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>document</th>\n","      <th>matched_pos</th>\n","      <th>pos</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Jim and Joe went to the big blue market next t...</td>\n","      <td>market</td>\n","      <td>[NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Jim and Joe went to the big blue market next t...</td>\n","      <td>town hall</td>\n","      <td>[NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Jim and Joe went to the big blue market next t...</td>\n","      <td>big blue</td>\n","      <td>[NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Jim and Joe went to the big blue market next t...</td>\n","      <td>next</td>\n","      <td>[NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-323740e8-0f62-4a6b-b6cf-cc6c172f53bc')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-323740e8-0f62-4a6b-b6cf-cc6c172f53bc button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-323740e8-0f62-4a6b-b6cf-cc6c172f53bc');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":[""],"metadata":{"id":"UzPlbd6SKiHi"},"execution_count":null,"outputs":[]}]}