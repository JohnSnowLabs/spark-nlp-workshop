{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLU_chunking_example.ipynb","provenance":[{"file_id":"1tW833T3HS8F5Lvn6LgeDd5LW5226syKN","timestamp":1599398724652},{"file_id":"1CYzHfQyFCdvIOVO2Z5aggVI9c0hDEOrw","timestamp":1599354735581}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"FgtBtiBmV1fD"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/chunkers/NLU_chunking_example.ipynb)\n","\n","# Grammatical Chunk Matching with NLU\n","With the chunker you can filter a data set based on Part of Speech Tags with Regex patterns.    \n"," \n","I.e. You could get all nouns or adjectives in your datset with the following parameterization.\n","```\n","pipe['default_chunker'].setRegexParsers(['<NN>+', '<JJ>+'])\n","```\n","\n","See [here](https://www.rexegg.com/regex-quickstart.html)  for a great reference of Regex operators\n","\n","## Overview of all Part of Speech Tags : \n","\n","\n","|Tag |Description | Example|\n","|------|------------|------|\n","|CC| Coordinating conjunction | This batch of mushroom stew is savory **and** delicious    |\n","|CD| Cardinal number | Here are **five** coins    |\n","|DT| Determiner | **The** bunny went home    |\n","|EX| Existential there | **There** is a storm coming    |\n","|FW| Foreign word | I'm having a **déjà vu**    |\n","|IN| Preposition or subordinating conjunction | He is cleverer **than** I am   |\n","|JJ| Adjective | She wore a **beautiful** dress    |\n","|JJR| Adjective, comparative | My house is **bigger** than yours    |\n","|JJS| Adjective, superlative | I am the **shortest** person in my family   |\n","|LS| List item marker | A number of things need to be considered before starting a business **,** such as premises **,** finance **,** product demand **,** staffing and access to customers |\n","|MD| Modal | You **must** stop when the traffic lights turn red    |\n","|NN| Noun, singular or mass | The **dog** likes to run    |\n","|NNS| Noun, plural | The **cars** are fast    |\n","|NNP| Proper noun, singular | I ordered the chair from **Amazon**  |\n","|NNPS| Proper noun, plural | We visted the **Kennedys**   |\n","|PDT| Predeterminer | **Both** the children had a toy   |\n","|POS| Possessive ending | I built the dog'**s** house    |\n","|PRP| Personal pronoun | **You** need to stop    |\n","|PRP$| Possessive pronoun | Remember not to judge a book by **its** cover |\n","|RB| Adverb | The dog barks **loudly**    |\n","|RBR| Adverb, comparative | Could you sing more **quietly** please?   |\n","|RBS| Adverb, superlative | Everyone in the race ran fast, but John ran **the fastest** of all    |\n","|RP| Particle | He ate **up** all his dinner    |\n","|SYM| Symbol | What are you doing **?**    |\n","|TO| to | Please send it back **to** me    |\n","|UH| Interjection | **Wow!** You look gorgeous    |\n","|VB| Verb, base form | We **play** soccer |\n","|VBD| Verb, past tense | I **worked** at a restaurant    |\n","|VBG| Verb, gerund or present participle | **Smoking** kills people   |\n","|VBN| Verb, past participle | She has **done** her homework    |\n","|VBP| Verb, non-3rd person singular present | You **flit** from place to place    |\n","|VBZ| Verb, 3rd person singular present | He never **calls** me    |\n","|WDT| Wh-determiner | The store honored the complaints, **which** were less than 25 days old    |\n","|WP| Wh-pronoun | **Who** can help me?    |\n","|WP\\$| Possessive wh-pronoun | **Whose** fault is it?    |\n","|WRB| Wh-adverb | **Where** are you going?  |\n","\n","\n","\n","\n","\n","\n","\n","\n","Chunks are Named \n"]},{"cell_type":"code","metadata":{"id":"M2-GiYL6xurJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620179685939,"user_tz":-300,"elapsed":125170,"user":{"displayName":"ahmed lone","photoUrl":"","userId":"02458088882398909889"}},"outputId":"eb2572e7-b80f-46a7-be9f-b4889ee9a952"},"source":["!wget https://setup.johnsnowlabs.com/nlu/colab.sh -O - | bash\n","  \n","\n","import nlu"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2021-05-05 01:52:41--  https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1671 (1.6K) [text/plain]\n","Saving to: ‘STDOUT’\n","\n","-                     0%[                    ]       0  --.-KB/s               Installing  NLU 3.0.0 with  PySpark 3.0.2 and Spark NLP 3.0.1 for Google Colab ...\n","-                   100%[===================>]   1.63K  --.-KB/s    in 0.001s  \n","\n","2021-05-05 01:52:41 (1.19 MB/s) - written to stdout [1671/1671]\n","\n","\u001b[K     |████████████████████████████████| 204.8MB 69kB/s \n","\u001b[K     |████████████████████████████████| 153kB 44.2MB/s \n","\u001b[K     |████████████████████████████████| 204kB 22.0MB/s \n","\u001b[K     |████████████████████████████████| 204kB 33.7MB/s \n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NYQRU3pRO146"},"source":["# 2. Load the Chunker and print parameters"]},{"cell_type":"code","metadata":{"id":"pmpZSNvGlyZQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604911109685,"user_tz":-60,"elapsed":116018,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"577793a4-9888-41f7-fd49-431b957b2166"},"source":["import nlu \n","\n","pipe = nlu.load('match.chunks')\n","# Now we print the info to see at which index which com,ponent is and what parameters we can configure on them \n","pipe.print_info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["match_chunks download started this may take some time.\n","Approx size to download 4.3 MB\n","[OK!]\n","The following parameters are configurable for this NLU pipeline (You can copy paste the examples) :\n",">>> pipe['document_assembler'] has settable params:\n","pipe['document_assembler'].setCleanupMode('disabled')         | Info: possible values: disabled, inplace, inplace_full, shrink, shrink_full, each, each_full, delete_full | Currently set to : disabled\n",">>> pipe['sentence_detector'] has settable params:\n","pipe['sentence_detector'].setCustomBounds([])                 | Info: characters used to explicitly mark sentence bounds | Currently set to : []\n","pipe['sentence_detector'].setDetectLists(True)                | Info: whether detect lists during sentence detection | Currently set to : True\n","pipe['sentence_detector'].setExplodeSentences(False)          | Info: whether to explode each sentence into a different row, for better parallelization. Defaults to false. | Currently set to : False\n","pipe['sentence_detector'].setMaxLength(99999)                 | Info: Set the maximum allowed length for each sentence | Currently set to : 99999\n","pipe['sentence_detector'].setMinLength(0)                     | Info: Set the minimum allowed length for each sentence. | Currently set to : 0\n","pipe['sentence_detector'].setUseAbbreviations(True)           | Info: whether to apply abbreviations at sentence detection | Currently set to : True\n","pipe['sentence_detector'].setUseCustomBoundsOnly(False)       | Info: Only utilize custom bounds in sentence detection | Currently set to : False\n",">>> pipe['regex_tokenizer'] has settable params:\n","pipe['regex_tokenizer'].setCaseSensitiveExceptions(True)      | Info: Whether to care for case sensitiveness in exceptions | Currently set to : True\n","pipe['regex_tokenizer'].setTargetPattern('\\S+')               | Info: pattern to grab from text as token candidates. Defaults \\S+ | Currently set to : \\S+\n","pipe['regex_tokenizer'].setMaxLength(99999)                   | Info: Set the maximum allowed length for each token | Currently set to : 99999\n","pipe['regex_tokenizer'].setMinLength(0)                       | Info: Set the minimum allowed length for each token | Currently set to : 0\n",">>> pipe['sentiment_dl'] has settable params:\n",">>> pipe['default_chunker'] has settable params:\n","pipe['default_chunker'].setRegexParsers(['<DT>?<JJ>*<NN>+'])  | Info: an array of grammar based chunk parsers | Currently set to : ['<DT>?<JJ>*<NN>+']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9RRmIv9ZbaX3"},"source":["# 3. Configure pipe to only match nounds and adjvectives and predict on data"]},{"cell_type":"code","metadata":{"id":"j2ZZZvr1uGpx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604911117028,"user_tz":-60,"elapsed":123353,"user":{"displayName":"Christian Kasim Loan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqAD-ircKP-s5Eh6JSdkDggDczfqQbJGU_IRb4Hw=s64","userId":"14469489166467359317"}},"outputId":"f773883e-31fa-4029-f9c6-86e9fa1387ff"},"source":["# Lets set our Chunker to only match NN\n","pipe['default_chunker'].setRegexParsers(['<NN>+', '<JJ>+'])\n","# Now we can predict with the configured pipeline\n","pipe.predict(\"Jim and Joe went to the big blue market next to the town hall\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pos</th>\n","      <th>chunk</th>\n","    </tr>\n","    <tr>\n","      <th>origin_index</th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...</td>\n","      <td>market</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>[NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...</td>\n","      <td>town hall</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>[NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...</td>\n","      <td>big blue</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>[NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...</td>\n","      <td>next</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                            pos      chunk\n","origin_index                                                              \n","0             [NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...     market\n","0             [NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...  town hall\n","0             [NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...   big blue\n","0             [NNP, CC, NNP, VBD, TO, DT, JJ, JJ, NN, JJ, TO...       next"]},"metadata":{"tags":[]},"execution_count":3}]}]}