{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLU_date_matching.ipynb","provenance":[{"file_id":"1svpqtC3cY6JnRGeJngIPl2raqxdowpyi","timestamp":1599400881246},{"file_id":"1tW833T3HS8F5Lvn6LgeDd5LW5226syKN","timestamp":1599398724652},{"file_id":"1CYzHfQyFCdvIOVO2Z5aggVI9c0hDEOrw","timestamp":1599354735581}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NYQRU3pRO146"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n","\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/nlu/blob/master/examples/colab/component_examples/matchers/NLU_date_matching.ipynb)\n","\n","# Date Matching\n","\n"]},{"cell_type":"code","metadata":{"id":"M2-GiYL6xurJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1649992271640,"user_tz":-300,"elapsed":105853,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}},"outputId":"1986a775-4dec-4348-d6d5-160d3a4edb4b"},"source":["!wget https://setup.johnsnowlabs.com/nlu/colab.sh -O - | bash\n","  \n","\n","import nlu"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-04-15 03:09:24--  https://setup.johnsnowlabs.com/nlu/colab.sh\n","Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n","Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:443... connected.\n","HTTP request sent, awaiting response... 302 Moved Temporarily\n","Location: https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh [following]\n","--2022-04-15 03:09:24--  https://raw.githubusercontent.com/JohnSnowLabs/nlu/master/scripts/colab_setup.sh\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1665 (1.6K) [text/plain]\n","Saving to: ‘STDOUT’\n","\n","-                   100%[===================>]   1.63K  --.-KB/s    in 0s      \n","\n","2022-04-15 03:09:25 (23.7 MB/s) - written to stdout [1665/1665]\n","\n","Installing  NLU 3.4.3rc2 with  PySpark 3.0.3 and Spark NLP 3.4.2 for Google Colab ...\n","Hit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:2 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Ign:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Get:5 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Get:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n","Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Get:8 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n","Hit:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n","Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Get:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,268 kB]\n","Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,134 kB]\n","Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,695 kB]\n","Get:18 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,490 kB]\n","Get:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [953 kB]\n","Get:21 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,947 kB]\n","Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [996 kB]\n","Get:23 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n","Fetched 13.8 MB in 5s (2,965 kB/s)\n","Reading package lists... Done\n","tar: spark-3.0.2-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n","tar: Error is not recoverable: exiting now\n","\u001b[K     |████████████████████████████████| 209.1 MB 61 kB/s \n","\u001b[K     |████████████████████████████████| 142 kB 39.7 MB/s \n","\u001b[K     |████████████████████████████████| 505 kB 48.1 MB/s \n","\u001b[K     |████████████████████████████████| 198 kB 69.6 MB/s \n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting nlu_tmp==3.4.3rc10\n","  Downloading nlu_tmp-3.4.3rc10-py3-none-any.whl (510 kB)\n","\u001b[K     |████████████████████████████████| 510 kB 4.9 MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from nlu_tmp==3.4.3rc10) (0.6)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nlu_tmp==3.4.3rc10) (1.21.5)\n","Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.7/dist-packages (from nlu_tmp==3.4.3rc10) (6.0.1)\n","Requirement already satisfied: spark-nlp<3.5.0,>=3.4.2 in /usr/local/lib/python3.7/dist-packages (from nlu_tmp==3.4.3rc10) (3.4.2)\n","Requirement already satisfied: pandas>=1.3.5 in /usr/local/lib/python3.7/dist-packages (from nlu_tmp==3.4.3rc10) (1.3.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.3.5->nlu_tmp==3.4.3rc10) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.3.5->nlu_tmp==3.4.3rc10) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.3.5->nlu_tmp==3.4.3rc10) (1.15.0)\n","Installing collected packages: nlu-tmp\n","Successfully installed nlu-tmp-3.4.3rc10\n"]}]},{"cell_type":"markdown","metadata":{"id":"Gph8XOL1Pzpl"},"source":["# NLU makes Date Matching easy. \n","\n"]},{"cell_type":"code","metadata":{"id":"pmpZSNvGlyZQ","colab":{"base_uri":"https://localhost:8080/","height":571},"executionInfo":{"status":"error","timestamp":1649992303361,"user_tz":-300,"elapsed":31734,"user":{"displayName":"ahmed lone","userId":"02458088882398909889"}},"outputId":"996acd83-93ba-4a98-af73-7b57db52f438"},"source":["import nlu \n","\n","example_text =  [\"A person like Jim or Joe\", \n"," \"An organisation like Microsoft or PETA\",\n"," \"A location like Germany\",\n"," \"Anything else like Playstation\", \n"," \"Person consisting of multiple tokens like Angela Merkel or Donald Trump\",\n"," \"Organisations consisting of multiple tokens like JP Morgan\",\n"," \"Locations consiting of multiple tokens like Los Angeles\", \n"," \"Anything else made up of multiple tokens like Super Nintendo\",]\n","\n","pipe = nlu.load('match.datetime',verbose = True)\n","pipe.predict(\"Jim and Joe went to the market next to the town hall\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Parsed Nlu_ref=match.datetime as lang=en\n","Parsed Nlu_ref=match.datetime as lang=en\n","Parsed Nlu_ref=en.match.datetime as lang=en\n"]},{"output_type":"stream","name":"stdout","text":["<class 'KeyError'>\n","None\n","None\n"]},{"output_type":"error","ename":"Exception","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nlu/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(request, path, verbose, gpu, streamlit_caching)\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mnlu_component\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlu_ref_to_component\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlu_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;31m# if we get a list of components, then the NLU reference is a pipeline, we do not need to check order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nlu/pipe/component_resolution.py\u001b[0m in \u001b[0;36mnlu_ref_to_component\u001b[0;34m(nlu_ref, detect_lang, authenticated)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mresolved_component\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_trained_component_for_nlp_model_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlu_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp_ref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlicense_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nlu/pipe/component_resolution.py\u001b[0m in \u001b[0;36mget_trained_component_for_nlp_model_ref\u001b[0;34m(lang, nlu_ref, nlp_ref, license_type, model_configs)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             model_configs: Optional[Dict[str, any]] = None) -> NluComponent:\n\u001b[0;32m--> 219\u001b[0;31m     \u001b[0manno_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpellbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp_ref_to_anno_class\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnlp_ref\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m     \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manno_class_to_empty_component\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manno_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: None","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-f95e50e7963a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m  \"Anything else made up of multiple tokens like Super Nintendo\",]\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'match.datetime'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Jim and Joe went to the market next to the town hall\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nlu/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(request, path, verbose, gpu, streamlit_caching)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         raise Exception(\n\u001b[0;32m--> 111\u001b[0;31m             f\"Something went wrong during creating the Spark NLP model for your request =  {request} Did you use a NLU Spell?\")\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0;31m# Complete Spark NLP Pipeline, which is defined as a DAG given by the starting Annotators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: Something went wrong during creating the Spark NLP model for your request =  match.datetime Did you use a NLU Spell?"]}]},{"cell_type":"markdown","metadata":{"id":"if5mQWqRxDst"},"source":["## Configure the date macher with custom parameters"]},{"cell_type":"code","metadata":{"id":"j2ZZZvr1uGpx"},"source":["pipe.print_info()\n","# Lets set our Chunker to only match NN\n","pipe['date_matcher'].setReadMonthFirst(True)   \n","\n","# Now we can predict with the configured pipeline\n","pipe.predict(\"2020/01/01 was a intresting day\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2z5D3cPrEhu9"},"source":["pipe['date_matcher'].setReadMonthFirst(False)   \n","\n","# Now we can predict with the configured pipeline\n","pipe.predict(\"2020/01/01 was a intresting day\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n3p0gLbvEodo"},"source":[""],"execution_count":null,"outputs":[]}]}