{"cells":[{"cell_type":"markdown","source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ac48b1a0-d86c-46c8-8830-4e482b8e5d23","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# 8. Advanced Sentence Segmentation"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ec786a7d-22f7-41f9-9cc9-296b23e6e43a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import sparknlp\nfrom sparknlp.base import *\nfrom sparknlp.annotator import *\nfrom pyspark.ml import PipelineModel\n\n\nprint(\"Spark NLP version\", sparknlp.version())\n\nprint(\"Apache Spark version:\", spark.version)\n\nspark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8b22ca02-3f21-4fa5-bd4f-4c565045bfba","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Spark NLP version 4.2.4\nApache Spark version: 3.1.2\nOut[1]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Spark NLP version 4.2.4\nApache Spark version: 3.1.2\nOut[1]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=7956323724731612#setting/sparkui/0616-152819-zhyjt0vc/driver-2464948806542488808\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.139.64.4:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=7956323724731612#setting/sparkui/0616-152819-zhyjt0vc/driver-2464948806542488808\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.139.64.4:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"markdown","source":["`SentenceDetectorDL` (SDDL) is based on a general-purpose neural network model for sentence boundary detection.  The task of sentence boundary detection is to identify sentences within a text. Many natural language processing tasks take a sentence as an input unit, such as part-of-speech tagging, dependency parsing, named entity recognition or machine translation.\n\nIn this model, we treated the sentence boundary detection task as a classification problem using a DL CNN architecture. We also modified the original implemenation a little bit to cover broken sentences and some impossible end of line chars.\n\nWe are releasing two pretrained SDDL models: `english` and `multilanguage` that are trained on `SETimes corpus (Tyers and Alperen, 2010)` and ` Europarl. Wong et al. (2014)` datasets.\n\nHere are the test metrics on various languages for `multilang` model"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5eb01b95-1046-4006-b361-17e96fbc8ab8","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["`bg Bulgarian`\n\n`bs Bosnian`\n\n`de German`\n\n`el Greek`\n\n`en English`\n\n`hr Croatian`\n\n`mk Macedonian`\n\n`ro Romanian`\n\n`sq Albanian`\n\n`sr Serbian`\n\n`tr Turkish`"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d23f868c-037a-4aea-bfad-6e8c01e0349c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["documenter = DocumentAssembler()\\\n    .setInputCol(\"text\")\\\n    .setOutputCol(\"document\")\n    \nsentencerDL = SentenceDetectorDLModel\\\n    .pretrained(\"sentence_detector_dl\", \"en\") \\\n    .setInputCols([\"document\"]) \\\n    .setOutputCol(\"sentences\")\n\nsd_pipeline = PipelineModel(stages=[documenter, sentencerDL])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d94ad132-bac3-46b8-bf09-83a5e6bf7a6c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">sentence_detector_dl download started this may take some time.\nApproximate size to download 354.6 KB\n\r[ | ]\r[OK!]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">sentence_detector_dl download started this may take some time.\nApproximate size to download 354.6 KB\n\r[ | ]\r[OK!]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["sd_model = LightPipeline(sd_pipeline)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"34a9e998-bded-441a-85f6-123b77c32199","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["text = \"\"\"John loves Mary.Mary loves Peter\nPeter loves Helen .Helen loves John; \nTotal: four people involved.\"\"\"\n\nfor anno in sd_model.fullAnnotate(text)[0][\"sentences\"]:\n    print(\"{}\\t{}\\t{}\\t{}\".format(\n        anno.metadata[\"sentence\"], anno.begin, anno.end, anno.result))\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2dae32bc-91bc-4f6d-bc50-1d5206d3aca3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">0\t0\t15\tJohn loves Mary.\n1\t16\t31\tMary loves Peter\n2\t33\t51\tPeter loves Helen .\n3\t52\t68\tHelen loves John;\n4\t71\t98\tTotal: four people involved.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0\t0\t15\tJohn loves Mary.\n1\t16\t31\tMary loves Peter\n2\t33\t51\tPeter loves Helen .\n3\t52\t68\tHelen loves John;\n4\t71\t98\tTotal: four people involved.\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Testing with a broken text (random `\\n` chars added)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e1ca4462-4747-4edd-a54a-e3076fd33890","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["text = '''\nThere are many NLP tasks like text summarization, question-answering, sentence prediction to name a few. One method to get\\n these tasks done is using a pre-trained model. Instead of training \na model from scratch for NLP tasks using millions of annotated texts each time, a general language representation is created by training a model on a huge amount of data. This is called a pre-trained model. This pre-trained model is \nthen fine-tuned for each NLP tasks according to need.\nLet’s just peek into the pre-BERT world…\nFor creating models, we need words to be represented in a form \\n understood by the training network, ie, numbers. Thus many algorithms were used to convert words into vectors or more precisely, word embeddings. \nOne of the earliest algorithms used for this purpose is word2vec. However, the drawback of word2vec models was that they were context-free. One problem caused by this is that they cannot accommodate polysemy. For example, the word ‘letter’ has a different meaning according to the context. It can mean ‘single element of alphabet’ or ‘document addressed to another person’. But in word2vec both the letter returns same embeddings.\n'''\n\nfor anno in sd_model.fullAnnotate(text)[0][\"sentences\"]:\n  \n    print(\"{}\\t{}\\t{}\\t{}\".format(\n        anno.metadata[\"sentence\"], anno.begin, anno.end, anno.result.replace('\\n',''))) # removing \\n to beutify printing\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"75fcb557-0ef9-42f6-aa1a-17d226a39d5c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">0\t1\t104\tThere are many NLP tasks like text summarization, question-answering, sentence prediction to name a few.\n1\t106\t170\tOne method to get these tasks done is using a pre-trained model.\n2\t172\t362\tInstead of training a model from scratch for NLP tasks using millions of annotated texts each time, a general language representation is created by training a model on a huge amount of data.\n3\t364\t398\tThis is called a pre-trained model.\n4\t400\t479\tThis pre-trained model is then fine-tuned for each NLP tasks according to need.\n5\t481\t520\tLet’s just peek into the pre-BERT world…\n6\t522\t634\tFor creating models, we need words to be represented in a form  understood by the training network, ie, numbers.\n7\t636\t731\tThus many algorithms were used to convert words into vectors or more precisely, word embeddings.\n8\t734\t798\tOne of the earliest algorithms used for this purpose is word2vec.\n9\t800\t872\tHowever, the drawback of word2vec models was that they were context-free.\n10\t874\t941\tOne problem caused by this is that they cannot accommodate polysemy.\n11\t943\t1022\tFor example, the word ‘letter’ has a different meaning according to the context.\n12\t1024\t1106\tIt can mean ‘single element of alphabet’ or ‘document addressed to another person’.\n13\t1108\t1163\tBut in word2vec both the letter returns same embeddings.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0\t1\t104\tThere are many NLP tasks like text summarization, question-answering, sentence prediction to name a few.\n1\t106\t170\tOne method to get these tasks done is using a pre-trained model.\n2\t172\t362\tInstead of training a model from scratch for NLP tasks using millions of annotated texts each time, a general language representation is created by training a model on a huge amount of data.\n3\t364\t398\tThis is called a pre-trained model.\n4\t400\t479\tThis pre-trained model is then fine-tuned for each NLP tasks according to need.\n5\t481\t520\tLet’s just peek into the pre-BERT world…\n6\t522\t634\tFor creating models, we need words to be represented in a form  understood by the training network, ie, numbers.\n7\t636\t731\tThus many algorithms were used to convert words into vectors or more precisely, word embeddings.\n8\t734\t798\tOne of the earliest algorithms used for this purpose is word2vec.\n9\t800\t872\tHowever, the drawback of word2vec models was that they were context-free.\n10\t874\t941\tOne problem caused by this is that they cannot accommodate polysemy.\n11\t943\t1022\tFor example, the word ‘letter’ has a different meaning according to the context.\n12\t1024\t1106\tIt can mean ‘single element of alphabet’ or ‘document addressed to another person’.\n13\t1108\t1163\tBut in word2vec both the letter returns same embeddings.\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Compare with Spacy Sentence Splitter"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d9f6ac5f-c864-4260-90bd-2eed8404b34c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sh /databricks/python3/bin/pip install --upgrade spacy==3.2.1 markupsafe==2.0.1\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8a94ad4b-fd44-4241-99ca-dee9394210f8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Requirement already satisfied: spacy==3.2.1 in /databricks/python3/lib/python3.8/site-packages (3.2.1)\nRequirement already satisfied: markupsafe==2.0.1 in /databricks/python3/lib/python3.8/site-packages (2.0.1)\nRequirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.8 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (3.0.11)\nRequirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (1.0.4)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (2.25.1)\nRequirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.1 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (2.4.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy==3.2.1) (52.0.0)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (4.64.1)\nRequirement already satisfied: numpy&gt;=1.15.0 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (1.22.0)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (3.0.8)\nRequirement already satisfied: pathy&gt;=0.3.5 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (0.10.1)\nRequirement already satisfied: blis&lt;0.8.0,&gt;=0.4.0 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (0.7.9)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.9.0,&gt;=1.7.4 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (1.8.2)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (1.0.9)\nRequirement already satisfied: typer&lt;0.5.0,&gt;=0.3.0 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (0.4.2)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (2.11.3)\nRequirement already satisfied: thinc&lt;8.1.0,&gt;=8.0.12 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (8.0.17)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (2.0.7)\nRequirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (3.3.0)\nRequirement already satisfied: packaging&gt;=20.0 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (20.9)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.8.1 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (0.10.1)\nRequirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (2.0.8)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from packaging&gt;=20.0-&gt;spacy==3.2.1) (2.4.7)\nRequirement already satisfied: smart-open&lt;7.0.0,&gt;=5.2.1 in /databricks/python3/lib/python3.8/site-packages (from pathy&gt;=0.3.5-&gt;spacy==3.2.1) (6.3.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /databricks/python3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,&lt;1.9.0,&gt;=1.7.4-&gt;spacy==3.2.1) (4.4.0)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy==3.2.1) (1.25.11)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy==3.2.1) (2.10)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy==3.2.1) (2020.12.5)\nRequirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy==3.2.1) (4.0.0)\nRequirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /databricks/python3/lib/python3.8/site-packages (from typer&lt;0.5.0,&gt;=0.3.0-&gt;spacy==3.2.1) (8.1.3)\nWARNING: You are using pip version 21.0.1; however, version 22.3.1 is available.\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Requirement already satisfied: spacy==3.2.1 in /databricks/python3/lib/python3.8/site-packages (3.2.1)\nRequirement already satisfied: markupsafe==2.0.1 in /databricks/python3/lib/python3.8/site-packages (2.0.1)\nRequirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.8 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (3.0.11)\nRequirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (1.0.4)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (2.25.1)\nRequirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.1 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (2.4.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy==3.2.1) (52.0.0)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (4.64.1)\nRequirement already satisfied: numpy&gt;=1.15.0 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (1.22.0)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (3.0.8)\nRequirement already satisfied: pathy&gt;=0.3.5 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (0.10.1)\nRequirement already satisfied: blis&lt;0.8.0,&gt;=0.4.0 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (0.7.9)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.9.0,&gt;=1.7.4 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (1.8.2)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (1.0.9)\nRequirement already satisfied: typer&lt;0.5.0,&gt;=0.3.0 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (0.4.2)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (2.11.3)\nRequirement already satisfied: thinc&lt;8.1.0,&gt;=8.0.12 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (8.0.17)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (2.0.7)\nRequirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (3.3.0)\nRequirement already satisfied: packaging&gt;=20.0 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (20.9)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.8.1 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (0.10.1)\nRequirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /databricks/python3/lib/python3.8/site-packages (from spacy==3.2.1) (2.0.8)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from packaging&gt;=20.0-&gt;spacy==3.2.1) (2.4.7)\nRequirement already satisfied: smart-open&lt;7.0.0,&gt;=5.2.1 in /databricks/python3/lib/python3.8/site-packages (from pathy&gt;=0.3.5-&gt;spacy==3.2.1) (6.3.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /databricks/python3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,&lt;1.9.0,&gt;=1.7.4-&gt;spacy==3.2.1) (4.4.0)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy==3.2.1) (1.25.11)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy==3.2.1) (2.10)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy==3.2.1) (2020.12.5)\nRequirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy==3.2.1) (4.0.0)\nRequirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /databricks/python3/lib/python3.8/site-packages (from typer&lt;0.5.0,&gt;=0.3.0-&gt;spacy==3.2.1) (8.1.3)\nWARNING: You are using pip version 21.0.1; however, version 22.3.1 is available.\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["%sh python -m spacy download en_core_web_sm "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"04f198f5-4263-4810-b41a-0be5128d4309","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">2023-01-08 20:21:52.548800: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libcudart.so.11.0&#39;; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-01-08 20:21:52.548848: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\nCollecting en-core-web-sm==3.2.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\nRequirement already satisfied: spacy&lt;3.3.0,&gt;=3.2.0 in /databricks/python3/lib/python3.8/site-packages (from en-core-web-sm==3.2.0) (3.2.1)\nRequirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.8 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (3.0.11)\nRequirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (1.0.4)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (2.25.1)\nRequirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.1 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (2.4.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (52.0.0)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (4.64.1)\nRequirement already satisfied: numpy&gt;=1.15.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (1.22.0)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (3.0.8)\nRequirement already satisfied: pathy&gt;=0.3.5 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (0.10.1)\nRequirement already satisfied: blis&lt;0.8.0,&gt;=0.4.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (0.7.9)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.9.0,&gt;=1.7.4 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (1.8.2)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (1.0.9)\nRequirement already satisfied: typer&lt;0.5.0,&gt;=0.3.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (0.4.2)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (2.11.3)\nRequirement already satisfied: thinc&lt;8.1.0,&gt;=8.0.12 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (8.0.17)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (2.0.7)\nRequirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (3.3.0)\nRequirement already satisfied: packaging&gt;=20.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (20.9)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.8.1 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (0.10.1)\nRequirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (2.0.8)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from packaging&gt;=20.0-&gt;spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (2.4.7)\nRequirement already satisfied: smart-open&lt;7.0.0,&gt;=5.2.1 in /databricks/python3/lib/python3.8/site-packages (from pathy&gt;=0.3.5-&gt;spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (6.3.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /databricks/python3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,&lt;1.9.0,&gt;=1.7.4-&gt;spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (4.4.0)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (1.25.11)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (2.10)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (2020.12.5)\nRequirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (4.0.0)\nRequirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /databricks/python3/lib/python3.8/site-packages (from typer&lt;0.5.0,&gt;=0.3.0-&gt;spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (8.1.3)\nRequirement already satisfied: MarkupSafe&gt;=0.23 in /databricks/python3/lib/python3.8/site-packages (from jinja2-&gt;spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (2.0.1)\nWARNING: You are using pip version 21.0.1; however, version 22.3.1 is available.\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.\n<span class=\"ansi-green-fg\">✔ Download and installation successful</span>\nYou can now load the package via spacy.load(&#39;en_core_web_sm&#39;)\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2023-01-08 20:21:52.548800: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library &#39;libcudart.so.11.0&#39;; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n2023-01-08 20:21:52.548848: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\nCollecting en-core-web-sm==3.2.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\nRequirement already satisfied: spacy&lt;3.3.0,&gt;=3.2.0 in /databricks/python3/lib/python3.8/site-packages (from en-core-web-sm==3.2.0) (3.2.1)\nRequirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.8 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (3.0.11)\nRequirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (1.0.4)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (2.25.1)\nRequirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.1 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (2.4.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (52.0.0)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (4.64.1)\nRequirement already satisfied: numpy&gt;=1.15.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (1.22.0)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (3.0.8)\nRequirement already satisfied: pathy&gt;=0.3.5 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (0.10.1)\nRequirement already satisfied: blis&lt;0.8.0,&gt;=0.4.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (0.7.9)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.9.0,&gt;=1.7.4 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (1.8.2)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (1.0.9)\nRequirement already satisfied: typer&lt;0.5.0,&gt;=0.3.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (0.4.2)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (2.11.3)\nRequirement already satisfied: thinc&lt;8.1.0,&gt;=8.0.12 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (8.0.17)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (2.0.7)\nRequirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (3.3.0)\nRequirement already satisfied: packaging&gt;=20.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (20.9)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.8.1 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (0.10.1)\nRequirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (2.0.8)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from packaging&gt;=20.0-&gt;spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (2.4.7)\nRequirement already satisfied: smart-open&lt;7.0.0,&gt;=5.2.1 in /databricks/python3/lib/python3.8/site-packages (from pathy&gt;=0.3.5-&gt;spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (6.3.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /databricks/python3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,&lt;1.9.0,&gt;=1.7.4-&gt;spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (4.4.0)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (1.25.11)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (2.10)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (2020.12.5)\nRequirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (4.0.0)\nRequirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /databricks/python3/lib/python3.8/site-packages (from typer&lt;0.5.0,&gt;=0.3.0-&gt;spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (8.1.3)\nRequirement already satisfied: MarkupSafe&gt;=0.23 in /databricks/python3/lib/python3.8/site-packages (from jinja2-&gt;spacy&lt;3.3.0,&gt;=3.2.0-&gt;en-core-web-sm==3.2.0) (2.0.1)\nWARNING: You are using pip version 21.0.1; however, version 22.3.1 is available.\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.\n<span class=\"ansi-green-fg\">✔ Download and installation successful</span>\nYou can now load the package via spacy.load(&#39;en_core_web_sm&#39;)\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import spacy\nimport en_core_web_sm\nnlp = en_core_web_sm.load()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ad0c7f8f-8965-4fbf-9819-4e5094cc2a4b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["nlp = en_core_web_sm.load()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"eaf503d7-7ea4-4625-b9a4-7c7e9b2870cc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["text = \"\"\"John loves Mary.Mary loves Peter\nPeter loves Helen .Helen loves John; \nTotal: four people involved.\"\"\"\n\nfor sent in nlp(text).sents:\n    print(sent)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"39da4329-9240-4726-aae7-c50d6d83c20b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">John loves Mary.\nMary loves Peter\nPeter loves Helen .Helen\nloves John; \nTotal: four people involved.\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">John loves Mary.\nMary loves Peter\nPeter loves Helen .Helen\nloves John; \nTotal: four people involved.\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Test with another random broken sentence"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"38e23eae-b987-42c8-b14d-d840efdb1f0a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["random_broken_text = '''\nA California woman who vanished in Utah’s Zion National Park earlier \nthis month was found and reunited with her family, \nofficials said Sunday. Holly Suzanne Courtier, 38, was located within the park after a visitor \nsaw her and alerted rangers, the National Park Service said in a statement.\nAdditional details about how she \nsurvived or where she was found were not immediately available. In the statement, Courtier’s relatives said they were “overjoyed” that she’d been found.\nCourtier, of Los Angeles, disappeared after a private shuttle dropped her off on Oct. 6 at the Grotto park area \ninside the 232-square-mile national park. She was scheduled to be picked up later that \nafternoon but didn't show up, park officials said. The search included K-9 units and federal, \nstate and local rescue teams. Volunteers also joined the effort.\n'''\n\nprint ('with Spark NLP SentenceDetectorDL')\nprint ('===================================')\n\nfor anno in sd_model.fullAnnotate(random_broken_text)[0][\"sentences\"]:\n  \n    print(\"{}\\t{}\".format(\n        anno.metadata[\"sentence\"], anno.result.replace('\\n',''))) # removing \\n to beutify printing\n\nprint()\nprint ('with Spacy Sentence Detection')\nprint ('===================================')\nfor i,sent in enumerate(nlp(random_broken_text).sents):\n    print(i, '\\t',str(sent).replace('\\n',''))# removing \\n to beutify printing"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"845ac8ce-7de1-4bd2-a1e2-b9453360de0c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">with Spark NLP SentenceDetectorDL\n===================================\n0\tA California woman who vanished in Utah’s Zion National Park earlier this month was found and reunited with her family, officials said Sunday.\n1\tHolly Suzanne Courtier, 38, was located within the park after a visitor saw her and alerted rangers, the National Park Service said in a statement.\n2\tAdditional details about how she survived or where she was found were not immediately available.\n3\tIn the statement, Courtier’s relatives said they were “overjoyed” that she’d been found.\n4\tCourtier, of Los Angeles, disappeared after a private shuttle dropped her off on Oct. 6 at the Grotto park area inside the 232-square-mile national park.\n5\tShe was scheduled to be picked up later that afternoon but didn&#39;t show up, park officials said.\n6\tThe search included K-9 units and federal, state and local rescue teams.\n7\tVolunteers also joined the effort.\n\nwith Spacy Sentence Detection\n===================================\n0 \t A California woman who vanished in Utah’s Zion National Park earlier this month was found and reunited with her family, officials said Sunday.\n1 \t Holly Suzanne Courtier, 38, was located within the park after a visitor saw her and alerted rangers, the National Park Service said in a statement.\n2 \t Additional details about how she survived or where she was found were not immediately available.\n3 \t In the statement, Courtier’s relatives said they were “overjoyed” that she’d been found.\n4 \t Courtier, of Los Angeles, disappeared after a private shuttle dropped her off on Oct. 6 at the Grotto park area inside the 232-square-mile national park.\n5 \t She was scheduled to be picked up later that afternoon but didn&#39;t show up, park officials said.\n6 \t The search included K-9 units and federal, state and local rescue teams.\n7 \t Volunteers also joined the effort.\n8 \t \n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">with Spark NLP SentenceDetectorDL\n===================================\n0\tA California woman who vanished in Utah’s Zion National Park earlier this month was found and reunited with her family, officials said Sunday.\n1\tHolly Suzanne Courtier, 38, was located within the park after a visitor saw her and alerted rangers, the National Park Service said in a statement.\n2\tAdditional details about how she survived or where she was found were not immediately available.\n3\tIn the statement, Courtier’s relatives said they were “overjoyed” that she’d been found.\n4\tCourtier, of Los Angeles, disappeared after a private shuttle dropped her off on Oct. 6 at the Grotto park area inside the 232-square-mile national park.\n5\tShe was scheduled to be picked up later that afternoon but didn&#39;t show up, park officials said.\n6\tThe search included K-9 units and federal, state and local rescue teams.\n7\tVolunteers also joined the effort.\n\nwith Spacy Sentence Detection\n===================================\n0 \t A California woman who vanished in Utah’s Zion National Park earlier this month was found and reunited with her family, officials said Sunday.\n1 \t Holly Suzanne Courtier, 38, was located within the park after a visitor saw her and alerted rangers, the National Park Service said in a statement.\n2 \t Additional details about how she survived or where she was found were not immediately available.\n3 \t In the statement, Courtier’s relatives said they were “overjoyed” that she’d been found.\n4 \t Courtier, of Los Angeles, disappeared after a private shuttle dropped her off on Oct. 6 at the Grotto park area inside the 232-square-mile national park.\n5 \t She was scheduled to be picked up later that afternoon but didn&#39;t show up, park officials said.\n6 \t The search included K-9 units and federal, state and local rescue teams.\n7 \t Volunteers also joined the effort.\n8 \t \n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Multilanguage Sentence Detector DL"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2350fd03-1243-46cb-a5e4-a6b425788f64","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["sentencerDL_multilang = SentenceDetectorDLModel\\\n  .pretrained(\"sentence_detector_dl\", \"xx\") \\\n  .setInputCols([\"document\"]) \\\n  .setOutputCol(\"sentences\")\n\nsd_pipeline_multi = PipelineModel(stages=[documenter, sentencerDL_multilang])\n\nsd_model_multi = LightPipeline(sd_pipeline_multi)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c67fdb94-3808-4412-844c-6447a5ce0087","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">sentence_detector_dl download started this may take some time.\nApproximate size to download 514.9 KB\n\r[ | ]\r[OK!]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">sentence_detector_dl download started this may take some time.\nApproximate size to download 514.9 KB\n\r[ | ]\r[OK!]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["gr_text= '''\nΌπως ίσως θα γνωρίζει, όταν εγκαθιστάς μια νέα εφαρμογή, θα έχεις διαπιστώσει \nλίγο μετά, ότι το PC αρχίζει να επιβραδύνεται. Στη συνέχεια, όταν επισκέπτεσαι την οθόνη ή από την διαχείριση εργασιών, θα διαπιστώσεις ότι η εν λόγω εφαρμογή έχει προστεθεί στη \nλίστα των προγραμμάτων που εκκινούν αυτόματα, όταν ξεκινάς το PC.\nΠροφανώς, κάτι τέτοιο δεν αποτελεί μια ιδανική κατάσταση, ιδίως για τους λιγότερο γνώστες, οι \nοποίοι ίσως δεν θα συνειδητοποιήσουν ότι κάτι τέτοιο συνέβη. Όσο περισσότερες εφαρμογές στη λίστα αυτή, τόσο πιο αργή γίνεται η \nεκκίνηση, ιδίως αν πρόκειται για απαιτητικές εφαρμογές. Τα ευχάριστα νέα είναι ότι η τελευταία και πιο πρόσφατη preview build της έκδοσης των Windows 10 που θα καταφθάσει στο πρώτο μισό του 2021, οι εφαρμογές θα \nενημερώνουν το χρήστη ότι έχουν προστεθεί στη λίστα των εφαρμογών που εκκινούν μόλις ανοίγεις το PC.\n'''\n\nprint ('with Spark NLP SentenceDetectorDL')\nprint ('===================================')\n\nfor anno in sd_model_multi.fullAnnotate(gr_text)[0][\"sentences\"]:\n  \n    print(\"{}\\t{}\".format(\n        anno.metadata[\"sentence\"], anno.result.replace('\\n',''))) # removing \\n to beutify printing\n\nprint()\nprint ('with Spacy Sentence Detection')\nprint ('===================================')\nfor i,sent in enumerate(nlp(gr_text).sents):\n    print(i, '\\t',str(sent).replace('\\n',''))# removing \\n to beutify printing"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"67763ab2-81b9-45c6-98f2-83bad6509861","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">with Spark NLP SentenceDetectorDL\n===================================\n0\tΌπως ίσως θα γνωρίζει, όταν εγκαθιστάς μια νέα εφαρμογή, θα έχεις διαπιστώσει λίγο μετά, ότι το PC αρχίζει να επιβραδύνεται.\n1\tΣτη συνέχεια, όταν επισκέπτεσαι την οθόνη ή από την διαχείριση εργασιών, θα διαπιστώσεις ότι η εν λόγω εφαρμογή έχει προστεθεί στη λίστα των προγραμμάτων που εκκινούν αυτόματα, όταν ξεκινάς το PC.\n2\tΠροφανώς, κάτι τέτοιο δεν αποτελεί μια ιδανική κατάσταση, ιδίως για τους λιγότερο γνώστες, οι οποίοι ίσως δεν θα συνειδητοποιήσουν ότι κάτι τέτοιο συνέβη.\n3\tΌσο περισσότερες εφαρμογές στη λίστα αυτή, τόσο πιο αργή γίνεται η εκκίνηση, ιδίως αν πρόκειται για απαιτητικές εφαρμογές.\n4\tΤα ευχάριστα νέα είναι ότι η τελευταία και πιο πρόσφατη preview build της έκδοσης των Windows 10 που θα καταφθάσει στο πρώτο μισό του 2021, οι εφαρμογές θα ενημερώνουν το χρήστη ότι έχουν προστεθεί στη λίστα των εφαρμογών που εκκινούν μόλις ανοίγεις το PC.\n\nwith Spacy Sentence Detection\n===================================\n0 \t Όπως ίσως θα γνωρίζει, όταν εγκαθιστάς μια νέα εφαρμογή, θα έχεις διαπιστώσει λίγο μετά, ότι το PC αρχίζει να επιβραδύνεται.\n1 \t Στη συνέχεια, όταν επισκέπτεσαι την οθόνη ή από την διαχείριση εργασιών, θα διαπιστώσεις ότι η εν λόγω εφαρμογή έχει προστεθεί στη λίστα των προγραμμάτων που εκκινούν αυτόματα, όταν ξεκινάς το PC.\n2 \t Προφανώς, κάτι τέτοιο δεν αποτελεί μια ιδανική κατάσταση, ιδίως για τους λιγότερο γνώστες, οι οποίοι ίσως δεν θα συνειδητοποιήσουν ότι κάτι τέτοιο συνέβη.\n3 \t Όσο περισσότερες εφαρμογές στη λίστα αυτή, τόσο πιο αργή γίνεται η εκκίνηση, ιδίως αν πρόκειται για απαιτητικές εφαρμογές.\n4 \t Τα ευχάριστα νέα είναι ότι η τελευταία και πιο πρόσφατη preview build της έκδοσης\n5 \t των Windows 10 που θα καταφθάσει στο πρώτο μισό του 2021, οι εφαρμογές θα ενημερώνουν το χρήστη ότι έχουν προστεθεί στη λίστα των εφαρμογών που εκκινούν μόλις ανοίγεις το PC.\n6 \t \n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">with Spark NLP SentenceDetectorDL\n===================================\n0\tΌπως ίσως θα γνωρίζει, όταν εγκαθιστάς μια νέα εφαρμογή, θα έχεις διαπιστώσει λίγο μετά, ότι το PC αρχίζει να επιβραδύνεται.\n1\tΣτη συνέχεια, όταν επισκέπτεσαι την οθόνη ή από την διαχείριση εργασιών, θα διαπιστώσεις ότι η εν λόγω εφαρμογή έχει προστεθεί στη λίστα των προγραμμάτων που εκκινούν αυτόματα, όταν ξεκινάς το PC.\n2\tΠροφανώς, κάτι τέτοιο δεν αποτελεί μια ιδανική κατάσταση, ιδίως για τους λιγότερο γνώστες, οι οποίοι ίσως δεν θα συνειδητοποιήσουν ότι κάτι τέτοιο συνέβη.\n3\tΌσο περισσότερες εφαρμογές στη λίστα αυτή, τόσο πιο αργή γίνεται η εκκίνηση, ιδίως αν πρόκειται για απαιτητικές εφαρμογές.\n4\tΤα ευχάριστα νέα είναι ότι η τελευταία και πιο πρόσφατη preview build της έκδοσης των Windows 10 που θα καταφθάσει στο πρώτο μισό του 2021, οι εφαρμογές θα ενημερώνουν το χρήστη ότι έχουν προστεθεί στη λίστα των εφαρμογών που εκκινούν μόλις ανοίγεις το PC.\n\nwith Spacy Sentence Detection\n===================================\n0 \t Όπως ίσως θα γνωρίζει, όταν εγκαθιστάς μια νέα εφαρμογή, θα έχεις διαπιστώσει λίγο μετά, ότι το PC αρχίζει να επιβραδύνεται.\n1 \t Στη συνέχεια, όταν επισκέπτεσαι την οθόνη ή από την διαχείριση εργασιών, θα διαπιστώσεις ότι η εν λόγω εφαρμογή έχει προστεθεί στη λίστα των προγραμμάτων που εκκινούν αυτόματα, όταν ξεκινάς το PC.\n2 \t Προφανώς, κάτι τέτοιο δεν αποτελεί μια ιδανική κατάσταση, ιδίως για τους λιγότερο γνώστες, οι οποίοι ίσως δεν θα συνειδητοποιήσουν ότι κάτι τέτοιο συνέβη.\n3 \t Όσο περισσότερες εφαρμογές στη λίστα αυτή, τόσο πιο αργή γίνεται η εκκίνηση, ιδίως αν πρόκειται για απαιτητικές εφαρμογές.\n4 \t Τα ευχάριστα νέα είναι ότι η τελευταία και πιο πρόσφατη preview build της έκδοσης\n5 \t των Windows 10 που θα καταφθάσει στο πρώτο μισό του 2021, οι εφαρμογές θα ενημερώνουν το χρήστη ότι έχουν προστεθεί στη λίστα των εφαρμογών που εκκινούν μόλις ανοίγεις το PC.\n6 \t \n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["cyrillic_text = '''\nB чeтвъpтъĸ Gооglе oбяви няĸoлĸo aĸтyaлизaции нa cвoятa тъpcaчĸa, зaявявaйĸи чe e \nвъвeлa изĸycтвeн интeлeĸт (Аl) и мaшиннo oбyчeниe зa пoдoбpявaнe нa пoтpeбитeлcĸoтo изживявaнe.\nΠoтpeбитeлитe вeчe мoгaт дa cи тaнaниĸaт, cвиpят или пeят мeлoдия нa пeceн нa Gооglе чpeз мoбилнoтo пpилoжeниe, \nĸaтo дoĸocнaт иĸoнaтa нa миĸpoфoнa и зaдaдaт въпpoca: Koя e тaзи пeceн?\nTaнaниĸaнeтo в пpoдължeниe нa 10-15 ceĸyнди щe дaдe шaнc нa aлгopитъмa c мaшиннo oбyчeниe нa Gооglе дa нaмepи и извeдe peзyлтaт ĸoя e пpипявaнaтa пeceн.\nΠoнacтoящeм фyнĸциятa e дocтъпнa нa aнглийcĸи eзиĸ зa Іоѕ и нa oĸoлo 20 eзиĸa зa Аndrоіd, \nĸaтo в бъдeщe и зa двeтe oпepaциoнни cиcтeми щe бъдe пpeдлoжeн eднaĸъв нaбop oт пoддъpжaни eзици, ĸaзвaт oт Gооglе.\nAl aĸтyaлизaциитe нa тъpceщия гигaнт cъщo oбxвaщaт пpaвoпиca и oбщитe зaявĸи зa тъpceнe.\nCpeд пoдoбpeниятa e вĸлючeн нoв пpaвoпиceн aлгopитъм, ĸoйтo изпoлзвa нeвpoннa мpeжa \nc дълбoĸo oбyчeниe, зa ĸoятo Gооglе твъpди, чe идвa cъc знaчитeлнo пoдoбpeнa cпocoбнocт зa \nдeшифpиpaнe нa пpaвoпиcни гpeшĸи.\n'''\n\nprint ('with Spark NLP SentenceDetectorDL')\nprint ('===================================')\n\nfor anno in sd_model_multi.fullAnnotate(cyrillic_text)[0][\"sentences\"]:\n  \n    print(\"{}\\t{}\".format(\n        anno.metadata[\"sentence\"], anno.result.replace('\\n',''))) # removing \\n to beutify printing\n\nprint()\nprint ('with Spacy Sentence Detection')\nprint ('===================================')\nfor i,sent in enumerate(nlp(cyrillic_text).sents):\n    print(i, '\\t',str(sent).replace('\\n',''))# removing \\n to beutify printing"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"08a1d40c-0d63-42dd-abce-7070a97e0e2e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">with Spark NLP SentenceDetectorDL\n===================================\n0\tB чeтвъpтъĸ Gооglе oбяви няĸoлĸo aĸтyaлизaции нa cвoятa тъpcaчĸa, зaявявaйĸи чe e въвeлa изĸycтвeн интeлeĸт (Аl) и мaшиннo oбyчeниe зa пoдoбpявaнe нa пoтpeбитeлcĸoтo изживявaнe.\n1\tΠoтpeбитeлитe вeчe мoгaт дa cи тaнaниĸaт, cвиpят или пeят мeлoдия нa пeceн нa Gооglе чpeз мoбилнoтo пpилoжeниe, ĸaтo дoĸocнaт иĸoнaтa нa миĸpoфoнa и зaдaдaт въпpoca: Koя e тaзи пeceн?\n2\tTaнaниĸaнeтo в пpoдължeниe нa 10-15 ceĸyнди щe дaдe шaнc нa aлгopитъмa c мaшиннo oбyчeниe нa Gооglе дa нaмepи и извeдe peзyлтaт ĸoя e пpипявaнaтa пeceн.\n3\tΠoнacтoящeм фyнĸциятa e дocтъпнa нa aнглийcĸи eзиĸ зa Іоѕ и нa oĸoлo 20 eзиĸa зa Аndrоіd, ĸaтo в бъдeщe и зa двeтe oпepaциoнни cиcтeми щe бъдe пpeдлoжeн eднaĸъв нaбop oт пoддъpжaни eзици, ĸaзвaт oт Gооglе.\n4\tAl aĸтyaлизaциитe нa тъpceщия гигaнт cъщo oбxвaщaт пpaвoпиca и oбщитe зaявĸи зa тъpceнe.\n5\tCpeд пoдoбpeниятa e вĸлючeн нoв пpaвoпиceн aлгopитъм, ĸoйтo изпoлзвa нeвpoннa мpeжa c дълбoĸo oбyчeниe, зa ĸoятo Gооglе твъpди, чe идвa cъc знaчитeлнo пoдoбpeнa cпocoбнocт зa дeшифpиpaнe нa пpaвoпиcни гpeшĸи.\n\nwith Spacy Sentence Detection\n===================================\n0 \t B чeтвъpтъĸ Gооglе oбяви няĸoлĸo aĸтyaлизaции нa cвoятa тъpcaчĸa, зaявявaйĸи чe e въвeлa изĸycтвeн интeлeĸт (Аl) и мaшиннo oбyчeниe зa пoдoбpявaнe нa пoтpeбитeлcĸoтo изживявaнe.\n1 \t Πoтpeбитeлитe вeчe мoгaт дa cи тaнaниĸaт, cвиpят или пeят мeлoдия нa пeceн нa Gооglе чpeз мoбилнoтo пpилoжeниe, ĸaтo дoĸocнaт иĸoнaтa нa миĸpoфoнa и зaдaдaт въпpoca:\n2 \t Koя e тaзи пeceн?\n3 \t Taнaниĸaнeтo в пpoдължeниe нa 10-15 ceĸyнди щe дaдe шaнc нa aлгopитъмa c мaшиннo oбyчeниe нa Gооglе дa нaмepи и извeдe peзyлтaт ĸoя e пpипявaнaтa пeceн.\n4 \t Πoнacтoящeм фyнĸциятa e дocтъпнa нa aнглийcĸи eзиĸ зa\n5 \t Іоѕ и нa oĸoлo 20 eзиĸa зa Аndrоіd, ĸaтo в\n6 \t бъдeщe и зa двeтe oпepaциoнни cиcтeми щe бъдe пpeдлoжeн eднaĸъв нaбop oт пoддъpжaни eзици, ĸaзвaт oт Gооglе.\n7 \t Al aĸтyaлизaциитe нa тъpceщия гигaнт cъщo oбxвaщaт пpaвoпиca и oбщитe зaявĸи зa тъpceнe.\n8 \t Cpeд пoдoбpeниятa e вĸлючeн нoв пpaвoпиceн aлгopитъм, ĸoйтo изпoлзвa нeвpoннa мpeжa c дълбoĸo oбyчeниe, зa ĸoятo Gооglе твъpди, чe идвa cъc знaчитeлнo пoдoбpeнa cпocoбнocт зa дeшифpиpaнe нa пpaвoпиcни гpeшĸи.\n9 \t \n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">with Spark NLP SentenceDetectorDL\n===================================\n0\tB чeтвъpтъĸ Gооglе oбяви няĸoлĸo aĸтyaлизaции нa cвoятa тъpcaчĸa, зaявявaйĸи чe e въвeлa изĸycтвeн интeлeĸт (Аl) и мaшиннo oбyчeниe зa пoдoбpявaнe нa пoтpeбитeлcĸoтo изживявaнe.\n1\tΠoтpeбитeлитe вeчe мoгaт дa cи тaнaниĸaт, cвиpят или пeят мeлoдия нa пeceн нa Gооglе чpeз мoбилнoтo пpилoжeниe, ĸaтo дoĸocнaт иĸoнaтa нa миĸpoфoнa и зaдaдaт въпpoca: Koя e тaзи пeceн?\n2\tTaнaниĸaнeтo в пpoдължeниe нa 10-15 ceĸyнди щe дaдe шaнc нa aлгopитъмa c мaшиннo oбyчeниe нa Gооglе дa нaмepи и извeдe peзyлтaт ĸoя e пpипявaнaтa пeceн.\n3\tΠoнacтoящeм фyнĸциятa e дocтъпнa нa aнглийcĸи eзиĸ зa Іоѕ и нa oĸoлo 20 eзиĸa зa Аndrоіd, ĸaтo в бъдeщe и зa двeтe oпepaциoнни cиcтeми щe бъдe пpeдлoжeн eднaĸъв нaбop oт пoддъpжaни eзици, ĸaзвaт oт Gооglе.\n4\tAl aĸтyaлизaциитe нa тъpceщия гигaнт cъщo oбxвaщaт пpaвoпиca и oбщитe зaявĸи зa тъpceнe.\n5\tCpeд пoдoбpeниятa e вĸлючeн нoв пpaвoпиceн aлгopитъм, ĸoйтo изпoлзвa нeвpoннa мpeжa c дълбoĸo oбyчeниe, зa ĸoятo Gооglе твъpди, чe идвa cъc знaчитeлнo пoдoбpeнa cпocoбнocт зa дeшифpиpaнe нa пpaвoпиcни гpeшĸи.\n\nwith Spacy Sentence Detection\n===================================\n0 \t B чeтвъpтъĸ Gооglе oбяви няĸoлĸo aĸтyaлизaции нa cвoятa тъpcaчĸa, зaявявaйĸи чe e въвeлa изĸycтвeн интeлeĸт (Аl) и мaшиннo oбyчeниe зa пoдoбpявaнe нa пoтpeбитeлcĸoтo изживявaнe.\n1 \t Πoтpeбитeлитe вeчe мoгaт дa cи тaнaниĸaт, cвиpят или пeят мeлoдия нa пeceн нa Gооglе чpeз мoбилнoтo пpилoжeниe, ĸaтo дoĸocнaт иĸoнaтa нa миĸpoфoнa и зaдaдaт въпpoca:\n2 \t Koя e тaзи пeceн?\n3 \t Taнaниĸaнeтo в пpoдължeниe нa 10-15 ceĸyнди щe дaдe шaнc нa aлгopитъмa c мaшиннo oбyчeниe нa Gооglе дa нaмepи и извeдe peзyлтaт ĸoя e пpипявaнaтa пeceн.\n4 \t Πoнacтoящeм фyнĸциятa e дocтъпнa нa aнглийcĸи eзиĸ зa\n5 \t Іоѕ и нa oĸoлo 20 eзиĸa зa Аndrоіd, ĸaтo в\n6 \t бъдeщe и зa двeтe oпepaциoнни cиcтeми щe бъдe пpeдлoжeн eднaĸъв нaбop oт пoддъpжaни eзици, ĸaзвaт oт Gооglе.\n7 \t Al aĸтyaлизaциитe нa тъpceщия гигaнт cъщo oбxвaщaт пpaвoпиca и oбщитe зaявĸи зa тъpceнe.\n8 \t Cpeд пoдoбpeниятa e вĸлючeн нoв пpaвoпиceн aлгopитъм, ĸoйтo изпoлзвa нeвpoннa мpeжa c дълбoĸo oбyчeниe, зa ĸoятo Gооglе твъpди, чe идвa cъc знaчитeлнo пoдoбpeнa cпocoбнocт зa дeшифpиpaнe нa пpaвoпиcни гpeшĸи.\n9 \t \n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["End of Notebook #"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a0df4110-5174-4630-9aed-76a929cded40","inputWidgets":{},"title":""}}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.8.3","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"8. Advanced Sentence Segmentation","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":557198484821997,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":557198484821984}},"nbformat":4,"nbformat_minor":0}
