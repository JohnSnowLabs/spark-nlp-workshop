{"cells":[{"cell_type":"markdown","source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"db8c4840-cfa1-4388-866b-a2c869f60a73","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["# 5. Contextual Spell Checking and Correction"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4f4a4d6a-60da-4759-ac4e-96c5bb1af002","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<H1> Noisy Channel Model Spell Checker - Introduction </H1>\n\nblogpost : https://medium.com/spark-nlp/applying-context-aware-spell-checking-in-spark-nlp-3c29c46963bc\n\n<div>\n<p><br/>\nThe idea for this annotator is to have a flexible, configurable and \"re-usable by parts\" model.<br/>\nFlexibility is the ability to accommodate different use cases for spell checking like OCR text, keyboard-input text, ASR text, and general spelling problems due to orthographic errors.<br/>\nWe say this is a configurable annotator, as you can adapt it yourself to different use cases avoiding re-training as much as possible.<br/>\n</p>\n</div>\n\n\n<b> Spell Checking at three levels: </b>\nThe final ranking of a correction sequence is affected by three things, \n\n\n1. Different correction candidates for each word - __word level__.\n2. The surrounding text of each word, i.e. it's context - __sentence level__.\n3. The relative cost of different correction candidates according to the edit operations at the character level it requires - __subword level__."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8fa0d257-1bd4-4b80-aec2-76c91048458d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import sparknlp\nfrom sparknlp.base import *\nfrom sparknlp.annotator import *\n\nprint(\"Spark NLP version\", sparknlp.version())\n\nprint(\"Apache Spark version:\", spark.version)\n\nspark"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c032f1aa-6782-4455-903c-fe060c0ca81d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Spark NLP version 4.2.4\nApache Spark version: 3.1.2\nOut[32]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Spark NLP version 4.2.4\nApache Spark version: 3.1.2\nOut[32]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=7956323724731612#setting/sparkui/0616-152819-zhyjt0vc/driver-817352295916841936\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.139.64.5:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=7956323724731612#setting/sparkui/0616-152819-zhyjt0vc/driver-817352295916841936\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.139.64.5:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        "]}}],"execution_count":0},{"cell_type":"markdown","source":["### Initial Setup\nAs it's usual in Spark-NLP let's start with building a pipeline; a _spell correction pipeline_. We will use a pretrained model from our library."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fbdc7c15-c660-4355-94e4-66032433077c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from sparknlp.annotator import *\nfrom sparknlp.common import *\nfrom sparknlp.base import *\n\nfrom IPython.utils.text import columnize"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1acd3053-6b7a-4f8f-91e3-9f70f698e8bc","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["documentAssembler = DocumentAssembler()\\\n    .setInputCol(\"text\")\\\n    .setOutputCol(\"document\")\n\ntokenizer = RecursiveTokenizer()\\\n    .setInputCols([\"document\"])\\\n    .setOutputCol(\"token\")\\\n    .setPrefixes([\"\\\"\", \"(\", \"[\", \"\\n\"])\\\n    .setSuffixes([\".\", \",\", \"?\", \")\",\"!\", \"'s\"])\n\nspellModel = ContextSpellCheckerModel\\\n    .pretrained('spellcheck_dl')\\\n    .setInputCols(\"token\")\\\n    .setOutputCol(\"checked\")\\\n    .setErrorThreshold(4.0)\\\n    .setTradeoff(6.0)\n\nfinisher = Finisher()\\\n    .setInputCols(\"checked\")\n\npipeline = Pipeline(\n    stages = [\n    documentAssembler,\n    tokenizer,\n    spellModel,\n    finisher\n  ])\n\nempty_ds = spark.createDataFrame([[\"\"]]).toDF(\"text\")\nlp = LightPipeline(pipeline.fit(empty_ds))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"30a57ca6-2b4b-4fab-bca5-a158dd020f54","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">spellcheck_dl download started this may take some time.\nApproximate size to download 95.1 MB\n\r[ | ]\r[OK!]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">spellcheck_dl download started this may take some time.\nApproximate size to download 95.1 MB\n\r[ | ]\r[OK!]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Ok!, at this point we have our spell checking pipeline as expected. Let's see what we can do with it,"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"de4de86c-6631-4a12-88fa-8bd5c9e3994f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["lp.annotate(\"Plaese alliow me tao introdduce myhelf, I am a man of waelth und tiaste\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9589ddf1-6db2-4867-891c-0be893d15678","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[35]: {&#39;checked&#39;: [&#39;Phase&#39;,\n  &#39;allow&#39;,\n  &#39;me&#39;,\n  &#39;to&#39;,\n  &#39;introduce&#39;,\n  &#39;myself&#39;,\n  &#39;,&#39;,\n  &#39;I&#39;,\n  &#39;am&#39;,\n  &#39;a&#39;,\n  &#39;man&#39;,\n  &#39;of&#39;,\n  &#39;warmth&#39;,\n  &#39;and&#39;,\n  &#39;taste&#39;]}</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[35]: {&#39;checked&#39;: [&#39;Phase&#39;,\n  &#39;allow&#39;,\n  &#39;me&#39;,\n  &#39;to&#39;,\n  &#39;introduce&#39;,\n  &#39;myself&#39;,\n  &#39;,&#39;,\n  &#39;I&#39;,\n  &#39;am&#39;,\n  &#39;a&#39;,\n  &#39;man&#39;,\n  &#39;of&#39;,\n  &#39;warmth&#39;,\n  &#39;and&#39;,\n  &#39;taste&#39;]}</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Word Level Corrections\nContinuing with our pretrained model, let's try to see how corrections work at the word level. Each Context Spell Checker model that you can find in Spark-NLP library comes with two sources for word candidates: \n+ a general vocabulary that is built during training(and remains unmutable during the life of the model), and\n+ special classes for dealing with special types of words like numbers or dates. These are dynamic, and you can modify them so they adjust better to your data.\n\nThe general vocabulary is learned during training, and cannot be modified, however, the special classes can be updated after training has happened on a pre-trained model.\nThis means you can modify how existing classes produce corrections, but not the number or type of the classes.\nLet's see how we can accomplish this."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"802aa3c8-9d48-44eb-b87b-73404a82086a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# First let's start with a loaded model, and check which classes it has been trained with\nspellModel.getWordClasses()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f5ea73bb-b9f4-455a-bc8e-30d365ff18c9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[36]: [&#39;(_DATE_,RegexParser)&#39;,\n &#39;(_LOC_,VocabParser)&#39;,\n &#39;(_NAME_,VocabParser)&#39;,\n &#39;(_NUM_,RegexParser)&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[36]: [&#39;(_DATE_,RegexParser)&#39;,\n &#39;(_LOC_,VocabParser)&#39;,\n &#39;(_NAME_,VocabParser)&#39;,\n &#39;(_NUM_,RegexParser)&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["We have five classes, of two different types: some are vocabulary based and others are regex based,\n+ __Vocabulary based classes__ can propose correction candidates from the provided vocabulary, for example a dictionary of names.\n+ __Regex classes__ are defined by a regular expression, and they can be used to generate correction candidates for things like numbers. Internally, the Spell Checker will enumerate your regular expression and build a fast automaton, not only for recognizing the word(number in this example) as valid and preserve it, but also for generating a correction candidate.\nThus the regex should be a finite regex(it must define a finite regular language).\n\nNow suppose that you have a new friend from Poland whose name is 'Jowita', let's see how the pretrained Spell Checker does with this name."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c375e83e-91f0-4078-8584-f405de0e297e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["beautify = lambda annotations: [columnize(sent['checked']) for sent in annotations]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"66847f3b-694f-4911-82af-9c35215d8845","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Foreign name without errors\nsample = 'We are going to meet Jowita in the city hall.'\nbeautify([lp.annotate(sample)])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"23292123-ad60-4a3f-bb5d-acfbda228874","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[38]: [&#39;We  are  going  to  meet  Jowita  in  the  city  hall  .\\n&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[38]: [&#39;We  are  going  to  meet  Jowita  in  the  city  hall  .\\n&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Well, the result is not very good, that's because the Spell Checker has been trained mainly with American English texts. At least, the surrounding words are helping to obtain a correction that is a name. We can do better, let's see how.\n\n## Updating a predefined word class\n\n### Vocabulary Classes\n\nIn order for the Spell Checker to be able to preserve words, like a foreign name, we have the option to update existing classes so they can cover new words."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3572480a-8a32-44c3-9dd0-2def36cf9a45","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# add some more, in case we need them\nspellModel.updateVocabClass('_NAME_', ['Monika', 'Agnieszka', 'Inga', 'Jowita', 'Melania'], True)\n\n# Let's see what we get now\nsample = 'We are going to meet Jowita at the city hall.'\nbeautify([lp.annotate(sample)])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"da157182-f1c9-4646-adc6-6c08ef75574f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[39]: [&#39;We  are  going  to  meet  Jowita  at  the  city  hall  .\\n&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[39]: [&#39;We  are  going  to  meet  Jowita  at  the  city  hall  .\\n&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Much better, right? Now suppose that we want to be able to not only preserve the word, but also to propose meaningful corrections to the name of our foreign friend."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4da2bba2-5a69-4ae2-949e-6bbf03bdccda","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Foreign name with an error\nsample = 'We are going to meet Jovita in the city hall.'\nbeautify([lp.annotate(sample)])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4014592f-a2e3-4f51-8e4b-0ef9e5ba19bf","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[40]: [&#39;We  are  going  to  meet  Jowita  in  the  city  hall  .\\n&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[40]: [&#39;We  are  going  to  meet  Jowita  in  the  city  hall  .\\n&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Here we were able to add the new word to the class and propose corrections for it, but also, the new word has been treated as a name, that meaning that the model used information about the typical context for names in order to produce the best correction."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c36d377a-e366-4d28-8ba3-c00ff0b0b3e6","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Regex Classes\nWe can do something similar for classes defined by regex. We can add a regex, to for example deal with a special format for dates, that will not only preserve the date with the special format, but also be able to correct it."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8a4440c3-a352-4cec-bcff-43472ddd9e30","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# Date with custom format\nsample = 'We are going to meet her in the city hall on february-3.'\nbeautify([lp.annotate(sample)])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"caba9324-deb8-437d-a1b3-07bceea518a1","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[41]: [&#39;We  are  going  to  meet  her  in  the  city  hall  on  february-3  .\\n&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[41]: [&#39;We  are  going  to  meet  her  in  the  city  hall  on  february-3  .\\n&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# this is a sample regex, for simplicity not covering all months\nspellModel.updateRegexClass('_DATE_', '(january|february|march)-[0-31]')\nbeautify([lp.annotate(sample)])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"992eff59-b785-49ce-b40a-bcaafd7ddde6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[42]: [&#39;We  are  going  to  meet  her  in  the  city  hall  on  february-3  .\\n&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[42]: [&#39;We  are  going  to  meet  her  in  the  city  hall  on  february-3  .\\n&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Now our date wasn't destroyed!"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e79a5a48-0f9b-491e-b819-986e114b623f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# now check that it produces good corrections to the date\nsample = 'We are going to meet her in the city hall on mebbruary-3.'\nbeautify([lp.annotate(sample)])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e66dd5a9-f0b8-4c31-9eff-dd3718938eeb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[43]: [&#39;We  are  going  to  meet  her  in  the  city  hall  on  mebbruary-3  .\\n&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[43]: [&#39;We  are  going  to  meet  her  in  the  city  hall  on  mebbruary-3  .\\n&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["And the model produces good corrections for the special regex class. Remember that each regex that you enter to the model must be finite. In all these examples the new definitions for our classes didn't prevent the model to continue using the context to produce corrections. Let's see why being able to use the context is important.\n### Sentence Level Corrections\nThe Spell Checker can leverage the context of words for ranking different correction sequences. Let's take a look at some examples,"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"22bbfd7a-420a-4d3b-908f-1ded50ecebca","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# check for the different occurrences of the word \"siter\"\nexample1 = [\"I will call my siter.\",\\\n            \"Due to bad weather, we had to move to a different siter.\",\\\n            \"We travelled to three siter in the summer.\"]\nbeautify(lp.annotate(example1))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5f8df8b5-4038-4252-97ec-880f3105070c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[44]: [&#39;I  will  call  my  site  .\\n&#39;,\n &#39;Due  to  bad  weather  ,  we  had  to  move  to  a  different  site  .\\n&#39;,\n &#39;We  travelled  to  three  sites  in  the  summer  .\\n&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[44]: [&#39;I  will  call  my  site  .\\n&#39;,\n &#39;Due  to  bad  weather  ,  we  had  to  move  to  a  different  site  .\\n&#39;,\n &#39;We  travelled  to  three  sites  in  the  summer  .\\n&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# check for the different occurrences of the word \"ueather\"\nexample2 = [\"During the summer we have the best ueather.\",\\\n            \"I have a black ueather jacket, so nice.\"]\nbeautify(lp.annotate(example2))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f13d3114-f7e7-4b07-82a1-37d73e884246","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[45]: [&#39;During  the  summer  we  have  the  best  weather  .\\n&#39;,\n &#39;I  have  a  black  leather  jacket  ,  so  nice  .\\n&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[45]: [&#39;During  the  summer  we  have  the  best  weather  .\\n&#39;,\n &#39;I  have  a  black  leather  jacket  ,  so  nice  .\\n&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Notice that in the first example, 'siter' is indeed a valid English word, <br/> https://www.merriam-webster.com/dictionary/siter <br/>\nThe only way to customize how the use of context is performed is to train the language model by training a Spell Checker from scratch. If you want to be able to train your custom language model, please refer to the Training notebook.\nNow we've learned how the context can help to pick the best possible correction, and why it is important to be able to leverage the context even when the other parts of the Spell Checker were updated."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"61af13b9-6323-44d3-a31d-69682d90f1cd","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["### Subword level corrections\nAnother fine tunning that our Spell Checker accepts is to assign different costs to different edit operations that are necessary to transform a word into a correction candidate. \nSo, why is this important? Errors can come from different sources,\n+ Homophones are words that sound similar, but are written differently and have different meaning. Some examples, {there, their, they're}, {see, sea}, {to, too, two}. You will typically see these errors in text obtained by Automatic Speech Recognition(ASR).\n+ Characters can also be confused because of looking similar. So a 0(zero) can be confused with a O(capital o), or a 1(number one) with an l(lowercase l). These errors typically come from OCR.\n+ Input device related, sometimes keyboards cause certain patterns to be more likely than others due to letter locations, for example in a QWERTY keyboard.\n+ Last but not least, ortographic errors, related to the writter making mistakes. Forgetting a double consonant, or using it in the wrong place, interchanging letters(i.e., 'becuase' for 'because'), and many others.\n\nThe goal is to continue using all the other features of the model and still be able to adapt the model to handle each of these cases in the best possible way. Let's see how to accomplish this."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6f540ec0-3ea5-4215-b6db-1d4dada347e6","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# sending or lending ?\nsample = 'I will be 1ending him my car'\nlp.annotate(sample)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e6569840-af13-4e73-806f-c3a5f37d788b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[46]: {&#39;checked&#39;: [&#39;I&#39;, &#39;will&#39;, &#39;be&#39;, &#39;lending&#39;, &#39;him&#39;, &#39;my&#39;, &#39;car&#39;]}</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[46]: {&#39;checked&#39;: [&#39;I&#39;, &#39;will&#39;, &#39;be&#39;, &#39;lending&#39;, &#39;him&#39;, &#39;my&#39;, &#39;car&#39;]}</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# let's make the replacement of an '1' for an 'l' cheaper\nweights = {'1': {'l': .01}}\nspellModel.setWeights(weights)\nlp.annotate(sample)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8e45d2b6-ae90-4cca-901f-a590a6601621","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[47]: {&#39;checked&#39;: [&#39;I&#39;, &#39;will&#39;, &#39;be&#39;, &#39;lending&#39;, &#39;him&#39;, &#39;my&#39;, &#39;car&#39;]}</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[47]: {&#39;checked&#39;: [&#39;I&#39;, &#39;will&#39;, &#39;be&#39;, &#39;lending&#39;, &#39;him&#39;, &#39;my&#39;, &#39;car&#39;]}</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Assembling this matrix by hand could be a daunting challenge. There is one script in Python that can do this for you.\nThis is something to be soon included like an option during training for the Context Spell Checker. Stay tuned on new releases!"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"154c5d80-6574-44f4-b77c-7c8bdfed778d","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["## Advanced - the mysterious tradeoff parameter \nThere's a clear tension between two forces here,\n+ The context information: by which the model wants to change words based on the surrounding words.\n+ The word information: by which the model wants to preserve as much an input word as possible to avoid destroying the input.\n\nChanging words that are in the vocabulary for others that seem more suitable according to the context is one of the most challenging tasks in spell correction. This is because you run into the risk of destroying existing 'good' words.\nThe models that you will find in the Spark-NLP library have already been configured in a way that balances these two forces and produces good results in most of the situations. But your dataset can be different from the one used to train the model.\nSo we encourage the user to play a bit with the hyperparameters, and for you to have an idea on how it can be modified, we're going to see the following example,"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1b7ba6c2-e3df-420d-9ec2-0c3b99d64a71","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["sample = 'have you been two the falls?'\nbeautify([lp.annotate(sample)])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c55c64a4-75c6-495e-af33-b61bb2f558ec","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[48]: [&#39;have  you  been  to  the  falls  ?\\n&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[48]: [&#39;have  you  been  to  the  falls  ?\\n&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Here 'two' is clearly wrong, probably a typo, and the model should be able to choose the right correction candidate according to the context. <br/>\nEvery path is scored with a cost, and the higher the cost the less chances for the path being chosen as the final answer.<br/>\nIn order for the model to rely more on the context and less on word information, we have the setTradeoff() method. You can think of the tradeoff as how much a single edition(insert, delete, etc) operation affects the influence of a word when competing inside a path in the graph.<br/>\nSo the lower the tradeoff, the less we care about the edit operations in the word, and the more we care about the word fitting properly into its context. The tradeoff parameter typically ranges between 5 and 25. <br/>\nLet's see what happens when we relax how much the model cares about individual words in our example,"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"254ae828-10da-48f8-ac95-c7e6b0a3a420","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["spellModel.getTradeoff()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7a94d343-afa8-4a9d-b36a-ba2221c6b834","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[49]: 6.0</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[49]: 6.0</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# let's decrease the influence of word-level errors\n# TODO a nicer way of doing this other than re-creating the pipeline?\nspellModel.setTradeoff(2.0)\n\npipeline = Pipeline(\n    stages = [\n    documentAssembler,\n    tokenizer,\n    spellModel,\n    finisher\n  ])\n\nempty_ds = spark.createDataFrame([[\"\"]]).toDF(\"text\")\nlp = LightPipeline(pipeline.fit(empty_ds))\n\nbeautify([lp.annotate(sample)])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bdf520f3-d8b7-4860-bc80-da8ae3976efa","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[50]: [&#39;have  you  been  to  the  falls  ?\\n&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[50]: [&#39;have  you  been  to  the  falls  ?\\n&#39;]</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Advanced - performance"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b4c60ab7-641b-456d-b64e-cb9c5da6fdd2","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["The discussion about performance revolves around _error detection_. The more errors the model detects the more populated is the candidate diagram we showed above[TODO add diagram or convert this into blogpost], and the more alternative paths need to be evaluated. </br>\nBasically the error detection stage of the model can decide whether a word needs a correction or not; with two reasons for a word to be considered as incorrect, \n+ The word is OOV: the word is out of the vocabulary.\n+ The context: the word doesn't fit well within its neighbouring words. \nThe only parameter that we can control at this point is the second one, and we do so with the setErrorThreshold() method that contains a max perplexity above which the word will be considered suspicious and a good candidate for being corrected.</br>\nThe parameter that comes with the pretrained model has been set so you can get both a decent performance and accuracy. For reference, this is how the F-score, and time varies in a sample dataset for different values of the errorThreshold,\n\n\n|fscore |totaltime|threshold|\n|-------|---------|---------|\n|52.69  |405s | 8f|\n|52.43  |357s |10f|\n|52.25  |279s |12f|\n|52.14  |234s |14f|\n\nYou can trade some minor points in accuracy for a nice speedup."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"72942a15-6706-49c0-b6a3-d89a6a7b3917","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["def sparknlp_spell_check(text):\n\n  return beautify([lp.annotate(text)])[0].rstrip()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c47cdc37-6ff6-4298-b8d3-4c7f0778fb08","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["sparknlp_spell_check('I will go to Philadelhia tomorrow')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2db5f0fd-12ea-42a4-9c35-b39e84199113","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[52]: &#39;I  will  go  to  Philadelphia  tomorrow&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[52]: &#39;I  will  go  to  Philadelphia  tomorrow&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["sparknlp_spell_check('I will go to Philadhelpia tomorrow')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"41064eb9-9c0e-4e4c-8531-51b04b4780de","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[53]: &#39;I  will  go  to  Philadelphia  tomorrow&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[53]: &#39;I  will  go  to  Philadelphia  tomorrow&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["sparknlp_spell_check('I will go to Piladelphia tomorrow')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ad6811de-e2c5-40e2-95cc-891ce9659b8a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[54]: &#39;I  will  go  to  Philadelphia  tomorrow&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[54]: &#39;I  will  go  to  Philadelphia  tomorrow&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["sparknlp_spell_check('I will go to Philadedlhia tomorrow')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f1b8acb1-bdb8-4094-bb22-fa6eaea3bae2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[55]: &#39;I  will  go  to  Philadelphia  tomorrow&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[55]: &#39;I  will  go  to  Philadelphia  tomorrow&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["sparknlp_spell_check('I will go to Phieladelphia tomorrow')"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e1087e39-e075-455d-b198-91a9e60dfce8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[56]: &#39;I  will  go  to  Philadelphia  tomorrow&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[56]: &#39;I  will  go  to  Philadelphia  tomorrow&#39;</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## ContextSpellCheckerApproach\n\nTrains a deep-learning based Noisy Channel Model Spell Algorithm.\n\nCorrection candidates are extracted combining context information and word information.\n\n1.   Different correction candidates for each word   **word level**\n2.   The surrounding text of each word, i.e. itâ€™s context  **sentence level**.\n3.   The relative cost of different correction candidates according to the edit operations at the character level it requires  **subword level**."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"11bd947a-249d-4d02-a1d3-e221ad005a77","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# For this example, we will use the first Sherlock Holmes book as the training dataset.\n\ndocumentAssembler = DocumentAssembler() \\\n    .setInputCol(\"text\") \\\n    .setOutputCol(\"document\")\n\ntokenizer = Tokenizer()\\\n    .setInputCols(\"document\") \\\n    .setOutputCol(\"token\")\n\nspellChecker = ContextSpellCheckerApproach() \\\n    .setInputCols(\"token\") \\\n    .setOutputCol(\"corrected\") \\\n    .setWordMaxDistance(3) \\\n    .setBatchSize(24) \\\n    .setEpochs(8) \\\n    .setLanguageModelClasses(1650)  # dependant on vocabulary size\n    # .addVocabClass(\"_NAME_\", names) # Extra classes for correction could be added like this\n\npipeline = Pipeline().setStages([\n    documentAssembler,\n    tokenizer,\n    spellChecker\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a1afce5f-abb6-48c7-89d2-911f7f249350","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/holmes.txt\n  \ndbutils.fs.cp(\"file:/databricks/driver/holmes.txt\", \"dbfs:/\") \n  "],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"45877686-e2ff-48ae-a6c5-18514f13af8b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[58]: True</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[58]: True</div>"]}}],"execution_count":0},{"cell_type":"code","source":["path = \"/holmes.txt\"\n\ndataset = spark.read.text(path).toDF(\"text\")\n\ndataset.show(truncate=100)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8c729f61-ac70-4cd3-b041-579d461cc2b3","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+----------------------------------------------------------------------------------------------------+\n|                                                                                                text|\n+----------------------------------------------------------------------------------------------------+\n|THE ADVENTURES OF SHERLOCK HOLMESArthur Conan Doyle Table of contents A Scandal in Bohemia The Re...|\n+----------------------------------------------------------------------------------------------------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------------------------------------------------------------------------------------+\n                                                                                                text|\n+----------------------------------------------------------------------------------------------------+\nTHE ADVENTURES OF SHERLOCK HOLMESArthur Conan Doyle Table of contents A Scandal in Bohemia The Re...|\n+----------------------------------------------------------------------------------------------------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["pipelineModel = pipeline.fit(dataset)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"46a80c2d-5079-4094-be3a-163b32d71ccf","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["lp = LightPipeline(pipelineModel)\nresult = lp.annotate(\"Plaese alliow me tao introdduce myhelf, I am a man of waelth und tiaste\")\nresult[\"corrected\"]"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e016fd81-1609-4d1c-a71c-e4b6f5f006d5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[61]: [&#39;Please&#39;,\n &#39;allow&#39;,\n &#39;me&#39;,\n &#39;to&#39;,\n &#39;introduce&#39;,\n &#39;myself&#39;,\n &#39;,&#39;,\n &#39;I&#39;,\n &#39;am&#39;,\n &#39;a&#39;,\n &#39;man&#39;,\n &#39;of&#39;,\n &#39;wealth&#39;,\n &#39;and&#39;,\n &#39;taste&#39;]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[61]: [&#39;Please&#39;,\n &#39;allow&#39;,\n &#39;me&#39;,\n &#39;to&#39;,\n &#39;introduce&#39;,\n &#39;myself&#39;,\n &#39;,&#39;,\n &#39;I&#39;,\n &#39;am&#39;,\n &#39;a&#39;,\n &#39;man&#39;,\n &#39;of&#39;,\n &#39;wealth&#39;,\n &#39;and&#39;,\n &#39;taste&#39;]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["import pandas as pd\n\npd.DataFrame(zip(result[\"token\"],result[\"corrected\"]),columns=[\"orginal\",\"corrected\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2c4de780-921e-4dc8-9afa-32fba8f6ea45","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[62]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[62]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>orginal</th>\n      <th>corrected</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Plaese</td>\n      <td>Please</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>alliow</td>\n      <td>allow</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>me</td>\n      <td>me</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>tao</td>\n      <td>to</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>introdduce</td>\n      <td>introduce</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>myhelf</td>\n      <td>myself</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>,</td>\n      <td>,</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>I</td>\n      <td>I</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>am</td>\n      <td>am</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>a</td>\n      <td>a</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>man</td>\n      <td>man</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>of</td>\n      <td>of</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>waelth</td>\n      <td>wealth</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>und</td>\n      <td>and</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>tiaste</td>\n      <td>taste</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>orginal</th>\n      <th>corrected</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Plaese</td>\n      <td>Please</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>alliow</td>\n      <td>allow</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>me</td>\n      <td>me</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>tao</td>\n      <td>to</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>introdduce</td>\n      <td>introduce</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>myhelf</td>\n      <td>myself</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>,</td>\n      <td>,</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>I</td>\n      <td>I</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>am</td>\n      <td>am</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>a</td>\n      <td>a</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>man</td>\n      <td>man</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>of</td>\n      <td>of</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>waelth</td>\n      <td>wealth</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>und</td>\n      <td>and</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>tiaste</td>\n      <td>taste</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### End of Notebook # 5"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9cadf7a0-9b65-4537-881c-66c5164f5b4b","inputWidgets":{},"title":""}}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.8.3","nbconvert_exporter":"python","file_extension":".py"},"application/vnd.databricks.v1+notebook":{"notebookName":"5. Contextual Spell Checking and Correction","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":557198484821703}},"nbformat":4,"nbformat_minor":0}
