{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c469c9ee-3832-4f99-96ee-679bf24df825",
     "showTitle": false,
     "title": ""
    },
    "id": "QJOsfqRGI_3n"
   },
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a104cee-cb11-4845-86f6-17fbe57a4840",
     "showTitle": false,
     "title": ""
    },
    "id": "sI93AChbI_3v"
   },
   "source": [
    "<H1>Context Based Clinical Spell Checker</H1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf19bd8-d6fa-4bd7-ac3a-3db6c7a82c5c",
     "showTitle": false,
     "title": ""
    },
    "id": "wdUI6jnVI_3w",
    "outputId": "69563866-9a6a-402b-bf03-20b89f5370d0"
   },
   "outputs": [],
   "source": [
    "from johnsnowlabs import nlp, medical\n",
    "\n",
    "# After uploading your license run this to install all licensed Python Wheels and pre-download Jars the Spark Session JVM\n",
    "#nlp.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aec5b99e-ee02-4c2a-af2f-8dd758051ffa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=7956323724731612#setting/sparkui/1112-201629-lg699ln8/driver-5312365348760152798\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.139.64.10:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=7956323724731612#setting/sparkui/1112-201629-lg699ln8/driver-5312365348760152798\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.139.64.10:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql as SQL\n",
    "\n",
    "import os\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "pd.set_option('max_colwidth', 100)\n",
    "pd.set_option('display.max_columns', 100)  \n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59bcdce5-ad28-4093-92ba-baaa55562973",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## spellcheck_clinical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d260ffe0-e641-4035-a530-0f100eae8e12",
     "showTitle": false,
     "title": ""
    },
    "id": "-KEqAocbI_3y",
    "outputId": "c20fae82-4fe1-4624-ebc3-2e1672a2bb28"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spellcheck_clinical download started this may take some time.\nApproximate size to download 134.7 MB\n\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[ \\ ]\r[ | ]\r[ / ]\r[ — ]\r[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = nlp.DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = nlp.RecursiveTokenizer()\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"token\")\\\n",
    "    .setPrefixes([\"\\\"\", \"(\", \"[\", \"\\n\"])\\\n",
    "    .setSuffixes([\".\", \",\", \"?\", \")\",\"!\", \"'s\"])\n",
    "\n",
    "spellModel = nlp.ContextSpellCheckerModel.pretrained('spellcheck_clinical', 'en', 'clinical/models')\\\n",
    "    .setInputCols(\"token\")\\\n",
    "    .setOutputCol(\"checked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "833444d3-5a30-47dc-a00d-19061e93f585",
     "showTitle": false,
     "title": ""
    },
    "id": "L8pETwfiI_31",
    "outputId": "144fefed-0690-44d5-a8dd-8e3735a228b6"
   },
   "outputs": [],
   "source": [
    "pipeline = nlp.Pipeline(\n",
    "    stages = [\n",
    "    documentAssembler,\n",
    "    tokenizer,\n",
    "    spellModel\n",
    "  ])\n",
    "\n",
    "empty_ds = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "lp = nlp.LightPipeline(pipeline.fit(empty_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8466d0a-cd9c-4829-a383-157fe8562838",
     "showTitle": false,
     "title": ""
    },
    "id": "o-wXCw6mI_33"
   },
   "source": [
    "Ok!, at this point we have our spell checking pipeline as expected. Let's see what we can do with it, see these errors,\n",
    "\n",
    "\n",
    "\n",
    "_She was **treathed** with a five day course of **amoxicilin** for a **resperatory** **truct** infection._\n",
    "\n",
    "_With pain well controlled on **orall** **meditation**, she was discharged to **reihabilitation** **facilitay**._\n",
    "\n",
    "\n",
    "_Her **adominal** examination is soft, nontender, and **nonintended**_\n",
    "\n",
    "_The patient was seen by the **entocrinology** service and she was discharged on 40 units of **unsilin** glargine at night_\n",
    "      \n",
    "_No __cute__ distress_\n",
    "\n",
    "\n",
    "Check that some of the errors are valid English words, only by considering the context the right choice can be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47569d82-239e-43e0-a485-2e685951045c",
     "showTitle": false,
     "title": ""
    },
    "id": "zjHnqUteI_33",
    "outputId": "b8e18c61-c76b-476e-f650-245e9c678ded"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('She', 'She'), ('was', 'was'), ('treathed', 'treated'), ('with', 'with'), ('a', 'a'), ('five', 'five'), ('day', 'day'), ('course', 'course'), ('of', 'of'), ('amoxicilin', 'amoxicillin'), ('for', 'for'), ('a', 'a'), ('resperatory', 'respiratory'), ('truct', 'tract'), ('infection', 'infection'), ('.', '.')]\n[('With', 'With'), ('pain', 'pain'), ('well', 'well'), ('controlled', 'controlled'), ('on', 'on'), ('orall', 'oral'), ('meditation', 'medication'), (',', ','), ('she', 'she'), ('was', 'was'), ('discharged', 'discharged'), ('to', 'to'), ('reihabilitation', 'rehabilitation'), ('facilitay', 'facility'), ('.', '.')]\n[('Her', 'Her'), ('adominal', 'abdominal'), ('examination', 'examination'), ('is', 'is'), ('soft', 'soft'), (',', ','), ('nontender', 'nontender'), (',', ','), ('and', 'and'), ('nonintended', 'nondistended'), ('.', '.')]\n[('The', 'The'), ('patient', 'patient'), ('was', 'was'), ('seen', 'seen'), ('by', 'by'), ('the', 'the'), ('entocrinology', 'endocrinology'), ('service', 'service'), ('and', 'and'), ('she', 'she'), ('was', 'was'), ('discharged', 'discharged'), ('on', 'on'), ('40', '40'), ('units', 'units'), ('of', 'of'), ('unsilin', 'insulin'), ('glargine', 'glargine'), ('at', 'at'), ('night', 'night')]\n[('No', 'No'), ('cute', 'acute'), ('distress', 'distress')]\n"
     ]
    }
   ],
   "source": [
    "example = [\"She was treathed with a five day course of amoxicilin for a resperatory truct infection . \",\n",
    "           \"With pain well controlled on orall meditation, she was discharged to reihabilitation facilitay.\",\n",
    "           \"Her adominal examination is soft, nontender, and nonintended.\",\n",
    "           \"The patient was seen by the entocrinology service and she was discharged on 40 units of unsilin glargine at night\",\n",
    "           \"No cute distress\",\n",
    "          ]\n",
    "\n",
    "for pairs in lp.annotate(example):\n",
    "\n",
    "  print (list(zip(pairs['token'],pairs['checked'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f582fa70-6edd-43b4-aa39-4bdd4538fc17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected tokens:\n\nOut[6]: [('treathed', 'treated'),\n ('amoxicilin', 'amoxicillin'),\n ('resperatory', 'respiratory'),\n ('truct', 'tract'),\n ('orall', 'oral'),\n ('meditation', 'medication'),\n ('reihabilitation', 'rehabilitation'),\n ('facilitay', 'facility'),\n ('adominal', 'abdominal'),\n ('nonintended', 'nondistended'),\n ('entocrinology', 'endocrinology'),\n ('unsilin', 'insulin'),\n ('cute', 'acute')]"
     ]
    }
   ],
   "source": [
    "print(\"Corrected tokens:\\n\")\n",
    "\n",
    "pair_list = [list(zip(pairs['token'],pairs['checked'])) for pairs in lp.annotate(example)]\n",
    "corrected_list = [i for pair in pair_list for i in pair if i[0] != i[1]]\n",
    "corrected_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "27370fb9-3045-4730-946f-17c897c19289",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## spellcheck_drug_norvig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8053347-6022-47db-893a-1d5283ba4515",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spellcheck_drug_norvig download started this may take some time.\nApproximate size to download 4.3 MB\n\r[ | ]\r[ / ]\r[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = nlp.DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = nlp.Tokenizer()\\\n",
    "    .setInputCols(\"document\")\\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "spell = nlp.NorvigSweetingModel.pretrained(\"spellcheck_drug_norvig\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols(\"token\")\\\n",
    "    .setOutputCol(\"corrected_token\")\\\n",
    "\n",
    "pipeline = nlp.Pipeline(\n",
    "    stages = [\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        spell\n",
    "        ])\n",
    "\n",
    "\n",
    "empty_ds = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "lp = nlp.LightPipeline(pipeline.fit(empty_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3220b4a9-658b-40c8-b577-873c3287c5ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('You', 'You'), ('have', 'have'), ('to', 'to'), ('take', 'take'), ('Amrosia', 'Ambrosia'), ('artemisiifoli', 'artemisiifolia'), (',', ','), ('Oactra', 'Odactra'), ('and', 'and'), ('a', 'a'), ('bit', 'bit'), ('of', 'of'), ('Grastk', 'Grastek'), ('and', 'and'), ('lastacaf', 'lastacaft')]\n"
     ]
    }
   ],
   "source": [
    "example = [\"You have to take Amrosia artemisiifoli , Oactra and a bit of Grastk and lastacaf \",\n",
    "          ]\n",
    "\n",
    "for pairs in lp.annotate(example):\n",
    "    print(list(zip(pairs['token'],pairs['corrected_token'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f7fb8e6-880a-4156-853a-05b030b34ff9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected tokens:\n\nOut[9]: [('Amrosia', 'Ambrosia'),\n ('artemisiifoli', 'artemisiifolia'),\n ('Oactra', 'Odactra'),\n ('Grastk', 'Grastek'),\n ('lastacaf', 'lastacaft')]"
     ]
    }
   ],
   "source": [
    "print(\"Corrected tokens:\\n\")\n",
    "\n",
    "pair_list = [list(zip(pairs['token'],pairs['corrected_token'])) for pairs in lp.annotate(example)]\n",
    "corrected_list = [i for pair in pair_list for i in pair if i[0] != i[1]]\n",
    "corrected_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97ac90f2-d3fb-4c3d-a2e5-c5226fc0535a",
     "showTitle": false,
     "title": ""
    },
    "id": "jTs-UhZzI_35"
   },
   "source": [
    "End of Notebook #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a4cf8d4-70aa-43dc-8d12-b91541cd86dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "12.0.Clinical_Context_Spell_Checker",
   "widgets": {}
  },
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
