{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wxZDXLDCXkk_"
   },
   "source": [
    "\n",
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KLqW6FOnEvov"
   },
   "source": [
    "# Train Finance Assertion\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "okhT7AcXxben"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115199,
     "status": "ok",
     "timestamp": 1664816113389,
     "user": {
      "displayName": "Damla",
      "userId": "03285166568766987047"
     },
     "user_tz": -120
    },
    "id": "dmcB5zVBHZO8",
    "outputId": "cd366e47-7f4d-457a-dfe5-3ed5174d4a0c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from johnsnowlabs import *\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "spark = start_spark()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JYBQyxEd0uR0"
   },
   "source": [
    "## Data Prep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AVBmGFcQ03La"
   },
   "outputs": [],
   "source": [
    "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/finance-nlp/data/assertion_df.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8iJF_HCw1Lgh"
   },
   "outputs": [],
   "source": [
    "training_df = pd.read_csv('./assertion_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JREBeTzb8ov-",
    "outputId": "57dc8ff5-0c8f-4710-a68a-cd869ec9c4b3"
   },
   "outputs": [],
   "source": [
    "training_data = spark.createDataFrame(training_df)\n",
    "training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ET8GD3y3-17e",
    "outputId": "6e611d6a-f2d9-421a-c850-8e8bc8b16b07"
   },
   "outputs": [],
   "source": [
    "training_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6xa4jp8Szs0",
    "outputId": "91b737ae-ef4e-4090-b603-793fb8fdc81d"
   },
   "outputs": [],
   "source": [
    "%time training_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxcHD_Q_-_lD",
    "outputId": "b8c5c979-b181-43eb-e03b-49e079895084"
   },
   "outputs": [],
   "source": [
    "(train_data, test_data) = training_data.randomSplit([0.9, 0.1], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(training_data.count()))\n",
    "print(\"Test Dataset Count: \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFN_BuHU84HF",
    "outputId": "9168daec-ed16-4910-a8f0-9fe75c276c3b"
   },
   "outputs": [],
   "source": [
    "train_data.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2WZDqlZA_kmb"
   },
   "source": [
    "## Using Bert Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7qfJh8ap_nI2",
    "outputId": "334d9ddc-15cf-494a-a1fc-20897c3f7c9f"
   },
   "outputs": [],
   "source": [
    "bert_embeddings = nlp.BertEmbeddings.pretrained(\"bert_embeddings_sec_bert_base\", \"en\") \\\n",
    "  .setInputCols(\"document\", \"token\") \\\n",
    "  .setOutputCol(\"embeddings\")\\\n",
    "  .setMaxSentenceLength(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fe0957BT_rcy"
   },
   "outputs": [],
   "source": [
    "document = nlp.DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "chunk = nlp.Doc2Chunk()\\\n",
    "    .setInputCols(\"document\")\\\n",
    "    .setOutputCol(\"doc_chunk\")\\\n",
    "    .setChunkCol(\"target\")\\\n",
    "    .setStartCol(\"start\")\\\n",
    "    .setStartColByTokenIndex(True)\\\n",
    "    .setFailOnMissing(False)\\\n",
    "    .setLowerCase(False)\n",
    "\n",
    "token = nlp.Tokenizer()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LFTO0PlI9-3e"
   },
   "source": [
    "We save the test data in parquet format to use in `AssertionDLApproach()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M9u4c65G9VaC"
   },
   "outputs": [],
   "source": [
    "assertion_pipeline = nlp.Pipeline(\n",
    "    stages = [\n",
    "    document,\n",
    "    chunk,\n",
    "    token,\n",
    "    bert_embeddings])\n",
    "\n",
    "assertion_train_data = assertion_pipeline.fit(training_data).transform(training_data)\n",
    "assertion_test_data = assertion_pipeline.fit(test_data).transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r-UREmtI9Vd3",
    "outputId": "d3a6f2b5-67d1-4446-edc4-b7667a049691"
   },
   "outputs": [],
   "source": [
    "assertion_test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kBaiXx78BTLT"
   },
   "outputs": [],
   "source": [
    "assertion_train_data.write.mode('overwrite').parquet('train_data.parquet')\n",
    "assertion_test_data.write.mode('overwrite').parquet('test_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BtxnrvcA9VlN",
    "outputId": "1a8b0ddb-88eb-4cd0-da4a-5bc533b18a9e"
   },
   "outputs": [],
   "source": [
    "assertion_train_data.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "uTishXbut1MS"
   },
   "source": [
    "## Graph setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "0ShZT8BBo4FY"
   },
   "source": [
    "We will use TFGraphBuilder annotator which can be used to create graphs in the model training pipeline. \n",
    "\n",
    "TFGraphBuilder inspects the data and creates the proper graph if a suitable version of TensorFlow (<= 2.7 ) is available. The graph is stored in the defined folder and loaded by the approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhU0L1OXdaLN"
   },
   "outputs": [],
   "source": [
    "graph_folder= \"./tf_graphs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miNgoTjio0mL"
   },
   "outputs": [],
   "source": [
    "assertion_graph_builder =  finance.TFGraphBuilder()\\\n",
    "    .setModelName(\"assertion_dl\")\\\n",
    "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "    .setLabelColumn(\"label\")\\\n",
    "    .setGraphFolder(graph_folder)\\\n",
    "    .setGraphFile(\"assertion_graph.pb\")\\\n",
    "    .setMaxSequenceLength(1200)\\\n",
    "    .setHiddenUnitsNumber(25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "6D0Ng7nMUjJa"
   },
   "source": [
    "**Setting the Scope Window (Target Area) Dynamically in Assertion Status Detection Models**\n",
    "\n",
    "\n",
    "This parameter allows you to train the Assertion Status Models to focus on specific context windows when resolving the status of a NER chunk. The window is in format `[X,Y]` being `X` the number of tokens to consider on the left of the chunk, and `Y` the max number of tokens to consider on the right. Let’s take a look at what different windows mean:\n",
    "\n",
    "\n",
    "*   By default, the window is `[-1,-1]` which means that the Assertion Status will look at all of the tokens in the sentence/document (up to a maximum of tokens set in `setMaxSentLen()` ).\n",
    "*   `[0,0]` means “don’t pay attention to any token except the ner_chunk”, what basically is not considering any context for the Assertion resolution.\n",
    "*   `[9,15]` is what empirically seems to be the best baseline, meaning that we look up to 9 tokens on the left and 15 on the right of the ner chunk to understand the context and resolve the status.\n",
    "\n",
    "\n",
    "Check this [Scope Window Tuning Assertion Status Detection notebook](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/2.1.Scope_window_tuning_assertion_status_detection.ipynb)  that illustrates the effect of the different windows and how to properly fine-tune your AssertionDLModels to get the best of them.\n",
    "\n",
    "In our case, the best Scope Window is around [10,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQxGbYks91go"
   },
   "outputs": [],
   "source": [
    "scope_window = [50, 50]\n",
    "\n",
    "assertionStatus = finance.AssertionDLApproach()\\\n",
    "    .setLabelCol(\"label\")\\\n",
    "    .setInputCols(\"document\", \"doc_chunk\", \"embeddings\")\\\n",
    "    .setOutputCol(\"assertion\")\\\n",
    "    .setBatchSize(128)\\\n",
    "    .setLearningRate(0.001)\\\n",
    "    .setEpochs(2)\\\n",
    "    .setStartCol(\"start\")\\\n",
    "    .setEndCol(\"end\")\\\n",
    "    .setMaxSentLen(1200)\\\n",
    "    .setEnableOutputLogs(True)\\\n",
    "    .setOutputLogsPath('training_logs/')\\\n",
    "    .setGraphFolder(graph_folder)\\\n",
    "    .setGraphFile(f\"{graph_folder}/assertion_graph.pb\")\\\n",
    "    .setTestDataset(path=\"test_data.parquet\", read_as='SPARK', options={'format': 'parquet'})\\\n",
    "    .setScopeWindow(scope_window)\n",
    "    #.setValidationSplit(0.2)\\    \n",
    "    #.setDropout(0.1)\\    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2MZLeCYATrS"
   },
   "outputs": [],
   "source": [
    "assertion_pipeline = nlp.Pipeline(\n",
    "    stages = [\n",
    "    #document,\n",
    "    #chunk,\n",
    "    #token,\n",
    "    #embeddings,\n",
    "    assertion_graph_builder,\n",
    "    assertionStatus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yIvnuaQP91j8",
    "outputId": "79f7598e-1b39-42de-eca1-2945ea10b9e8"
   },
   "outputs": [],
   "source": [
    "training_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ueJz0aiJ_7l4"
   },
   "outputs": [],
   "source": [
    "assertion_train_data = spark.read.parquet('train_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j1NCZ89T_7ol",
    "outputId": "ba16f9d7-59a1-425f-f408-91798ae7da28"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "assertion_model = assertion_pipeline.fit(assertion_train_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "30SmcTiSpnWa"
   },
   "source": [
    "Checking the results saved in the log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOiu1vuspKut",
    "outputId": "bd263606-10c3-4c3a-ad4f-499912a336a4"
   },
   "outputs": [],
   "source": [
    "log_files = os.listdir(\"training_logs\")\n",
    "log_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CcQV0-fIrJHz",
    "outputId": "67066ebf-ce47-44ee-e4dd-95e3679ad4f6"
   },
   "outputs": [],
   "source": [
    "with open(\"training_logs/\"+log_files[0]) as log_file:\n",
    "    print(log_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgcG00nT91nn"
   },
   "outputs": [],
   "source": [
    "assertion_test_data = spark.read.parquet('test_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-k2WrFkRyQyP",
    "outputId": "196171e4-acec-403c-dd25-41c653268957"
   },
   "outputs": [],
   "source": [
    "preds = assertion_model.transform(assertion_test_data).select('label','assertion.result')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yI73lwG2xk5"
   },
   "outputs": [],
   "source": [
    "preds_df = preds.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "yRXZFGlQ3Z2U",
    "outputId": "dc40c737-d13e-4334-a35f-9ed6a15818ba"
   },
   "outputs": [],
   "source": [
    "preds_df[\"result\"] = preds_df[\"result\"].apply(lambda x: x[0] if len(x) else pd.NA)\n",
    "preds_df.dropna(inplace=True)\n",
    "\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hb1kyGAE0Gn",
    "outputId": "d9827789-87be-4cc4-ae26-210016579da2"
   },
   "outputs": [],
   "source": [
    "# We are going to use sklearn to evalute the results on test dataset\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print (classification_report( preds_df['label'], preds_df['result']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WuJ5YZ9sXU13"
   },
   "source": [
    "## Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBcoOwvwXV8p",
    "outputId": "b01e856f-a162-49bd-ab34-ea0f6b730255"
   },
   "outputs": [],
   "source": [
    "assertion_model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ioMW1jSrA-wg",
    "outputId": "ad724ab9-d348-49de-f7cb-be0ef1c2d175"
   },
   "outputs": [],
   "source": [
    "# Save a Spark NLP model\n",
    "assertion_model.stages[-1].write().overwrite().save('Assertion')\n",
    "\n",
    "import shutil\n",
    "shutil.make_archive('Assertion', 'zip', 'Assertion')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
