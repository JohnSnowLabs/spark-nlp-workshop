{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9KViDTWeMEE",
   "metadata": {
    "id": "e9KViDTWeMEE"
   },
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35e5153",
   "metadata": {
    "id": "e35e5153"
   },
   "source": [
    "# Financial Word and Sentence Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a14890-bc41-4765-9557-89a969c04d8f",
   "metadata": {
    "id": "69a14890-bc41-4765-9557-89a969c04d8f"
   },
   "source": [
    "# Finance Word and Sentence Embeddings visualization using PCA (Principal Component Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f982ae9-0570-4f90-bd30-c9d55d219e5b",
   "metadata": {
    "id": "5f982ae9-0570-4f90-bd30-c9d55d219e5b"
   },
   "source": [
    "Modern NLP models work with a numerical representation of texts and their menaning. For token classification problems (inferring a class for a token, for example Name Entity Recognition) Word Embeddings are required. For sentences, paragraph, document classification - we use Sentence Embeddings.\n",
    "\n",
    "In this notebook, we got token embeddings using Spark NLP Finance Word Embeddings(**bert_embeddings_sec_bert_base**) and using these token embeddings we got sentence embeddings by sparknlp annotator SentenceEmbeddings to get those numerical representations of the semantics of the texts. The result is a 768 embeddings matrix, impossible to process by the human eye.\n",
    "\n",
    "There are many techniques we can use to visualize those embeddings. We are using one of them - Principal Component Analysis, a dimensionality reduction process, carried out by Spark MLLib. Both embeddings have 768 dimensions, so we will reduced this dimensions from **768** to **3** (X, Y, Z) and will use a color for the word / sentence legend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13955b63-0fe3-4768-a3fa-d83a230660eb",
   "metadata": {
    "id": "okhT7AcXxben"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8414a78-e996-493e-ae9a-ff941dd30edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install plotly\n",
    "%pip install -q tensorflow==2.7.0\n",
    "%pip install -q tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01644e1e-242b-4941-b93c-1e292f2e1ca1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115199,
     "status": "ok",
     "timestamp": 1664816113389,
     "user": {
      "displayName": "Damla",
      "userId": "03285166568766987047"
     },
     "user_tz": -120
    },
    "id": "dmcB5zVBHZO8",
    "outputId": "cd366e47-7f4d-457a-dfe5-3ed5174d4a0c"
   },
   "outputs": [],
   "source": [
    "from johnsnowlabs import *\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(\"Spark NLP Version :\", sparknlp.version())\n",
    "\n",
    "spark = start_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7c011d-eac8-4eda-b272-027bab6a8895",
   "metadata": {
    "id": "4b7c011d-eac8-4eda-b272-027bab6a8895"
   },
   "source": [
    "# Get sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XvEU36EbfUZH",
   "metadata": {
    "id": "XvEU36EbfUZH"
   },
   "outputs": [],
   "source": [
    "# Downloading sample datasets.\n",
    "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings_JSL/Finance/data/finance_pca_samples.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f411a9b-9744-4f77-bdb9-58702260b5b0",
   "metadata": {
    "id": "5f411a9b-9744-4f77-bdb9-58702260b5b0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"finance_pca_samples.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3977898f-24e8-43d6-a338-80f3bf8ad2a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3551,
     "status": "ok",
     "timestamp": 1664817521114,
     "user": {
      "displayName": "Damla",
      "userId": "03285166568766987047"
     },
     "user_tz": -120
    },
    "id": "3977898f-24e8-43d6-a338-80f3bf8ad2a1",
    "outputId": "c5f6dd89-4885-4278-d88d-1005d33d39af",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create spark dataframe\n",
    "sdf = spark.createDataFrame(df)\n",
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de27fdb-3fc1-4d58-8505-4f02238635b8",
   "metadata": {
    "id": "5de27fdb-3fc1-4d58-8505-4f02238635b8"
   },
   "source": [
    "# Pipeline with Spark NLP and Spark MLLIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YwQjfHcM_a4q",
   "metadata": {
    "id": "YwQjfHcM_a4q"
   },
   "outputs": [],
   "source": [
    "# We defined a generic pipeline for word and sentence embeddings\n",
    "\n",
    "def generic_pipeline():\n",
    "    document_assembler = nlp.DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "    tokenizer = nlp.Tokenizer()\\\n",
    "      .setInputCols(\"document\")\\\n",
    "      .setOutputCol(\"token\")\n",
    "\n",
    "    word_embeddings = nlp.BertEmbeddings.pretrained(\"bert_embeddings_sec_bert_base\",\"en\")\\\n",
    "      .setInputCols([\"document\", \"token\"])\\\n",
    "      .setOutputCol(\"word_embeddings\")\n",
    "\n",
    "    pipeline = Pipeline(stages = [\n",
    "      document_assembler,\n",
    "      tokenizer,\n",
    "      word_embeddings\n",
    "    ])\n",
    "\n",
    "    return pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d751fd45-9810-4b43-b893-0d373e8d4870",
   "metadata": {
    "id": "d751fd45-9810-4b43-b893-0d373e8d4870"
   },
   "source": [
    "## Sentence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b990cfd8-576f-4057-a548-0434d39897bd",
   "metadata": {
    "id": "b990cfd8-576f-4057-a548-0434d39897bd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings_sentence = nlp.SentenceEmbeddings()\\\n",
    "    .setInputCols([\"document\", \"word_embeddings\"])\\\n",
    "    .setOutputCol(\"sentence_embeddings\")\\\n",
    "    .setPoolingStrategy(\"AVERAGE\")\n",
    "# We used sparknlp SentenceEmbeddings anootator to get each sentence embeddings from token embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde3b08f-6187-43fd-8d7b-3fe6da054c18",
   "metadata": {
    "id": "fde3b08f-6187-43fd-8d7b-3fe6da054c18"
   },
   "source": [
    "# Custom transform to retrieve the numerical embeddings from Spark NLP and pass it to Spark MLLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cfff3f-487b-4808-8b32-f54fc443777b",
   "metadata": {
    "id": "30cfff3f-487b-4808-8b32-f54fc443777b"
   },
   "outputs": [],
   "source": [
    "# This class extracts the embeddings from the Spark NLP Annotation object\n",
    "# from pyspark import ml as ML\n",
    "class EmbeddingsUDF(\n",
    "    Transformer, ML.param.shared.HasInputCol, ML.param.shared.HasOutputCol,\n",
    "    ML.util.DefaultParamsReadable, ML.util.DefaultParamsWritable\n",
    "):\n",
    "    @keyword_only\n",
    "    def __init__(self):\n",
    "        super(EmbeddingsUDF, self).__init__()\n",
    "\n",
    "        def _sum(r):\n",
    "            result = 0.0\n",
    "            for e in r:\n",
    "                result += e\n",
    "            return result\n",
    "\n",
    "        self.udfs = {\n",
    "            'convertToVectorUDF': F.udf(lambda vs: ML.linalg.Vectors.dense(vs), ML.linalg.VectorUDT()),\n",
    "            'sumUDF': F.udf(lambda r: _sum(r), T.FloatType())\n",
    "        }\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "\n",
    "        results = dataset.select(\n",
    "            \"*\", F.explode(\"sentence_embeddings.embeddings\").alias(\"embeddings\")\n",
    "        )\n",
    "        results = results.withColumn(\n",
    "            \"features\",\n",
    "            self.udfs['convertToVectorUDF'](F.col(\"embeddings\"))\n",
    "        )\n",
    "        results = results.withColumn(\n",
    "            \"emb_sum\",\n",
    "            self.udfs['sumUDF'](F.col(\"embeddings\"))\n",
    "        )\n",
    "        # Remove those with embeddings all zeroes (so we can calculate cosine distance)\n",
    "        results = results.where(F.col(\"emb_sum\")!=0.0)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f82e139-1cc3-423d-9d53-c0ccfec835c2",
   "metadata": {
    "id": "5f82e139-1cc3-423d-9d53-c0ccfec835c2"
   },
   "outputs": [],
   "source": [
    "embeddings_for_pca = EmbeddingsUDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213dfcf-61bc-4db2-a8c7-797a318a13f4",
   "metadata": {
    "id": "5213dfcf-61bc-4db2-a8c7-797a318a13f4"
   },
   "outputs": [],
   "source": [
    "DIMENSIONS  = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285ba04a-dbe2-4f53-9aa0-fc66ba783e14",
   "metadata": {
    "id": "285ba04a-dbe2-4f53-9aa0-fc66ba783e14"
   },
   "outputs": [],
   "source": [
    "pca = ML.feature.PCA(k=DIMENSIONS, inputCol=\"features\", outputCol=\"pca_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f95669-0f8e-491b-b6d8-33f590aa06cf",
   "metadata": {
    "id": "82f95669-0f8e-491b-b6d8-33f590aa06cf"
   },
   "source": [
    "### Full Spark NLP + Spark MLLib pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c279de78-563f-474c-8d3e-3c72243bb04b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 38234,
     "status": "ok",
     "timestamp": 1664817559343,
     "user": {
      "displayName": "Damla",
      "userId": "03285166568766987047"
     },
     "user_tz": -120
    },
    "id": "c279de78-563f-474c-8d3e-3c72243bb04b",
    "outputId": "8444b2b0-6a47-4f5b-9ce3-4287e0b690d5"
   },
   "outputs": [],
   "source": [
    "# We did all process in one pipeline\n",
    "pipeline = Pipeline().setStages([generic_pipeline(), embeddings_sentence, embeddings_for_pca, pca])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46c615-d2b6-408a-99c3-bac0f824dcf4",
   "metadata": {
    "id": "0d46c615-d2b6-408a-99c3-bac0f824dcf4"
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d52ebe0-4257-4354-817f-5b8f2a363b8e",
   "metadata": {
    "id": "4d52ebe0-4257-4354-817f-5b8f2a363b8e"
   },
   "outputs": [],
   "source": [
    "result = model.transform(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b852cad-e1e7-48bc-94b0-9da7188a9727",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3070,
     "status": "ok",
     "timestamp": 1664817576376,
     "user": {
      "displayName": "Damla",
      "userId": "03285166568766987047"
     },
     "user_tz": -120
    },
    "id": "7b852cad-e1e7-48bc-94b0-9da7188a9727",
    "outputId": "695c8eff-0e43-4356-ce13-b255a90985f1"
   },
   "outputs": [],
   "source": [
    "result.select('pca_features', 'label').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf7811-b04c-4f63-88e4-10e51341ce0f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "executionInfo": {
     "elapsed": 1639,
     "status": "ok",
     "timestamp": 1664817578008,
     "user": {
      "displayName": "Damla",
      "userId": "03285166568766987047"
     },
     "user_tz": -120
    },
    "id": "adbf7811-b04c-4f63-88e4-10e51341ce0f",
    "outputId": "fd42928e-639d-42b0-f6e3-a5ec49664e74"
   },
   "outputs": [],
   "source": [
    "df = result.select('pca_features', 'label').toPandas()\n",
    "\n",
    "df\n",
    "# As you see, dimension values are inside a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3707de-8ff4-49b7-89c2-85e9616bc337",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1664817578008,
     "user": {
      "displayName": "Damla",
      "userId": "03285166568766987047"
     },
     "user_tz": -120
    },
    "id": "5c3707de-8ff4-49b7-89c2-85e9616bc337",
    "outputId": "d145e906-2fd3-4997-a0f8-806dda0b7e10"
   },
   "outputs": [],
   "source": [
    "# We extract the dimension values out off the list\n",
    "\n",
    "df[\"x\"] = df[\"pca_features\"].apply(lambda x: x[0])\n",
    "\n",
    "df[\"y\"] = df[\"pca_features\"].apply(lambda x: x[1])\n",
    "\n",
    "df[\"z\"] = df[\"pca_features\"].apply(lambda x: x[2])\n",
    "\n",
    "df = df[[\"x\", \"y\", \"z\", \"label\"]]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7281d091-9356-41a7-83e7-6b5ee541a09c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 616
    },
    "executionInfo": {
     "elapsed": 2675,
     "status": "ok",
     "timestamp": 1664817580676,
     "user": {
      "displayName": "Damla",
      "userId": "03285166568766987047"
     },
     "user_tz": -120
    },
    "id": "7281d091-9356-41a7-83e7-6b5ee541a09c",
    "outputId": "dc74381c-d519-4b5b-83bb-4d1c95dfd38e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_3d(df, x = 'x', y = 'y', z = 'z', color = 'label', width=800, height=600)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602bc911-284b-4e0e-8882-99ac444fbb51",
   "metadata": {
    "id": "602bc911-284b-4e0e-8882-99ac444fbb51"
   },
   "source": [
    "### Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bfbc63-cb97-4189-8d6c-6629bbc898e8",
   "metadata": {
    "id": "11bfbc63-cb97-4189-8d6c-6629bbc898e8"
   },
   "source": [
    "We can also visualize the semantics of words, instead of full texts, by using Word Embeddings. We will add a Tokenizer and a WordEmbeddings model to get those embeddings, and them apply PCA as before. Firstly we splitted the pipeline in two to get all token embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f0cced-e804-4415-aa57-6ae05587adf2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2573,
     "status": "ok",
     "timestamp": 1664817583245,
     "user": {
      "displayName": "Damla",
      "userId": "03285166568766987047"
     },
     "user_tz": -120
    },
    "id": "28f0cced-e804-4415-aa57-6ae05587adf2",
    "outputId": "93aa95f3-42c3-426c-c7c8-a8741b1ffdd4"
   },
   "outputs": [],
   "source": [
    "model = generic_pipeline().fit(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31090faa-0bbe-4e94-8384-e38a338e1989",
   "metadata": {
    "id": "31090faa-0bbe-4e94-8384-e38a338e1989"
   },
   "outputs": [],
   "source": [
    "result = model.transform(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1815b79d-81c5-45ec-ac80-418e31f9df8e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1138,
     "status": "ok",
     "timestamp": 1664817584378,
     "user": {
      "displayName": "Damla",
      "userId": "03285166568766987047"
     },
     "user_tz": -120
    },
    "id": "1815b79d-81c5-45ec-ac80-418e31f9df8e",
    "outputId": "5d2e9f6d-d3ae-4bc0-8e52-08763ac9d5f0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df = result.select(\"label\", F.explode(F.arrays_zip(result.token.result, result.word_embeddings.embeddings)).alias(\"cols\"))\\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"token\"),\n",
    "              \"label\",\n",
    "              F.expr(\"cols['1']\").alias(\"embeddings\"))\n",
    "\n",
    "result_df.show(truncate = 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6186b5d6-fa75-42db-b5f6-febbe0f6c957",
   "metadata": {
    "id": "6186b5d6-fa75-42db-b5f6-febbe0f6c957"
   },
   "outputs": [],
   "source": [
    "# Here we defined inheritance class from that defined previously EmbeddingsUDF class\n",
    "class WordEmbeddingsUDF(EmbeddingsUDF):    \n",
    "    def _transform(self, dataset):\n",
    "        \n",
    "        results = dataset.select('token', 'label', 'embeddings') # We changed this line because our embedding cloumn is already exploded\n",
    "\n",
    "        results = results.withColumn(\n",
    "            \"features\",\n",
    "            self.udfs['convertToVectorUDF'](F.col(\"embeddings\"))\n",
    "        )\n",
    "        results = results.withColumn(\n",
    "            \"emb_sum\",\n",
    "            self.udfs['sumUDF'](F.col(\"embeddings\"))\n",
    "        )\n",
    "        # Remove those with embeddings all zeroes (so we can calculate cosine distance)\n",
    "        results = results.where(F.col(\"emb_sum\")!=0.0)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e416012e-ba80-4ca8-98de-aa675660ecb1",
   "metadata": {
    "id": "e416012e-ba80-4ca8-98de-aa675660ecb1"
   },
   "outputs": [],
   "source": [
    "embeddings_for_pca = WordEmbeddingsUDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cca3d1f-0261-42de-96f8-f68f7fe6210a",
   "metadata": {
    "id": "0cca3d1f-0261-42de-96f8-f68f7fe6210a"
   },
   "outputs": [],
   "source": [
    "DIMENSIONS  = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9e30a2-cc23-463f-87d1-4846c7a321e7",
   "metadata": {
    "id": "2f9e30a2-cc23-463f-87d1-4846c7a321e7"
   },
   "outputs": [],
   "source": [
    "pca = ML.feature.PCA(k=DIMENSIONS, inputCol=\"features\", outputCol=\"pca_features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec00af2-8e16-4473-8b43-4f7cff630a3d",
   "metadata": {
    "id": "4ec00af2-8e16-4473-8b43-4f7cff630a3d"
   },
   "source": [
    "### Full Spark NLP + Spark MLLib pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b8590b-2bf4-45b9-924d-d7438b87f08a",
   "metadata": {
    "id": "69b8590b-2bf4-45b9-924d-d7438b87f08a"
   },
   "outputs": [],
   "source": [
    "# We run the second part of the pipeline. Here 768 dimensions is reduced to 3 dimensions\n",
    "\n",
    "pipeline = Pipeline().setStages([embeddings_for_pca, pca])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207c543-9fb0-4187-a359-61abad891934",
   "metadata": {
    "id": "8207c543-9fb0-4187-a359-61abad891934",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf674445-20d2-4ee9-8409-6a3782794aae",
   "metadata": {
    "id": "cf674445-20d2-4ee9-8409-6a3782794aae"
   },
   "outputs": [],
   "source": [
    "result = model.transform(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9093ca0e-e46e-40e8-8958-c47702fc4a43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 677,
     "status": "ok",
     "timestamp": 1664817592272,
     "user": {
      "displayName": "Damla",
      "userId": "03285166568766987047"
     },
     "user_tz": -120
    },
    "id": "9093ca0e-e46e-40e8-8958-c47702fc4a43",
    "outputId": "4f6cc062-314c-4bd4-a9d0-f09765182c16",
    "tags": []
   },
   "outputs": [],
   "source": [
    "result.select(\"token\", \"label\", \"pca_features\").show(truncate = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2299c9bc-8e8e-4c49-ad29-133df31da543",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "executionInfo": {
     "elapsed": 2142,
     "status": "ok",
     "timestamp": 1664817594411,
     "user": {
      "displayName": "Damla",
      "userId": "03285166568766987047"
     },
     "user_tz": -120
    },
    "id": "2299c9bc-8e8e-4c49-ad29-133df31da543",
    "outputId": "07519ee3-3bc8-444a-947f-a50a8fda2927"
   },
   "outputs": [],
   "source": [
    "df = result.select('token', 'label', 'pca_features').toPandas()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20443a2f-9e6e-4cd2-885e-b42720bcb840",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1664817594412,
     "user": {
      "displayName": "Damla",
      "userId": "03285166568766987047"
     },
     "user_tz": -120
    },
    "id": "20443a2f-9e6e-4cd2-885e-b42720bcb840",
    "outputId": "e34d00a7-ef11-4a82-9180-dda7716d03c5"
   },
   "outputs": [],
   "source": [
    "df[\"x\"] = df[\"pca_features\"].apply(lambda x: x[0])\n",
    "\n",
    "df[\"y\"] = df[\"pca_features\"].apply(lambda x: x[1])\n",
    "\n",
    "df[\"z\"] = df[\"pca_features\"].apply(lambda x: x[2])\n",
    "\n",
    "df = df[[\"token\", \"label\", \"x\", \"y\", \"z\"]]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d9799-e838-4cdc-903e-dbd5cdc37aa9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 816
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1664817594412,
     "user": {
      "displayName": "Damla",
      "userId": "03285166568766987047"
     },
     "user_tz": -120
    },
    "id": "e96d9799-e838-4cdc-903e-dbd5cdc37aa9",
    "outputId": "01f850f6-128a-4924-bdc1-26c5c5b449fa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_3d(df, x = 'x', y = 'y', z = 'z', color = \"label\", width=1000, height = 800, hover_data = [\"token\", \"label\"])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5571fc8b-91a2-41e9-9320-bff4185dc1a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ede4c5c-7fb9-4445-bbbf-6341526eeac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "ca1c4b8877e01dec1d65bc94ac0771fb7b4e7d433b24c0ced0afdc05f796f65d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}