{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "wxZDXLDCXkk_",
   "metadata": {
    "id": "wxZDXLDCXkk_"
   },
   "source": [
    "\n",
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "KLqW6FOnEvov",
   "metadata": {
    "id": "KLqW6FOnEvov"
   },
   "source": [
    "# Training Legal Assertion\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e11e1a1b-fa10-422b-baeb-31698cd3ee52",
   "metadata": {
    "id": "okhT7AcXxben"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4baa3e8-80c1-4639-9abc-f598a760e198",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 115199,
     "status": "ok",
     "timestamp": 1664816113389,
     "user": {
      "displayName": "Damla",
      "userId": "03285166568766987047"
     },
     "user_tz": -120
    },
    "id": "dmcB5zVBHZO8",
    "outputId": "cd366e47-7f4d-457a-dfe5-3ed5174d4a0c"
   },
   "outputs": [],
   "source": [
    "from johnsnowlabs import *\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "spark = start_spark()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc2e8327-8e17-4726-9010-b187335909f0",
   "metadata": {
    "id": "JYBQyxEd0uR0"
   },
   "source": [
    "# CoNLL Data Prep "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "JYBQyxEd0uR0",
   "metadata": {
    "id": "JYBQyxEd0uR0"
   },
   "source": [
    "# Data Prep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AVBmGFcQ03La",
   "metadata": {
    "id": "AVBmGFcQ03La"
   },
   "outputs": [],
   "source": [
    "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/legal-nlp/data/assertion_fin.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8iJF_HCw1Lgh",
   "metadata": {
    "id": "8iJF_HCw1Lgh"
   },
   "outputs": [],
   "source": [
    "training_df = pd.read_csv('./assertion_fin.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JREBeTzb8ov-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JREBeTzb8ov-",
    "outputId": "4714acfc-92dd-45fb-df1f-455c1171aeff"
   },
   "outputs": [],
   "source": [
    "training_data = spark.createDataFrame(training_df)\n",
    "training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ET8GD3y3-17e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ET8GD3y3-17e",
    "outputId": "5e478418-b365-4168-8de9-aab1a885d189"
   },
   "outputs": [],
   "source": [
    "training_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R6xa4jp8Szs0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6xa4jp8Szs0",
    "outputId": "6e8e951c-f1c7-445d-af8c-f6e76182174f"
   },
   "outputs": [],
   "source": [
    "%time training_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fxcHD_Q_-_lD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxcHD_Q_-_lD",
    "outputId": "1a114990-64d0-43c1-8522-ef4a9b2145fb"
   },
   "outputs": [],
   "source": [
    "(train_data, test_data) = training_data.randomSplit([0.9, 0.1], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(training_data.count()))\n",
    "print(\"Test Dataset Count: \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DFN_BuHU84HF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DFN_BuHU84HF",
    "outputId": "ea931a9b-a829-4fcc-90fd-19d7aedf982d"
   },
   "outputs": [],
   "source": [
    "train_data.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2WZDqlZA_kmb",
   "metadata": {
    "id": "2WZDqlZA_kmb"
   },
   "source": [
    "# Using RoBerta Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7qfJh8ap_nI2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7qfJh8ap_nI2",
    "outputId": "9bd7bd23-7a37-4961-bb94-8374b0abf7d8"
   },
   "outputs": [],
   "source": [
    "roberta_embeddings = nlp.RoBertaEmbeddings.pretrained(\"roberta_embeddings_legal_roberta_base\",\"en\") \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\") \\\n",
    "    .setMaxSentenceLength(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fe0957BT_rcy",
   "metadata": {
    "id": "Fe0957BT_rcy"
   },
   "outputs": [],
   "source": [
    "document = nlp.DocumentAssembler()\\\n",
    "    .setInputCol(\"sentence\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "chunk = nlp.Doc2Chunk()\\\n",
    "    .setInputCols(\"document\")\\\n",
    "    .setOutputCol(\"doc_chunk\")\\\n",
    "    .setChunkCol(\"chunk\")\\\n",
    "    .setStartCol(\"tkn_start\")\\\n",
    "    .setStartColByTokenIndex(True)\\\n",
    "    .setFailOnMissing(False)\\\n",
    "    .setLowerCase(False)\n",
    "\n",
    "token = nlp.Tokenizer()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('token')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "LFTO0PlI9-3e",
   "metadata": {
    "id": "LFTO0PlI9-3e"
   },
   "source": [
    "We save the test data in parquet format to use in `AssertionDLApproach()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M9u4c65G9VaC",
   "metadata": {
    "id": "M9u4c65G9VaC"
   },
   "outputs": [],
   "source": [
    "assertion_pipeline = nlp.Pipeline(\n",
    "    stages = [\n",
    "    document,\n",
    "    chunk,\n",
    "    token,\n",
    "    roberta_embeddings])\n",
    "\n",
    "assertion_test_data = assertion_pipeline.fit(test_data).transform(test_data)\n",
    "\n",
    "assertion_train_data = assertion_pipeline.fit(training_data).transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r-UREmtI9Vd3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r-UREmtI9Vd3",
    "outputId": "31cd6903-4929-491c-83a7-09c5ef42d9dd"
   },
   "outputs": [],
   "source": [
    "assertion_test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BtxnrvcA9VlN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BtxnrvcA9VlN",
    "outputId": "dd24e11a-9ad8-470b-9610-3d9eb1dfaac5"
   },
   "outputs": [],
   "source": [
    "assertion_train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kBaiXx78BTLT",
   "metadata": {
    "id": "kBaiXx78BTLT"
   },
   "outputs": [],
   "source": [
    "assertion_test_data.write.mode('overwrite').parquet('test_data.parquet')\n",
    "\n",
    "assertion_train_data.write.mode('overwrite').parquet('train_data.parquet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "uTishXbut1MS",
   "metadata": {
    "id": "uTishXbut1MS"
   },
   "source": [
    "## Graph setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ShZT8BBo4FY",
   "metadata": {
    "id": "0ShZT8BBo4FY"
   },
   "source": [
    "We will use TFGraphBuilder annotator which can be used to create graphs in the model training pipeline. \n",
    "\n",
    "TFGraphBuilder inspects the data and creates the proper graph if a suitable version of TensorFlow (<= 2.7 ) is available. The graph is stored in the defined folder and loaded by the approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XhU0L1OXdaLN",
   "metadata": {
    "id": "XhU0L1OXdaLN"
   },
   "outputs": [],
   "source": [
    "graph_folder= \"./tf_graphs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miNgoTjio0mL",
   "metadata": {
    "id": "miNgoTjio0mL"
   },
   "outputs": [],
   "source": [
    "assertion_graph_builder =  legal.TFGraphBuilder()\\\n",
    "    .setModelName(\"assertion_dl\")\\\n",
    "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "    .setLabelColumn(\"assertion_label\")\\\n",
    "    .setGraphFolder(graph_folder)\\\n",
    "    .setGraphFile(\"assertion_graph.pb\")\\\n",
    "    .setMaxSequenceLength(1200)\\\n",
    "    .setHiddenUnitsNumber(25)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6D0Ng7nMUjJa",
   "metadata": {
    "id": "6D0Ng7nMUjJa"
   },
   "source": [
    "**Setting the Scope Window (Target Area) Dynamically in Assertion Status Detection Models**\n",
    "\n",
    "\n",
    "This parameter allows you to train the Assertion Status Models to focus on specific context windows when resolving the status of a NER chunk. The window is in format `[X,Y]` being `X` the number of tokens to consider on the left of the chunk, and `Y` the max number of tokens to consider on the right. Let’s take a look at what different windows mean:\n",
    "\n",
    "\n",
    "*   By default, the window is `[-1,-1]` which means that the Assertion Status will look at all of the tokens in the sentence/document (up to a maximum of tokens set in `setMaxSentLen()` ).\n",
    "*   `[0,0]` means “don’t pay attention to any token except the ner_chunk”, what basically is not considering any context for the Assertion resolution.\n",
    "*   `[9,15]` is what empirically seems to be the best baseline, meaning that we look up to 9 tokens on the left and 15 on the right of the ner chunk to understand the context and resolve the status.\n",
    "\n",
    "\n",
    "Check this [Scope Window Tuning Assertion Status Detection notebook](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/2.1.Scope_window_tuning_assertion_status_detection.ipynb)  that illustrates the effect of the different windows and how to properly fine-tune your AssertionDLModels to get the best of them.\n",
    "\n",
    "In our case, the best Scope Window is around [10,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BQxGbYks91go",
   "metadata": {
    "id": "BQxGbYks91go"
   },
   "outputs": [],
   "source": [
    "scope_window = [50, 50]\n",
    "\n",
    "assertionStatus = legal.AssertionDLApproach()\\\n",
    "    .setLabelCol(\"assertion_label\")\\\n",
    "    .setInputCols(\"document\", \"doc_chunk\", \"embeddings\")\\\n",
    "    .setOutputCol(\"assertion\")\\\n",
    "    .setBatchSize(128)\\\n",
    "    .setLearningRate(0.001)\\\n",
    "    .setEpochs(2)\\\n",
    "    .setStartCol(\"tkn_start\")\\\n",
    "    .setEndCol(\"tkn_end\")\\\n",
    "    .setMaxSentLen(1200)\\\n",
    "    .setEnableOutputLogs(True)\\\n",
    "    .setOutputLogsPath('training_logs/')\\\n",
    "    .setGraphFolder(graph_folder)\\\n",
    "    .setGraphFile(f\"{graph_folder}/assertion_graph.pb\")\\\n",
    "    .setTestDataset(path=\"test_data.parquet\", read_as='SPARK', options={'format': 'parquet'})\\\n",
    "    .setScopeWindow(scope_window)\n",
    "    #.setValidationSplit(0.2)\\    \n",
    "    #.setDropout(0.1)\\    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "T2MZLeCYATrS",
   "metadata": {
    "id": "T2MZLeCYATrS"
   },
   "outputs": [],
   "source": [
    "assertion_pipeline = nlp.Pipeline(\n",
    "    stages = [\n",
    "    #document,\n",
    "    #chunk,\n",
    "    #token,\n",
    "    #embeddings,\n",
    "    assertion_graph_builder,\n",
    "    assertionStatus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yIvnuaQP91j8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yIvnuaQP91j8",
    "outputId": "54d68047-97f2-4cc0-d8ee-b7b57948145b"
   },
   "outputs": [],
   "source": [
    "training_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ueJz0aiJ_7l4",
   "metadata": {
    "id": "ueJz0aiJ_7l4"
   },
   "outputs": [],
   "source": [
    "assertion_train_data = spark.read.parquet('train_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j1NCZ89T_7ol",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j1NCZ89T_7ol",
    "outputId": "5a2a676d-0b61-4c59-c7db-6f2bc475c334"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "assertion_model = assertion_pipeline.fit(assertion_train_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30SmcTiSpnWa",
   "metadata": {
    "id": "30SmcTiSpnWa"
   },
   "source": [
    "Checking the results saved in the log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kOiu1vuspKut",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOiu1vuspKut",
    "outputId": "f62ec0f2-c361-4ecb-e094-bf659e956e17"
   },
   "outputs": [],
   "source": [
    "log_files = os.listdir(\"./training_logs\")\n",
    "log_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CcQV0-fIrJHz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CcQV0-fIrJHz",
    "outputId": "4239e398-1b0e-405a-cdd3-a608ea7687ca"
   },
   "outputs": [],
   "source": [
    "with open(\"./training_logs/\"+log_files[0]) as log_file:\n",
    "    print(log_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bgcG00nT91nn",
   "metadata": {
    "id": "bgcG00nT91nn"
   },
   "outputs": [],
   "source": [
    "assertion_test_data = spark.read.parquet('test_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-k2WrFkRyQyP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-k2WrFkRyQyP",
    "outputId": "de6214ba-b53d-4eb5-a3da-53410f470169"
   },
   "outputs": [],
   "source": [
    "preds = assertion_model.transform(assertion_test_data).select('assertion_label','assertion.result')\n",
    "\n",
    "preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4yI73lwG2xk5",
   "metadata": {
    "id": "4yI73lwG2xk5"
   },
   "outputs": [],
   "source": [
    "preds_df = preds.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yRXZFGlQ3Z2U",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "yRXZFGlQ3Z2U",
    "outputId": "103b1d38-b06e-4282-b878-5d1d64127b91"
   },
   "outputs": [],
   "source": [
    "preds_df[\"result\"] = preds_df[\"result\"].apply(lambda x: x[0] if len(x) else pd.NA)\n",
    "\n",
    "preds_df.dropna(inplace=True)\n",
    "\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1hb1kyGAE0Gn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1hb1kyGAE0Gn",
    "outputId": "468f0f48-2a58-494e-c827-8df5bb141c7a"
   },
   "outputs": [],
   "source": [
    "# We are going to use sklearn to evalute the results on test dataset\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print (classification_report( preds_df['assertion_label'], preds_df['result']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "WuJ5YZ9sXU13",
   "metadata": {
    "id": "WuJ5YZ9sXU13"
   },
   "source": [
    "### Saving the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KBcoOwvwXV8p",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBcoOwvwXV8p",
    "outputId": "85a27749-4cdc-498b-ffa1-02aa99a47912"
   },
   "outputs": [],
   "source": [
    "assertion_model.stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ioMW1jSrA-wg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ioMW1jSrA-wg",
    "outputId": "329f2545-0041-494f-8719-c641cb5b4c5e"
   },
   "outputs": [],
   "source": [
    "# Save a Spark NLP model\n",
    "assertion_model.stages[-1].write().overwrite().save('Assertion')\n",
    "\n",
    "import shutil\n",
    "shutil.make_archive('Assertion', 'zip', 'Assertion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec63a751-f945-4038-b9c8-275062f8e1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
