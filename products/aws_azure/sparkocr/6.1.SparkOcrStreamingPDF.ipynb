{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwFzpuFhVnFR"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxStFMMVVnFU"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-ocr-workshop/blob/master/tutorials/Certification_Trainings/6.1.SparkOcrStreamingPDF.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vh3AEyqVnFU"
      },
      "source": [
        "## Spark OCR Streaming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCQ2Pl-UVnFU"
      },
      "source": [
        "## Blogposts and videos\n",
        "\n",
        "- [Text Detection in Spark OCR](https://medium.com/spark-nlp/text-detection-in-spark-ocr-dcd8002bdc97)\n",
        "\n",
        "- [Table Detection & Extraction in Spark OCR](https://medium.com/spark-nlp/table-detection-extraction-in-spark-ocr-50765c6cedc9)\n",
        "\n",
        "- [Extract Tabular Data from PDF in Spark OCR](https://medium.com/spark-nlp/extract-tabular-data-from-pdf-in-spark-ocr-b02136bc0fcb)\n",
        "\n",
        "- [Signature Detection in Spark OCR](https://medium.com/spark-nlp/signature-detection-in-spark-ocr-32f9e6f91e3c)\n",
        "\n",
        "- [GPU image pre-processing in Spark OCR](https://medium.com/spark-nlp/gpu-image-pre-processing-in-spark-ocr-3-1-0-6fc27560a9bb)\n",
        "\n",
        "- [How to Setup Spark OCR on UBUNTU - Video](https://www.youtube.com/watch?v=cmt4WIcL0nI)\n",
        "\n",
        "\n",
        "**More examples here**\n",
        "\n",
        "https://github.com/JohnSnowLabs/spark-ocr-workshop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "M-ftVAZdVnFV"
      },
      "outputs": [],
      "source": [
        "import json, os\n",
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import files\n",
        "\n",
        "    if 'spark_ocr.json' not in os.listdir():\n",
        "      license_keys = files.upload()\n",
        "      os.rename(list(license_keys.keys())[0], 'spark_ocr.json')\n",
        "\n",
        "with open('spark_ocr.json') as f:\n",
        "    license_keys = json.load(f)\n",
        "\n",
        "# Defining license key-value pairs as local variables\n",
        "locals().update(license_keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Np3NFHkbVnFW",
        "outputId": "00a2c2ff-6ed4-490b-c4f5-66a71f0e6854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 281.4 MB 36 kB/s \n",
            "\u001b[K     |████████████████████████████████| 531 kB 14.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 59.7 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://pypi.johnsnowlabs.com/4.0.0-49cdb09f66ca01a93f959366f0e4a84d1a09b2df\n",
            "Collecting spark-ocr==4.0.0+spark32\n",
            "  Downloading https://pypi.johnsnowlabs.com/4.0.0-49cdb09f66ca01a93f959366f0e4a84d1a09b2df/spark-ocr/spark_ocr-4.0.0%2Bspark32-py3-none-any.whl (28.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 28.0 MB 346 kB/s \n",
            "\u001b[?25hRequirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from spark-ocr==4.0.0+spark32) (0.10.9.3)\n",
            "Collecting implicits==1.0.2\n",
            "  Downloading implicits-1.0.2-py3-none-any.whl (3.7 kB)\n",
            "Collecting craft-text-detector==0.4.2\n",
            "  Downloading craft_text_detector-0.4.2-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: pyspark==3.2.1 in /usr/local/lib/python3.7/dist-packages (from spark-ocr==4.0.0+spark32) (3.2.1)\n",
            "Collecting scikit-image==0.18.1\n",
            "  Downloading scikit_image-0.18.1-cp37-cp37m-manylinux1_x86_64.whl (29.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 29.2 MB 16.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spark-nlp==4.0.0 in /usr/local/lib/python3.7/dist-packages (from spark-ocr==4.0.0+spark32) (4.0.0)\n",
            "Collecting pillow==9.0.1\n",
            "  Downloading Pillow-9.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 39.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy==1.21.6 in /usr/local/lib/python3.7/dist-packages (from spark-ocr==4.0.0+spark32) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from craft-text-detector==0.4.2->spark-ocr==4.0.0+spark32) (1.12.0+cu113)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.7/dist-packages (from craft-text-detector==0.4.2->spark-ocr==4.0.0+spark32) (4.4.0)\n",
            "Requirement already satisfied: torchvision>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from craft-text-detector==0.4.2->spark-ocr==4.0.0+spark32) (0.13.0+cu113)\n",
            "Collecting opencv-python<4.5.4.62,>=3.4.8.29\n",
            "  Downloading opencv_python-4.5.4.60-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 60.3 MB 65 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.7/dist-packages (from craft-text-detector==0.4.2->spark-ocr==4.0.0+spark32) (1.7.3)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.18.1->spark-ocr==4.0.0+spark32) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.18.1->spark-ocr==4.0.0+spark32) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.18.1->spark-ocr==4.0.0+spark32) (2.6.3)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.18.1->spark-ocr==4.0.0+spark32) (2021.11.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.18.1->spark-ocr==4.0.0+spark32) (2.4.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown>=3.10.1->craft-text-detector==0.4.2->spark-ocr==4.0.0+spark32) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown>=3.10.1->craft-text-detector==0.4.2->spark-ocr==4.0.0+spark32) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown>=3.10.1->craft-text-detector==0.4.2->spark-ocr==4.0.0+spark32) (4.64.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown>=3.10.1->craft-text-detector==0.4.2->spark-ocr==4.0.0+spark32) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown>=3.10.1->craft-text-detector==0.4.2->spark-ocr==4.0.0+spark32) (3.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.18.1->spark-ocr==4.0.0+spark32) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.18.1->spark-ocr==4.0.0+spark32) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.18.1->spark-ocr==4.0.0+spark32) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.18.1->spark-ocr==4.0.0+spark32) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image==0.18.1->spark-ocr==4.0.0+spark32) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown>=3.10.1->craft-text-detector==0.4.2->spark-ocr==4.0.0+spark32) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown>=3.10.1->craft-text-detector==0.4.2->spark-ocr==4.0.0+spark32) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown>=3.10.1->craft-text-detector==0.4.2->spark-ocr==4.0.0+spark32) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown>=3.10.1->craft-text-detector==0.4.2->spark-ocr==4.0.0+spark32) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown>=3.10.1->craft-text-detector==0.4.2->spark-ocr==4.0.0+spark32) (1.7.1)\n",
            "Installing collected packages: pillow, opencv-python, scikit-image, implicits, craft-text-detector, spark-ocr\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.6.0.66\n",
            "    Uninstalling opencv-python-4.6.0.66:\n",
            "      Successfully uninstalled opencv-python-4.6.0.66\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.18.3\n",
            "    Uninstalling scikit-image-0.18.3:\n",
            "      Successfully uninstalled scikit-image-0.18.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed craft-text-detector-0.4.2 implicits-1.0.2 opencv-python-4.5.4.60 pillow-9.0.1 scikit-image-0.18.1 spark-ocr-4.0.0+spark32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Installing pyspark and spark-nlp\n",
        "%pip install --upgrade -q pyspark==3.2.1 spark-nlp==$PUBLIC_VERSION\n",
        "\n",
        "# Installing Spark OCR\n",
        "#! pip uninstall spark-ocr -Y\n",
        "%pip install spark-ocr==$OCR_VERSION\\+spark32 --extra-index-url=https://pypi.johnsnowlabs.com/$SPARK_OCR_SECRET --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKbt4xboVnFX"
      },
      "source": [
        "<b><h1><font color='darkred'>!!! ATTENTION !!! </font><h1><b>\n",
        "\n",
        "<b>After running previous cell, <font color='darkred'>RESTART the COLAB RUNTIME </font> and go ahead.<b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xetwftatVnFX"
      },
      "outputs": [],
      "source": [
        "import json, os\n",
        "\n",
        "with open(\"spark_ocr.json\", 'r') as f:\n",
        "  license_keys = json.load(f)\n",
        "\n",
        "# Adding license key-value pairs to environment variables\n",
        "os.environ.update(license_keys)\n",
        "\n",
        "# Defining license key-value pairs as local variables\n",
        "locals().update(license_keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "gfCDUTvNVnFY"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "import sparkocr\n",
        "import json\n",
        "import os\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import PipelineModel\n",
        "import pyspark.sql.functions as f\n",
        "\n",
        "from sparkocr.transformers import *\n",
        "from sparkocr.utils import display_images\n",
        "from sparkocr.enums import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEQ28LsCVnFY"
      },
      "source": [
        "## Initialization of spark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "id": "yshbVxejVnFZ",
        "outputId": "29e560a8-1b1d-4f31-d73c-1b0a92a851a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark version: 3.2.1\n",
            "Spark NLP version: 4.0.0\n",
            "Spark OCR version: 4.0.0\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fc182446190>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://d7daad8ab00e:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.2.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark OCR</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from sparkocr import start\n",
        "\n",
        "\n",
        "spark = start(secret=SPARK_OCR_SECRET, nlp_version=PUBLIC_VERSION)\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6jWQne42VnFa"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "from sparkocr.transformers import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4VmVAhtZVnFa"
      },
      "outputs": [],
      "source": [
        "# fill path to folder with PDF's here\n",
        "dataset_path = \"data/pdfs/*.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fFp-fzw6VnFa"
      },
      "outputs": [],
      "source": [
        "# read one file for infer schema\n",
        "pdfs_df = spark.read.format(\"binaryFile\").load(dataset_path).limit(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHHhEzKKVnFb"
      },
      "source": [
        "## Define OCR pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "lpUD5kghVnFb"
      },
      "outputs": [],
      "source": [
        "# Transform binary to image\n",
        "pdf_to_image = PdfToImage()\n",
        "pdf_to_image.setOutputCol(\"image\")\n",
        "\n",
        "# Run OCR for each region\n",
        "ocr = ImageToText()\n",
        "ocr.setInputCol(\"image\")\n",
        "ocr.setOutputCol(\"text\")\n",
        "ocr.setConfidenceThreshold(60)\n",
        "\n",
        "# OCR pipeline\n",
        "pipeline = PipelineModel(stages=[\n",
        "    pdf_to_image,\n",
        "    ocr\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poeHsXI3VnFb"
      },
      "source": [
        "## Define streaming pipeline and start it\n",
        "Note: each start erase previous results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jMohfzzXVnFb"
      },
      "outputs": [],
      "source": [
        "# count of files in one microbatch\n",
        "maxFilesPerTrigger = 4 \n",
        "\n",
        "# read files as stream\n",
        "pdf_stream_df = spark.readStream \\\n",
        ".format(\"binaryFile\") \\\n",
        ".schema(pdfs_df.schema) \\\n",
        ".option(\"maxFilesPerTrigger\", maxFilesPerTrigger) \\\n",
        ".load(dataset_path)\n",
        "\n",
        "# process files using OCR pipeline\n",
        "result = pipeline.transform(pdf_stream_df).withColumn(\"timestamp\", current_timestamp())\n",
        "\n",
        "# store results to memory table\n",
        "query = result.writeStream \\\n",
        " .format('memory') \\\n",
        " .queryName('result') \\\n",
        " .start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eUaWbXgNVnFc"
      },
      "outputs": [],
      "source": [
        "# get progress of streamig job\n",
        "query.lastProgress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ls3CjwnCVnFc"
      },
      "outputs": [],
      "source": [
        "# need to run for stop steraming job\n",
        "query.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqH2GjqOVnFc"
      },
      "source": [
        "## Show results from 'result' table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Nlsn48tVnFc",
        "outputId": "78ad67e3-106c-4a6a-99aa-efd2f280ed9a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# count of processed records (number of processed pages in results)\n",
        "spark.table(\"result\").count() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q04UZA8kVnFd",
        "outputId": "1feb4855-cef4-4139-86ed-09db5230f91c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------+--------------------+--------------------+\n",
            "|           timestamp|pagenum|                path|                text|\n",
            "+--------------------+-------+--------------------+--------------------+\n",
            "|2022-07-20 21:45:...|      0|file:/content/dat...|FOREWORD\\n\\nElect...|\n",
            "|2022-07-20 21:45:...|      0|file:/content/dat...|C nca Document fo...|\n",
            "|2022-07-20 21:56:...|      0|file:/content/dat...|6/13/22, 11:47 AM...|\n",
            "+--------------------+-------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# show results\n",
        "spark.table(\"result\").select(\"timestamp\",\"pagenum\", \"path\", \"text\").show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyPk7dD3VnFd"
      },
      "source": [
        "## Run streaming job for storing results to disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "z36PWzv-VnFd"
      },
      "outputs": [],
      "source": [
        "query = result.select(\"text\").writeStream \\\n",
        " .format('text') \\\n",
        " .option(\"path\", \"results/\") \\\n",
        " .option(\"checkpointLocation\", \"checkpointDir\") \\\n",
        " .start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6IxwNFfHVnFd"
      },
      "outputs": [],
      "source": [
        "# get progress of streamig job\n",
        "query.lastProgress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWQ6rveKVnFe"
      },
      "outputs": [],
      "source": [
        "# need to run for stop steraming job\n",
        "query.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGJa_r51VnFe"
      },
      "source": [
        "## Read results from disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUzerwK9VnFe",
        "outputId": "0723673e-3f41-490c-e5f1-24bdf3b0c20b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------------------------------------------------+\n",
            "|value                                                                                        |\n",
            "+---------------------------------------------------------------------------------------------+\n",
            "|                                                                                             |\n",
            "|                                                                                             |\n",
            "|                                                                                             |\n",
            "|                                                                                             |\n",
            "|                                                                                             |\n",
            "|                                                                                             |\n",
            "|                                                                                             |\n",
            "|                                                                                             |\n",
            "|                                                                                             |\n",
            "|He does report to me that he just had a recent evaluation by Dr, nd he has had allergy       |\n",
            "|fluticasone. He ordered Prilosec. There is one medication that he is not taking because he is|\n",
            "|worked as a school administrator. 3) Married and no children. 4) Li elong nonsmoker,         |\n",
            "|                                                                                             |\n",
            "|                                                                                             |\n",
            "|                                                                                             |\n",
            "|industries. They create ideas and use them in their designs, they stimu-                     |\n",
            "|To encourage this exchange of ideas, ELECTRONIC DESIGN                                       |\n",
            "|                                                                                             |\n",
            "|New York EDWARD E. GRAZDA                                                                    |\n",
            "|Payment Date Tuesday, June 14, 2022                                                          |\n",
            "+---------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "results = spark.read.format(\"text\").load(\"results/*.txt\")\n",
        "results.sample(.1).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BEWgzNPVnFe"
      },
      "source": [
        "## Clean results and checkpoint folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a5ZHZLcVnFe"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "rm -r -f results\n",
        "rm -r -f checkpointDir"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "6.1.SparkOcrStreamingPDF.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}