{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dM_VpZ33OQ2u"
   },
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V52RWZgkKskS"
   },
   "source": [
    "# 6 Context Spell Checker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPuaSBWsDy4U"
   },
   "source": [
    "## Start Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xZ1DPExi95xa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP Version : 4.2.4\n",
      "Spark Session already created, some configs may not take.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-15-180.ec2.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>John-Snow-Labs-Spark-Session ðŸš€ with Jars for: ðŸš€Spark-NLP==4.2.4, ðŸ’ŠSpark-Healthcare==4.2.4, ðŸ•¶Spark-OCR==4.2.4, running on âš¡ PySpark==3.1.2</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f0b224200a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Spark NLP Version :\", sparknlp.version())\n",
    "\n",
    "spark = sparknlp.start()\n",
    "# params =>> gpu=False\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOjVN8NKOQ22"
   },
   "source": [
    "<H1> Noisy Channel Model Spell Checker - Introduction </H1>\n",
    "\n",
    "blogpost : https://medium.com/spark-nlp/applying-context-aware-spell-checking-in-spark-nlp-3c29c46963bc\n",
    "\n",
    "<div>\n",
    "<p><br/>\n",
    "The idea for this annotator is to have a flexible, configurable and \"re-usable by parts\" model.<br/>\n",
    "Flexibility is the ability to accommodate different use cases for spell checking like OCR text, keyboard-input text, ASR text, and general spelling problems due to orthographic errors.<br/>\n",
    "We say this is a configurable annotator, as you can adapt it yourself to different use cases avoiding re-training as much as possible.<br/>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "<b> Spell Checking at three levels: </b>\n",
    "The final ranking of a correction sequence is affected by three things, \n",
    "\n",
    "\n",
    "1. Different correction candidates for each word - __word level__.\n",
    "2. The surrounding text of each word, i.e. it's context - __sentence level__.\n",
    "3. The relative cost of different correction candidates according to the edit operations at the character level it requires - __subword level__.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUCfqQbLOQ23"
   },
   "source": [
    "### Initial Setup\n",
    "As it's usual in Spark-NLP let's start with building a pipeline; a _spell correction pipeline_. We will use a pretrained model from our library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9wK6EnGvOQ24"
   },
   "outputs": [],
   "source": [
    "from sparknlp.common import *\n",
    "from IPython.utils.text import columnize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-cBsZyHaOQ27",
    "outputId": "be0d41c5-09bb-4fd5-af2a-86607af45286"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spellcheck_dl download started this may take some time.\n",
      "Approximate size to download 95.1 MB\n",
      "[ | ]spellcheck_dl download started this may take some time.\n",
      "Approximate size to download 95.1 MB\n",
      "Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ â€” ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:====================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = RecursiveTokenizer()\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"token\")\\\n",
    "    .setPrefixes([\"\\\"\", \"(\", \"[\", \"\\n\"])\\\n",
    "    .setSuffixes([\".\", \",\", \"?\", \")\",\"!\", \"'s\"])\n",
    "\n",
    "spellModel = ContextSpellCheckerModel\\\n",
    "    .pretrained('spellcheck_dl')\\\n",
    "    .setInputCols(\"token\")\\\n",
    "    .setOutputCol(\"checked\")\\\n",
    "    .setErrorThreshold(4.0)\\\n",
    "    .setTradeoff(6.0)\n",
    "\n",
    "finisher = Finisher()\\\n",
    "    .setInputCols(\"checked\")\n",
    "\n",
    "pipeline = Pipeline(stages = [\n",
    "     documentAssembler,\n",
    "     tokenizer,\n",
    "     spellModel,\n",
    "     finisher\n",
    "  ])\n",
    "\n",
    "empty_ds = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "lp = LightPipeline(pipeline.fit(empty_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Hcev_C7OQ3D"
   },
   "source": [
    "Ok!, at this point we have our spell checking pipeline as expected. Let's see what we can do with it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IytF5v0_OQ3E",
    "outputId": "238e70d2-aa33-4558-91fd-b228179b768f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checked': ['Phase',\n",
       "  'allow',\n",
       "  'me',\n",
       "  'to',\n",
       "  'introduce',\n",
       "  'myself',\n",
       "  ',',\n",
       "  'I',\n",
       "  'am',\n",
       "  'a',\n",
       "  'man',\n",
       "  'of',\n",
       "  'wealth',\n",
       "  'and',\n",
       "  'taste']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp.annotate(\"Plaese alliow me tao introdduce myhelf, I am a man of waelth und tiaste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EfgK96HuOQ3K"
   },
   "source": [
    "### Word Level Corrections\n",
    "Continuing with our pretrained model, let's try to see how corrections work at the word level. Each Context Spell Checker model that you can find in Spark-NLP library comes with two sources for word candidates: \n",
    "+ a general vocabulary that is built during training(and remains unmutable during the life of the model), and\n",
    "+ special classes for dealing with special types of words like numbers or dates. These are dynamic, and you can modify them so they adjust better to your data.\n",
    "\n",
    "The general vocabulary is learned during training, and cannot be modified, however, the special classes can be updated after training has happened on a pre-trained model.\n",
    "This means you can modify how existing classes produce corrections, but not the number or type of the classes.\n",
    "Let's see how we can accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xrECOVImOQ3L",
    "outputId": "54c1880d-5327-4950-a639-49b682779579"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(_DATE_,RegexParser)',\n",
       " '(_LOC_,VocabParser)',\n",
       " '(_NUM_,RegexParser)',\n",
       " '(_NAME_,VocabParser)']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's start with a loaded model, and check which classes it has been trained with\n",
    "spellModel.getWordClasses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnXKtN9JOQ3P"
   },
   "source": [
    "We have five classes, of two different types: some are vocabulary based and others are regex based,\n",
    "+ __Vocabulary based classes__ can propose correction candidates from the provided vocabulary, for example a dictionary of names.\n",
    "+ __Regex classes__ are defined by a regular expression, and they can be used to generate correction candidates for things like numbers. Internally, the Spell Checker will enumerate your regular expression and build a fast automaton, not only for recognizing the word(number in this example) as valid and preserve it, but also for generating a correction candidate.\n",
    "Thus the regex should be a finite regex(it must define a finite regular language).\n",
    "\n",
    "Now suppose that you have a new friend from Poland whose name is 'Jowita', let's see how the pretrained Spell Checker does with this name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LyNv27gBO4y4"
   },
   "outputs": [],
   "source": [
    "beautify = lambda annotations: [columnize(sent['checked']) for sent in annotations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j5rqzNm1OQ3P",
    "outputId": "0ec33f2c-6282-452c-98f5-ab71108b079c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We  are  going  to  meet  With  in  the  city  hall  .\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Foreign name without errors\n",
    "sample = 'We are going to meet Jowita in the city hall.'\n",
    "beautify([lp.annotate(sample)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF6roiiQOQ38"
   },
   "source": [
    "## Advanced - the mysterious tradeoff parameter \n",
    "There's a clear tension between two forces here,\n",
    "+ The context information: by which the model wants to change words based on the surrounding words.\n",
    "+ The word information: by which the model wants to preserve as much an input word as possible to avoid destroying the input.\n",
    "\n",
    "Changing words that are in the vocabulary for others that seem more suitable according to the context is one of the most challenging tasks in spell correction. This is because you run into the risk of destroying existing 'good' words.\n",
    "The models that you will find in the Spark-NLP library have already been configured in a way that balances these two forces and produces good results in most of the situations. But your dataset can be different from the one used to train the model.\n",
    "So we encourage the user to play a bit with the hyperparameters, and for you to have an idea on how it can be modified, we're going to see the following example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "trwTZ0YROQ38",
    "outputId": "fd58ea3b-4f8b-4b16-939c-b0b66f967233"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have  you  been  to  the  falls  ?\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 'have you been two the falls?'\n",
    "beautify([lp.annotate(sample)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvp4QocxOQ3-"
   },
   "source": [
    "Here 'two' is clearly wrong, probably a typo, and the model should be able to choose the right correction candidate according to the context. <br/>\n",
    "Every path is scored with a cost, and the higher the cost the less chances for the path being chosen as the final answer.<br/>\n",
    "In order for the model to rely more on the context and less on word information, we have the setTradeoff() method. You can think of the tradeoff as how much a single edition(insert, delete, etc) operation affects the influence of a word when competing inside a path in the graph.<br/>\n",
    "So the lower the tradeoff, the less we care about the edit operations in the word, and the more we care about the word fitting properly into its context. The tradeoff parameter typically ranges between 5 and 25. <br/>\n",
    "Let's see what happens when we relax how much the model cares about individual words in our example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WUpyUecvOQ3_",
    "outputId": "04d00c04-e096-4cc7-cd49-1d0eaf663d42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellModel.getTradeoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5zmQLB_UOQ4C",
    "outputId": "28e95fff-e5dd-4564-9a60-cd609edf7d77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have  you  been  to  the  falls  ?\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's decrease the influence of word-level errors\n",
    "# TODO a nicer way of doing this other than re-creating the pipeline?\n",
    "spellModel.setTradeoff(2.0)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "    documentAssembler,\n",
    "    tokenizer,\n",
    "    spellModel,\n",
    "    finisher\n",
    "  ])\n",
    "\n",
    "empty_ds = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "lp = LightPipeline(pipeline.fit(empty_ds))\n",
    "\n",
    "beautify([lp.annotate(sample)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LD1RZYWCOQ4F"
   },
   "source": [
    "## Advanced - performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-EyNb0HOQ4G"
   },
   "source": [
    "The discussion about performance revolves around _error detection_. The more errors the model detects the more populated is the candidate diagram we showed above[TODO add diagram or convert this into blogpost], and the more alternative paths need to be evaluated. </br>\n",
    "Basically the error detection stage of the model can decide whether a word needs a correction or not; with two reasons for a word to be considered as incorrect, \n",
    "+ The word is OOV: the word is out of the vocabulary.\n",
    "+ The context: the word doesn't fit well within its neighbouring words. \n",
    "The only parameter that we can control at this point is the second one, and we do so with the setErrorThreshold() method that contains a max perplexity above which the word will be considered suspicious and a good candidate for being corrected.</br>\n",
    "The parameter that comes with the pretrained model has been set so you can get both a decent performance and accuracy. For reference, this is how the F-score, and time varies in a sample dataset for different values of the errorThreshold,\n",
    "\n",
    "\n",
    "|fscore |totaltime|threshold|\n",
    "|-------|---------|---------|\n",
    "|52.69  |405s | 8f|\n",
    "|52.43  |357s |10f|\n",
    "|52.25  |279s |12f|\n",
    "|52.14  |234s |14f|\n",
    "\n",
    "You can trade some minor points in accuracy for a nice speedup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Pt7ca87zQaCP"
   },
   "outputs": [],
   "source": [
    "def sparknlp_spell_check(text):\n",
    "\n",
    "  return beautify([lp.annotate(text)])[0].rstrip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "vIFJmn6pPobo",
    "outputId": "5aaf58e5-15b9-4dd4-c275-acd6ee57b32c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I  will  go  to  Philadelphia  tomorrow'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparknlp_spell_check('I will go to Philadelhia tomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "OmCSDVc9Vz6H",
    "outputId": "c0701b85-d82c-4234-efab-ffded195ccd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I  will  go  to  Philadelphia  tomorrow'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparknlp_spell_check('I will go to Philadhelpia tomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "TG-bQtJnVz3Z",
    "outputId": "e96b853c-bf98-42b6-f35a-6be2818507e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I  will  go  to  Philadelphia  tomorrow'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparknlp_spell_check('I will go to Piladelphia tomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "3TmDb-ThVz0t",
    "outputId": "fa967b67-4fd5-4eef-a16c-8a3ed5c8d7af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I  will  go  to  Philadelphia  tomorrow'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparknlp_spell_check('I will go to Philadedlphia tomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "m50x-QYaVzx6",
    "outputId": "48808637-64c7-46b7-f787-338223f753ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I  will  go  to  Philadelphia  tomorrow'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparknlp_spell_check('I will go to Phieladelphia tomorrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ8oCQA4wKMJ"
   },
   "source": [
    "## ContextSpellCheckerApproach\n",
    "\n",
    "Trains a deep-learning based Noisy Channel Model Spell Algorithm.\n",
    "\n",
    "Correction candidates are extracted combining context information and word information.\n",
    "\n",
    "1.   Different correction candidates for each word   **word level**\n",
    "2.   The surrounding text of each word, i.e. itâ€™s context  **sentence level**.\n",
    "3.   The relative cost of different correction candidates according to the edit operations at the character level it requires  **subword level**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7NyXcrNSIr-X"
   },
   "outputs": [],
   "source": [
    "# For this example, we will use the first Sherlock Holmes book as the training dataset.\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols(\"document\") \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "spellChecker = ContextSpellCheckerApproach() \\\n",
    "    .setInputCols(\"token\") \\\n",
    "    .setOutputCol(\"corrected\") \\\n",
    "    .setWordMaxDistance(3) \\\n",
    "    .setBatchSize(24) \\\n",
    "    .setEpochs(8) \\\n",
    "    .setLanguageModelClasses(1650)  # dependant on vocabulary size\n",
    "    # .addVocabClass(\"_NAME_\", names) # Extra classes for correction could be added like this\n",
    "\n",
    "pipeline = Pipeline().setStages([\n",
    "    documentAssembler,\n",
    "    tokenizer,\n",
    "    spellChecker\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "dpaFhDADrZf5"
   },
   "outputs": [],
   "source": [
    "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/holmes.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XUCpkYHtrZdN",
    "outputId": "5731e0c3-01c8-46f4-84a6-59abd72364a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------+\n",
      "|                                                                                                text|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "|THE ADVENTURES OF SHERLOCK HOLMESArthur Conan Doyle Table of contents A Scandal in Bohemia The Re...|\n",
      "+----------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path = \"holmes.txt\"\n",
    "\n",
    "dataset = spark.read.text(path).toDF(\"text\")\n",
    "\n",
    "dataset.show(truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YVpujH03rZak",
    "outputId": "60cf71cb-95c4-44a6-b370-a76048bc862f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 59 ms, sys: 11.6 ms, total: 70.5 ms\n",
      "Wall time: 41.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pipelineModel = pipeline.fit(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "OmmcL2mjsUL5"
   },
   "outputs": [],
   "source": [
    "lp = LightPipeline(pipelineModel)\n",
    "result = lp.annotate(\"Plaese alliow me tao introdduce myhelf, I am a man of waelth und tiaste\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jtz0Ud95s76T",
    "outputId": "5ba68877-66d2-458e-82fb-b81219bc60fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Please',\n",
       " 'allow',\n",
       " 'me',\n",
       " 'to',\n",
       " 'introduce',\n",
       " 'myself',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'a',\n",
       " 'man',\n",
       " 'of',\n",
       " 'wealth',\n",
       " 'and',\n",
       " 'taste']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"corrected\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 520
    },
    "id": "0LIupsUKtTi8",
    "outputId": "0721bb8a-78fd-4cb7-d77f-38b6c61c3ad0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orginal</th>\n",
       "      <th>corrected</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plaese</td>\n",
       "      <td>Please</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alliow</td>\n",
       "      <td>allow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>me</td>\n",
       "      <td>me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tao</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>introdduce</td>\n",
       "      <td>introduce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>myhelf</td>\n",
       "      <td>myself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>am</td>\n",
       "      <td>am</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>man</td>\n",
       "      <td>man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>waelth</td>\n",
       "      <td>wealth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>und</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tiaste</td>\n",
       "      <td>taste</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       orginal  corrected\n",
       "0       Plaese     Please\n",
       "1       alliow      allow\n",
       "2           me         me\n",
       "3          tao         to\n",
       "4   introdduce  introduce\n",
       "5       myhelf     myself\n",
       "6            ,          ,\n",
       "7            I          I\n",
       "8           am         am\n",
       "9            a          a\n",
       "10         man        man\n",
       "11          of         of\n",
       "12      waelth     wealth\n",
       "13         und        and\n",
       "14      tiaste      taste"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(zip(result[\"token\"],result[\"corrected\"]),columns=[\"orginal\",\"corrected\"])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "jsl4.2.3",
   "language": "python",
   "name": "jsl4.2.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
