{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfzsuGYbvEGN"
   },
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rx8ANMi103y2"
   },
   "source": [
    "# **Chunk Mapping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DggoJ4cvwsP"
   },
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YvYsq4B1u8A6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Loading license number 0 from /home/ubuntu/.johnsnowlabs/licenses/license_number_0_for_Spark-Healthcare_Spark-OCR.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/12 16:18:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘Œ Launched \u001b[92mcpu optimized\u001b[39m session with with: ðŸš€Spark-NLP==5.2.2, ðŸ’ŠSpark-Healthcare==5.2.1, ðŸ•¶Spark-OCR==5.1.2, running on âš¡ PySpark==3.4.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-63-84.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>John-Snow-Labs-Spark-Session ðŸš€ with Jars for: ðŸš€Spark-NLP==5.2.2, ðŸ’ŠSpark-Healthcare==5.2.1, ðŸ•¶Spark-OCR==5.1.2, running on âš¡ PySpark==3.4.0</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe4b16f9bb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 39752)\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/usr/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 281, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 253, in poll\n",
      "    if func():\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/accumulators.py\", line 257, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "from johnsnowlabs import nlp, medical\n",
    "\n",
    "spark = start_spark()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7Go2B1RvP0C"
   },
   "source": [
    "# **1- Pretrained Chunk Mapper Models and Pretrained Pipelines**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjnUYOjvbVEc"
   },
   "source": [
    "**<center>MAPPER MODELS**\r\n",
    "\r\n",
    "|index|model|index|model|index|model|\r\n",
    "|-----:|:-----|-----:|:-----|-----:|:-----|\r\n",
    "| 1| [abbreviation_category_mapper](https://nlp.johnsnowlabs.com/2022/11/16/abbreviation_category_mapper_en.html)  | 2| [abbreviation_mapper](https://nlp.johnsnowlabs.com/2022/05/11/abbreviation_mapper_en_3_0.html)  | 3| [cvx_code_mapper](https://nlp.johnsnowlabs.com/2022/10/12/cvx_code_mapper_en.html)  |\r\n",
    "| 4| [cvx_name_mapper](https://nlp.johnsnowlabs.com/2022/10/12/cvx_name_mapper_en.html)  | 5| [drug_action_treatment_mapper](https://nlp.johnsnowlabs.com/2022/03/31/drug_action_treatment_mapper_en_3_0.html)  | 6| [drug_ade_mapper](https://nlp.johnsnowlabs.com/2022/08/23/drug_ade_mapper_en.html)  |\r\n",
    "| 7| [drug_brandname_ndc_mapper](https://nlp.johnsnowlabs.com/2022/05/11/drug_brandname_ndc_mapper_en_3_0.html)  | 8| [drug_category_mapper](https://nlp.johnsnowlabs.com/2022/12/18/drug_category_mapper_en.html)  | 9| [icd10_icd9_mapper](https://nlp.johnsnowlabs.com/2022/09/30/icd10_icd9_mapper_en.html)  |\r\n",
    "| 10| [icd10cm_mapper](https://nlp.johnsnowlabs.com/2022/10/29/icd10cm_mapper_en.html)  | 11| [icd10cm_snomed_mapper](https://nlp.johnsnowlabs.com/2022/06/26/icd10cm_snomed_mapper_en_3_0.html)  | 12| [icd10cm_umls_mapper](https://nlp.johnsnowlabs.com/2022/06/26/icd10cm_umls_mapper_en_3_0.html)  |\r\n",
    "| 13| [icd9_icd10_mapper](https://nlp.johnsnowlabs.com/2022/09/30/icd9_icd10_mapper_en.html)  | 14| [icd9_mapper](https://nlp.johnsnowlabs.com/2022/09/30/icd9_mapper_en.html)  | 15| [icdo_snomed_mapper](https://nlp.johnsnowlabs.com/2022/06/26/icdo_snomed_mapper_en_3_0.html)  |\r\n",
    "| 16| [kegg_disease_mapper](https://nlp.johnsnowlabs.com/2022/11/18/kegg_disease_mapper_en.html)  | 17| [kegg_drug_mapper](https://nlp.johnsnowlabs.com/2022/11/21/kegg_drug_mapper_en.html)  | 18| [mesh_umls_mapper](https://nlp.johnsnowlabs.com/2022/06/26/mesh_umls_mapper_en_3_0.html)  |\r\n",
    "| 19| [ndc_drug_brandname_mapper](https://nlp.johnsnowlabs.com/2023/02/22/ndc_drug_brandname_mapper_en.html)  | 20| [normalized_section_header_mapper](https://nlp.johnsnowlabs.com/2022/06/26/normalized_section_header_mapper_en_3_0.html)  | 21| [rxnorm_action_treatment_mapper](https://nlp.johnsnowlabs.com/2022/05/08/rxnorm_action_treatment_mapper_en_3_0.html)  |\r\n",
    "| 22| [rxnorm_drug_brandname_mapper](https://nlp.johnsnowlabs.com/2023/02/09/rxnorm_drug_brandname_mapper_en.html)  | 23| [rxnorm_mapper](https://nlp.johnsnowlabs.com/2022/06/27/rxnorm_mapper_en_3_0.html)  | 24| [rxnorm_ndc_mapper](https://nlp.johnsnowlabs.com/2022/05/20/rxnorm_ndc_mapper_en_3_0.html)  |\r\n",
    "| 25| [rxnorm_nih_mapper](https://nlp.johnsnowlabs.com/2023/02/23/rxnorm_nih_mapper_en.html)  | 26| [rxnorm_normalized_mapper](https://nlp.johnsnowlabs.com/2022/09/29/rxnorm_normalized_mapper_en.html)  | 27| [rxnorm_umls_mapper](https://nlp.johnsnowlabs.com/2022/06/26/rxnorm_umls_mapper_en_3_0.html)  |\r\n",
    "| 28| [snomed_icd10cm_mapper](https://nlp.johnsnowlabs.com/2022/06/26/snomed_icd10cm_mapper_en_3_0.html)  | 29| [snomed_icdo_mapper](https://nlp.johnsnowlabs.com/2022/06/26/snomed_icdo_mapper_en_3_0.html)  | 30| [snomed_umls_mapper](https://nlp.johnsnowlabs.com/2022/06/27/snomed_umls_mapper_en_3_0.html)  |\r\n",
    "| 31| [umls_clinical_drugs_mapper](https://nlp.johnsnowlabs.com/2022/07/06/umls_clinical_drugs_mapper_en_3_0.html)  | 32| [umls_clinical_findings_mapper](https://nlp.johnsnowlabs.com/2022/07/08/umls_clinical_findings_mapper_en_3_0.html)  | 33| [umls_disease_syndrome_mapper](https://nlp.johnsnowlabs.com/2022/07/11/umls_disease_syndrome_mapper_en_3_0.html)  |\r\n",
    "| 34| [umls_drug_substance_mapper](https://nlp.johnsnowlabs.com/2022/07/11/umls_drug_substance_mapper_en_3_0.html)  | 35| [umls_major_concepts_mapper](https://nlp.johnsnowlabs.com/2022/07/11/umls_major_concepts_mapper_en_3_0.html)  | 36| []()|\r\n",
    "\r\n",
    "**You can find all these models and more [NLP Models Hub](https://nlp.johnsnowlabs.com/models?q=Chunk+Mapping&edition=Spark+NLP+for+Healthcare)**\r\n",
    "\r\n",
    "<br>\r\n",
    "\r\n",
    "**<center>PRETRAINED MAPPER PIPELINES**\r\n",
    "\r\n",
    "|index|model|\r\n",
    "|-----:|:-----|\r\n",
    "| 1| [icd10_icd9_mapping](https://nlp.johnsnowlabs.com/2022/09/30/icd10_icd9_mapping_en.html)  |\r\n",
    "| 2| [icd10cm_snomed_mapping](https://nlp.johnsnowlabs.com/2022/06/27/icd10cm_snomed_mapping_en_3_0.html)  |\r\n",
    "| 3| [icd10cm_umls_mapping](https://nlp.johnsnowlabs.com/2021/05/04/icd10cm_umls_mapping_en.html)  |\r\n",
    "| 4| [icdo_snomed_mapping](https://nlp.johnsnowlabs.com/2022/06/27/icdo_snomed_mapping_en_3_0.html)  |\r\n",
    "| 5| [mesh_umls_mapping](https://nlp.johnsnowlabs.com/2021/05/04/mesh_umls_mapping_en.html)  |\r\n",
    "| 6| [rxnorm_mesh_mapping](https://nlp.johnsnowlabs.com/2021/05/04/rxnorm_mesh_mapping_en.html)  |\r\n",
    "| 7| [rxnorm_ndc_mapping](https://nlp.johnsnowlabs.com/2022/06/27/rxnorm_ndc_mapping_en_3_0.html)  |\r\n",
    "| 8| [rxnorm_umls_mapping](https://nlp.johnsnowlabs.com/2021/05/04/rxnorm_umls_mapping_en.html)  |\r\n",
    "| 9| [snomed_icd10cm_mapping](https://nlp.johnsnowlabs.com/2021/05/02/snomed_icd10cm_mapping_en.html)  |\r\n",
    "| 10| [snomed_icdo_mapping](https://nlp.johnsnowlabs.com/2022/06/27/snomed_icdo_mapping_en_3_0.html)  |\r\n",
    "| 11| [snomed_umls_mapping](https://nlp.johnsnowlabs.com/2021/05/04/snomed_umls_mapping_en.html)  |\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "You can check [Healthcare Code Mapping Notebook](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/healthcare-nlp/06.1.Code_Mapping_Pipelines.ipynb) for the examples of pretrained mapper pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tun6FxO1V443"
   },
   "source": [
    "## **1.1- Drug Action Treatment Mapper**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbpIBoXl94ii"
   },
   "source": [
    "Pretrained `drug_action_treatment_mapper` model maps drugs with their corresponding `action` and `treatment` through `ChunkMapperModel()` annotator. <br/>\n",
    "\n",
    "\n",
    "**Action** of drug refers to the function of a drug in various body systems. <br/>\n",
    "**Treatment** refers to which disease the drug is used to treat. \n",
    "\n",
    "We can choose which option we want to use by setting `setRel()` parameter of `ChunkMapperModel()`\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rioaJXFnS-q2"
   },
   "source": [
    "We will create a pipeline consisting `bert_token_classifier_drug_development_trials` ner model to extract ner chunk as well as `ChunkMapperModel()`. <br/>\n",
    " Also, we will set the `.setRel()` parameter with `action` and see the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LDE0GMq8xYin",
    "outputId": "e8a088c0-bdea-4a08-8fcc-2dcb7eede255"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_token_classifier_drug_development_trials download started this may take some time.\n",
      "[ | ]bert_token_classifier_drug_development_trials download started this may take some time.\n",
      "Approximate size to download 382.1 MB\n",
      "[ / ]Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/jars/spark-core_2.12-3.4.0.jar) to field java.lang.ref.Reference.referent\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n",
      "drug_action_treatment_mapper download started this may take some time.\n",
      "[ | ]drug_action_treatment_mapper download started this may take some time.\n",
      "Approximate size to download 8.1 MB\n",
      "Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ â€” ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "#ChunkMapper Pipeline\n",
    "document_assembler = nlp.DocumentAssembler()\\\n",
    "      .setInputCol('text')\\\n",
    "      .setOutputCol('document')\n",
    "\n",
    "sentence_detector = nlp.SentenceDetector()\\\n",
    "      .setInputCols([\"document\"])\\\n",
    "      .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = nlp.Tokenizer()\\\n",
    "      .setInputCols(\"sentence\")\\\n",
    "      .setOutputCol(\"token\")\n",
    "\n",
    "ner =  medical.BertForTokenClassification.pretrained(\"bert_token_classifier_drug_development_trials\", \"en\", \"clinical/models\")\\\n",
    "      .setInputCols(\"token\",\"sentence\")\\\n",
    "      .setOutputCol(\"ner\")\n",
    "\n",
    "nerconverter = medical.NerConverter()\\\n",
    "      .setInputCols(\"sentence\", \"token\", \"ner\")\\\n",
    "      .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "#drug_action_treatment_mapper with \"action\" mappings\n",
    "chunkerMapper= medical.ChunkMapperModel().pretrained(\"drug_action_treatment_mapper\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols([\"ner_chunk\"])\\\n",
    "    .setOutputCol(\"action_mappings\")\\\n",
    "    .setRels([\"action\"])\n",
    "    \n",
    "\n",
    "pipeline = nlp.Pipeline().setStages([document_assembler,\n",
    "                                 sentence_detector,\n",
    "                                 tokenizer,\n",
    "                                 ner, \n",
    "                                 nerconverter, \n",
    "                                 chunkerMapper])\n",
    "\n",
    "text = [[\"\"\"The patient was female and patient of Dr. X. and she was given Dermovate, Aspagin\"\"\"]]\n",
    "\n",
    "\n",
    "test_data = spark.createDataFrame(text).toDF(\"text\")\n",
    "\n",
    "res = pipeline.fit(test_data).transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0jetN39Hzj63"
   },
   "source": [
    "Chunks detected by ner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EDwLFe-QxYdr",
    "outputId": "e85d1ed6-15f3-44cc-c215-d8baeedffb84"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|chunks   |\n",
      "+---------+\n",
      "|Dermovate|\n",
      "|Aspagin  |\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "res.select(F.explode('ner_chunk.result').alias(\"chunks\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCLMKbAOTqTl"
   },
   "source": [
    "Checking mapping results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qAFTxkiaXNTs",
    "outputId": "ae101e1a-d51c-403a-e25a-cdd659ea1d96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|result                        |\n",
      "+------------------------------+\n",
      "|[anti-inflammatory, analgesic]|\n",
      "+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.select(\"action_mappings.result\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yFcIWEa7BSXV",
    "outputId": "d5b17b11-52c5-4ae7-9208-b9e32c751484",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|metadata                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[{chunk -> 0, __trained__ -> Dermovate, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.9997595, all_k_resolutions -> anti-inflammatory:::corticosteroids::: dermatological preparations:::very strong, target_text -> Dermovate, ner_source -> ner_chunk, ops -> 0.0, all_relations -> corticosteroids::: dermatological preparations:::very strong, entity -> Dermovate, resolved_text -> anti-inflammatory, distance -> 0.0, sentence -> 0, __relation_name__ -> action}, {chunk -> 1, __trained__ -> Aspagin, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.99668664, all_k_resolutions -> analgesic:::anti-inflammatory:::antipyretic, target_text -> Aspagin, ner_source -> ner_chunk, ops -> 0.0, all_relations -> anti-inflammatory:::antipyretic, entity -> Aspagin, resolved_text -> analgesic, distance -> 0.0, sentence -> 0, __relation_name__ -> action}]|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.selectExpr(\"action_mappings.metadata\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T25yMbYkVVKI"
   },
   "source": [
    "As you see above under the ***metadata*** column, if exist, we can see all the relations for each chunk. <br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j3kYyv-OX-QM",
    "outputId": "20adfd0d-69de-444d-a376-12d3afac4608"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+------------------------------------------------------------+\n",
      "|ner_chunk|mapping_result   |all_relations                                               |\n",
      "+---------+-----------------+------------------------------------------------------------+\n",
      "|Dermovate|anti-inflammatory|corticosteroids::: dermatological preparations:::very strong|\n",
      "|Aspagin  |analgesic        |anti-inflammatory:::antipyretic                             |\n",
      "+---------+-----------------+------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.select(F.explode(F.arrays_zip(res.ner_chunk.result, res.action_mappings.result, res.action_mappings.metadata)).alias(\"col\"))\\\n",
    "    .select(F.expr(\"col['0']\").alias(\"ner_chunk\"),\n",
    "            F.expr(\"col['1']\").alias(\"mapping_result\"),\n",
    "            F.expr(\"col['2']['all_relations']\").alias(\"all_relations\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsTnjIm90IT7"
   },
   "source": [
    "Now, let's set the `.setRel(\"treatment\")` and see the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2UQk9dIKzp_v",
    "outputId": "b36d99b4-83b2-483d-a6fa-834564fbe479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug_action_treatment_mapper download started this may take some time.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "#drug_action_treatment_mapper with \"treatment\" mappings\n",
    "chunkerMapper= medical.ChunkMapperModel().pretrained(\"drug_action_treatment_mapper\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols([\"ner_chunk\"])\\\n",
    "    .setOutputCol(\"action_mappings\")\\\n",
    "    .setRels([\"treatment\"])\n",
    "\n",
    "pipeline = nlp.Pipeline().setStages([document_assembler,\n",
    "                                 sentence_detector,\n",
    "                                 tokenizer,\n",
    "                                 ner, \n",
    "                                 nerconverter, \n",
    "                                 chunkerMapper])\n",
    "\n",
    "text = [[\"\"\"The patient was female and patient of Dr. X. and she was given Dermovate, Aspagin\"\"\"]]\n",
    "\n",
    "test_data = spark.createDataFrame(text).toDF(\"text\")\n",
    "\n",
    "res = pipeline.fit(test_data).transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WIa_Lioxzp5V",
    "outputId": "88cf7db7-f893-46a2-c5e2-bf6da2158e3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|chunks   |\n",
      "+---------+\n",
      "|Dermovate|\n",
      "|Aspagin  |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.select(F.explode('ner_chunk.result').alias(\"chunks\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J6YxxPlOSGAF",
    "outputId": "88daa7bc-a497-4285-a1a5-0595dc2be060",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|metadata                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[{chunk -> 0, __trained__ -> Dermovate, relation -> treatment, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.9997595, all_k_resolutions -> lupus:::discoid lupus erythematosus:::empeines:::psoriasis:::eczema, target_text -> Dermovate, ner_source -> ner_chunk, ops -> 0.0, all_relations -> discoid lupus erythematosus:::empeines:::psoriasis:::eczema, entity -> Dermovate, resolved_text -> lupus, distance -> 0.0, sentence -> 0, __relation_name__ -> treatment}, {chunk -> 1, __trained__ -> Aspagin, relation -> treatment, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.99668664, all_k_resolutions -> ankylosing spondylitis:::arthralgia:::pain:::bursitis:::headache:::migraine:::myositis:::neuralgia:::osteoarthritis:::gout:::rheumatoid arthritis:::spondylitis:::spondyloarthritis:::tendinitis:::tenosynovitis:::crush injury:::golfer's elbow, target_text -> Aspagin, ner_source -> ner_chunk, ops -> 0.0, all_relations -> arthralgia:::pain:::bursitis:::headache:::migraine:::myositis:::neuralgia:::osteoarthritis:::gout:::rheumatoid arthritis:::spondylitis:::spondyloarthritis:::tendinitis:::tenosynovitis:::crush injury:::golfer's elbow, entity -> Aspagin, resolved_text -> ankylosing spondylitis, distance -> 0.0, sentence -> 0, __relation_name__ -> treatment}]|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.selectExpr(\"action_mappings.metadata\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFpNGIVzWNl1"
   },
   "source": [
    "Here are the ***treatment*** mappings and all relations under the metadata column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gftZgABhSFtq",
    "outputId": "e57260a2-0159-4dbf-e2c3-4740dce342da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ner_chunk|mapping_result        |all_relations                                                                                                                                                                                                          |\n",
      "+---------+----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Dermovate|lupus                 |discoid lupus erythematosus:::empeines:::psoriasis:::eczema                                                                                                                                                            |\n",
      "|Aspagin  |ankylosing spondylitis|arthralgia:::pain:::bursitis:::headache:::migraine:::myositis:::neuralgia:::osteoarthritis:::gout:::rheumatoid arthritis:::spondylitis:::spondyloarthritis:::tendinitis:::tenosynovitis:::crush injury:::golfer's elbow|\n",
      "+---------+----------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.select(F.explode(F.arrays_zip(res.ner_chunk.result, \n",
    "                                  res.action_mappings.result, \n",
    "                                  res.action_mappings.metadata)).alias(\"col\"))\\\n",
    "    .select(F.expr(\"col['0']\").alias(\"ner_chunk\"),\n",
    "            F.expr(\"col['1']\").alias(\"mapping_result\"),\n",
    "            F.expr(\"col['2']['all_relations']\").alias(\"all_relations\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQMHFp1TvRVX"
   },
   "source": [
    "## **1.2- Section Header Normalizer Mapper**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tue53lJXvfdK"
   },
   "source": [
    "We have `normalized_section_header_mapper` model that normalizes the section headers in clinical notes. It returns two levels of normalization called `level_1` and `level_2`. <br/>\n",
    "\n",
    "**level_1** refers to the most comprehensive \"section header\" for the corresponding chunk while **level_2** refers to the second comprehensive one.\n",
    "\n",
    "Let's create a piepline with `normalized_section_header_mapper` and see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "26Ih_X1JvSV_",
    "outputId": "5d14605a-34bf-468e-965f-aeef3e3e5538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[ | ]embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n",
      "ner_jsl_slim download started this may take some time.\n",
      "[ | ]ner_jsl_slim download started this may take some time.\n",
      "Approximate size to download 14.4 MB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n",
      "normalized_section_header_mapper download started this may take some time.\n",
      "[ | ]normalized_section_header_mapper download started this may take some time.\n",
      "Approximate size to download 18.8 KB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = nlp.DocumentAssembler()\\\n",
    "       .setInputCol('text')\\\n",
    "       .setOutputCol('document')\n",
    "\n",
    "sentence_detector =  nlp.SentenceDetector()\\\n",
    "       .setInputCols([\"document\"])\\\n",
    "       .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer =  nlp.Tokenizer()\\\n",
    "      .setInputCols(\"sentence\")\\\n",
    "      .setOutputCol(\"token\")\n",
    "\n",
    "embeddings =  nlp.WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\",\"clinical/models\")\\\n",
    "      .setInputCols([\"sentence\", \"token\"])\\\n",
    "      .setOutputCol(\"word_embeddings\")\n",
    "\n",
    "clinical_ner = medical.NerModel.pretrained(\"ner_jsl_slim\", \"en\", \"clinical/models\")\\\n",
    "      .setInputCols([\"sentence\",\"token\", \"word_embeddings\"])\\\n",
    "      .setOutputCol(\"ner\")\n",
    "\n",
    "ner_converter =  medical.NerConverter()\\\n",
    "      .setInputCols([\"sentence\", \"token\", \"ner\"])\\\n",
    "      .setOutputCol(\"ner_chunk\")\\\n",
    "      .setWhiteList([\"Header\"])\n",
    "\n",
    "chunkerMapper = medical.ChunkMapperModel.pretrained(\"normalized_section_header_mapper\", \"en\", \"clinical/models\") \\\n",
    "       .setInputCols(\"ner_chunk\")\\\n",
    "       .setOutputCol(\"mappings\")\\\n",
    "       .setRels([\"level_1\"]) #or level_2\n",
    "\n",
    "pipeline = nlp.Pipeline().setStages([document_assembler,\n",
    "                                sentence_detector,\n",
    "                                tokenizer, \n",
    "                                embeddings,\n",
    "                                clinical_ner, \n",
    "                                ner_converter, \n",
    "                                chunkerMapper])\n",
    "\n",
    "sentences = [\n",
    "    [\"\"\"ADMISSION DIAGNOSIS Right pleural effusion and suspected malignant mesothelioma.\n",
    "        PRINCIPAL DIAGNOSIS Right pleural effusion, suspected malignant mesothelioma.\n",
    "        GENERAL REVIEW Right pleural effusion, firm nodules, diffuse scattered throughout the right pleura and diaphragmatic surface.\n",
    "    \"\"\"]]\n",
    "\n",
    "test_data = spark.createDataFrame(sentences).toDF(\"text\")\n",
    "res = pipeline.fit(test_data).transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fTn2JPWyxOk"
   },
   "source": [
    "Checking the headers detected by ner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QfgyrLomxXv_",
    "outputId": "345757b8-4b6b-43ff-a0ca-aff024adbd65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|chunks             |\n",
      "+-------------------+\n",
      "|ADMISSION DIAGNOSIS|\n",
      "|PRINCIPAL DIAGNOSIS|\n",
      "|GENERAL REVIEW     |\n",
      "+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "res.select(F.explode('ner_chunk.result').alias(\"chunks\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pv_4VLWzFw0"
   },
   "source": [
    "Checking mapping results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LYZdPVHwxXr9",
    "outputId": "5c43f991-96f8-415d-a13d-4cf09265acd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|result                             |\n",
      "+-----------------------------------+\n",
      "|[DIAGNOSIS, DIAGNOSIS, REVIEW TYPE]|\n",
      "+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.select(\"mappings.result\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cqgpO0ANvSOu",
    "outputId": "d7c86a33-102c-444b-d491-ed9c1aaf3953"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+\n",
      "|ner_chunk          |mapping_result|\n",
      "+-------------------+--------------+\n",
      "|ADMISSION DIAGNOSIS|DIAGNOSIS     |\n",
      "|PRINCIPAL DIAGNOSIS|DIAGNOSIS     |\n",
      "|GENERAL REVIEW     |REVIEW TYPE   |\n",
      "+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.select(F.explode(F.arrays_zip(res.ner_chunk.result, res.mappings.result)).alias(\"col\"))\\\n",
    "    .select(F.expr(\"col['0']\").alias(\"ner_chunk\"),\n",
    "            F.expr(\"col['1']\").alias(\"mapping_result\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5OHH23MzjDl"
   },
   "source": [
    "As you see above, we can see the \"level_1\" based normalized version of each section header."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIUlv3Ka_iwy"
   },
   "source": [
    "## **1.3- Drug Brand Name NDC Mapper**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfYOYPF0ALiH"
   },
   "source": [
    "We have `drug_brandname_ndc_mapper` model that maps drug brand names to corresponding National Drug Codes (NDC). Product NDCs for each strength are returned in result and metadata. <br/>\n",
    "\n",
    "It has one relation type called `Strength_NDC`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsdyOYV1DHdU"
   },
   "source": [
    "Let's create a pipeline with `drug_brandname_ndc_mapper` and see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6wGtPSK4_ikU",
    "outputId": "66f9f19f-c9b6-49bc-bb80-41187e2af869"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug_brandname_ndc_mapper download started this may take some time.\n",
      "[ | ]drug_brandname_ndc_mapper download started this may take some time.\n",
      "Approximate size to download 2.9 MB\n",
      "Download done! Loading the resource.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler =  nlp.DocumentAssembler()\\\n",
    "      .setInputCol(\"text\")\\\n",
    "      .setOutputCol(\"chunk\")\n",
    "\n",
    "chunkerMapper = medical.ChunkMapperModel.pretrained(\"drug_brandname_ndc_mapper\", \"en\", \"clinical/models\")\\\n",
    "      .setInputCols([\"chunk\"])\\\n",
    "      .setOutputCol(\"ndc\")\\\n",
    "      .setRels([\"Strength_NDC\"])\n",
    "\n",
    "pipeline = nlp.Pipeline().setStages([document_assembler,\n",
    "                                 chunkerMapper])  \n",
    "\n",
    "model = pipeline.fit(spark.createDataFrame([['']]).toDF('text')) \n",
    "\n",
    "lp = nlp.LightPipeline(model)\n",
    "\n",
    "res = lp.fullAnnotate('ZYVOX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9fdHJi2zDSww"
   },
   "source": [
    "Checking mapping results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "98Najib1Aoba",
    "outputId": "b2619369-6354-4136-8333-0b1b3e68db39"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand_Name</th>\n",
       "      <th>Strenth_NDC</th>\n",
       "      <th>Other_NDC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZYVOX</td>\n",
       "      <td>600 mg/300mL | 0009-4992</td>\n",
       "      <td>600 mg/300mL | 66298-7807:::600 mg/300mL | 0009-7807:::600 mg/300mL | 0009-5140:::100 mg/5mL | 0009-5136:::600 mg/1 | 70518-1226:::600 mg/300mL | 66298-5140:::200 mg/100mL | 66298-5137:::200 mg/100mL | 0009-5137:::600 mg/1 | 0009-5138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Brand_Name               Strenth_NDC  \\\n",
       "0      ZYVOX  600 mg/300mL | 0009-4992   \n",
       "\n",
       "                                                                                                                                                                                                                                    Other_NDC  \n",
       "0  600 mg/300mL | 66298-7807:::600 mg/300mL | 0009-7807:::600 mg/300mL | 0009-5140:::100 mg/5mL | 0009-5136:::600 mg/1 | 70518-1226:::600 mg/300mL | 66298-5140:::200 mg/100mL | 66298-5137:::200 mg/100mL | 0009-5137:::600 mg/1 | 0009-5138  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunks = []\n",
    "mappings = []\n",
    "all_re= []\n",
    "\n",
    "for m, n in list(zip(res[0]['chunk'], res[0][\"ndc\"])):\n",
    "        \n",
    "    chunks.append(m.result)\n",
    "    mappings.append(n.result) \n",
    "    all_re.append(n.metadata[\"all_relations\"])\n",
    "    \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df = pd.DataFrame({'Brand_Name':chunks, 'Strenth_NDC': mappings, 'Other_NDC':all_re})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ntq2uH7lDkAq"
   },
   "source": [
    "As you see, we can see corresponding \"NDC\" mappings of each \"brand names\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBKhGobLESSP"
   },
   "source": [
    "## **1.4- RxNorm NDC Mapper**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODX-0_GxEbFF"
   },
   "source": [
    "We have `rxnorm_ndc_mapper` model that maps RxNorm and RxNorm Extension codes with corresponding National Drug Codes (NDC).\n",
    "\n",
    "It has two relation types that can be defined in `setRel()` parameter; **Product NDC** and **Package NDC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRNTaLYFFET9"
   },
   "source": [
    "Let's create a pipeline with `rxnorm_ndc_mapper` model by setting the  relation as `setRel(\"Product NDC\")` and see the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bR1XvJzKEM3n",
    "outputId": "66842769-a855-4c30-a534-b5b03b7220e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbiobert_base_cased_mli download started this may take some time.\n",
      "Approximate size to download 384.3 MB\n",
      "[ | ]sbiobert_base_cased_mli download started this may take some time.\n",
      "Approximate size to download 384.3 MB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n",
      "sbiobertresolve_rxnorm_augmented download started this may take some time.\n",
      "[ | ]sbiobertresolve_rxnorm_augmented download started this may take some time.\n",
      "Approximate size to download 1.2 GB\n",
      "Download done! Loading the resource.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n",
      "rxnorm_ndc_mapper download started this may take some time.\n",
      "[ | ]rxnorm_ndc_mapper download started this may take some time.\n",
      "Approximate size to download 1.9 MB\n",
      "Download done! Loading the resource.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler =  nlp.DocumentAssembler()\\\n",
    "      .setInputCol('text')\\\n",
    "      .setOutputCol('ner_chunk')\n",
    "\n",
    "sbert_embedder =  nlp.BertSentenceEmbeddings.pretrained('sbiobert_base_cased_mli', 'en','clinical/models')\\\n",
    "      .setInputCols([\"ner_chunk\"])\\\n",
    "      .setOutputCol(\"sentence_embeddings\")\\\n",
    "      .setCaseSensitive(False)\n",
    "    \n",
    "rxnorm_resolver = medical.SentenceEntityResolverModel.pretrained(\"sbiobertresolve_rxnorm_augmented\",\"en\", \"clinical/models\") \\\n",
    "      .setInputCols([\"sentence_embeddings\"]) \\\n",
    "      .setOutputCol(\"rxnorm_code\")\\\n",
    "      .setDistanceFunction(\"EUCLIDEAN\")\n",
    "\n",
    "chunkerMapper_product = medical.ChunkMapperModel.pretrained(\"rxnorm_ndc_mapper\", \"en\", \"clinical/models\")\\\n",
    "      .setInputCols([\"rxnorm_code\"])\\\n",
    "      .setOutputCol(\"Product NDC\")\\\n",
    "      .setRels([\"Product NDC\"]) #or Package NDC\n",
    "\n",
    "pipeline = nlp.Pipeline().setStages([document_assembler,\n",
    "                                 sbert_embedder,\n",
    "                                 rxnorm_resolver,\n",
    "                                 chunkerMapper_product\n",
    "                                 ])\n",
    "\n",
    "model = pipeline.fit(spark.createDataFrame([['']]).toDF('text')) \n",
    "\n",
    "lp = nlp.LightPipeline(model)\n",
    "\n",
    "result = lp.fullAnnotate('macadamia nut 100 MG/ML')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IeGDZ0-vSRp6"
   },
   "source": [
    "Checking the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "cf1w7hszGwc9",
    "outputId": "fda984db-386f-4d98-ccdb-1f20e2e99f00"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ner_chunk</th>\n",
       "      <th>rxnorm_code</th>\n",
       "      <th>Product NDC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>macadamia nut 100 MG/ML</td>\n",
       "      <td>212433</td>\n",
       "      <td>00187-1474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ner_chunk rxnorm_code Product NDC\n",
       "0  macadamia nut 100 MG/ML      212433  00187-1474"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunks = []\n",
    "rxnorm_code = []\n",
    "product= []\n",
    "\n",
    "\n",
    "for m, n, j in list(zip(result[0]['ner_chunk'], result[0][\"rxnorm_code\"], result[0][\"Product NDC\"])):\n",
    "\n",
    "    chunks.append(m.result)\n",
    "    rxnorm_code.append(n.result) \n",
    "    product.append(j.result)\n",
    "    \n",
    "df = pd.DataFrame({'ner_chunk':chunks,\n",
    "                   'rxnorm_code': rxnorm_code,\n",
    "                   'Product NDC': product})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvGK4Z7PSns8"
   },
   "source": [
    "As you see, we can see corresponding \"Product NDC\" mappings of each \"RxNorm codes\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ci6QTFAaNvon"
   },
   "source": [
    "## **1.5- RxNorm Action Treatment Mapper**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2L7A0ruN4JD"
   },
   "source": [
    "We have `rxnorm_action_treatment_mapper` model that maps RxNorm and RxNorm Extension codes with their corresponding action and treatment. It has two relation types that can be defined in `setRel()` parameter; <br/>\n",
    "\n",
    "**Action** of drug refers to the function of a drug in various body systems. <br/>\n",
    "**Treatment** refers to which disease the drug is used to treat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kX0S24f7cA3H"
   },
   "source": [
    "Let's create a pipeline and see how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NndvMrLFNmjk",
    "outputId": "febe485f-3ce3-4161-f25b-222b59d05d75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sbiobert_base_cased_mli download started this may take some time.\n",
      "Approximate size to download 384.3 MB\n",
      "[OK!]\n",
      "sbiobertresolve_rxnorm_augmented download started this may take some time.\n",
      "[OK!]\n",
      "rxnorm_action_treatment_mapper download started this may take some time.\n",
      "[ | ]rxnorm_action_treatment_mapper download started this may take some time.\n",
      "Approximate size to download 20.1 MB\n",
      "Download done! Loading the resource.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = nlp.DocumentAssembler()\\\n",
    "      .setInputCol('text')\\\n",
    "      .setOutputCol('ner_chunk')\n",
    "\n",
    "sbert_embedder = nlp.BertSentenceEmbeddings.pretrained('sbiobert_base_cased_mli', 'en','clinical/models')\\\n",
    "      .setInputCols([\"ner_chunk\"])\\\n",
    "      .setOutputCol(\"sentence_embeddings\")\\\n",
    "      .setCaseSensitive(False)\n",
    "    \n",
    "rxnorm_resolver = medical.SentenceEntityResolverModel.pretrained(\"sbiobertresolve_rxnorm_augmented\",\"en\", \"clinical/models\") \\\n",
    "      .setInputCols([\"sentence_embeddings\"]) \\\n",
    "      .setOutputCol(\"rxnorm_code\")\\\n",
    "      .setDistanceFunction(\"EUCLIDEAN\")\n",
    "\n",
    "chunkerMapper_action =  medical.ChunkMapperModel.pretrained(\"rxnorm_action_treatment_mapper\", \"en\", \"clinical/models\")\\\n",
    "      .setInputCols([\"rxnorm_code\"])\\\n",
    "      .setOutputCol(\"Action\")\\\n",
    "      .setRels([\"action\"]) #or treatment\n",
    "\n",
    "pipeline = nlp.Pipeline().setStages([document_assembler,\n",
    "                                 sbert_embedder,\n",
    "                                 rxnorm_resolver,\n",
    "                                 chunkerMapper_action\n",
    "                                 ])\n",
    "\n",
    "model = pipeline.fit(spark.createDataFrame([['']]).toDF('text')) \n",
    "\n",
    "lp = nlp.LightPipeline(model)\n",
    "\n",
    "res = lp.fullAnnotate('Zonalon 50 mg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFRzialBWYmM"
   },
   "source": [
    "Checking the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 111
    },
    "id": "c0EPkC-IPHfz",
    "outputId": "60645093-cb07-49c5-c608-59156b8e658e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ner_chunk</th>\n",
       "      <th>rxnorm_code</th>\n",
       "      <th>Action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zonalon 50 mg</td>\n",
       "      <td>103971</td>\n",
       "      <td>Analgesic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ner_chunk rxnorm_code     Action\n",
       "0  Zonalon 50 mg      103971  Analgesic"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = []\n",
    "rxnorm_code = []\n",
    "action= []\n",
    "\n",
    "\n",
    "for m, n, j in list(zip(res[0]['ner_chunk'], res[0][\"rxnorm_code\"], res[0][\"Action\"])):\n",
    "\n",
    "    chunks.append(m.result)\n",
    "    rxnorm_code.append(n.result) \n",
    "    action.append(j.result)\n",
    "    \n",
    "df = pd.DataFrame({'ner_chunk':chunks,\n",
    "                   'rxnorm_code': rxnorm_code,\n",
    "                   'Action': action})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lOTEXeeWSyW"
   },
   "source": [
    "As you see, we can see corresponding \"Action\" mappings of each \"RxNorm codes\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTKofeLuWj42"
   },
   "source": [
    "## **1.6- Abbreviation Mapper**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwYOGi8DXCp8"
   },
   "source": [
    "We have `abbreviation_mapper` model that maps abbreviations and acronyms of medical regulatory activities with their definitions. <br/> It has one relation type that can be defined in `setRel(\"definition\")` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHW9JxRjXZFy"
   },
   "source": [
    "Let's create a pipeline consisting `ner_abbreviation_clinical` to extract abbreviations from text, and feed the `abbreviation_mapper` with it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I6TcC8MvPHZE",
    "outputId": "d4893b02-6283-440c-807e-1b1a3ac8b727"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n",
      "ner_abbreviation_clinical download started this may take some time.\n",
      "[ | ]ner_abbreviation_clinical download started this may take some time.\n",
      "Approximate size to download 13.9 MB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n",
      "abbreviation_mapper download started this may take some time.\n",
      "[ | ]abbreviation_mapper download started this may take some time.\n",
      "Approximate size to download 214.7 KB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = nlp.DocumentAssembler()\\\n",
    "      .setInputCol('text')\\\n",
    "      .setOutputCol('document')\n",
    "\n",
    "sentence_detector = nlp.SentenceDetector()\\\n",
    "      .setInputCols([\"document\"])\\\n",
    "      .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = nlp.Tokenizer()\\\n",
    "      .setInputCols(\"sentence\")\\\n",
    "      .setOutputCol(\"token\")\n",
    "\n",
    "word_embeddings = nlp.WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "      .setInputCols([\"sentence\", \"token\"])\\\n",
    "      .setOutputCol(\"embeddings\")\n",
    "\n",
    "#NER model to detect abbreviations in the text\n",
    "abbr_ner =  medical.NerModel.pretrained('ner_abbreviation_clinical', 'en', 'clinical/models') \\\n",
    "      .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "      .setOutputCol(\"abbr_ner\")\n",
    "\n",
    "abbr_converter = medical.NerConverter() \\\n",
    "      .setInputCols([\"sentence\", \"token\", \"abbr_ner\"]) \\\n",
    "      .setOutputCol(\"abbr_ner_chunk\")\\\n",
    "\n",
    "chunkerMapper =  medical.ChunkMapperModel.pretrained(\"abbreviation_mapper\", \"en\", \"clinical/models\")\\\n",
    "      .setInputCols([\"abbr_ner_chunk\"])\\\n",
    "      .setOutputCol(\"mappings\")\\\n",
    "      .setRels([\"definition\"]) \n",
    "\n",
    "pipeline = nlp.Pipeline().setStages([document_assembler,\n",
    "                                 sentence_detector,\n",
    "                                 tokenizer, \n",
    "                                 word_embeddings,\n",
    "                                 abbr_ner, \n",
    "                                 abbr_converter, \n",
    "                                 chunkerMapper])\n",
    "\n",
    "text = [\"\"\"Gravid with estimated fetal weight of 6-6/12 pounds.\n",
    "           LABORATORY DATA: Laboratory tests include a CBC which is normal. \n",
    "           HIV: Negative. One-Hour Glucose: 117. Group B strep has not been done as yet.\"\"\"]\n",
    "\n",
    "test_data = spark.createDataFrame([text]).toDF(\"text\")\n",
    "\n",
    "model = pipeline.fit(test_data)\n",
    "res= model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icqK31njXpbk"
   },
   "source": [
    "Checking the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15gReqinY1Y3",
    "outputId": "69eb1260-fd30-4cd5-c582-731a25c7f253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|    result|\n",
      "+----------+\n",
      "|[CBC, HIV]|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#abbreviations extracted by ner model\n",
    "res.select(\"abbr_ner_chunk.result\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fH_PG8ntXePz",
    "outputId": "82c4e79a-7b6a-4721-b82f-ee1afbc7b913"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------------+\n",
      "|Abbreviation|Definition                  |\n",
      "+------------+----------------------------+\n",
      "|CBC         |complete blood count        |\n",
      "|HIV         |human immunodeficiency virus|\n",
      "+------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.select(F.explode(F.arrays_zip(res.abbr_ner_chunk.result, res.mappings.result)).alias(\"col\"))\\\n",
    "    .select(F.expr(\"col['0']\").alias(\"Abbreviation\"),\n",
    "            F.expr(\"col['1']\").alias(\"Definition\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hB4S4r_eYNUc"
   },
   "source": [
    "As you see, we can see corresponding \"definition\" mappings of each \"abbreviation\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tTo9cdLBfyG9"
   },
   "source": [
    "# **2- Creating a Mapper Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SW9g1bjtgFQM"
   },
   "source": [
    "There is a `ChunkMapperApproach()` to create your own mapper model. <br/>\n",
    "\n",
    "This receives an `ner_chunk` and a Json with a mapping of ner entities and relations, and returns the `ner_chunk` augmented with the relations from the Json ontology. <br/> We give the path of json file to the `setDictionary()` parameter.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFDHG9EVgayJ"
   },
   "source": [
    "Let's create an example Json, then create a drug mapper model. This model will match the given drug name (only \"metformin\" for our example) with correpsonding action and treatment.  \n",
    "\n",
    "The format of json file should be like following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ekp8gF4wgjLs"
   },
   "outputs": [],
   "source": [
    "data_set= {\n",
    "  \"mappings\": [\n",
    "    {\n",
    "      \"key\": \"metformin\",\n",
    "      \"relations\": [\n",
    "        {\n",
    "          \"key\": \"action\",\n",
    "          \"values\" : [\"hypoglycemic\", \"Drugs Used In Diabetes\"]\n",
    "        },\n",
    "        {\n",
    "          \"key\": \"treatment\",\n",
    "          \"values\" : [\"diabetes\", \"t2dm\"]\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "with open('sample_drug.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_set, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqjQ7rj_hxch"
   },
   "source": [
    "By using `setRel()` parameter, we tell the model which type of mapping we want. In our case, if we want from our model to return **action** mapping, we set the parameter as `setRel(\"action\")`,  we set as `setRel(\"treatment\")` for **treatment**\n",
    "\n",
    "Let's create a pipeline and see it in action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PbuDGXzKf2r7",
    "outputId": "69f1d55e-2ed3-4eb0-c515-1895594f0610"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n",
      "ner_posology_small download started this may take some time.\n",
      "[ | ]ner_posology_small download started this may take some time.\n",
      "Approximate size to download 13.9 MB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = nlp.DocumentAssembler()\\\n",
    "      .setInputCol('text')\\\n",
    "      .setOutputCol('document')\n",
    "\n",
    "sentence_detector = nlp.SentenceDetector()\\\n",
    "      .setInputCols([\"document\"])\\\n",
    "      .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = nlp.Tokenizer()\\\n",
    "      .setInputCols(\"sentence\")\\\n",
    "      .setOutputCol(\"token\")\n",
    "\n",
    "word_embeddings = nlp.WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "      .setInputCols([\"sentence\", \"token\"])\\\n",
    "      .setOutputCol(\"embeddings\")\n",
    "\n",
    "#NER model to detect drug in the text\n",
    "clinical_ner =  medical.NerModel.pretrained(\"ner_posology_small\",\"en\",\"clinical/models\")\\\n",
    "      .setInputCols([\"sentence\",\"token\",\"embeddings\"])\\\n",
    "      .setOutputCol(\"ner\")\\\n",
    "      .setLabelCasing(\"upper\")\n",
    " \n",
    "ner_converter = medical.NerConverter()\\\n",
    "      .setInputCols([\"sentence\", \"token\", \"ner\"])\\\n",
    "      .setOutputCol(\"ner_chunk\")\\\n",
    "      .setWhiteList([\"DRUG\"])\n",
    "\n",
    "chunkerMapper =  medical.ChunkMapperApproach()\\\n",
    "      .setInputCols([\"ner_chunk\"])\\\n",
    "      .setOutputCol(\"mappings\")\\\n",
    "      .setDictionary(\"./sample_drug.json\")\\\n",
    "      .setRels([\"action\"]) #or treatment\n",
    "\n",
    "pipeline = nlp.Pipeline().setStages([document_assembler,\n",
    "                                 sentence_detector,\n",
    "                                 tokenizer, \n",
    "                                 word_embeddings,\n",
    "                                 clinical_ner, \n",
    "                                 ner_converter, \n",
    "                                 chunkerMapper])\n",
    "\n",
    "text = [\"The patient was given 1 unit of metformin daily.\"]\n",
    "\n",
    "test_data = spark.createDataFrame([text]).toDF(\"text\")\n",
    "\n",
    "model = pipeline.fit(test_data)\n",
    "res= model.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nDPk6Uhtrwbf",
    "outputId": "dbd07a9c-fbe1-49aa-b3a9-9880b2fb1ef3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- text: string (nullable = true)\n",
      " |-- document: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- sentence: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- token: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- embeddings: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- ner: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- ner_chunk: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      " |-- mappings: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- annotatorType: string (nullable = true)\n",
      " |    |    |-- begin: integer (nullable = false)\n",
      " |    |    |-- end: integer (nullable = false)\n",
      " |    |    |-- result: string (nullable = true)\n",
      " |    |    |-- metadata: map (nullable = true)\n",
      " |    |    |    |-- key: string\n",
      " |    |    |    |-- value: string (valueContainsNull = true)\n",
      " |    |    |-- embeddings: array (nullable = true)\n",
      " |    |    |    |-- element: float (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWe7PtYLrQT7"
   },
   "source": [
    "Checking the ner result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bG41lCKTf89Y",
    "outputId": "2c8c9834-a1c8-4d9c-9026-200c24c78ca7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|chunks   |\n",
      "+---------+\n",
      "|metformin|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "res.select(F.explode('ner_chunk.result').alias(\"chunks\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AsGy0OXshpV"
   },
   "source": [
    "Checking the mapper result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTRODkcHhzgw",
    "outputId": "51aadfd6-74aa-4dd1-dd95-65f52474bcb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|metadata                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[{chunk -> 0, __trained__ -> metformin, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.9994, all_k_resolutions -> hypoglycemic:::Drugs Used In Diabetes, target_text -> metformin, ner_source -> ner_chunk, ops -> 0.0, all_relations -> Drugs Used In Diabetes, entity -> metformin, resolved_text -> hypoglycemic, distance -> 0.0, sentence -> 0, __relation_name__ -> action}]|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.selectExpr(\"mappings.metadata\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DpWUhBkmDeve",
    "outputId": "da121470-40eb-444b-f5c5-349d21a4b27e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+----------------------+\n",
      "|ner_chunk|mapping_result|all_relations         |\n",
      "+---------+--------------+----------------------+\n",
      "|metformin|hypoglycemic  |Drugs Used In Diabetes|\n",
      "+---------+--------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.select(F.explode(F.arrays_zip(res.ner_chunk.result, \n",
    "                                  res.mappings.result, \n",
    "                                  res.mappings.metadata)).alias(\"col\"))\\\n",
    "    .select(F.expr(\"col['0']\").alias(\"ner_chunk\"),\n",
    "            F.expr(\"col['1']\").alias(\"mapping_result\"),\n",
    "            F.expr(\"col['2']['all_relations']\").alias(\"all_relations\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hdWLMiss7uT"
   },
   "source": [
    "As you see, the model that we created with `ChunkMapperApproach()` succesfully mapped \"metformin\". Under the metadata, we can see all relations that we defined in Json. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJPH_FaOtqpP"
   },
   "source": [
    "### **2.1- Save the model to disk** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5SHrUZcu2iR"
   },
   "source": [
    "Now, we will save our model and use it with `ChunkMapperModel()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EaNF5kWUs46N",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.stages[-1].write().save(\"models/drug_mapper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEBFO85yuw07"
   },
   "source": [
    "Using the saved model. This time we will check 'treatment' mappings results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9xdx4s-MuutS",
    "outputId": "22c05e16-1f17-4fdc-8671-bfbf366aeeb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n",
      "ner_posology_small download started this may take some time.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = nlp.DocumentAssembler()\\\n",
    "      .setInputCol('text')\\\n",
    "      .setOutputCol('document')\n",
    "\n",
    "sentence_detector = nlp.SentenceDetector()\\\n",
    "      .setInputCols([\"document\"])\\\n",
    "      .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = nlp.Tokenizer()\\\n",
    "      .setInputCols(\"sentence\")\\\n",
    "      .setOutputCol(\"token\")\n",
    "\n",
    "word_embeddings = nlp.WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "      .setInputCols([\"sentence\", \"token\"])\\\n",
    "      .setOutputCol(\"embeddings\")\n",
    "\n",
    "#NER model to detect drug in the text\n",
    "clinical_ner =  medical.NerModel.pretrained(\"ner_posology_small\",\"en\",\"clinical/models\")\\\n",
    "\t    .setInputCols([\"sentence\",\"token\",\"embeddings\"])\\\n",
    "\t    .setOutputCol(\"ner\")\\\n",
    "      .setLabelCasing(\"upper\")\n",
    " \n",
    "ner_converter = medical.NerConverter()\\\n",
    "      .setInputCols([\"sentence\", \"token\", \"ner\"])\\\n",
    "      .setOutputCol(\"ner_chunk\")\\\n",
    "      .setWhiteList([\"DRUG\"])\n",
    "\n",
    "chunkerMapper =  medical.ChunkMapperModel.load(\"./models/drug_mapper\")\\\n",
    "      .setInputCols([\"ner_chunk\"])\\\n",
    "      .setOutputCol(\"mappings\")\\\n",
    "      .setRels([\"treatment\"]) \n",
    "\n",
    "pipeline = nlp.Pipeline().setStages([document_assembler,\n",
    "                                 sentence_detector,\n",
    "                                 tokenizer, \n",
    "                                 word_embeddings,\n",
    "                                 clinical_ner, \n",
    "                                 ner_converter, \n",
    "                                 chunkerMapper])\n",
    "\n",
    "text = [\"The patient was given 1 unit of metformin daily.\"]\n",
    "\n",
    "test_data = spark.createDataFrame([text]).toDF(\"text\")\n",
    "\n",
    "model = pipeline.fit(test_data)\n",
    "res= model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pl3Wbxe1uugR",
    "outputId": "91190d03-c0b3-4ced-c622-d7d64484928b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|metadata                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[{chunk -> 0, __trained__ -> metformin, relation -> treatment, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.9994, all_k_resolutions -> diabetes:::t2dm, target_text -> metformin, ner_source -> ner_chunk, ops -> 0.0, all_relations -> t2dm, entity -> metformin, resolved_text -> diabetes, distance -> 0.0, sentence -> 0, __relation_name__ -> treatment}]|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.selectExpr(\"mappings.metadata\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1-DCeMsjDnw0",
    "outputId": "b656f0f7-cb03-41c3-9007-a1c894afe517"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+-------------+\n",
      "|ner_chunk|mapping_result|all_relations|\n",
      "+---------+--------------+-------------+\n",
      "|metformin|diabetes      |t2dm         |\n",
      "+---------+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.select(F.explode(F.arrays_zip(res.ner_chunk.result, res.mappings.result, res.mappings.metadata)).alias(\"col\"))\\\n",
    "    .select(F.expr(\"col['0']\").alias(\"ner_chunk\"),\n",
    "            F.expr(\"col['1']\").alias(\"mapping_result\"),\n",
    "            F.expr(\"col['2']['all_relations']\").alias(\"all_relations\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJW7n4Resju8"
   },
   "source": [
    "As you see above, we created our own drug mapper model successfully. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3SlO9WCfu79"
   },
   "source": [
    "### **2.2- Create a Model with Upper Cased or Lower Cased**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqbPFVfAf7Je"
   },
   "source": [
    "We can set the case status of `ChunkMapperApproach` while creating a model by using `setLowerCase()` parameter.\n",
    "\n",
    "Let's create a new mapping dictionary and see how it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "XaN7B3YJiTut"
   },
   "outputs": [],
   "source": [
    "data_set= {\n",
    "    \"mappings\": [\n",
    "        {\n",
    "            \"key\": \"Warfarina lusa\",\n",
    "            \"relations\": [\n",
    "                {\n",
    "                    \"key\": \"action\",\n",
    "                    \"values\": [\n",
    "                        \"Analgesic\",\n",
    "                        \"Antipyretic\"\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"key\": \"treatment\",\n",
    "                    \"values\": [\n",
    "                        \"diabetes\",\n",
    "                        \"t2dm\"\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('mappings.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_set, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "DBxQ1kJfmEA_"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "        [\"\"\"The patient was given Warfarina Lusa and amlodipine 10 MG.The patient was given Aspagin, coumadin 5 mg, coumadin, and he has metamorfin\"\"\"]\n",
    "    ]\n",
    "\n",
    "\n",
    "test_data = spark.createDataFrame(sentences).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fUlSk0y9mnyC"
   },
   "source": [
    "**`setLowerCase(True)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwTkmZUcfnDB",
    "outputId": "d18cfe54-c2ca-42f5-c078-72b0e6f3ccbb",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|col                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{labeled_dependency, 22, 35, Analgesic, {chunk -> 0, __trained__ -> Warfarina lusa, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66565, all_k_resolutions -> Analgesic:::Antipyretic, target_text -> Warfarina Lusa, ner_source -> ner_chunk, ops -> 0.0, all_relations -> Antipyretic, entity -> Warfarina Lusa, resolved_text -> Analgesic, distance -> 0.07142857142857142, sentence -> 0, __relation_name__ -> action}, []}|\n",
      "|{labeled_dependency, 41, 50, NONE, {chunk -> 1, confidence -> 0.9999, ner_source -> ner_chunk, entity -> amlodipine, sentence -> 0}, []}                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|{labeled_dependency, 80, 86, NONE, {chunk -> 2, confidence -> 0.9905, ner_source -> ner_chunk, entity -> Aspagin, sentence -> 0}, []}                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|{labeled_dependency, 89, 96, NONE, {chunk -> 3, confidence -> 0.9997, ner_source -> ner_chunk, entity -> coumadin, sentence -> 0}, []}                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|{labeled_dependency, 104, 111, NONE, {chunk -> 4, confidence -> 0.9994, ner_source -> ner_chunk, entity -> coumadin, sentence -> 0}, []}                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|{labeled_dependency, 125, 134, NONE, {chunk -> 5, confidence -> 0.9989, ner_source -> ner_chunk, entity -> metamorfin, sentence -> 0}, []}                                                                                                                                                                                                                                                                                                                                                |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunkerMapper = medical.ChunkMapperApproach() \\\n",
    "        .setInputCols([\"ner_chunk\"]) \\\n",
    "        .setOutputCol(\"mappings\") \\\n",
    "        .setDictionary(\"mappings.json\") \\\n",
    "        .setRels([\"action\"]) \\\n",
    "        .setLowerCase(True) \\\n",
    "\n",
    "pipeline = nlp.Pipeline().setStages([document_assembler,\n",
    "                                 sentence_detector,\n",
    "                                 tokenizer, \n",
    "                                 word_embeddings,\n",
    "                                 clinical_ner, \n",
    "                                 ner_converter, \n",
    "                                 chunkerMapper])\n",
    "\n",
    "\n",
    "result_df = pipeline.fit(test_data).transform(test_data)\n",
    "result_df.selectExpr(\"explode(mappings)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhByE9vSm_to"
   },
   "source": [
    "\"Warfarina lusa\" is in lower case in the source json file, and in upper case(Warfarina Lusa) in our example training sentence. We trained that model in lower case, the model mapped the entity even though our training sentence is uppercased. <br/>\n",
    "\n",
    "Let's check with `setLowerCase(False)` and see the difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pv3UQrGZfm-I",
    "outputId": "43a336b2-a7c5-452c-b8a7-139e22ccd7e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|col                                                                                                                                          |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{labeled_dependency, 22, 35, NONE, {chunk -> 0, confidence -> 0.66565, ner_source -> ner_chunk, entity -> Warfarina Lusa, sentence -> 0}, []}|\n",
      "|{labeled_dependency, 41, 50, NONE, {chunk -> 1, confidence -> 0.9999, ner_source -> ner_chunk, entity -> amlodipine, sentence -> 0}, []}     |\n",
      "|{labeled_dependency, 80, 86, NONE, {chunk -> 2, confidence -> 0.9905, ner_source -> ner_chunk, entity -> Aspagin, sentence -> 0}, []}        |\n",
      "|{labeled_dependency, 89, 96, NONE, {chunk -> 3, confidence -> 0.9997, ner_source -> ner_chunk, entity -> coumadin, sentence -> 0}, []}       |\n",
      "|{labeled_dependency, 104, 111, NONE, {chunk -> 4, confidence -> 0.9994, ner_source -> ner_chunk, entity -> coumadin, sentence -> 0}, []}     |\n",
      "|{labeled_dependency, 125, 134, NONE, {chunk -> 5, confidence -> 0.9989, ner_source -> ner_chunk, entity -> metamorfin, sentence -> 0}, []}   |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunkerMapper =  medical.ChunkMapperApproach() \\\n",
    "        .setInputCols([\"ner_chunk\"]) \\\n",
    "        .setOutputCol(\"mappings\") \\\n",
    "        .setDictionary(\"mappings.json\") \\\n",
    "        .setRels([\"action\"]) \\\n",
    "        .setLowerCase(False) \\\n",
    "\n",
    "pipeline = nlp.Pipeline().setStages([document_assembler,\n",
    "                                 sentence_detector,\n",
    "                                 tokenizer, \n",
    "                                 word_embeddings,\n",
    "                                 clinical_ner, \n",
    "                                 ner_converter, \n",
    "                                 chunkerMapper])\n",
    "\n",
    "\n",
    "result_df = pipeline.fit(test_data).transform(test_data)\n",
    "result_df.selectExpr(\"explode(mappings)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ooQvQ2YJp1ae"
   },
   "source": [
    "As you see, our model couldn't map the given uppercased \"Warfarine Lura\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5iORj5g_rAV7"
   },
   "source": [
    "### **2.3- Selecting Multiple Relations** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp4fqhZPrWG-"
   },
   "source": [
    "We can select multiple relations for the same chunk with the `setRels()` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38I9ft6BfmxY",
    "outputId": "4d899a0a-1939-49ef-f65c-53d2ae9b24c1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|col                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{labeled_dependency, 22, 35, Analgesic, {chunk -> 0, __trained__ -> Warfarina lusa, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66565, all_k_resolutions -> Analgesic:::Antipyretic, target_text -> Warfarina Lusa, ner_source -> ner_chunk, ops -> 0.0, all_relations -> Antipyretic, entity -> Warfarina Lusa, resolved_text -> Analgesic, distance -> 0.07142857142857142, sentence -> 0, __relation_name__ -> action}, []}|\n",
      "|{labeled_dependency, 22, 35, diabetes, {chunk -> 0, __trained__ -> Warfarina lusa, relation -> treatment, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66565, all_k_resolutions -> diabetes:::t2dm, target_text -> Warfarina Lusa, ner_source -> ner_chunk, ops -> 0.0, all_relations -> t2dm, entity -> Warfarina Lusa, resolved_text -> diabetes, distance -> 0.07142857142857142, sentence -> 0, __relation_name__ -> treatment}, []}           |\n",
      "|{labeled_dependency, 41, 50, NONE, {chunk -> 1, confidence -> 0.9999, ner_source -> ner_chunk, entity -> amlodipine, sentence -> 0}, []}                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|{labeled_dependency, 80, 86, NONE, {chunk -> 2, confidence -> 0.9905, ner_source -> ner_chunk, entity -> Aspagin, sentence -> 0}, []}                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|{labeled_dependency, 89, 96, NONE, {chunk -> 3, confidence -> 0.9997, ner_source -> ner_chunk, entity -> coumadin, sentence -> 0}, []}                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|{labeled_dependency, 104, 111, NONE, {chunk -> 4, confidence -> 0.9994, ner_source -> ner_chunk, entity -> coumadin, sentence -> 0}, []}                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|{labeled_dependency, 125, 134, NONE, {chunk -> 5, confidence -> 0.9989, ner_source -> ner_chunk, entity -> metamorfin, sentence -> 0}, []}                                                                                                                                                                                                                                                                                                                                                |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunkerMapper =  medical.ChunkMapperApproach() \\\n",
    "        .setInputCols([\"ner_chunk\"]) \\\n",
    "        .setOutputCol(\"mappings\") \\\n",
    "        .setDictionary(\"mappings.json\") \\\n",
    "        .setLowerCase(True) \\\n",
    "        .setRels([\"action\", \"treatment\"])\n",
    "\n",
    "pipeline = nlp.Pipeline().setStages([document_assembler,\n",
    "                                 sentence_detector,\n",
    "                                 tokenizer, \n",
    "                                 word_embeddings,\n",
    "                                 clinical_ner, \n",
    "                                 ner_converter, \n",
    "                                 chunkerMapper])\n",
    "\n",
    "\n",
    "result_df = pipeline.fit(test_data).transform(test_data)\n",
    "result_df.selectExpr(\"explode(mappings)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LU7XurIxrz4T"
   },
   "source": [
    "As you see, we are able to see all the relations(action, treatment) at the same time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYgl8fvAsELm"
   },
   "source": [
    "### **2.4- Filtering Multi-token Chunks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1-8izXXsnDe"
   },
   "source": [
    "If the chunk includes multi-tokens splitted by a whitespace, we can filter that chunk by using `setAllowMultiTokenChunk()` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_yzlVT23sKvA",
    "outputId": "009512c3-783c-45f4-bcc7-2b65af830184"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|col                                                                                                                                          |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{labeled_dependency, 22, 35, NONE, {chunk -> 0, confidence -> 0.66565, ner_source -> ner_chunk, entity -> Warfarina Lusa, sentence -> 0}, []}|\n",
      "|{labeled_dependency, 41, 50, NONE, {chunk -> 1, confidence -> 0.9999, ner_source -> ner_chunk, entity -> amlodipine, sentence -> 0}, []}     |\n",
      "|{labeled_dependency, 80, 86, NONE, {chunk -> 2, confidence -> 0.9905, ner_source -> ner_chunk, entity -> Aspagin, sentence -> 0}, []}        |\n",
      "|{labeled_dependency, 89, 96, NONE, {chunk -> 3, confidence -> 0.9997, ner_source -> ner_chunk, entity -> coumadin, sentence -> 0}, []}       |\n",
      "|{labeled_dependency, 104, 111, NONE, {chunk -> 4, confidence -> 0.9994, ner_source -> ner_chunk, entity -> coumadin, sentence -> 0}, []}     |\n",
      "|{labeled_dependency, 125, 134, NONE, {chunk -> 5, confidence -> 0.9989, ner_source -> ner_chunk, entity -> metamorfin, sentence -> 0}, []}   |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunkerMapper =  medical.ChunkMapperApproach() \\\n",
    "        .setInputCols([\"ner_chunk\"]) \\\n",
    "        .setOutputCol(\"mappings\") \\\n",
    "        .setDictionary(\"mappings.json\") \\\n",
    "        .setLowerCase(True) \\\n",
    "        .setRels([\"action\", \"treatment\"]) \\\n",
    "        .setAllowMultiTokenChunk(False)\n",
    "\n",
    "pipeline = nlp.Pipeline().setStages([document_assembler,\n",
    "                                 sentence_detector,\n",
    "                                 tokenizer, \n",
    "                                 word_embeddings,\n",
    "                                 clinical_ner, \n",
    "                                 ner_converter, \n",
    "                                 chunkerMapper])\n",
    "\n",
    "\n",
    "result_df = pipeline.fit(test_data).transform(test_data)\n",
    "result_df.selectExpr(\"explode(mappings)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkYb6byStDTA"
   },
   "source": [
    "The chunk \"Warfarina Lusa\" is a multi-token. Therefore, our mapper model skip that entity. <br/>\n",
    "So, let's set `.setAllowMultiTokenChunk(True)` and see the difference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9pTV_9BLtifw",
    "outputId": "b5f0f4e8-bbc2-4cec-da1b-bf6bbe34793f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|col                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{labeled_dependency, 22, 35, Analgesic, {chunk -> 0, __trained__ -> Warfarina lusa, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66565, all_k_resolutions -> Analgesic:::Antipyretic, target_text -> Warfarina Lusa, ner_source -> ner_chunk, ops -> 0.0, all_relations -> Antipyretic, entity -> Warfarina Lusa, resolved_text -> Analgesic, distance -> 0.07142857142857142, sentence -> 0, __relation_name__ -> action}, []}|\n",
      "|{labeled_dependency, 22, 35, diabetes, {chunk -> 0, __trained__ -> Warfarina lusa, relation -> treatment, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66565, all_k_resolutions -> diabetes:::t2dm, target_text -> Warfarina Lusa, ner_source -> ner_chunk, ops -> 0.0, all_relations -> t2dm, entity -> Warfarina Lusa, resolved_text -> diabetes, distance -> 0.07142857142857142, sentence -> 0, __relation_name__ -> treatment}, []}           |\n",
      "|{labeled_dependency, 41, 50, NONE, {chunk -> 1, confidence -> 0.9999, ner_source -> ner_chunk, entity -> amlodipine, sentence -> 0}, []}                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|{labeled_dependency, 80, 86, NONE, {chunk -> 2, confidence -> 0.9905, ner_source -> ner_chunk, entity -> Aspagin, sentence -> 0}, []}                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|{labeled_dependency, 89, 96, NONE, {chunk -> 3, confidence -> 0.9997, ner_source -> ner_chunk, entity -> coumadin, sentence -> 0}, []}                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|{labeled_dependency, 104, 111, NONE, {chunk -> 4, confidence -> 0.9994, ner_source -> ner_chunk, entity -> coumadin, sentence -> 0}, []}                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|{labeled_dependency, 125, 134, NONE, {chunk -> 5, confidence -> 0.9989, ner_source -> ner_chunk, entity -> metamorfin, sentence -> 0}, []}                                                                                                                                                                                                                                                                                                                                                |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunkerMapper =  medical.ChunkMapperApproach() \\\n",
    "        .setInputCols([\"ner_chunk\"]) \\\n",
    "        .setOutputCol(\"mappings\") \\\n",
    "        .setDictionary(\"mappings.json\") \\\n",
    "        .setLowerCase(True) \\\n",
    "        .setRels([\"action\", \"treatment\"]) \\\n",
    "        .setAllowMultiTokenChunk(True)\n",
    "\n",
    "pipeline = nlp.Pipeline().setStages([document_assembler,\n",
    "                                 sentence_detector,\n",
    "                                 tokenizer, \n",
    "                                 word_embeddings,\n",
    "                                 clinical_ner, \n",
    "                                 ner_converter, \n",
    "                                 chunkerMapper])\n",
    "\n",
    "\n",
    "result_df = pipeline.fit(test_data).transform(test_data)\n",
    "result_df.selectExpr(\"explode(mappings)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5- Lexical Fuzzy Matching Options in the ChunkMapper annotator\n",
    "There are multiple options to achieve fuzzy matching using the ChunkMapper annotation:\n",
    "- Partial Token NGram Fingerprinting: Specially useful to combine two frequent usecases; when there are noisy non informative tokens at the beginning / end of the chunk and the order of the chunk is not absolutely relevant. i.e. stomach acute pain --> acute pain stomach ; metformin 100 mg --> metformin.\n",
    "- Char NGram Fingerprinting: Specially useful in usecases that involve typos or different spacing patterns for chunks. i.e. head ache / ache head --> headache ; metformini / metformoni / metformni --> metformin\n",
    "- Fuzzy Distance (Slow): Specially useful when the mapping can be defined in terms of edit distance thresholds using functions like char based like Levenshtein, Hamming, LongestCommonSubsequence or token based like Cosine, Jaccard.\n",
    "\n",
    "The mapping logic will be run in the previous order also ordering by longest key inside each option as an intuitive way to minimize false positives.\n",
    "\n",
    "For more information please visit the followng links:  \n",
    "https://en.wikipedia.org/wiki/Fingerprint_(computing)  \n",
    "https://openrefine.org/docs/technical-reference/clustering-in-depth  \n",
    "https://commons.apache.org/proper/commons-text/apidocs/org/apache/commons/text/similarity/package-summary.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set_mappings = [\n",
    "        {\n",
    "            \"key\": \"Warfarina lusa\",\n",
    "            \"relations\": [\n",
    "                {\n",
    "                    \"key\": \"action\",\n",
    "                    \"values\": [\n",
    "                        \"Analgesic\",\n",
    "                        \"Antipyretic\"\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"key\": \"treatment\",\n",
    "                    \"values\": [\n",
    "                        \"diabetes\",\n",
    "                        \"t2dm\"\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"amlodipine\",\n",
    "            \"relations\": [\n",
    "                {\n",
    "                    \"key\": \"action\",\n",
    "                    \"values\": [\n",
    "                        \"Calcium Ions Inhibitor\"\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"key\": \"treatment\",\n",
    "                    \"values\": [\n",
    "                        \"hypertension\"\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"coumadin\",\n",
    "            \"relations\": [\n",
    "                {\n",
    "                    \"key\": \"action\",\n",
    "                    \"values\": [\n",
    "                        \"Coagulation Inhibitor\"\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"key\": \"treatment\",\n",
    "                    \"values\": [\n",
    "                        \"hypertension\"\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"aspagin\",\n",
    "            \"relations\": [\n",
    "                {\n",
    "                    \"key\": \"action\",\n",
    "                    \"values\": [\n",
    "                        \"Cycooxygenase Inhibitor\"\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"key\": \"treatment\",\n",
    "                    \"values\": [\n",
    "                        \"arthritis\"\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"key\": \"metformin\",\n",
    "            \"relations\": [\n",
    "                {\n",
    "                  \"key\": \"action\",\n",
    "                  \"values\" : [\"hypoglycemic\", \"Drugs Used In Diabetes\"]\n",
    "                },\n",
    "                {\n",
    "                  \"key\": \"treatment\",\n",
    "                  \"values\" : [\"diabetes\", \"t2dm\"]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different mapping sizes to test ChunkMappers sensitivity in terms of speed and efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keys to test speed and efficiency\n",
    "extra_keys = {\n",
    "    \"s500\": [{\"key\": f\"short key {i}\", \"relations\": [\n",
    "                {\n",
    "                    \"key\": \"any\",\n",
    "                    \"values\": [\n",
    "                        \"anyvalue\",\n",
    "                        \"anyvalue\"\n",
    "                    ]\n",
    "                }]} for i in range(500)],\n",
    "    \"s5000\": [{\"key\": f\"short key {i}\", \"relations\": [\n",
    "                {\n",
    "                    \"key\": \"any\",\n",
    "                    \"values\": [\n",
    "                        \"anyvalue\",\n",
    "                        \"anyvalue\"\n",
    "                    ]\n",
    "                }]} for i in range(5000)],\n",
    "    \"l5000\": [{\"key\": f\"a bit longer key {i}\", \"relations\": [\n",
    "                {\n",
    "                    \"key\": \"any\",\n",
    "                    \"values\": [\n",
    "                        \"anyvalue\",\n",
    "                        \"anyvalue\"\n",
    "                    ]\n",
    "                }]} for i in range(5000)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "for c, extra_mappings in extra_keys.items():\n",
    "    with open(f'mappings_{c}.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump({'mappings': data_set_mappings + extra_mappings}, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "        [\"\"\"The patient was given Lusa Warfarina 5mg and amlodipine 10 MG.The patient was given Aspaginaspa, coumadin 5 mg, coumadin, and he has metamorfin\"\"\"]\n",
    "    ]\n",
    "\n",
    "test_data = spark.createDataFrame(sentences).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Posology for longer and more illustrative chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = nlp.DocumentAssembler()\\\n",
    "      .setInputCol('text')\\\n",
    "      .setOutputCol('document')\n",
    "\n",
    "sentence_detector = nlp.SentenceDetector()\\\n",
    "      .setInputCols([\"document\"])\\\n",
    "      .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = nlp.Tokenizer()\\\n",
    "      .setInputCols(\"sentence\")\\\n",
    "      .setOutputCol(\"token\")\n",
    "\n",
    "word_embeddings = nlp.WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\",\"clinical/models\")\\\n",
    "      .setInputCols([\"sentence\", \"token\"])\\\n",
    "      .setOutputCol(\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_posology_greedy download started this may take some time.\n",
      "[ | ]ner_posology_greedy download started this may take some time.\n",
      "Approximate size to download 13.9 MB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|chunk                                                                                                                              |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{chunk, 22, 39, Lusa Warfarina 5mg, {chunk -> 0, confidence -> 0.8111, ner_source -> ner_chunk, entity -> DRUG, sentence -> 0}, []}|\n",
      "|{chunk, 45, 57, amlodipine 10, {chunk -> 1, confidence -> 0.66709995, ner_source -> ner_chunk, entity -> DRUG, sentence -> 0}, []} |\n",
      "|{chunk, 84, 94, Aspaginaspa, {chunk -> 2, confidence -> 0.9827, ner_source -> ner_chunk, entity -> DRUG, sentence -> 0}, []}       |\n",
      "|{chunk, 97, 109, coumadin 5 mg, {chunk -> 3, confidence -> 0.7287, ner_source -> ner_chunk, entity -> DRUG, sentence -> 0}, []}    |\n",
      "|{chunk, 112, 119, coumadin, {chunk -> 4, confidence -> 0.9969, ner_source -> ner_chunk, entity -> DRUG, sentence -> 0}, []}        |\n",
      "|{chunk, 133, 142, metamorfin, {chunk -> 5, confidence -> 0.9666, ner_source -> ner_chunk, entity -> DRUG, sentence -> 0}, []}      |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#NER model to detect drug in the text\n",
    "clinical_ner = medical.NerModel.pretrained(\"ner_posology_greedy\",\"en\",\"clinical/models\")\\\n",
    "        .setInputCols([\"sentence\",\"token\",\"embeddings\"])\\\n",
    "        .setOutputCol(\"ner\")\\\n",
    "        .setLabelCasing(\"upper\")\n",
    "\n",
    "ner_converter = medical.NerConverter()\\\n",
    "      .setInputCols(\"sentence\", \"token\", \"ner\")\\\n",
    "      .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "pipeline = nlp.Pipeline().setStages([document_assembler,\n",
    "                                 sentence_detector,\n",
    "                                 tokenizer, \n",
    "                                 word_embeddings,\n",
    "                                 clinical_ner, \n",
    "                                 ner_converter])\n",
    "cached_df = pipeline.fit(test_data).transform(test_data).cache()\n",
    "cached_df.selectExpr(\"explode(ner_chunk) as chunk\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example with just token fingerprinting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|col                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{labeled_dependency, 22, 39, Analgesic, {chunk -> 0, __trained__ -> Warfarina lusa, relation -> action, all_k_distances -> 0.0:::0.0, confidence -> 0.8111, all_k_resolutions -> Analgesic:::Antipyretic, target_text -> Lusa Warfarina 5mg, ner_source -> ner_chunk, ops -> -1.0, all_relations -> Antipyretic, entity -> Lusa Warfarina 5mg, resolved_text -> Analgesic, sentence -> 0, __relation_name__ -> action}, []}                                                                              |\n",
      "|{labeled_dependency, 22, 39, diabetes, {chunk -> 0, __trained__ -> Warfarina lusa, relation -> treatment, all_k_distances -> 0.0:::0.0, confidence -> 0.8111, all_k_resolutions -> diabetes:::t2dm, target_text -> Lusa Warfarina 5mg, ner_source -> ner_chunk, ops -> -1.0, all_relations -> t2dm, entity -> Lusa Warfarina 5mg, resolved_text -> diabetes, sentence -> 0, __relation_name__ -> treatment}, []}                                                                                         |\n",
      "|{labeled_dependency, 45, 57, Calcium Ions Inhibitor, {chunk -> 1, __trained__ -> amlodipine, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> Calcium Ions Inhibitor:::, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> -1.0, all_relations -> , entity -> amlodipine 10, resolved_text -> Calcium Ions Inhibitor, distance -> 0.23076923076923078, sentence -> 0, __relation_name__ -> action}, []}|\n",
      "|{labeled_dependency, 45, 57, hypertension, {chunk -> 1, __trained__ -> amlodipine, relation -> treatment, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> hypertension:::, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> -1.0, all_relations -> , entity -> amlodipine 10, resolved_text -> hypertension, distance -> 0.23076923076923078, sentence -> 0, __relation_name__ -> treatment}, []}                        |\n",
      "|{labeled_dependency, 84, 94, NONE, {chunk -> 2, confidence -> 0.9827, ner_source -> ner_chunk, entity -> Aspaginaspa, sentence -> 0}, []}                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|{labeled_dependency, 97, 109, Coagulation Inhibitor, {chunk -> 3, __trained__ -> coumadin, relation -> action, all_k_distances -> 0.0:::0.0, confidence -> 0.7287, all_k_resolutions -> Coagulation Inhibitor:::, target_text -> coumadin 5 mg, ner_source -> ner_chunk, ops -> -1.0, all_relations -> , entity -> coumadin 5 mg, resolved_text -> Coagulation Inhibitor, sentence -> 0, __relation_name__ -> action}, []}                                                                               |\n",
      "|{labeled_dependency, 97, 109, hypertension, {chunk -> 3, __trained__ -> coumadin, relation -> treatment, all_k_distances -> 0.0:::0.0, confidence -> 0.7287, all_k_resolutions -> hypertension:::, target_text -> coumadin 5 mg, ner_source -> ner_chunk, ops -> -1.0, all_relations -> , entity -> coumadin 5 mg, resolved_text -> hypertension, sentence -> 0, __relation_name__ -> treatment}, []}                                                                                                    |\n",
      "|{labeled_dependency, 112, 119, Coagulation Inhibitor, {chunk -> 4, __trained__ -> coumadin, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.9969, all_k_resolutions -> Coagulation Inhibitor:::, target_text -> coumadin, ner_source -> ner_chunk, ops -> 0.0, all_relations -> , entity -> coumadin, resolved_text -> Coagulation Inhibitor, distance -> 0.0, sentence -> 0, __relation_name__ -> action}, []}                                  |\n",
      "|{labeled_dependency, 112, 119, hypertension, {chunk -> 4, __trained__ -> coumadin, relation -> treatment, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.9969, all_k_resolutions -> hypertension:::, target_text -> coumadin, ner_source -> ner_chunk, ops -> 0.0, all_relations -> , entity -> coumadin, resolved_text -> hypertension, distance -> 0.0, sentence -> 0, __relation_name__ -> treatment}, []}                                                       |\n",
      "|{labeled_dependency, 133, 142, NONE, {chunk -> 5, confidence -> 0.9666, ner_source -> ner_chunk, entity -> metamorfin, sentence -> 0}, []}                                                                                                                                                                                                                                                                                                                                                               |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = medical.ChunkMapperApproach() \\\n",
    "        .setInputCols([\"ner_chunk\"]) \\\n",
    "        .setLowerCase(True) \\\n",
    "        .setRels([\"action\", \"treatment\"]) \\\n",
    "        .setAllowMultiTokenChunk(True) \\\n",
    "        .setEnableTokenFingerprintMatching(True) \\\n",
    "        .setMinTokenNgramFingerprint(1) \\\n",
    "        .setMaxTokenNgramFingerprint(3) \\\n",
    "        .setMaxTokenNgramDroppingCharsRatio(0.5)\n",
    "\n",
    "chunkerMappers = [\n",
    "    cm.copy().setOutputCol(f\"mappings_{c}\").setDictionary(f\"mappings_{c}.json\") \\\n",
    "    for c in extra_keys]\n",
    "\n",
    "result_df = nlp.Pipeline(stages=chunkerMappers).fit(cached_df).transform(cached_df)\n",
    "result_df.selectExpr(\"explode(mappings_s500)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+----------------------+---------+\n",
      "|ner_chunk         |fixed_chunk   |action_mapping_result |relation |\n",
      "+------------------+--------------+----------------------+---------+\n",
      "|Lusa Warfarina 5mg|Warfarina lusa|Analgesic             |action   |\n",
      "|Lusa Warfarina 5mg|Warfarina lusa|diabetes              |treatment|\n",
      "|amlodipine 10     |amlodipine    |Calcium Ions Inhibitor|action   |\n",
      "|amlodipine 10     |amlodipine    |hypertension          |treatment|\n",
      "|Aspaginaspa       |null          |NONE                  |null     |\n",
      "|coumadin 5 mg     |coumadin      |Coagulation Inhibitor |action   |\n",
      "|coumadin 5 mg     |coumadin      |hypertension          |treatment|\n",
      "|coumadin          |coumadin      |Coagulation Inhibitor |action   |\n",
      "|coumadin          |coumadin      |hypertension          |treatment|\n",
      "|metamorfin        |null          |NONE                  |null     |\n",
      "+------------------+--------------+----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.select(F.explode(F.arrays_zip(result_df.mappings_s500.result, \n",
    "                                  result_df.mappings_s500.metadata)).alias(\"col\"))\\\n",
    "    .select(F.expr(\"col['1']['entity']\").alias(\"ner_chunk\"),\n",
    "            F.expr(\"col['1']['__trained__']\").alias(\"fixed_chunk\"),\n",
    "            F.expr(\"col['0']\").alias(\"action_mapping_result\"),\n",
    "            F.expr(\"col['1']['relation']\").alias(\"relation \")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212 ms Â± 14.8 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "result_df.selectExpr(\"explode(mappings_s500)\").write.mode(\"overwrite\").save(\"timing_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180 ms Â± 4.45 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "result_df.selectExpr(\"explode(mappings_s5000)\").write.mode(\"overwrite\").save(\"timing_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170 ms Â± 4.7 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "result_df.selectExpr(\"explode(mappings_l5000)\").write.mode(\"overwrite\").save(\"timing_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be seen that fingerprinting is pretty much insensitive to the mappings size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example with token and char fingerprinting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|col                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{labeled_dependency, 22, 39, Analgesic, {chunk -> 0, __trained__ -> Warfarina lusa, relation -> action, all_k_distances -> 0.0:::0.0, confidence -> 0.8111, all_k_resolutions -> Analgesic:::Antipyretic, target_text -> Lusa Warfarina 5mg, ner_source -> ner_chunk, ops -> -1.0, all_relations -> Antipyretic, entity -> Lusa Warfarina 5mg, resolved_text -> Analgesic, sentence -> 0, __relation_name__ -> action}, []}                                                                              |\n",
      "|{labeled_dependency, 22, 39, diabetes, {chunk -> 0, __trained__ -> Warfarina lusa, relation -> treatment, all_k_distances -> 0.0:::0.0, confidence -> 0.8111, all_k_resolutions -> diabetes:::t2dm, target_text -> Lusa Warfarina 5mg, ner_source -> ner_chunk, ops -> -1.0, all_relations -> t2dm, entity -> Lusa Warfarina 5mg, resolved_text -> diabetes, sentence -> 0, __relation_name__ -> treatment}, []}                                                                                         |\n",
      "|{labeled_dependency, 45, 57, Calcium Ions Inhibitor, {chunk -> 1, __trained__ -> amlodipine, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> Calcium Ions Inhibitor:::, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> -1.0, all_relations -> , entity -> amlodipine 10, resolved_text -> Calcium Ions Inhibitor, distance -> 0.23076923076923078, sentence -> 0, __relation_name__ -> action}, []}|\n",
      "|{labeled_dependency, 45, 57, hypertension, {chunk -> 1, __trained__ -> amlodipine, relation -> treatment, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> hypertension:::, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> -1.0, all_relations -> , entity -> amlodipine 10, resolved_text -> hypertension, distance -> 0.23076923076923078, sentence -> 0, __relation_name__ -> treatment}, []}                        |\n",
      "|{labeled_dependency, 84, 94, Cycooxygenase Inhibitor, {chunk -> 2, __trained__ -> aspagin, relation -> action, all_k_distances -> 0.0:::0.0, confidence -> 0.9827, all_k_resolutions -> Cycooxygenase Inhibitor:::, target_text -> Aspaginaspa, ner_source -> ner_chunk, ops -> 12.0, all_relations -> , entity -> Aspaginaspa, resolved_text -> Cycooxygenase Inhibitor, sentence -> 0, __relation_name__ -> action}, []}                                                                               |\n",
      "|{labeled_dependency, 84, 94, arthritis, {chunk -> 2, __trained__ -> aspagin, relation -> treatment, all_k_distances -> 0.0:::0.0, confidence -> 0.9827, all_k_resolutions -> arthritis:::, target_text -> Aspaginaspa, ner_source -> ner_chunk, ops -> 12.0, all_relations -> , entity -> Aspaginaspa, resolved_text -> arthritis, sentence -> 0, __relation_name__ -> treatment}, []}                                                                                                                   |\n",
      "|{labeled_dependency, 97, 109, Coagulation Inhibitor, {chunk -> 3, __trained__ -> coumadin, relation -> action, all_k_distances -> 0.0:::0.0, confidence -> 0.7287, all_k_resolutions -> Coagulation Inhibitor:::, target_text -> coumadin 5 mg, ner_source -> ner_chunk, ops -> -1.0, all_relations -> , entity -> coumadin 5 mg, resolved_text -> Coagulation Inhibitor, sentence -> 0, __relation_name__ -> action}, []}                                                                               |\n",
      "|{labeled_dependency, 97, 109, hypertension, {chunk -> 3, __trained__ -> coumadin, relation -> treatment, all_k_distances -> 0.0:::0.0, confidence -> 0.7287, all_k_resolutions -> hypertension:::, target_text -> coumadin 5 mg, ner_source -> ner_chunk, ops -> -1.0, all_relations -> , entity -> coumadin 5 mg, resolved_text -> hypertension, sentence -> 0, __relation_name__ -> treatment}, []}                                                                                                    |\n",
      "|{labeled_dependency, 112, 119, Coagulation Inhibitor, {chunk -> 4, __trained__ -> coumadin, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.9969, all_k_resolutions -> Coagulation Inhibitor:::, target_text -> coumadin, ner_source -> ner_chunk, ops -> 0.0, all_relations -> , entity -> coumadin, resolved_text -> Coagulation Inhibitor, distance -> 0.0, sentence -> 0, __relation_name__ -> action}, []}                                  |\n",
      "|{labeled_dependency, 112, 119, hypertension, {chunk -> 4, __trained__ -> coumadin, relation -> treatment, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.9969, all_k_resolutions -> hypertension:::, target_text -> coumadin, ner_source -> ner_chunk, ops -> 0.0, all_relations -> , entity -> coumadin, resolved_text -> hypertension, distance -> 0.0, sentence -> 0, __relation_name__ -> treatment}, []}                                                       |\n",
      "|{labeled_dependency, 133, 142, NONE, {chunk -> 5, confidence -> 0.9666, ner_source -> ner_chunk, entity -> metamorfin, sentence -> 0}, []}                                                                                                                                                                                                                                                                                                                                                               |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = medical.ChunkMapperApproach() \\\n",
    "        .setInputCols([\"ner_chunk\"]) \\\n",
    "        .setLowerCase(True) \\\n",
    "        .setRels([\"action\", \"treatment\"]) \\\n",
    "        .setAllowMultiTokenChunk(True) \\\n",
    "        .setEnableTokenFingerprintMatching(True) \\\n",
    "        .setMinTokenNgramFingerprint(1) \\\n",
    "        .setMaxTokenNgramFingerprint(3) \\\n",
    "        .setMaxTokenNgramDroppingCharsRatio(0.5) \\\n",
    "        .setEnableCharFingerprintMatching(True) \\\n",
    "        .setMinCharNgramFingerprint(1) \\\n",
    "        .setMaxCharNgramFingerprint(3)\n",
    "\n",
    "chunkerMappers = [\n",
    "    cm.copy().setOutputCol(f\"mappings_{c}\").setDictionary(f\"mappings_{c}.json\") \\\n",
    "    for c in extra_keys]\n",
    "\n",
    "result_df = nlp.Pipeline(stages=chunkerMappers).fit(cached_df).transform(cached_df)\n",
    "result_df.selectExpr(\"explode(mappings_s500)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+-----------------------+---------+\n",
      "|ner_chunk         |fixed_chunk   |action_mapping_result  |relation |\n",
      "+------------------+--------------+-----------------------+---------+\n",
      "|Lusa Warfarina 5mg|Warfarina lusa|Analgesic              |action   |\n",
      "|Lusa Warfarina 5mg|Warfarina lusa|diabetes               |treatment|\n",
      "|amlodipine 10     |amlodipine    |Calcium Ions Inhibitor |action   |\n",
      "|amlodipine 10     |amlodipine    |hypertension           |treatment|\n",
      "|Aspaginaspa       |aspagin       |Cycooxygenase Inhibitor|action   |\n",
      "|Aspaginaspa       |aspagin       |arthritis              |treatment|\n",
      "|coumadin 5 mg     |coumadin      |Coagulation Inhibitor  |action   |\n",
      "|coumadin 5 mg     |coumadin      |hypertension           |treatment|\n",
      "|coumadin          |coumadin      |Coagulation Inhibitor  |action   |\n",
      "|coumadin          |coumadin      |hypertension           |treatment|\n",
      "|metamorfin        |null          |NONE                   |null     |\n",
      "+------------------+--------------+-----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.select(F.explode(F.arrays_zip(result_df.mappings_s500.result, \n",
    "                                  result_df.mappings_s500.metadata)).alias(\"col\"))\\\n",
    "    .select(F.expr(\"col['1']['entity']\").alias(\"ner_chunk\"),\n",
    "            F.expr(\"col['1']['__trained__']\").alias(\"fixed_chunk\"),\n",
    "            F.expr(\"col['0']\").alias(\"action_mapping_result\"),\n",
    "            F.expr(\"col['1']['relation']\").alias(\"relation \")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168 ms Â± 3.64 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "result_df.selectExpr(\"explode(mappings_s500)\").write.mode(\"overwrite\").save(\"timing_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164 ms Â± 5.72 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "result_df.selectExpr(\"explode(mappings_s5000)\").write.mode(\"overwrite\").save(\"timing_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164 ms Â± 1.36 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "result_df.selectExpr(\"explode(mappings_l5000)\").write.mode(\"overwrite\").save(\"timing_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be seen that fingerprinting is pretty much insensitive to the mappings size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example with token and char fingerprinting plus fuzzy distance calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|col                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{labeled_dependency, 22, 39, Analgesic, {chunk -> 0, __trained__ -> Warfarina lusa, relation -> action, all_k_distances -> 0.0:::0.0, confidence -> 0.8111, all_k_resolutions -> Analgesic:::Antipyretic, target_text -> Lusa Warfarina 5mg, ner_source -> ner_chunk, ops -> -1.0, all_relations -> Antipyretic, entity -> Lusa Warfarina 5mg, resolved_text -> Analgesic, sentence -> 0, __relation_name__ -> action}, []}                                                                              |\n",
      "|{labeled_dependency, 22, 39, diabetes, {chunk -> 0, __trained__ -> Warfarina lusa, relation -> treatment, all_k_distances -> 0.0:::0.0, confidence -> 0.8111, all_k_resolutions -> diabetes:::t2dm, target_text -> Lusa Warfarina 5mg, ner_source -> ner_chunk, ops -> -1.0, all_relations -> t2dm, entity -> Lusa Warfarina 5mg, resolved_text -> diabetes, sentence -> 0, __relation_name__ -> treatment}, []}                                                                                         |\n",
      "|{labeled_dependency, 45, 57, Calcium Ions Inhibitor, {chunk -> 1, __trained__ -> amlodipine, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> Calcium Ions Inhibitor:::, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> -1.0, all_relations -> , entity -> amlodipine 10, resolved_text -> Calcium Ions Inhibitor, distance -> 0.23076923076923078, sentence -> 0, __relation_name__ -> action}, []}|\n",
      "|{labeled_dependency, 45, 57, hypertension, {chunk -> 1, __trained__ -> amlodipine, relation -> treatment, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> hypertension:::, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> -1.0, all_relations -> , entity -> amlodipine 10, resolved_text -> hypertension, distance -> 0.23076923076923078, sentence -> 0, __relation_name__ -> treatment}, []}                        |\n",
      "|{labeled_dependency, 84, 94, Cycooxygenase Inhibitor, {chunk -> 2, __trained__ -> aspagin, relation -> action, all_k_distances -> 0.0:::0.0, confidence -> 0.9827, all_k_resolutions -> Cycooxygenase Inhibitor:::, target_text -> Aspaginaspa, ner_source -> ner_chunk, ops -> 12.0, all_relations -> , entity -> Aspaginaspa, resolved_text -> Cycooxygenase Inhibitor, sentence -> 0, __relation_name__ -> action}, []}                                                                               |\n",
      "|{labeled_dependency, 84, 94, arthritis, {chunk -> 2, __trained__ -> aspagin, relation -> treatment, all_k_distances -> 0.0:::0.0, confidence -> 0.9827, all_k_resolutions -> arthritis:::, target_text -> Aspaginaspa, ner_source -> ner_chunk, ops -> 12.0, all_relations -> , entity -> Aspaginaspa, resolved_text -> arthritis, sentence -> 0, __relation_name__ -> treatment}, []}                                                                                                                   |\n",
      "|{labeled_dependency, 97, 109, Coagulation Inhibitor, {chunk -> 3, __trained__ -> coumadin, relation -> action, all_k_distances -> 0.0:::0.0, confidence -> 0.7287, all_k_resolutions -> Coagulation Inhibitor:::, target_text -> coumadin 5 mg, ner_source -> ner_chunk, ops -> -1.0, all_relations -> , entity -> coumadin 5 mg, resolved_text -> Coagulation Inhibitor, sentence -> 0, __relation_name__ -> action}, []}                                                                               |\n",
      "|{labeled_dependency, 97, 109, hypertension, {chunk -> 3, __trained__ -> coumadin, relation -> treatment, all_k_distances -> 0.0:::0.0, confidence -> 0.7287, all_k_resolutions -> hypertension:::, target_text -> coumadin 5 mg, ner_source -> ner_chunk, ops -> -1.0, all_relations -> , entity -> coumadin 5 mg, resolved_text -> hypertension, sentence -> 0, __relation_name__ -> treatment}, []}                                                                                                    |\n",
      "|{labeled_dependency, 112, 119, Coagulation Inhibitor, {chunk -> 4, __trained__ -> coumadin, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.9969, all_k_resolutions -> Coagulation Inhibitor:::, target_text -> coumadin, ner_source -> ner_chunk, ops -> 0.0, all_relations -> , entity -> coumadin, resolved_text -> Coagulation Inhibitor, distance -> 0.0, sentence -> 0, __relation_name__ -> action}, []}                                  |\n",
      "|{labeled_dependency, 112, 119, hypertension, {chunk -> 4, __trained__ -> coumadin, relation -> treatment, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.9969, all_k_resolutions -> hypertension:::, target_text -> coumadin, ner_source -> ner_chunk, ops -> 0.0, all_relations -> , entity -> coumadin, resolved_text -> hypertension, distance -> 0.0, sentence -> 0, __relation_name__ -> treatment}, []}                                                       |\n",
      "|{labeled_dependency, 133, 142, hypoglycemic, {chunk -> 5, __trained__ -> metformin, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.9666, all_k_resolutions -> hypoglycemic:::Drugs Used In Diabetes, target_text -> metamorfin, ner_source -> ner_chunk, ops -> 0.3, all_relations -> Drugs Used In Diabetes, entity -> metamorfin, resolved_text -> hypoglycemic, sentence -> 0, __relation_name__ -> action}, []}                             |\n",
      "|{labeled_dependency, 133, 142, diabetes, {chunk -> 5, __trained__ -> metformin, relation -> treatment, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.9666, all_k_resolutions -> diabetes:::t2dm, target_text -> metamorfin, ner_source -> ner_chunk, ops -> 0.3, all_relations -> t2dm, entity -> metamorfin, resolved_text -> diabetes, sentence -> 0, __relation_name__ -> treatment}, []}                                                                       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cm = medical.ChunkMapperApproach() \\\n",
    "        .setInputCols([\"ner_chunk\"]) \\\n",
    "        .setOutputCol(\"mappings\") \\\n",
    "        .setDictionary(\"mappings.json\") \\\n",
    "        .setLowerCase(True) \\\n",
    "        .setRels([\"action\", \"treatment\"]) \\\n",
    "        .setAllowMultiTokenChunk(True) \\\n",
    "        .setEnableTokenFingerprintMatching(True) \\\n",
    "        .setMinTokenNgramFingerprint(1) \\\n",
    "        .setMaxTokenNgramFingerprint(3) \\\n",
    "        .setMaxTokenNgramDroppingCharsRatio(0.5) \\\n",
    "        .setEnableCharFingerprintMatching(True) \\\n",
    "        .setMinCharNgramFingerprint(1) \\\n",
    "        .setMaxCharNgramFingerprint(3) \\\n",
    "        .setEnableFuzzyMatching(True) \\\n",
    "        .setFuzzyMatchingDistanceThresholds(0.31)\n",
    "\n",
    "chunkerMappers = [\n",
    "    cm.copy().setOutputCol(f\"mappings_{c}\").setDictionary(f\"mappings_{c}.json\") \\\n",
    "    for c in extra_keys]\n",
    "\n",
    "result_df = nlp.Pipeline(stages=chunkerMappers).fit(cached_df).transform(cached_df)\n",
    "result_df.selectExpr(\"explode(mappings_s500)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+-----------------------+---------+\n",
      "|ner_chunk         |fixed_chunk   |action_mapping_result  |relation |\n",
      "+------------------+--------------+-----------------------+---------+\n",
      "|Lusa Warfarina 5mg|Warfarina lusa|Analgesic              |action   |\n",
      "|Lusa Warfarina 5mg|Warfarina lusa|diabetes               |treatment|\n",
      "|amlodipine 10     |amlodipine    |Calcium Ions Inhibitor |action   |\n",
      "|amlodipine 10     |amlodipine    |hypertension           |treatment|\n",
      "|Aspaginaspa       |aspagin       |Cycooxygenase Inhibitor|action   |\n",
      "|Aspaginaspa       |aspagin       |arthritis              |treatment|\n",
      "|coumadin 5 mg     |coumadin      |Coagulation Inhibitor  |action   |\n",
      "|coumadin 5 mg     |coumadin      |hypertension           |treatment|\n",
      "|coumadin          |coumadin      |Coagulation Inhibitor  |action   |\n",
      "|coumadin          |coumadin      |hypertension           |treatment|\n",
      "|metamorfin        |metformin     |hypoglycemic           |action   |\n",
      "|metamorfin        |metformin     |diabetes               |treatment|\n",
      "+------------------+--------------+-----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.select(F.explode(F.arrays_zip(result_df.mappings_s500.result, \n",
    "                                  result_df.mappings_s500.metadata)).alias(\"col\"))\\\n",
    "    .select(F.expr(\"col['1']['entity']\").alias(\"ner_chunk\"),\n",
    "            F.expr(\"col['1']['__trained__']\").alias(\"fixed_chunk\"),\n",
    "            F.expr(\"col['0']\").alias(\"action_mapping_result\"),\n",
    "            F.expr(\"col['1']['relation']\").alias(\"relation \")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194 ms Â± 3.41 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "result_df.selectExpr(\"explode(mappings_s500)\").write.mode(\"overwrite\").save(\"timing_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510 ms Â± 18.1 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "result_df.selectExpr(\"explode(mappings_s5000)\").write.mode(\"overwrite\").save(\"timing_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "523 ms Â± 15.7 ms per loop (mean Â± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "result_df.selectExpr(\"explode(mappings_l5000)\").write.mode(\"overwrite\").save(\"timing_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be seen that distance functions are really affected by the mappings size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example with fuzzy distance calculation using a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drug_action_treatment_mapper download started this may take some time.\n",
      "[OK!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 588:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|col                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{labeled_dependency, 22, 39, lipid modifying agents, {chunk -> 0, __trained__ -> pravastatina fg, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.8111, all_k_resolutions -> lipid modifying agents:::, target_text -> Lusa Warfarina 5mg, ner_source -> ner_chunk, ops -> 0.5555555555555556, all_relations -> , entity -> Lusa Warfarina 5mg, resolved_text -> lipid modifying agents, sentence -> 0, __relation_name__ -> action}, []}                                                                                                                                                                                                                                                              |\n",
      "|{labeled_dependency, 22, 39, anticoagulant, {chunk -> 0, __trained__ -> warfarin pmcs, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.8111, all_k_resolutions -> anticoagulant:::, target_text -> Lusa Warfarina 5mg, ner_source -> ner_chunk, ops -> 0.5, all_relations -> , entity -> Lusa Warfarina 5mg, resolved_text -> anticoagulant, sentence -> 0, __relation_name__ -> action}, []}                                                                                                                                                                                                                                                                                                          |\n",
      "|{labeled_dependency, 22, 39, anticoagulant, {chunk -> 0, __trained__ -> warfarina mk, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.8111, all_k_resolutions -> anticoagulant:::, target_text -> Lusa Warfarina 5mg, ner_source -> ner_chunk, ops -> 0.3888888888888889, all_relations -> , entity -> Lusa Warfarina 5mg, resolved_text -> anticoagulant, sentence -> 0, __relation_name__ -> action}, []}                                                                                                                                                                                                                                                                                            |\n",
      "|{labeled_dependency, 45, 57, antianginal, {chunk -> 1, __trained__ -> boie amlodipine besilate, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> antianginal:::antihypertensive:::antispasmodic:::hypotensive:::vasodilator, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> 0.5416666666666666, all_relations -> antihypertensive:::antispasmodic:::hypotensive:::vasodilator, entity -> amlodipine 10, resolved_text -> antianginal, sentence -> 0, __relation_name__ -> action}, []}                                                                                                                                                                    |\n",
      "|{labeled_dependency, 45, 57, antitumour, {chunk -> 1, __trained__ -> azathioprine eg, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> antitumour:::cytostatic:::immunosuppressive, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> 0.5333333333333333, all_relations -> cytostatic:::immunosuppressive, entity -> amlodipine 10, resolved_text -> antitumour, sentence -> 0, __relation_name__ -> action}, []}                                                                                                                                                                                                                                            |\n",
      "|{labeled_dependency, 45, 57, antianginal, {chunk -> 1, __trained__ -> pharex amlodipine besylate, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> antianginal:::antihypertensive:::antispasmodic:::hypotensive:::vasodilator, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> 0.5769230769230769, all_relations -> antihypertensive:::antispasmodic:::hypotensive:::vasodilator, entity -> amlodipine 10, resolved_text -> antianginal, sentence -> 0, __relation_name__ -> action}, []}                                                                                                                                                                  |\n",
      "|{labeled_dependency, 45, 57, antianginal, {chunk -> 1, __trained__ -> temax (amlodipine), relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> antianginal:::antihypertensive:::antispasmodic:::hypotensive:::vasodilator, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> 0.5555555555555556, all_relations -> antihypertensive:::antispasmodic:::hypotensive:::vasodilator, entity -> amlodipine 10, resolved_text -> antianginal, sentence -> 0, __relation_name__ -> action}, []}                                                                                                                                                                          |\n",
      "|{labeled_dependency, 45, 57, angioprotective, {chunk -> 1, __trained__ -> bilobil intense 120, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> angioprotective:::antihypoxic:::antioxidant:::improving cerebral and peripheral circulation:::microcirculatory improver:::neurometabolic:::nootropic, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> 0.5263157894736842, all_relations -> antihypoxic:::antioxidant:::improving cerebral and peripheral circulation:::microcirculatory improver:::neurometabolic:::nootropic, entity -> amlodipine 10, resolved_text -> angioprotective, sentence -> 0, __relation_name__ -> action}, []}                 |\n",
      "|{labeled_dependency, 45, 57, antianginal, {chunk -> 1, __trained__ -> felodipin hf retard, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> antianginal:::antihypertensive:::calcium channel blocking:::hypotensive, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> 0.5789473684210527, all_relations -> antihypertensive:::calcium channel blocking:::hypotensive, entity -> amlodipine 10, resolved_text -> antianginal, sentence -> 0, __relation_name__ -> action}, []}                                                                                                                                                                               |\n",
      "|{labeled_dependency, 45, 57, arteriodilating, {chunk -> 1, __trained__ -> nimodipina mepha, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> arteriodilating:::neuroprotective:::nootropic:::vasodilator, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> 0.5625, all_relations -> neuroprotective:::nootropic:::vasodilator, entity -> amlodipine 10, resolved_text -> arteriodilating, sentence -> 0, __relation_name__ -> action}, []}                                                                                                                                                                                                                  |\n",
      "|{labeled_dependency, 45, 57, alpha-adrenergic blocking, {chunk -> 1, __trained__ -> tamsulosine eg lp, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> alpha-adrenergic blocking:::alpha-adrenolytic:::antidisuric:::normalizing the function of the prostate:::urologicals, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> 0.5882352941176471, all_relations -> alpha-adrenolytic:::antidisuric:::normalizing the function of the prostate:::urologicals, entity -> amlodipine 10, resolved_text -> alpha-adrenergic blocking, sentence -> 0, __relation_name__ -> action}, []}                                                                         |\n",
      "|{labeled_dependency, 45, 57, agents acting in the renin-angiotensin system, {chunk -> 1, __trained__ -> perindopril/amlodipine krka, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> agents acting in the renin-angiotensin system:::antihypertensive, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> 0.5925925925925926, all_relations -> antihypertensive, entity -> amlodipine 10, resolved_text -> agents acting in the renin-angiotensin system, sentence -> 0, __relation_name__ -> action}, []}                                                                                                                                                   |\n",
      "|{labeled_dependency, 45, 57, antianginal, {chunk -> 1, __trained__ -> felodipin - 1 a pharma, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> antianginal:::antihypertensive:::calcium channel blocking:::hypotensive, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> 0.5909090909090909, all_relations -> antihypertensive:::calcium channel blocking:::hypotensive, entity -> amlodipine 10, resolved_text -> antianginal, sentence -> 0, __relation_name__ -> action}, []}                                                                                                                                                                            |\n",
      "|{labeled_dependency, 45, 57, antifungal broad spectrum, {chunk -> 1, __trained__ -> sporiline 1%, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> antifungal broad spectrum:::, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> 0.46153846153846156, all_relations -> , entity -> amlodipine 10, resolved_text -> antifungal broad spectrum, sentence -> 0, __relation_name__ -> action}, []}                                                                                                                                                                                                                                                             |\n",
      "|{labeled_dependency, 45, 57, antiulcer, {chunk -> 1, __trained__ -> famotidine 10% ohara, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> antiulcer:::blocking histamine h2-receptors:::drugs for acid related disorders:::gastric acid secretion inhibitor:::oppressive secretion of the gastrointestinal tract, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> 0.55, all_relations -> blocking histamine h2-receptors:::drugs for acid related disorders:::gastric acid secretion inhibitor:::oppressive secretion of the gastrointestinal tract, entity -> amlodipine 10, resolved_text -> antiulcer, sentence -> 0, __relation_name__ -> action}, []}|\n",
      "|{labeled_dependency, 45, 57, antiglaucomatous, {chunk -> 1, __trained__ -> adsorbocarpine 1%, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> antiglaucomatous:::dental agent:::direct acting miotic:::m-cholinomimetic:::other nervous system drugs, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> 0.5294117647058824, all_relations -> dental agent:::direct acting miotic:::m-cholinomimetic:::other nervous system drugs, entity -> amlodipine 10, resolved_text -> antiglaucomatous, sentence -> 0, __relation_name__ -> action}, []}                                                                                                              |\n",
      "|{labeled_dependency, 45, 57, antianginal, {chunk -> 1, __trained__ -> felodipin-mepha, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> antianginal:::antihypertensive:::calcium channel blocking:::hypotensive, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> 0.4666666666666667, all_relations -> antihypertensive:::calcium channel blocking:::hypotensive, entity -> amlodipine 10, resolved_text -> antianginal, sentence -> 0, __relation_name__ -> action}, []}                                                                                                                                                                                   |\n",
      "|{labeled_dependency, 45, 57, antianginal, {chunk -> 1, __trained__ -> adipine xl, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> antianginal:::antihypertensive:::arteriodilating:::calcium channel blocking:::coronary deglutating:::hypotensive:::vasodilator, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> 0.38461538461538464, all_relations -> antihypertensive:::arteriodilating:::calcium channel blocking:::coronary deglutating:::hypotensive:::vasodilator, entity -> amlodipine 10, resolved_text -> antianginal, sentence -> 0, __relation_name__ -> action}, []}                                                                         |\n",
      "|{labeled_dependency, 45, 57, antibacterials for systemic use, {chunk -> 1, __trained__ -> ofloxacine cf, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.66709995, all_k_resolutions -> antibacterials for systemic use:::bactericidal:::ophthalmologicals, target_text -> amlodipine 10, ner_source -> ner_chunk, ops -> 0.5384615384615384, all_relations -> bactericidal:::ophthalmologicals, entity -> amlodipine 10, resolved_text -> antibacterials for systemic use, sentence -> 0, __relation_name__ -> action}, []}                                                                                                                                                                           |\n",
      "|{labeled_dependency, 84, 94, anticoagulant, {chunk -> 2, __trained__ -> heparina sodica, relation -> action, all_k_distances -> 0.0:::0.0, __distance_function__ -> levenshtein, confidence -> 0.9827, all_k_resolutions -> anticoagulant:::anti-inflammatory:::antiproliferative:::antithrombotic:::decongestant:::fibrinolytic:::local anesthetic:::vasoprotective, target_text -> Aspaginaspa, ner_source -> ner_chunk, ops -> 0.5333333333333333, all_relations -> anti-inflammatory:::antiproliferative:::antithrombotic:::decongestant:::fibrinolytic:::local anesthetic:::vasoprotective, entity -> Aspaginaspa, resolved_text -> anticoagulant, sentence -> 0, __relation_name__ -> action}, []}                                                       |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "chunkerMapper_action = medical.ChunkMapperModel.pretrained(\"drug_action_treatment_mapper\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols([\"ner_chunk\"])\\\n",
    "    .setOutputCol(\"Action\")\\\n",
    "    .setRels([\"action\"]) \\\n",
    "    .setAllowMultiTokenChunk(True) \\\n",
    "    .setEnableFuzzyMatching(True) \\\n",
    "    .setFuzzyMatchingDistanceThresholds(0.6)\n",
    "\n",
    "\n",
    "result_df = chunkerMapper_action.transform(cached_df)\n",
    "result_df.selectExpr(\"explode(Action)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 591:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------------------+---------------------------------------------+---------+\n",
      "|ner_chunk         |fixed_chunk                |action_mapping_result                        |relation |\n",
      "+------------------+---------------------------+---------------------------------------------+---------+\n",
      "|Lusa Warfarina 5mg|pravastatina fg            |lipid modifying agents                       |action   |\n",
      "|Lusa Warfarina 5mg|warfarin pmcs              |anticoagulant                                |action   |\n",
      "|Lusa Warfarina 5mg|warfarina mk               |anticoagulant                                |action   |\n",
      "|amlodipine 10     |boie amlodipine besilate   |antianginal                                  |action   |\n",
      "|amlodipine 10     |azathioprine eg            |antitumour                                   |action   |\n",
      "|amlodipine 10     |pharex amlodipine besylate |antianginal                                  |action   |\n",
      "|amlodipine 10     |temax (amlodipine)         |antianginal                                  |action   |\n",
      "|amlodipine 10     |bilobil intense 120        |angioprotective                              |action   |\n",
      "|amlodipine 10     |felodipin hf retard        |antianginal                                  |action   |\n",
      "|amlodipine 10     |nimodipina mepha           |arteriodilating                              |action   |\n",
      "|amlodipine 10     |tamsulosine eg lp          |alpha-adrenergic blocking                    |action   |\n",
      "|amlodipine 10     |perindopril/amlodipine krka|agents acting in the renin-angiotensin system|action   |\n",
      "|amlodipine 10     |felodipin - 1 a pharma     |antianginal                                  |action   |\n",
      "|amlodipine 10     |sporiline 1%               |antifungal broad spectrum                    |action   |\n",
      "|amlodipine 10     |famotidine 10% ohara       |antiulcer                                    |action   |\n",
      "|amlodipine 10     |adsorbocarpine 1%          |antiglaucomatous                             |action   |\n",
      "|amlodipine 10     |felodipin-mepha            |antianginal                                  |action   |\n",
      "|amlodipine 10     |adipine xl                 |antianginal                                  |action   |\n",
      "|amlodipine 10     |ofloxacine cf              |antibacterials for systemic use              |action   |\n",
      "|Aspaginaspa       |heparina sodica            |anticoagulant                                |action   |\n",
      "+------------------+---------------------------+---------------------------------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_df.select(F.explode(F.arrays_zip(result_df.Action.result, \n",
    "                                  result_df.Action.metadata)).alias(\"col\"))\\\n",
    "    .select(F.expr(\"col['1']['entity']\").alias(\"ner_chunk\"),\n",
    "            F.expr(\"col['1']['__trained__']\").alias(\"fixed_chunk\"),\n",
    "            F.expr(\"col['0']\").alias(\"action_mapping_result\"),\n",
    "            F.expr(\"col['1']['relation']\").alias(\"relation \")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-ZTtXsmtzJM"
   },
   "source": [
    "# **3- ChunkMapperFilterer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pm2VP4MXuDSK"
   },
   "source": [
    "`ChunkMapperFilterer` annotator allows filtering of the chunks that were passed through the ChunkMapperModel. <br/>\n",
    "\n",
    "We can filter chunks by setting the `.setReturnCriteria()` parameter. It has 2 options; <br/>\n",
    "\n",
    "\n",
    "**success:** Returns the chunks which are mapped by ChunkMapper <br/>\n",
    "\n",
    "**fail:** Returns the chunks which are not mapped by ChunkMapper <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5gJw22BvZ5b"
   },
   "source": [
    "Let's apply the both options and check the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8MUfkhRtiZK",
    "outputId": "8dda562e-d37c-4f7b-b839-2111dd011a15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|col                                                                                                                                             |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{labeled_dependency, 22, 39, NONE, {chunk -> 0, confidence -> 0.8111, ner_source -> ner_chunk, entity -> Lusa Warfarina 5mg, sentence -> 0}, []}|\n",
      "|{labeled_dependency, 45, 57, NONE, {chunk -> 1, confidence -> 0.66709995, ner_source -> ner_chunk, entity -> amlodipine 10, sentence -> 0}, []} |\n",
      "|{labeled_dependency, 84, 94, NONE, {chunk -> 2, confidence -> 0.9827, ner_source -> ner_chunk, entity -> Aspaginaspa, sentence -> 0}, []}       |\n",
      "|{labeled_dependency, 97, 109, NONE, {chunk -> 3, confidence -> 0.7287, ner_source -> ner_chunk, entity -> coumadin 5 mg, sentence -> 0}, []}    |\n",
      "|{labeled_dependency, 112, 119, NONE, {chunk -> 4, confidence -> 0.9969, ner_source -> ner_chunk, entity -> coumadin, sentence -> 0}, []}        |\n",
      "|{labeled_dependency, 133, 142, NONE, {chunk -> 5, confidence -> 0.9666, ner_source -> ner_chunk, entity -> metamorfin, sentence -> 0}, []}      |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunkerMapper =  medical.ChunkMapperApproach() \\\n",
    "        .setInputCols([\"ner_chunk\"]) \\\n",
    "        .setOutputCol(\"mappings\") \\\n",
    "        .setDictionary(\"mappings.json\") \\\n",
    "        .setRel(\"action\") \\\n",
    "        .setLowerCase(True) \\\n",
    "        .setRels([\"action\", \"treatment\"]) \\\n",
    "\n",
    "pipeline = nlp.Pipeline().setStages([document_assembler,\n",
    "                                 sentence_detector,\n",
    "                                 tokenizer, \n",
    "                                 word_embeddings,\n",
    "                                 clinical_ner, \n",
    "                                 ner_converter, \n",
    "                                 chunkerMapper])\n",
    "\n",
    "\n",
    "result_df = pipeline.fit(test_data).transform(test_data)\n",
    "result_df.selectExpr(\"explode(mappings)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2V0T1b7vuFS"
   },
   "source": [
    "**`.setReturnCriteria(\"success\")`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IssbtA3Xqsgm",
    "outputId": "fcc9bab9-1993-40cb-fae4-d5894fd2259f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|col|\n",
      "+---+\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfModel =  medical.ChunkMapperFilterer() \\\n",
    "        .setInputCols([\"ner_chunk\",\"mappings\"]) \\\n",
    "        .setOutputCol(\"chunks_filtered\")\\\n",
    "        .setReturnCriteria(\"success\")\n",
    "\n",
    "cfModel.transform(result_df).selectExpr(\"explode(chunks_filtered)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTODYW0pvyl_"
   },
   "source": [
    "**`.setReturnCriteria(\"fail\")`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DZ9UNFk4qsaS",
    "outputId": "bdd8089a-20ed-4c6f-d0e0-e6d153f8e923"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|col                                                                                                                                |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{chunk, 22, 39, Lusa Warfarina 5mg, {chunk -> 0, confidence -> 0.8111, ner_source -> ner_chunk, entity -> DRUG, sentence -> 0}, []}|\n",
      "|{chunk, 45, 57, amlodipine 10, {chunk -> 1, confidence -> 0.66709995, ner_source -> ner_chunk, entity -> DRUG, sentence -> 0}, []} |\n",
      "|{chunk, 84, 94, Aspaginaspa, {chunk -> 2, confidence -> 0.9827, ner_source -> ner_chunk, entity -> DRUG, sentence -> 0}, []}       |\n",
      "|{chunk, 97, 109, coumadin 5 mg, {chunk -> 3, confidence -> 0.7287, ner_source -> ner_chunk, entity -> DRUG, sentence -> 0}, []}    |\n",
      "|{chunk, 112, 119, coumadin, {chunk -> 4, confidence -> 0.9969, ner_source -> ner_chunk, entity -> DRUG, sentence -> 0}, []}        |\n",
      "|{chunk, 133, 142, metamorfin, {chunk -> 5, confidence -> 0.9666, ner_source -> ner_chunk, entity -> DRUG, sentence -> 0}, []}      |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cfModel = medical.ChunkMapperFilterer() \\\n",
    "        .setInputCols([\"ner_chunk\",\"mappings\"]) \\\n",
    "        .setOutputCol(\"chunks_filtered\")\\\n",
    "        .setReturnCriteria(\"fail\")\n",
    "\n",
    "cfModel.transform(result_df).selectExpr(\"explode(chunks_filtered)\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJ9tNW9Gt5Sb"
   },
   "source": [
    "# 4- ResolverMerger - Using Sentence Entity Resolver and `ChunkMapperModel` Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tlEvAPvu6Lb"
   },
   "source": [
    "We can merge the results of `ChunkMapperModel` and `SentenceEntityResolverModel` by using `ResolverMerger` annotator. \n",
    "\n",
    "We can detect our results that fail by `ChunkMapperModel` with `ChunkMapperFilterer` and then merge the resolver and mapper results with `ResolverMerger`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "e3ABtM79mf86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n",
      "ner_posology_greedy download started this may take some time.\n",
      "[OK!]\n",
      "rxnorm_mapper download started this may take some time.\n",
      "[ | ]rxnorm_mapper download started this may take some time.\n",
      "Approximate size to download 1.9 MB\n",
      "Download done! Loading the resource.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 604:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n",
      "sbiobert_base_cased_mli download started this may take some time.\n",
      "Approximate size to download 384.3 MB\n",
      "[OK!]\n",
      "sbiobertresolve_rxnorm_augmented download started this may take some time.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = nlp.DocumentAssembler()\\\n",
    "      .setInputCol('text')\\\n",
    "      .setOutputCol('document')\n",
    "\n",
    "sentence_detector = nlp.SentenceDetector()\\\n",
    "      .setInputCols([\"document\"])\\\n",
    "      .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = nlp.Tokenizer()\\\n",
    "      .setInputCols(\"sentence\")\\\n",
    "      .setOutputCol(\"token\")\n",
    "\n",
    "word_embeddings = nlp.WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "      .setInputCols([\"sentence\", \"token\"])\\\n",
    "      .setOutputCol(\"embeddings\")\n",
    "\n",
    "ner_model = medical.NerModel.pretrained(\"ner_posology_greedy\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
    "    .setOutputCol(\"ner\")\n",
    "\n",
    "ner_converter = medical.NerConverter()\\\n",
    "    .setInputCols(\"sentence\", \"token\", \"ner\")\\\n",
    "    .setOutputCol(\"chunk\")\n",
    "\n",
    "chunkerMapper = medical.ChunkMapperModel.pretrained(\"rxnorm_mapper\", \"en\", \"clinical/models\")\\\n",
    "      .setInputCols([\"chunk\"])\\\n",
    "      .setOutputCol(\"RxNorm_Mapper\")\\\n",
    "      .setRels([\"rxnorm_code\"])\n",
    "\n",
    "cfModel = medical.ChunkMapperFilterer() \\\n",
    "    .setInputCols([\"chunk\", \"RxNorm_Mapper\"]) \\\n",
    "    .setOutputCol(\"chunks_fail\") \\\n",
    "    .setReturnCriteria(\"fail\")\n",
    "\n",
    "chunk2doc = nlp.Chunk2Doc() \\\n",
    "    .setInputCols(\"chunks_fail\") \\\n",
    "    .setOutputCol(\"chunk_doc\")\n",
    "\n",
    "sbert_embedder = nlp.BertSentenceEmbeddings.pretrained('sbiobert_base_cased_mli', 'en','clinical/models')\\\n",
    "      .setInputCols([\"chunk_doc\"])\\\n",
    "      .setOutputCol(\"sentence_embeddings\")\\\n",
    "      .setCaseSensitive(False)\n",
    "\n",
    "resolver = medical.SentenceEntityResolverModel.pretrained(\"sbiobertresolve_rxnorm_augmented\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentence_embeddings\"]) \\\n",
    "    .setOutputCol(\"resolver_code\") \\\n",
    "    .setDistanceFunction(\"EUCLIDEAN\")\n",
    "\n",
    "resolverMerger =medical.ResolverMerger()\\\n",
    "    .setInputCols([\"resolver_code\",\"RxNorm_Mapper\"])\\\n",
    "    .setOutputCol(\"RxNorm\")\n",
    "\n",
    "mapper_pipeline = nlp.Pipeline(\n",
    "      stages = [\n",
    "          document_assembler,\n",
    "          sentence_detector,\n",
    "          tokenizer,\n",
    "          word_embeddings,\n",
    "          ner_model,\n",
    "          ner_converter,\n",
    "          chunkerMapper,\n",
    "          chunkerMapper,\n",
    "          cfModel,\n",
    "          chunk2doc,\n",
    "          sbert_embedder,\n",
    "          resolver,\n",
    "          resolverMerger\n",
    "      ])\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "model = mapper_pipeline.fit(empty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wuab7yVluOLY",
    "outputId": "dc2550cc-2889-4c75-cf31-40ea2f34f58c"
   },
   "outputs": [],
   "source": [
    "samples = [['The patient was given Adapin 10 MG, coumadn 5 mg'],\n",
    "           ['The patient was given Avandia 4 mg, Tegretol, zitiga'] ]\n",
    "\n",
    "result = model.transform(spark.createDataFrame(samples).toDF(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fWqZRiCquOqe",
    "outputId": "80b23574-a08a-461e-8ba3-0f3ec1e06245"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 607:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+----------------------+--------------+-------------+-------------------------+\n",
      "|chunk                           |RxNorm_Mapper         |chunks_fail   |resolver_code|RxNorm                   |\n",
      "+--------------------------------+----------------------+--------------+-------------+-------------------------+\n",
      "|[Adapin 10 MG, coumadn 5 mg]    |[1000049, NONE]       |[coumadn 5 mg]|[200883]     |[1000049, 200883]        |\n",
      "|[Avandia 4 mg, Tegretol, zitiga]|[261242, 203029, NONE]|[zitiga]      |[2589614]    |[261242, 203029, 2589614]|\n",
      "+--------------------------------+----------------------+--------------+-------------+-------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result.selectExpr('chunk.result as chunk', \n",
    "                  'RxNorm_Mapper.result as RxNorm_Mapper', \n",
    "                  'chunks_fail.result as chunks_fail', \n",
    "                  'resolver_code.result as resolver_code',\n",
    "                  'RxNorm.result as RxNorm'\n",
    "              ).show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYQZjb7H4PAH"
   },
   "source": [
    "# 5- Section Header Normalizer Mapper with ChunkSentenceSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ChunkSentenceSplitter()` annotator splits documents or sentences by chunks provided. <br/> For detailed usage of this annotator, visit [this notebook](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings_JSL/Healthcare/18.Chunk_Sentence_Splitter.ipynb) <br/>\n",
    "\n",
    "In this section, we will do the following steps; \n",
    "- Detect \"section headers\" in given text through Ner model\n",
    "- Split the given text by headers with `ChunkSentenceSplitter()`\n",
    "- Normalize the `ChunkSentenceSplitter()` outputs with `normalized_section_header_mapper` model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with creating Ner pipeline to detect \"Header\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24021,
     "status": "ok",
     "timestamp": 1656513200012,
     "user": {
      "displayName": "Ahmet Emin Tek",
      "userId": "14855809472179427810"
     },
     "user_tz": -60
    },
    "id": "ZKz2nJtc4Mp1",
    "outputId": "4f5c2378-9dfc-4d94-fd45-9670a9e98221"
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    [\"\"\"ADMISSION DIAGNOSIS Right pleural effusion and suspected malignant mesothelioma.\n",
    "        PRINCIPAL DIAGNOSIS Right pleural effusion, suspected malignant mesothelioma.\n",
    "        REVIEW OF SYSTEMS Right pleural effusion, firm nodules, diffuse scattered throughout the right pleura and diaphragmatic surface.\n",
    "    \"\"\"]]\n",
    "\n",
    "df= spark.createDataFrame(sentences).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1656513200939,
     "user": {
      "displayName": "Ahmet Emin Tek",
      "userId": "14855809472179427810"
     },
     "user_tz": -60
    },
    "id": "vyFDnF2s4MeQ",
    "outputId": "8fe86163-7e84-4c86-f5ef-f2bca2af1e52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_token_classifier_ner_jsl_slim download started this may take some time.\n",
      "[ | ]bert_token_classifier_ner_jsl_slim download started this may take some time.\n",
      "Approximate size to download 385.7 MB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = nlp.DocumentAssembler()\\\n",
    "      .setInputCol(\"text\")\\\n",
    "      .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer= nlp.Tokenizer()\\\n",
    "      .setInputCols([\"document\"])\\\n",
    "      .setOutputCol(\"token\")\n",
    "\n",
    "tokenClassifier = medical.BertForTokenClassification.pretrained(\"bert_token_classifier_ner_jsl_slim\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols(\"token\", \"document\")\\\n",
    "    .setOutputCol(\"ner\")\\\n",
    "    .setCaseSensitive(True)\n",
    "\n",
    "ner_converter = medical.NerConverter() \\\n",
    "      .setInputCols([\"document\", \"token\", \"ner\"]) \\\n",
    "      .setOutputCol(\"ner_chunk\")\\\n",
    "      .setWhiteList([\"Header\"])\n",
    "\n",
    "pipeline = nlp.Pipeline(\n",
    "    stages = [\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        tokenClassifier,\n",
    "        ner_converter\n",
    "    ])\n",
    " \n",
    "empty_df = spark.createDataFrame([[\"\"]]).toDF('text')\n",
    "pipeline_model = pipeline.fit(empty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 614:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|col                                                                                                                                        |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|{chunk, 0, 18, ADMISSION DIAGNOSIS, {chunk -> 0, confidence -> 0.9994346, ner_source -> ner_chunk, entity -> Header, sentence -> 0}, []}   |\n",
      "|{chunk, 89, 107, PRINCIPAL DIAGNOSIS, {chunk -> 1, confidence -> 0.99020165, ner_source -> ner_chunk, entity -> Header, sentence -> 0}, []}|\n",
      "|{chunk, 175, 191, REVIEW OF SYSTEMS, {chunk -> 2, confidence -> 0.9989373, ner_source -> ner_chunk, entity -> Header, sentence -> 0}, []}  |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result = pipeline_model.transform(df)\n",
    "result.selectExpr('explode(ner_chunk)').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have our header entities. We will split the text by the headers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying ChunkSentenceSplitter \n",
    "chunkSentenceSplitter = medical.ChunkSentenceSplitter()\\\n",
    "    .setInputCols(\"document\",\"ner_chunk\")\\\n",
    "    .setOutputCol(\"paragraphs\")\\\n",
    "    .setGroupBySentences(False)\n",
    "\n",
    "paragraphs = chunkSentenceSplitter.transform(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|               token|                 ner|           ner_chunk|          paragraphs|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|ADMISSION DIAGNOS...|[{document, 0, 30...|[{token, 0, 8, AD...|[{named_entity, 0...|[{chunk, 0, 18, A...|[{document, 0, 89...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paragraphs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "      <th>entity</th>\n",
       "      <th>splitter_chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADMISSION DIAGNOSIS Right pleural effusion and suspected malignant mesothelioma.\\n</td>\n",
       "      <td>Header</td>\n",
       "      <td>ADMISSION DIAGNOSIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PRINCIPAL DIAGNOSIS Right pleural effusion, suspected malignant mesothelioma.\\n</td>\n",
       "      <td>Header</td>\n",
       "      <td>PRINCIPAL DIAGNOSIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REVIEW OF SYSTEMS Right pleural effusion, firm nodules, diffuse scattered throughout the right pleura and diaphragmatic surface.\\n</td>\n",
       "      <td>Header</td>\n",
       "      <td>REVIEW OF SYSTEMS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                  result  \\\n",
       "0                                             ADMISSION DIAGNOSIS Right pleural effusion and suspected malignant mesothelioma.\\n           \n",
       "1                                                PRINCIPAL DIAGNOSIS Right pleural effusion, suspected malignant mesothelioma.\\n           \n",
       "2  REVIEW OF SYSTEMS Right pleural effusion, firm nodules, diffuse scattered throughout the right pleura and diaphragmatic surface.\\n      \n",
       "\n",
       "   entity       splitter_chunk  \n",
       "0  Header  ADMISSION DIAGNOSIS  \n",
       "1  Header  PRINCIPAL DIAGNOSIS  \n",
       "2  Header    REVIEW OF SYSTEMS  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "result_df = paragraphs.selectExpr(\"explode(paragraphs) as result\").selectExpr(\"result.result\",\"result.metadata.entity\", \"result.metadata.splitter_chunk\").toPandas()\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, we have our splitted text and **section headers**. <br/>\n",
    "Now we will normalize this section headers with `normalized_section_header_mapper`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized_section_header_mapper download started this may take some time.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "chunkerMapper = medical.ChunkMapperModel.pretrained(\"normalized_section_header_mapper\", \"en\", \"clinical/models\") \\\n",
    "       .setInputCols(\"ner_chunk\")\\\n",
    "       .setOutputCol(\"mappings\")\\\n",
    "       .setRels([\"level_1\"]) #or level_2\n",
    "\n",
    "normalized_df= chunkerMapper.transform(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|               token|                 ner|           ner_chunk|          paragraphs|            mappings|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|ADMISSION DIAGNOS...|[{document, 0, 30...|[{token, 0, 8, AD...|[{named_entity, 0...|[{chunk, 0, 18, A...|[{document, 0, 89...|[{labeled_depende...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "normalized_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ner_chunk</th>\n",
       "      <th>normalized_headers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADMISSION DIAGNOSIS</td>\n",
       "      <td>DIAGNOSIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PRINCIPAL DIAGNOSIS</td>\n",
       "      <td>DIAGNOSIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REVIEW OF SYSTEMS</td>\n",
       "      <td>REVIEW TYPE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             ner_chunk normalized_headers\n",
       "0  ADMISSION DIAGNOSIS          DIAGNOSIS\n",
       "1  PRINCIPAL DIAGNOSIS          DIAGNOSIS\n",
       "2    REVIEW OF SYSTEMS        REVIEW TYPE"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df= normalized_df.select(F.explode(F.arrays_zip(normalized_df.ner_chunk.result,\n",
    "                                                           normalized_df.mappings.result)).alias(\"col\"))\\\n",
    "                            .select(F.expr(\"col['0']\").alias(\"ner_chunk\"),\n",
    "                                    F.expr(\"col['1']\").alias(\"normalized_headers\")).toPandas()\n",
    "normalized_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have our normalized headers. We will merge it with `ChunkSentenceSplitter()` output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_df= normalized_df.rename(columns={\"ner_chunk\": \"splitter_chunk\"})\n",
    "df= pd.merge(result_df, normalized_df, on=[\"splitter_chunk\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "      <th>entity</th>\n",
       "      <th>splitter_chunk</th>\n",
       "      <th>normalized_headers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADMISSION DIAGNOSIS Right pleural effusion and suspected malignant mesothelioma.\\n</td>\n",
       "      <td>Header</td>\n",
       "      <td>ADMISSION DIAGNOSIS</td>\n",
       "      <td>DIAGNOSIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PRINCIPAL DIAGNOSIS Right pleural effusion, suspected malignant mesothelioma.\\n</td>\n",
       "      <td>Header</td>\n",
       "      <td>PRINCIPAL DIAGNOSIS</td>\n",
       "      <td>DIAGNOSIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>REVIEW OF SYSTEMS Right pleural effusion, firm nodules, diffuse scattered throughout the right pleura and diaphragmatic surface.\\n</td>\n",
       "      <td>Header</td>\n",
       "      <td>REVIEW OF SYSTEMS</td>\n",
       "      <td>REVIEW TYPE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                  result  \\\n",
       "0                                             ADMISSION DIAGNOSIS Right pleural effusion and suspected malignant mesothelioma.\\n           \n",
       "1                                                PRINCIPAL DIAGNOSIS Right pleural effusion, suspected malignant mesothelioma.\\n           \n",
       "2  REVIEW OF SYSTEMS Right pleural effusion, firm nodules, diffuse scattered throughout the right pleura and diaphragmatic surface.\\n      \n",
       "\n",
       "   entity       splitter_chunk normalized_headers  \n",
       "0  Header  ADMISSION DIAGNOSIS          DIAGNOSIS  \n",
       "1  Header  PRINCIPAL DIAGNOSIS          DIAGNOSIS  \n",
       "2  Header    REVIEW OF SYSTEMS        REVIEW TYPE  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, we have splitted paragraphs, headers and normalized headers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- Pretrained Mapper Pipelines\n",
    "\n",
    "We will show an example of `rxnorm_umls_mapping` pipeline here. But you can check [Healthcare Code Mapping Notebook](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings_JSL/Healthcare/11.1.Healthcare_Code_Mapping.ipynb) for the examples of pretrained mapper pipelines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rxnorm_umls_mapping download started this may take some time.\n",
      "Approx size to download 1.8 MB\n",
      "[ | ]rxnorm_umls_mapping download started this may take some time.\n",
      "Approximate size to download 1.8 MB\n",
      "Download done! Loading the resource.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 633:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "rxnorm_umls_pipeline= nlp.PretrainedPipeline(\"rxnorm_umls_mapping\", \"en\", \"clinical/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': ['1161611 315677 343663'],\n",
       " 'rxnorm_code': ['1161611', '315677', '343663'],\n",
       " 'umls_code': ['C3215948', 'C0984912', 'C1146501']}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rxnorm_umls_pipeline.annotate(\"1161611 315677 343663\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**RxNorm Code** | **RxNorm Details** | **UMLS Code** | **UMLS Details** |\n",
    "| ---------- | -----------:| ---------- | -----------:|\n",
    "| 1161611 |  metformin Pill | C3215948 | metformin pill |\n",
    "| 315677 | cimetidine 100 mg | C0984912 | cimetidine 100 mg |\n",
    "| 343663 | insulin lispro 50 UNT/ML | C1146501 | insulin lispro 50 unt/ml |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
