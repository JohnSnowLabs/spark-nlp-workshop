{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8W51t04BN6B"
   },
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-vwpSj3BSbj"
   },
   "source": [
    "# ContextualParser (Rule Based NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SBh1CNgOg54"
   },
   "source": [
    "## **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53Qom33h4nvu",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "from johnsnowlabs import nlp, medical\n",
    "\n",
    "spark = start_spark()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZzN6yNBvb2b"
   },
   "source": [
    "# **How the ContextualParser Works**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nSO62FEY0iof"
   },
   "source": [
    "Spark NLP's `ContextualParser` is a licensed annotator that allows users to extract entities from a document based on pattern matching. It provides more functionality than its open-source counterpart `EntityRuler` by allowing users to customize specific characteristics for pattern matching. You're able to find entities using regex rules for full and partial matches, a dictionary with normalizing options and context parameters to take into account things such as token distances. \n",
    "\n",
    "There are 3 components necessary to understand when using the `ContextualParser` annotator:\n",
    "\n",
    "1. `ContextualParser` annotator's parameters\n",
    "2. JSON configuration file\n",
    "3. Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0_3bAzaPbip"
   },
   "source": [
    "## **1. ContextualParser Annotator Parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTSYc7RUQgKh"
   },
   "source": [
    "Here are all the parameters available to use with the `ContextualParserApproach`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vF4_Dm7qVTty"
   },
   "source": [
    "```\n",
    "contextualParser = ContextualParserApproach() \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"entity\") \\\n",
    "    .setCaseSensitive(True) \\\n",
    "    .setJsonPath(\"context_config.json\") \\\n",
    "    .setPrefixAndSuffixMatch(True) \\\n",
    "    .setDictionary(\"dictionary.tsv\", options={\"orientation\":\"vertical\"})\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVmRKjoBZ7HQ"
   },
   "source": [
    "We will dive deeper into the details of each parameter, but here's a quick overview:\n",
    "\n",
    "- `setCaseSensitive`: do you want the matching to be case sensitive (applies to all JSON properties apart from the regex property)\n",
    "- `setJsonPath`: the path to your JSON configuration file\n",
    "- `setPrefixAndSuffixMatch`: do you want to match using both the prefix AND suffix properties from the JSON configuration file\n",
    "- `setDictionary`: the path to your dictionary, used for normalizing entities\n",
    "\n",
    "Let's start by looking at the JSON configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GVO5m215TjXf"
   },
   "source": [
    "## **2. JSON Configuration File**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNJr5ISlaJsl"
   },
   "source": [
    "Here is a fully utilized JSON configuration file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4q1UuczZVhD_"
   },
   "source": [
    "```\n",
    "{\n",
    "  \"entity\": \"Gender\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"regex\": \"girl|boy\",\n",
    "  \"completeMatchRegex\": \"true\",\n",
    "  \"matchScope\": \"token\",\n",
    "  \"prefix\": [\"birth\", \"growing\", \"assessment\"],\n",
    "  \"suffix\": [\"faster\", \"velocities\"],\n",
    "  \"contextLength\": 100,\n",
    "  \"contextException\": [\"slightly\"],\n",
    "  \"exceptionDistance\": 40\n",
    " }\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GChnk1cXaUIZ"
   },
   "source": [
    "### **2.1. Basic Properties**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-kJhmUe0f13"
   },
   "source": [
    "There are 5 basic properties you can set in your JSON configuration file:\n",
    "\n",
    "- `entity`\n",
    "- `ruleScope`\n",
    "- `regex`\n",
    "- `completeMatchRegex`\n",
    "- `matchScope`\n",
    "\n",
    "Let's first look at the 3 most essential properties to set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RP8mwtgVkcj"
   },
   "source": [
    "```\n",
    "{\n",
    "  \"entity\": \"Digit\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"regex\": \"\\\\d+\" # Note here: backslashes are escape characters in JSON, so for regex pattern \"\\d+\" we need to write it out as \"\\\\d+\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pHSsBgoNcJiw"
   },
   "source": [
    "Here, we're looking for tokens in our text that match the regex: \"`\\d+`\" and assign the \"`Digit`\" entity to those tokens. When `ruleScope` is set to \"`sentence`\", we're looking for a match on each *token* of a **sentence**. You can change it to \"`document`\" to look for a match on each *sentence* of a **document**. The latter is particularly useful when working with multi-word matches, but we'll explore this at a later stage.\n",
    "\n",
    "The next properties to look at are `completeMatchRegex` and `matchScope`. To understand their use case, let's take a look at an example where we're trying to match all digits in our text. \n",
    "\n",
    "Let's say we come across the following string: ***XYZ987***\n",
    "\n",
    "Depending on how we set the `completeMatchRegex` and `matchScope` properties, we'll get the following results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOxfFn8_VngD"
   },
   "source": [
    "```\n",
    "{\n",
    "  \"entity\": \"Digit\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"regex\": \"\\\\d+\",\n",
    "  \"completeMatchRegex\": \"false\",\n",
    "  \"matchScope\": \"token\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZzkfdtDyavl"
   },
   "source": [
    "`OUTPUT: [XYZ987]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K37Ucw75Vrog"
   },
   "source": [
    "```\n",
    "{\n",
    "  \"entity\": \"Digit\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"regex\": \"\\\\d+\",  \n",
    "  \"completeMatchRegex\": \"false\",\n",
    "  \"matchScope\": \"sub-token\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkOfYVHGyb20"
   },
   "source": [
    "`OUTPUT: [987]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Jpr2IkFVwKw"
   },
   "source": [
    "```\n",
    "{\n",
    "  \"entity\": \"Digit\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"regex\": \"\\\\d+\",\n",
    "  \"completeMatchRegex\": \"true\"\n",
    "  # matchScope is ignored here\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiYD0oF7gJtw"
   },
   "source": [
    "`OUTPUT: []`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFE9Ri2N4xxT"
   },
   "source": [
    "`\"completeMatchRegex\": \"true\"` will only return an output if our string was modified in the following way (to get a complete, exact match): **XYZ 987**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-_sXg5l5NBg"
   },
   "source": [
    "```\n",
    "{\n",
    "  \"entity\": \"Digit\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"regex\": \"\\\\d+\",  \n",
    "  \"completeMatchRegex\": \"true\",\n",
    "  \"matchScope\": \"token\" # Note here: sub-token would return the same output\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUeuct_05p3f"
   },
   "source": [
    "`OUTPUT: [987]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlIUAPKazpT9"
   },
   "source": [
    "### **2.2. Context Awareness Properties**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9D2UfFFBz7fX"
   },
   "source": [
    "There are 5 properties related to context awareness:\n",
    "\n",
    "- `contextLength`\n",
    "- `prefix`\n",
    "- `suffix`\n",
    "- `contextException`\n",
    "- `exceptionDistance`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcUlaqmgDL9R"
   },
   "source": [
    "Let's look at a similar example. Say we have the following text: ***At birth, the typical boy is growing slightly faster than the typical girl, but growth rates become equal at about seven months.***\n",
    "\n",
    "If we want to match the gender that grows faster at birth, we can start by defining our regex: \"`girl|boy`\"\n",
    "\n",
    "Next, we add a prefix (\"`birth`\") and suffix (\"`faster`\") to ask the parser to match the regex only if the word \"`birth`\" comes before and only if the word \"`faster`\" comes after. Finally, we will need to set the `contextLength` - this is the maximum number of tokens after the prefix and before the suffix that will be searched to find a regex match.\n",
    "\n",
    "Here's what the JSON configuration file would look like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5Tq5OJBVzCL"
   },
   "source": [
    "```\n",
    "{\n",
    "  \"entity\": \"Gender\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"regex\": \"girl|boy\",\n",
    "  \"contextLength\": 50,\n",
    "  \"prefix\": [\"birth\"],\n",
    "  \"suffix\": [\"faster\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5Y5r9pO92B0"
   },
   "source": [
    "`OUTPUT: [boy]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXchjZZC_Gm0"
   },
   "source": [
    "If you remember, the annotator has a `setPrefixAndSuffixMatch()` parameter. If you set it to `True`, the previous output would remain as is. However, if you had set it to `False` and used the following JSON configuration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xO64udOnV1GJ"
   },
   "source": [
    "```\n",
    "{\n",
    "  \"entity\": \"Gender\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"regex\": \"girl|boy\",\n",
    "  \"contextLength\": 50,\n",
    "  \"prefix\": [\"birth\"],\n",
    "  \"suffix\": [\"faster\", \"rates\"]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xm-y_c_RAJpF"
   },
   "source": [
    "`OUTPUT: [boy, girl]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yk0nox1sAdw_"
   },
   "source": [
    "The parser now takes into account either the prefix OR suffix, only one of the condition has to be fulfilled for a match to count."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdEFJZM1DGQ1"
   },
   "source": [
    "Here's the sentence again: ***At birth, the typical boy is growing slightly faster than the typical girl, but growth rates become equal at about seven months.***\n",
    "\n",
    "The last 2 properties related to context awareness are `contextException` and `exceptionDistance`. This rules out matches based on a given exception:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JluGupMV5SR"
   },
   "source": [
    "```\n",
    "{\n",
    "  \"entity\": \"Gender\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"regex\": \"girl|boy\",\n",
    "  \"contextLength\": 50,\n",
    "  \"prefix\": [\"birth\"],\n",
    "  \"suffix\": [\"faster\", \"rates\"],\n",
    "  \"contextException\": [\"At\"],\n",
    "  \"exceptionDistance\": 5\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnFzqpHlC3Qz"
   },
   "source": [
    "`OUTPUT: [girl]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JT09xrE-DCiO"
   },
   "source": [
    "Here we've asked the parser to ignore a match if the token \"`At`\" is within 5 tokens of the matched regex. This caused the token \"`boy`\" to be ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VzFSjw7aVO2b"
   },
   "source": [
    "## **3. Dictionary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NPiJZx-Va8b"
   },
   "source": [
    "Another key feature of the `ContextualParser` annotator is the use of dictionaries. You can specify a path to a dictionary in `tsv` or `csv` format using the `setDictionary()` parameter. Using a dictionary is a useful when you have a list of exact words that you want the parser to pick up when processing some text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmB9hadyXYeM"
   },
   "source": [
    "### **3.1. Orientation**\n",
    "\n",
    "The first feature to be aware of when it comes to feeding dictionaries is the format of the dictionaries. The `ContextualParser` annotator will accept dictionaries in the horizontal format and in a vertical format. This is how they would look in practice:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyDmIqLmWGRy"
   },
   "source": [
    "Horizontal:\n",
    "\n",
    "| normalize | word1 | word2 | word3     |\n",
    "|-----------|-------|-------|-----------|\n",
    "| female    | woman | girl  | lady      |\n",
    "| male      | man   | boy   | gentleman |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToFbLxsDYInk"
   },
   "source": [
    "\n",
    "Vertical:\n",
    "\n",
    "| female    | normalize |\n",
    "|-----------|-----------|\n",
    "| woman     | word1     |\n",
    "| girl      | word2     |\n",
    "| lady      | word3     | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCrIw4CRYeBC"
   },
   "source": [
    "As you can see, your dictionary needs to have a `normalize` field that lets the annotator know which entity labels to use, and another field that lets the annotator know a list of words it should be looking to match. Here's how to set the format that your dictionary uses:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_epZRQiV85Y"
   },
   "source": [
    "```\n",
    "contextualParser = ContextualParserApproach() \\\n",
    "    .setDictionary(\"dictionary.tsv\", options={\"orientation\":\"vertical\"}) # default is horizontal\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CvM-FgLZgKk"
   },
   "source": [
    "### **3.2. Dictionary-related JSON Properties**\n",
    "\n",
    "When working with dictionaries, there are 2 properties in the JSON configuration file to be aware of:\n",
    "\n",
    "- `ruleScope`\n",
    "- `matchScope`\n",
    "\n",
    "This is especially true when you have multi-word entities in your dictionary.\n",
    "\n",
    "Let's take an example of a dictionary that contains a list of cities, sometimes made up of multiple words:\n",
    "\n",
    "| normalize | word1 | word2 | word3     |\n",
    "|-----------|-------|-------|-----------|\n",
    "| City      | New York | Salt Lake City  | Washington      |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5weyyiIMzNr"
   },
   "source": [
    "Let's say we're working with the following text: ***I love New York. Salt Lake City is nice too.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xg8eJwuTJcB6"
   },
   "source": [
    "With the following JSON properties, here's what you would get:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1TNWtywoGWb"
   },
   "source": [
    "```\n",
    "{\n",
    "  \"entity\": \"City\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"matchScope\": \"sub-token\",\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wweg5_C9JuWm"
   },
   "source": [
    "`OUTPUT: []`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNYWPr2eLJ5d"
   },
   "source": [
    "When `ruleScope` is set to `\"sentence\"`, the annotator attempts to find matches at the token level, parsing through each token in the sentence one by one, looking for a match with the dictionary items. Since `\"New York\"` and `\"Salt Lake City\"` are made up of multiple tokens, the annotator would never find a match from the dictionary. Let's change `ruleScope` to `\"document\"`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xib9zHN7oBvV"
   },
   "source": [
    "```\n",
    "{\n",
    "  \"entity\": \"City\",\n",
    "  \"ruleScope\": \"document\",\n",
    "  \"matchScope\": \"sub-token\",\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYmkGdtALXzK"
   },
   "source": [
    "`OUTPUT: [New York, Salt Lake City]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nmc7rvfsdFK9"
   },
   "source": [
    "When `ruleScope` is set to `\"document\"`, the annotator attempts to find matches by parsing through each sentence in the document one by one, looking for a match with the dictionary items. Beware of how you set `matchScope`. Taking the previous example, if we were to set `matchScope` to `\"token\"` instead of `\"sub-token\"`, here's what would happen:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkzDTcgen9aS"
   },
   "source": [
    "```\n",
    "{\n",
    "  \"entity\": \"City\",\n",
    "  \"ruleScope\": \"document\",\n",
    "  \"matchScope\": \"token\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "np7rBSQmetA9"
   },
   "source": [
    "`OUTPUT: [I love New York., Salt Lake City is nice too.]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5nNtvLaeOv9"
   },
   "source": [
    "As you can see, when `ruleScope` is at the document level, if you set your `matchScope` to the token level, the annotator will output each sentence containing the matched entities as individual chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zTtAFwqQdNy"
   },
   "source": [
    "### **3.3. Working with Multi-Word Matches**\n",
    "\n",
    "Although not directly related to dictionaries, if we build on top of what we've just seen, there is a use-case that is particularly in demand when working with the `ContextualParser` annotator: finding regex matches for chunks of words that span across multiple tokens. \n",
    "\n",
    "Let's re-iterate how the `ruleScope` property works: when `ruleScope` is set to `\"sentence\"`, we're looking for a match on each token of a sentence. When `ruleScope` is set to `\"document\"`, we're looking for a match on each sentence of a document. \n",
    "\n",
    "So now let's imagine you're parsing through medical documents trying to tag the *Family History* headers in those documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eJWXRrg0Qu0q"
   },
   "source": [
    "```\n",
    "{\n",
    "  \"entity\": \"Family History Header\",\n",
    "  \"regex\": \"[f|F]amily\\s+[h|H]istory\",  \n",
    "  \"ruleScope\": \"document\",\n",
    "  \"matchScope\": \"sub-token\"\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OP4H1t3CQyg5"
   },
   "source": [
    "`OUTPUT: [Family History, family history, Family history]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8PgD_N9RFTP"
   },
   "source": [
    "If you had set `ruleScope` to  `\"sentence\"`, here's what would have happened:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ljqiVjWaRJPe"
   },
   "source": [
    "```\n",
    "{\n",
    "  \"entity\": \"Family History Header\",\n",
    "  \"regex\": \"[f|F]amily\\s+[h|H]istory\",  \n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"matchScope\": \"sub-token\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GfcGKW81RMKN"
   },
   "source": [
    "`OUTPUT: []`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yMMgQdK5RPYb"
   },
   "source": [
    "Since Family History is divided into two different tokens, the annotator will never find a match since it's now looking for a match on each token of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehWiHjziPfGV"
   },
   "source": [
    "# **Running a Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VGz-nXwCDLgb"
   },
   "source": [
    "## **Example 1: Detecting Cities**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thaF2bObwDXd"
   },
   "source": [
    "Let's try running through some examples to build on top of what you've learned so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mcWKcPZO-upM"
   },
   "outputs": [],
   "source": [
    "# Here's some sample text\n",
    "sample_text = \"\"\"Peter Parker is a nice guy and lives in New York . Bruce Wayne is also a nice guy and lives in San Antonio and Gotham City . \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y4Ipms-4PwoN",
    "outputId": "40da49b7-1140-43ac-e7ab-a4d2b4817d8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City\n",
      "New York\n",
      "Gotham City\n",
      "San Antonio\n",
      "Salt Lake City"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to detect cities\n",
    "cities = \"\"\"City\\nNew York\\nGotham City\\nSan Antonio\\nSalt Lake City\"\"\"\n",
    "\n",
    "with open('cities.tsv', 'w') as f:\n",
    "    f.write(cities)\n",
    "\n",
    "# Check what dictionary looks like\n",
    "!cat cities.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Jvi1mbqY_LeA"
   },
   "outputs": [],
   "source": [
    "# Create JSON file\n",
    "cities = {\n",
    "  \"entity\": \"City\",\n",
    "  \"ruleScope\": \"document\", \n",
    "  \"matchScope\":\"sub-token\",\n",
    "  \"completeMatchRegex\": \"false\"\n",
    "} \n",
    "\n",
    "import json\n",
    "with open('cities.json', 'w') as f:\n",
    "    json.dump(cities, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yjQmEjWNQHvx"
   },
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "document_assembler = nlp.DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = nlp.SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = nlp.Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "contextual_parser = medical.ContextualParserApproach() \\\n",
    "    .setInputCols([\"sentence\", \"token\"])\\\n",
    "    .setOutputCol(\"entity\")\\\n",
    "    .setJsonPath(\"cities.json\")\\\n",
    "    .setCaseSensitive(True)\\\n",
    "    .setDictionary('cities.tsv', options={\"orientation\":\"vertical\"})\n",
    "\n",
    "chunk_converter = medical.ChunkConverter() \\\n",
    "    .setInputCols([\"entity\"]) \\\n",
    "    .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "parserPipeline = nlp.Pipeline(stages=[\n",
    "        document_assembler, \n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        contextual_parser,\n",
    "        chunk_converter,\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxPVaa4fYCb5"
   },
   "outputs": [],
   "source": [
    "# Create a lightpipeline model\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "parserModel = parserPipeline.fit(empty_data)\n",
    "\n",
    "light_model = nlp.LightPipeline(parserModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3T1xnn0vYOMA"
   },
   "outputs": [],
   "source": [
    "# Annotate the sample text\n",
    "annotations = light_model.fullAnnotate(sample_text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NY5zFoeSb8fg",
    "outputId": "a95d20b5-b28c-4f12-8a47-5351bd85d465"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Annotation(chunk, 40, 47, New York, {'field': 'City', 'tokenIndex': '9', 'confidence': '0.50', 'ner_source': 'ner_chunk', 'normalized': 'City', 'entity': 'City', 'sentence': '0'}, []),\n",
       " Annotation(chunk, 95, 105, San Antonio, {'field': 'City', 'tokenIndex': '10', 'confidence': '0.50', 'ner_source': 'ner_chunk', 'normalized': 'City', 'entity': 'City', 'sentence': '1'}, []),\n",
       " Annotation(chunk, 111, 121, Gotham City, {'field': 'City', 'tokenIndex': '13', 'confidence': '0.50', 'ner_source': 'ner_chunk', 'normalized': 'City', 'entity': 'City', 'sentence': '1'}, [])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check outputs\n",
    "annotations.get('ner_chunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "BBRie-vWXzK6",
    "outputId": "7641088d-6d5e-4bb9-e05b-0362d0e0502a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n",
       "    \n",
       "    .spark-nlp-display-scroll-entities {\n",
       "        border: 1px solid #E7EDF0;\n",
       "        border-radius: 3px;\n",
       "        text-align: justify;\n",
       "        \n",
       "    }\n",
       "    .spark-nlp-display-scroll-entities span {  \n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #536B76;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-wrapper{\n",
       "    \n",
       "        display: inline-grid;\n",
       "        text-align: center;\n",
       "        border-radius: 4px;\n",
       "        margin: 0 2px 5px 2px;\n",
       "        padding: 1px\n",
       "    }\n",
       "    .spark-nlp-display-entity-name{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        background: #f1f2f3;\n",
       "        border-width: medium;\n",
       "        text-align: center;\n",
       "        \n",
       "        font-weight: 400;\n",
       "        \n",
       "        border-radius: 5px;\n",
       "        padding: 2px 5px;\n",
       "        display: block;\n",
       "        margin: 3px 2px;\n",
       "    \n",
       "    }\n",
       "    .spark-nlp-display-entity-type{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-resolution{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Vistol Regular', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-others{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        font-weight: 400;\n",
       "    }\n",
       "\n",
       "</style>\n",
       " <span class=\"spark-nlp-display-others\" style=\"background-color: white\">Peter Parker is a nice guy and lives in </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #684986\"><span class=\"spark-nlp-display-entity-name\">New York </span><span class=\"spark-nlp-display-entity-type\">City</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> . Bruce Wayne is also a nice guy and lives in </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #684986\"><span class=\"spark-nlp-display-entity-name\">San Antonio </span><span class=\"spark-nlp-display-entity-type\">City</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #684986\"><span class=\"spark-nlp-display-entity-name\">Gotham City </span><span class=\"spark-nlp-display-entity-type\">City</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> . </span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize outputs\n",
    "from sparknlp_display import NerVisualizer\n",
    "\n",
    "visualiser = NerVisualizer()\n",
    "\n",
    "visualiser.display(annotations, label_col='ner_chunk', document_col='document', save_path=\"display_result.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtD1HJ7SABU7"
   },
   "source": [
    "Feel free to experiment with the annotator parameters and JSON properties to see how the output might change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lR6FnTsyBAjn"
   },
   "source": [
    "## **Example 2: Detect Gender and Age**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "tXK5CLYfDhNB"
   },
   "outputs": [],
   "source": [
    "# Here's some sample text\n",
    "sample_text = \"\"\"A 28 year old female with a history of gestational diabetes mellitus diagnosed 8 years ago. \n",
    "                 3 years ago, he reported an episode of HTG-induced pancreatitis . \n",
    "                 5 months old boy with repeated concussions.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0OZqYhDFDhNB",
    "outputId": "c79e0ff8-a2f7-4f9f-804f-a14a4d9c7a9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "male,man,male,boy,gentleman,he,him\n",
      "female,woman,female,girl,lady,old-lady,she,her\n",
      "neutral,they,neutral,it"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to detect gender\n",
    "gender = '''male,man,male,boy,gentleman,he,him\n",
    "female,woman,female,girl,lady,old-lady,she,her\n",
    "neutral,they,neutral,it'''\n",
    "\n",
    "with open('./gender.csv', 'w') as f:\n",
    "    f.write(gender)\n",
    "\n",
    "# Check what dictionary looks like\n",
    "!cat gender.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "U5-tYW1iDhNB"
   },
   "outputs": [],
   "source": [
    "# Create JSON file for gender\n",
    "gender = {\n",
    "  \"entity\": \"Gender\",\n",
    "  \"ruleScope\": \"sentence\", \n",
    "  \"completeMatchRegex\": \"true\",\n",
    "  \"matchScope\":\"token\"\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('./gender.json', 'w') as f:\n",
    "    json.dump(gender, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "bynTPmlQHEPv"
   },
   "outputs": [],
   "source": [
    "# Create JSON file for age\n",
    "age = {\n",
    "  \"entity\": \"Age\",\n",
    "  \"ruleScope\": \"sentence\",\n",
    "  \"matchScope\":\"token\",\n",
    "  \"regex\":\"\\\\d{1,3}\",\n",
    "  \"prefix\":[\"age of\", \"age\"],\n",
    "  \"suffix\": [\"-years-old\", \"years-old\", \"-year-old\",\n",
    "             \"-months-old\", \"-month-old\", \"-months-old\",\n",
    "             \"-day-old\", \"-days-old\", \"month old\",\n",
    "             \"days old\", \"year old\", \"years old\", \n",
    "             \"years\", \"year\", \"months\", \"old\"],\n",
    "  \"contextLength\": 25,\n",
    "  \"contextException\": [\"ago\"],\n",
    "  \"exceptionDistance\": 10\n",
    "}\n",
    "\n",
    "with open('./age.json', 'w') as f:\n",
    "    json.dump(age, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "pBQVujx1DhNC"
   },
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "document_assembler = nlp.DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = nlp.SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = nlp.Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "gender_contextual_parser = medical.ContextualParserApproach() \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"chunk_gender\") \\\n",
    "    .setJsonPath(\"gender.json\") \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setDictionary('gender.csv', options={\"delimiter\":\",\"}) \\\n",
    "    .setPrefixAndSuffixMatch(False)      \n",
    "\n",
    "age_contextual_parser = medical.ContextualParserApproach() \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"chunk_age\") \\\n",
    "    .setJsonPath(\"age.json\") \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setPrefixAndSuffixMatch(False)\\\n",
    "    .setShortestContextMatch(True)\\\n",
    "    .setOptionalContextRules(False) \n",
    "\n",
    "chunk_merger = medical.ChunkMergeApproach() \\\n",
    "    .setInputCols([\"chunk_gender\", \"chunk_age\"]) \\\n",
    "    .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "parserPipeline = nlp.Pipeline(stages=[\n",
    "        document_assembler, \n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        gender_contextual_parser,\n",
    "        age_contextual_parser,\n",
    "        chunk_merger\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36ZxeslFDhNC"
   },
   "outputs": [],
   "source": [
    "# Create a lightpipeline model\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "parserModel = parserPipeline.fit(empty_data)\n",
    "\n",
    "light_model = nlp.LightPipeline(parserModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OIhdQ4IjDhNC"
   },
   "outputs": [],
   "source": [
    "# Annotate the sample text\n",
    "annotations = light_model.fullAnnotate(sample_text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1tdgMbaWDhNC",
    "outputId": "e9e9df19-49b0-489d-fa85-0ac5ebf35cb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Annotation(chunk, 2, 3, 28, {'tokenIndex': '1', 'entity': 'Age', 'confidence': '0.74', 'field': 'Age', 'chunk': '0', 'normalized': '', 'sentence': '0'}, []),\n",
       " Annotation(chunk, 14, 19, female, {'tokenIndex': '4', 'entity': 'Gender', 'confidence': '0.50', 'field': 'Gender', 'chunk': '1', 'normalized': 'female', 'sentence': '0'}, []),\n",
       " Annotation(chunk, 79, 79, 8, {'tokenIndex': '13', 'entity': 'Age', 'confidence': '0.74', 'field': 'Age', 'chunk': '2', 'normalized': '', 'sentence': '0'}, []),\n",
       " Annotation(chunk, 110, 110, 3, {'tokenIndex': '0', 'entity': 'Age', 'confidence': '0.74', 'field': 'Age', 'chunk': '3', 'normalized': '', 'sentence': '1'}, []),\n",
       " Annotation(chunk, 123, 124, he, {'tokenIndex': '4', 'entity': 'Gender', 'confidence': '0.50', 'field': 'Gender', 'chunk': '4', 'normalized': 'male', 'sentence': '1'}, []),\n",
       " Annotation(chunk, 194, 194, 5, {'tokenIndex': '0', 'entity': 'Age', 'confidence': '0.74', 'field': 'Age', 'chunk': '5', 'normalized': '', 'sentence': '2'}, []),\n",
       " Annotation(chunk, 207, 209, boy, {'tokenIndex': '3', 'entity': 'Gender', 'confidence': '0.50', 'field': 'Gender', 'chunk': '6', 'normalized': 'male', 'sentence': '2'}, [])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check outputs\n",
    "annotations.get('ner_chunk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "id": "xw0mQKMxDhND",
    "outputId": "998092f8-649f-44d0-c6db-b89f134ab181"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n",
       "    \n",
       "    .spark-nlp-display-scroll-entities {\n",
       "        border: 1px solid #E7EDF0;\n",
       "        border-radius: 3px;\n",
       "        text-align: justify;\n",
       "        \n",
       "    }\n",
       "    .spark-nlp-display-scroll-entities span {  \n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #536B76;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-wrapper{\n",
       "    \n",
       "        display: inline-grid;\n",
       "        text-align: center;\n",
       "        border-radius: 4px;\n",
       "        margin: 0 2px 5px 2px;\n",
       "        padding: 1px\n",
       "    }\n",
       "    .spark-nlp-display-entity-name{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        background: #f1f2f3;\n",
       "        border-width: medium;\n",
       "        text-align: center;\n",
       "        \n",
       "        font-weight: 400;\n",
       "        \n",
       "        border-radius: 5px;\n",
       "        padding: 2px 5px;\n",
       "        display: block;\n",
       "        margin: 3px 2px;\n",
       "    \n",
       "    }\n",
       "    .spark-nlp-display-entity-type{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-resolution{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Vistol Regular', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-others{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        font-weight: 400;\n",
       "    }\n",
       "\n",
       "</style>\n",
       " <span class=\"spark-nlp-display-others\" style=\"background-color: white\">A </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #ffe0ac\"><span class=\"spark-nlp-display-entity-name\">28 </span><span class=\"spark-nlp-display-entity-type\">Age</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> year old </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #ffacb7\"><span class=\"spark-nlp-display-entity-name\">female </span><span class=\"spark-nlp-display-entity-type\">Gender</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> with a history of gestational diabetes mellitus diagnosed </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #ffe0ac\"><span class=\"spark-nlp-display-entity-name\">8 </span><span class=\"spark-nlp-display-entity-type\">Age</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> years ago. <br>                 </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #ffe0ac\"><span class=\"spark-nlp-display-entity-name\">3 </span><span class=\"spark-nlp-display-entity-type\">Age</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> years ago, </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #ffacb7\"><span class=\"spark-nlp-display-entity-name\">he </span><span class=\"spark-nlp-display-entity-type\">Gender</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> reported an episode of HTG-induced pancreatitis . <br>                 </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #ffe0ac\"><span class=\"spark-nlp-display-entity-name\">5 </span><span class=\"spark-nlp-display-entity-type\">Age</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> months old </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #ffacb7\"><span class=\"spark-nlp-display-entity-name\">boy </span><span class=\"spark-nlp-display-entity-type\">Gender</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> with repeated concussions.</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize outputs\n",
    "from sparknlp_display import NerVisualizer\n",
    "\n",
    "visualiser = NerVisualizer()\n",
    "\n",
    "visualiser.display(annotations, label_col='ner_chunk', document_col='document', save_path=\"display_result_2.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOtVACnl_t3n"
   },
   "source": [
    "Feel free to experiment with the annotator parameters and JSON properties to see how the output might change. If you're looking to work on running the pipeline on a full dataset, just make sure to use the `fit()` and `transform()` methods directly on your dataset instead of using the lightpipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L6nRUUrRRfoB",
    "outputId": "a6c86904-7f0b-4c25-bc71-75e986a246f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                text|            document|            sentence|               token|        chunk_gender|           chunk_age|           ner_chunk|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|A 28 year old fem...|[{document, 0, 23...|[{document, 0, 90...|[{token, 0, 0, A,...|[{chunk, 14, 19, ...|[{chunk, 2, 3, 28...|[{chunk, 2, 3, 28...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create example dataframe with sample text\n",
    "data = spark.createDataFrame([[sample_text]]).toDF(\"text\")\n",
    "\n",
    "# Fit and show\n",
    "results = parserPipeline.fit(data).transform(data)\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orB8zsF9G6H4",
    "outputId": "e1aa5031-3c40-4cda-dadd-691a563161dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|       result|\n",
      "+-------------+\n",
      "|[28, 8, 3, 5]|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select(\"chunk_age.result\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qzn7Qv3_HERi",
    "outputId": "32442bf2-de48-4c58-d7ce-4dde3af77fe5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|           result|\n",
      "+-----------------+\n",
      "|[female, he, boy]|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select(\"chunk_gender.result\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Example 3: Detect Test Result and Dates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Medical text has a complex structure. Sometimes, our deid ner model mistakenly identifies certain entities as `dates`, such as test results or dimensions. In such cases, we utilize a rule-based NER (contextual parser)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(\n",
    "    {'text': [\n",
    "        '''Mark White was born 06-20-1990. Mark White is 45 years old. Test Result: RHC 11-22-33, LHC 11\\\\22\\\\33, Wedge 11-16-1972.''',\n",
    "        '''John was born on 07-25-2000 and he was discharged on 03/15/2022. Test Result: RV 26/2. Left Ventricle 26-2.  Wedge 11/16/19.''',\n",
    "        '''John Moore was born 03/20/2012 and he is 18 years old. Test Result: Pulmonary Artery 07\\\\31\\\\19 ( PA 07/31/19 ).'''\n",
    "]})\n",
    "\n",
    "# pre-process text\n",
    "#data['text'].replace('\\\\', '\\\\\\\\', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|text                                                                                                                        |\n",
      "+----------------------------------------------------------------------------------------------------------------------------+\n",
      "|Mark White was born 06-20-1990. Mark White is 45 years old. Test Result: RHC 11-22-33, LHC 11\\22\\33, Wedge 11-16-1972.      |\n",
      "|John was born on 07-25-2000 and he was discharged on 03/15/2022. Test Result: RV 26/2. Left Ventricle 26-2.  Wedge 11/16/19.|\n",
      "|John Moore was born 03/20/2012 and he is 18 years old. Test Result: Pulmonary Artery 07\\31\\19 ( PA 07/31/19 ).              |\n",
      "+----------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert data for Spark processing\n",
    "input_df = spark.createDataFrame(data)\n",
    "input_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create JSON file for test result patterns (to be used with ContextualizedParser)\n",
    "test_result_rules = {\n",
    "    'entity': 'test_result',\n",
    "    'ruleScope': 'sentence',\n",
    "    'matchScope': 'token',\n",
    "    'regex': '(\\d{2}.?\\d{2}.?\\d{2})|(\\d{2}.?\\d{2}.?\\d{4})|(\\d{2}.?\\d{1})',\n",
    "    'prefix': ['Right atrium', 'RA',\n",
    "               'Left atrium', 'LA',\n",
    "               'Wedge', \"Catheterization\",\n",
    "               'Right Heart Catheterization', 'RHC',\n",
    "               'Left Heart Catheterization', 'LHC',\n",
    "               'PA', 'pulmonary artery',\n",
    "               'RV', 'right ventricle'],\n",
    "    'suffix': ['.', ','],\n",
    "    'contextLength': 45,\n",
    "    'completeMatchRegex': 'true',\n",
    "    \"contextException\": [\"born\",  \"on\"],\n",
    "    \"exceptionDistance\":10,\n",
    "}\n",
    "\n",
    "with open('test_result_rules.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_result_rules, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[ | ]embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n",
      "ner_deid_subentity_augmented download started this may take some time.\n",
      "[ | ]ner_deid_subentity_augmented download started this may take some time.\n",
      "Approximate size to download 14.1 MB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "embeddings = nlp.WordEmbeddingsModel \\\n",
    "    .pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols([\"sentence\", \"token\"])\\\n",
    "    .setOutputCol(\"word_embeddings\")\n",
    "\n",
    "# identify test results\n",
    "test_contextual_parser = medical.ContextualParserApproach() \\\n",
    "    .setInputCols(['sentence', 'token']) \\\n",
    "    .setOutputCol('test_result') \\\n",
    "    .setJsonPath('test_result_rules.json') \\\n",
    "    .setCaseSensitive(False) \\\n",
    "    .setPrefixAndSuffixMatch(True) \\\n",
    "    .setShortestContextMatch(True) \\\n",
    "    .setOptionalContextRules(False)\n",
    "\n",
    "test_contextual_parser_converter = medical.ChunkConverter() \\\n",
    "    .setInputCols(['test_result']) \\\n",
    "    .setOutputCol('test_result_chunk')\n",
    "\n",
    "# Deid NER\n",
    "deid_ner = medical.NerModel \\\n",
    "    .pretrained('ner_deid_subentity_augmented', 'en', 'clinical/models') \\\n",
    "    .setInputCols(['sentence', 'token', 'word_embeddings']) \\\n",
    "    .setOutputCol('deid_ner')\n",
    "\n",
    "deid_ner_converter = medical.NerConverter() \\\n",
    "    .setInputCols(['sentence', 'token', 'deid_ner']) \\\n",
    "    .setOutputCol('deid_ner_chunk') \\\n",
    "    .setWhiteList(['date'])\n",
    "\n",
    "# merge\n",
    "chunk_merger = medical.ChunkMergeApproach() \\\n",
    "    .setInputCols(['test_result_chunk', 'deid_ner_chunk']) \\\n",
    "    .setOutputCol('ner_chunk') \\\n",
    "    .setMergeOverlapping(True)\n",
    "\n",
    "parserPipeline = nlp.Pipeline().setStages([\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    embeddings,\n",
    "    test_contextual_parser,\n",
    "    test_contextual_parser_converter,\n",
    "    deid_ner,\n",
    "    deid_ner_converter,\n",
    "    chunk_merger,\n",
    "])\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "pipeline_model = parserPipeline.fit(empty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = pipeline_model.transform(input_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(result_col, output):\n",
    "\n",
    "    output.select(F.explode(F.arrays_zip(output[result_col].result,\n",
    "                                         output[result_col].begin,\n",
    "                                         output[result_col].end,\n",
    "                                         output[result_col].metadata,)).alias(\"cols\")) \\\n",
    "          .select(F.expr(\"cols['0']\").alias(chunk_col),\n",
    "                  F.expr(\"cols['1']\").alias(\"begin\"),\n",
    "                  F.expr(\"cols['2']\").alias(\"end\"),\n",
    "                  F.expr(\"cols['3']['entity']\").alias(\"entity\"),\n",
    "                  F.expr(\"cols['3']['confidence']\").alias(\"confidence\")) \\\n",
    "          .show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De-identified NER Results\n",
      "=========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+---+------+----------+\n",
      "|deid_ner_chunk|begin|end|entity|confidence|\n",
      "+--------------+-----+---+------+----------+\n",
      "|06-20-1990    |20   |29 |DATE  |0.9677    |\n",
      "|11-22-33      |77   |84 |DATE  |0.9997    |\n",
      "|11-16-1972    |107  |116|DATE  |0.9966    |\n",
      "|07-25-2000    |17   |26 |DATE  |0.987     |\n",
      "|03/15/2022    |53   |62 |DATE  |1.0       |\n",
      "|11/16/19      |115  |122|DATE  |1.0       |\n",
      "|03/20/2012    |20   |29 |DATE  |0.9998    |\n",
      "|07/31/19      |99   |106|DATE  |1.0       |\n",
      "+--------------+-----+---+------+----------+\n",
      "\n",
      "Contextual Test Results\n",
      "=======================\n",
      "\n",
      "+-----------------+-----+---+-----------+----------+\n",
      "|test_result_chunk|begin|end|entity     |confidence|\n",
      "+-----------------+-----+---+-----------+----------+\n",
      "|11-22-33         |77   |84 |test_result|0.74      |\n",
      "|11\\22\\33         |91   |98 |test_result|0.66      |\n",
      "|11-16-1972       |107  |116|test_result|0.57      |\n",
      "|26/2             |81   |84 |test_result|0.74      |\n",
      "|11/16/19         |115  |122|test_result|0.73      |\n",
      "|07\\31\\19         |85   |92 |test_result|0.66      |\n",
      "|07/31/19         |99   |106|test_result|0.59      |\n",
      "+-----------------+-----+---+-----------+----------+\n",
      "\n",
      "Merged NER Results\n",
      "==================\n",
      "\n",
      "+----------+-----+---+-----------+----------+\n",
      "|ner_chunk |begin|end|entity     |confidence|\n",
      "+----------+-----+---+-----------+----------+\n",
      "|06-20-1990|20   |29 |DATE       |0.9677    |\n",
      "|11-22-33  |77   |84 |test_result|0.74      |\n",
      "|11\\22\\33  |91   |98 |test_result|0.66      |\n",
      "|11-16-1972|107  |116|test_result|0.57      |\n",
      "|07-25-2000|17   |26 |DATE       |0.987     |\n",
      "|03/15/2022|53   |62 |DATE       |1.0       |\n",
      "|26/2      |81   |84 |test_result|0.74      |\n",
      "|11/16/19  |115  |122|test_result|0.73      |\n",
      "|03/20/2012|20   |29 |DATE       |0.9998    |\n",
      "|07\\31\\19  |85   |92 |test_result|0.66      |\n",
      "|07/31/19  |99   |106|test_result|0.59      |\n",
      "+----------+-----+---+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_list = {\"deid_ner_chunk\": \"De-identified NER Results\",\n",
    "               \"test_result_chunk\": \"Contextual Test Results\",\n",
    "               \"ner_chunk\": \"Merged NER Results\"}\n",
    "\n",
    "for chunk_col, title in output_list.items():\n",
    "    print(f\"{title}\\n{'=' * len(title)}\\n\")\n",
    "    process_output(chunk_col, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De-identified NER Results\n",
      "=========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n",
       "    \n",
       "    .spark-nlp-display-scroll-entities {\n",
       "        border: 1px solid #E7EDF0;\n",
       "        border-radius: 3px;\n",
       "        text-align: justify;\n",
       "        \n",
       "    }\n",
       "    .spark-nlp-display-scroll-entities span {  \n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #536B76;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-wrapper{\n",
       "    \n",
       "        display: inline-grid;\n",
       "        text-align: center;\n",
       "        border-radius: 4px;\n",
       "        margin: 0 2px 5px 2px;\n",
       "        padding: 1px\n",
       "    }\n",
       "    .spark-nlp-display-entity-name{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        background: #f1f2f3;\n",
       "        border-width: medium;\n",
       "        text-align: center;\n",
       "        \n",
       "        font-weight: 400;\n",
       "        \n",
       "        border-radius: 5px;\n",
       "        padding: 2px 5px;\n",
       "        display: block;\n",
       "        margin: 3px 2px;\n",
       "    \n",
       "    }\n",
       "    .spark-nlp-display-entity-type{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-resolution{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Vistol Regular', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-others{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        font-weight: 400;\n",
       "    }\n",
       "\n",
       "</style>\n",
       " <span class=\"spark-nlp-display-others\" style=\"background-color: white\">Mark White was born </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #a6b1e1\"><span class=\"spark-nlp-display-entity-name\">06-20-1990 </span><span class=\"spark-nlp-display-entity-type\">DATE</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">. Mark White is 45 years old. Test Result: RHC </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #a6b1e1\"><span class=\"spark-nlp-display-entity-name\">11-22-33 </span><span class=\"spark-nlp-display-entity-type\">DATE</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">, LHC 11\\22\\33, Wedge </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #a6b1e1\"><span class=\"spark-nlp-display-entity-name\">11-16-1972 </span><span class=\"spark-nlp-display-entity-type\">DATE</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">.</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n",
       "    \n",
       "    .spark-nlp-display-scroll-entities {\n",
       "        border: 1px solid #E7EDF0;\n",
       "        border-radius: 3px;\n",
       "        text-align: justify;\n",
       "        \n",
       "    }\n",
       "    .spark-nlp-display-scroll-entities span {  \n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #536B76;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-wrapper{\n",
       "    \n",
       "        display: inline-grid;\n",
       "        text-align: center;\n",
       "        border-radius: 4px;\n",
       "        margin: 0 2px 5px 2px;\n",
       "        padding: 1px\n",
       "    }\n",
       "    .spark-nlp-display-entity-name{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        background: #f1f2f3;\n",
       "        border-width: medium;\n",
       "        text-align: center;\n",
       "        \n",
       "        font-weight: 400;\n",
       "        \n",
       "        border-radius: 5px;\n",
       "        padding: 2px 5px;\n",
       "        display: block;\n",
       "        margin: 3px 2px;\n",
       "    \n",
       "    }\n",
       "    .spark-nlp-display-entity-type{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-resolution{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Vistol Regular', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-others{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        font-weight: 400;\n",
       "    }\n",
       "\n",
       "</style>\n",
       " <span class=\"spark-nlp-display-others\" style=\"background-color: white\">John was born on </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #a6b1e1\"><span class=\"spark-nlp-display-entity-name\">07-25-2000 </span><span class=\"spark-nlp-display-entity-type\">DATE</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and he was discharged on </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #a6b1e1\"><span class=\"spark-nlp-display-entity-name\">03/15/2022 </span><span class=\"spark-nlp-display-entity-type\">DATE</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">. Test Result: RV 26/2. Left Ventricle 26-2.  Wedge </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #a6b1e1\"><span class=\"spark-nlp-display-entity-name\">11/16/19 </span><span class=\"spark-nlp-display-entity-type\">DATE</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">.</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n",
       "    \n",
       "    .spark-nlp-display-scroll-entities {\n",
       "        border: 1px solid #E7EDF0;\n",
       "        border-radius: 3px;\n",
       "        text-align: justify;\n",
       "        \n",
       "    }\n",
       "    .spark-nlp-display-scroll-entities span {  \n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #536B76;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-wrapper{\n",
       "    \n",
       "        display: inline-grid;\n",
       "        text-align: center;\n",
       "        border-radius: 4px;\n",
       "        margin: 0 2px 5px 2px;\n",
       "        padding: 1px\n",
       "    }\n",
       "    .spark-nlp-display-entity-name{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        background: #f1f2f3;\n",
       "        border-width: medium;\n",
       "        text-align: center;\n",
       "        \n",
       "        font-weight: 400;\n",
       "        \n",
       "        border-radius: 5px;\n",
       "        padding: 2px 5px;\n",
       "        display: block;\n",
       "        margin: 3px 2px;\n",
       "    \n",
       "    }\n",
       "    .spark-nlp-display-entity-type{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-resolution{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Vistol Regular', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-others{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        font-weight: 400;\n",
       "    }\n",
       "\n",
       "</style>\n",
       " <span class=\"spark-nlp-display-others\" style=\"background-color: white\">John Moore was born </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #a6b1e1\"><span class=\"spark-nlp-display-entity-name\">03/20/2012 </span><span class=\"spark-nlp-display-entity-type\">DATE</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and he is 18 years old. Test Result: Pulmonary Artery 07\\31\\19 ( PA </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #a6b1e1\"><span class=\"spark-nlp-display-entity-name\">07/31/19 </span><span class=\"spark-nlp-display-entity-type\">DATE</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> ).</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual Test Results\n",
      "=======================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n",
       "    \n",
       "    .spark-nlp-display-scroll-entities {\n",
       "        border: 1px solid #E7EDF0;\n",
       "        border-radius: 3px;\n",
       "        text-align: justify;\n",
       "        \n",
       "    }\n",
       "    .spark-nlp-display-scroll-entities span {  \n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #536B76;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-wrapper{\n",
       "    \n",
       "        display: inline-grid;\n",
       "        text-align: center;\n",
       "        border-radius: 4px;\n",
       "        margin: 0 2px 5px 2px;\n",
       "        padding: 1px\n",
       "    }\n",
       "    .spark-nlp-display-entity-name{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        background: #f1f2f3;\n",
       "        border-width: medium;\n",
       "        text-align: center;\n",
       "        \n",
       "        font-weight: 400;\n",
       "        \n",
       "        border-radius: 5px;\n",
       "        padding: 2px 5px;\n",
       "        display: block;\n",
       "        margin: 3px 2px;\n",
       "    \n",
       "    }\n",
       "    .spark-nlp-display-entity-type{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-resolution{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Vistol Regular', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-others{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        font-weight: 400;\n",
       "    }\n",
       "\n",
       "</style>\n",
       " <span class=\"spark-nlp-display-others\" style=\"background-color: white\">Mark White was born 06-20-1990. Mark White is 45 years old. Test Result: RHC </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #AD1B5B\"><span class=\"spark-nlp-display-entity-name\">11-22-33 </span><span class=\"spark-nlp-display-entity-type\">test_result</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">, LHC </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #AD1B5B\"><span class=\"spark-nlp-display-entity-name\">11\\22\\33 </span><span class=\"spark-nlp-display-entity-type\">test_result</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">, Wedge </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #AD1B5B\"><span class=\"spark-nlp-display-entity-name\">11-16-1972 </span><span class=\"spark-nlp-display-entity-type\">test_result</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">.</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n",
       "    \n",
       "    .spark-nlp-display-scroll-entities {\n",
       "        border: 1px solid #E7EDF0;\n",
       "        border-radius: 3px;\n",
       "        text-align: justify;\n",
       "        \n",
       "    }\n",
       "    .spark-nlp-display-scroll-entities span {  \n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #536B76;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-wrapper{\n",
       "    \n",
       "        display: inline-grid;\n",
       "        text-align: center;\n",
       "        border-radius: 4px;\n",
       "        margin: 0 2px 5px 2px;\n",
       "        padding: 1px\n",
       "    }\n",
       "    .spark-nlp-display-entity-name{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        background: #f1f2f3;\n",
       "        border-width: medium;\n",
       "        text-align: center;\n",
       "        \n",
       "        font-weight: 400;\n",
       "        \n",
       "        border-radius: 5px;\n",
       "        padding: 2px 5px;\n",
       "        display: block;\n",
       "        margin: 3px 2px;\n",
       "    \n",
       "    }\n",
       "    .spark-nlp-display-entity-type{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-resolution{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Vistol Regular', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-others{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        font-weight: 400;\n",
       "    }\n",
       "\n",
       "</style>\n",
       " <span class=\"spark-nlp-display-others\" style=\"background-color: white\">John was born on 07-25-2000 and he was discharged on 03/15/2022. Test Result: RV </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #7D1B05\"><span class=\"spark-nlp-display-entity-name\">26/2 </span><span class=\"spark-nlp-display-entity-type\">test_result</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">. Left Ventricle 26-2.  Wedge </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #7D1B05\"><span class=\"spark-nlp-display-entity-name\">11/16/19 </span><span class=\"spark-nlp-display-entity-type\">test_result</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">.</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n",
       "    \n",
       "    .spark-nlp-display-scroll-entities {\n",
       "        border: 1px solid #E7EDF0;\n",
       "        border-radius: 3px;\n",
       "        text-align: justify;\n",
       "        \n",
       "    }\n",
       "    .spark-nlp-display-scroll-entities span {  \n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #536B76;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-wrapper{\n",
       "    \n",
       "        display: inline-grid;\n",
       "        text-align: center;\n",
       "        border-radius: 4px;\n",
       "        margin: 0 2px 5px 2px;\n",
       "        padding: 1px\n",
       "    }\n",
       "    .spark-nlp-display-entity-name{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        background: #f1f2f3;\n",
       "        border-width: medium;\n",
       "        text-align: center;\n",
       "        \n",
       "        font-weight: 400;\n",
       "        \n",
       "        border-radius: 5px;\n",
       "        padding: 2px 5px;\n",
       "        display: block;\n",
       "        margin: 3px 2px;\n",
       "    \n",
       "    }\n",
       "    .spark-nlp-display-entity-type{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-resolution{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Vistol Regular', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-others{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        font-weight: 400;\n",
       "    }\n",
       "\n",
       "</style>\n",
       " <span class=\"spark-nlp-display-others\" style=\"background-color: white\">John Moore was born 03/20/2012 and he is 18 years old. Test Result: Pulmonary Artery </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #694C20\"><span class=\"spark-nlp-display-entity-name\">07\\31\\19 </span><span class=\"spark-nlp-display-entity-type\">test_result</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> ( PA </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #694C20\"><span class=\"spark-nlp-display-entity-name\">07/31/19 </span><span class=\"spark-nlp-display-entity-type\">test_result</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> ).</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged NER Results\n",
      "==================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n",
       "    \n",
       "    .spark-nlp-display-scroll-entities {\n",
       "        border: 1px solid #E7EDF0;\n",
       "        border-radius: 3px;\n",
       "        text-align: justify;\n",
       "        \n",
       "    }\n",
       "    .spark-nlp-display-scroll-entities span {  \n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #536B76;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-wrapper{\n",
       "    \n",
       "        display: inline-grid;\n",
       "        text-align: center;\n",
       "        border-radius: 4px;\n",
       "        margin: 0 2px 5px 2px;\n",
       "        padding: 1px\n",
       "    }\n",
       "    .spark-nlp-display-entity-name{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        background: #f1f2f3;\n",
       "        border-width: medium;\n",
       "        text-align: center;\n",
       "        \n",
       "        font-weight: 400;\n",
       "        \n",
       "        border-radius: 5px;\n",
       "        padding: 2px 5px;\n",
       "        display: block;\n",
       "        margin: 3px 2px;\n",
       "    \n",
       "    }\n",
       "    .spark-nlp-display-entity-type{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-resolution{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Vistol Regular', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-others{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        font-weight: 400;\n",
       "    }\n",
       "\n",
       "</style>\n",
       " <span class=\"spark-nlp-display-others\" style=\"background-color: white\">Mark White was born </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #a6b1e1\"><span class=\"spark-nlp-display-entity-name\">06-20-1990 </span><span class=\"spark-nlp-display-entity-type\">DATE</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">. Mark White is 45 years old. Test Result: RHC </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #88A26B\"><span class=\"spark-nlp-display-entity-name\">11-22-33 </span><span class=\"spark-nlp-display-entity-type\">test_result</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">, LHC </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #88A26B\"><span class=\"spark-nlp-display-entity-name\">11\\22\\33 </span><span class=\"spark-nlp-display-entity-type\">test_result</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">, Wedge </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #88A26B\"><span class=\"spark-nlp-display-entity-name\">11-16-1972 </span><span class=\"spark-nlp-display-entity-type\">test_result</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">.</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n",
       "    \n",
       "    .spark-nlp-display-scroll-entities {\n",
       "        border: 1px solid #E7EDF0;\n",
       "        border-radius: 3px;\n",
       "        text-align: justify;\n",
       "        \n",
       "    }\n",
       "    .spark-nlp-display-scroll-entities span {  \n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #536B76;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-wrapper{\n",
       "    \n",
       "        display: inline-grid;\n",
       "        text-align: center;\n",
       "        border-radius: 4px;\n",
       "        margin: 0 2px 5px 2px;\n",
       "        padding: 1px\n",
       "    }\n",
       "    .spark-nlp-display-entity-name{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        background: #f1f2f3;\n",
       "        border-width: medium;\n",
       "        text-align: center;\n",
       "        \n",
       "        font-weight: 400;\n",
       "        \n",
       "        border-radius: 5px;\n",
       "        padding: 2px 5px;\n",
       "        display: block;\n",
       "        margin: 3px 2px;\n",
       "    \n",
       "    }\n",
       "    .spark-nlp-display-entity-type{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-resolution{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Vistol Regular', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-others{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        font-weight: 400;\n",
       "    }\n",
       "\n",
       "</style>\n",
       " <span class=\"spark-nlp-display-others\" style=\"background-color: white\">John was born on </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #a6b1e1\"><span class=\"spark-nlp-display-entity-name\">07-25-2000 </span><span class=\"spark-nlp-display-entity-type\">DATE</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and he was discharged on </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #a6b1e1\"><span class=\"spark-nlp-display-entity-name\">03/15/2022 </span><span class=\"spark-nlp-display-entity-type\">DATE</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">. Test Result: RV </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #0B7582\"><span class=\"spark-nlp-display-entity-name\">26/2 </span><span class=\"spark-nlp-display-entity-type\">test_result</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">. Left Ventricle 26-2.  Wedge </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #0B7582\"><span class=\"spark-nlp-display-entity-name\">11/16/19 </span><span class=\"spark-nlp-display-entity-type\">test_result</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">.</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n",
       "    \n",
       "    .spark-nlp-display-scroll-entities {\n",
       "        border: 1px solid #E7EDF0;\n",
       "        border-radius: 3px;\n",
       "        text-align: justify;\n",
       "        \n",
       "    }\n",
       "    .spark-nlp-display-scroll-entities span {  \n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #536B76;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-wrapper{\n",
       "    \n",
       "        display: inline-grid;\n",
       "        text-align: center;\n",
       "        border-radius: 4px;\n",
       "        margin: 0 2px 5px 2px;\n",
       "        padding: 1px\n",
       "    }\n",
       "    .spark-nlp-display-entity-name{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        background: #f1f2f3;\n",
       "        border-width: medium;\n",
       "        text-align: center;\n",
       "        \n",
       "        font-weight: 400;\n",
       "        \n",
       "        border-radius: 5px;\n",
       "        padding: 2px 5px;\n",
       "        display: block;\n",
       "        margin: 3px 2px;\n",
       "    \n",
       "    }\n",
       "    .spark-nlp-display-entity-type{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-resolution{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Vistol Regular', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-others{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        font-weight: 400;\n",
       "    }\n",
       "\n",
       "</style>\n",
       " <span class=\"spark-nlp-display-others\" style=\"background-color: white\">John Moore was born </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #a6b1e1\"><span class=\"spark-nlp-display-entity-name\">03/20/2012 </span><span class=\"spark-nlp-display-entity-type\">DATE</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and he is 18 years old. Test Result: Pulmonary Artery </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #34B337\"><span class=\"spark-nlp-display-entity-name\">07\\31\\19 </span><span class=\"spark-nlp-display-entity-type\">test_result</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> ( PA </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #34B337\"><span class=\"spark-nlp-display-entity-name\">07/31/19 </span><span class=\"spark-nlp-display-entity-type\">test_result</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> ).</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sparknlp_display import NerVisualizer\n",
    "results = output.collect()\n",
    "output_list = {\"deid_ner_chunk\": \"De-identified NER Results\",\n",
    "               \"test_result_chunk\": \"Contextual Test Results\",\n",
    "               \"ner_chunk\": \"Merged NER Results\"}\n",
    "\n",
    "visualizer = NerVisualizer()\n",
    "for chunk_col, title in output_list.items():\n",
    "    print(f\"{title}\\n{'=' * len(title)}\\n\")\n",
    "\n",
    "    for i in range(len(results)):\n",
    "         visualizer.display(results[i], label_col=chunk_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained Contextual Parser Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><b>Contextual Parser Model List</b>\r\n",
    "\r\n",
    "|index|model| description|\r\n",
    "|-----:|:-----|:-----|\r\n",
    "| 1| [date_of_birth_parser](https://nlp.johnsnowlabs.com/2023/08/22/date_of_birth_parser_en.html)  | This model can extract date-of-birth (DOB) entities in clinical texts. |\r\n",
    "| 1| [date_of_death_parser](https://nlp.johnsnowlabs.com/2023/08/22/date_of_birth_parser_en.html)  | This model can extract date-of-death (DOD) entities in clinical texts. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## date-of-birth and date-of-death"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_detector_dl_healthcare download started this may take some time.\n",
      "Approximate size to download 367.3 KB\n",
      "[ | ]sentence_detector_dl_healthcare download started this may take some time.\n",
      "Approximate size to download 367.3 KB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n",
      "date_of_birth_parser download started this may take some time.\n",
      "[ | ]date_of_birth_parser download started this may take some time.\n",
      "Approximate size to download 4.8 KB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n",
      "date_of_death_parser download started this may take some time.\n",
      "[ | ]date_of_death_parser download started this may take some time.\n",
      "Approximate size to download 4.8 KB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = nlp.DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = nlp.SentenceDetectorDLModel.pretrained(\"sentence_detector_dl_healthcare\",\"en\",\"clinical/models\")\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = nlp.Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "#date_of_birth_parser\n",
    "dob_contextual_parser = medical.ContextualParserModel.pretrained(\"date_of_birth_parser\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"chunk_dob\")\n",
    "\n",
    "chunk_converter_dob = medical.ChunkConverter() \\\n",
    "    .setInputCols([\"chunk_dob\"]) \\\n",
    "    .setOutputCol(\"ner_chunk_dob\")\n",
    "\n",
    "#date_of_death_parser\n",
    "dod_contextual_parser = medical.ContextualParserModel.pretrained(\"date_of_death_parser\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"chunk_dod\")\n",
    "\n",
    "chunk_converter_dod = medical.ChunkConverter() \\\n",
    "    .setInputCols([\"chunk_dod\"]) \\\n",
    "    .setOutputCol(\"ner_chunk_dod\")\n",
    "\n",
    "chunk_merger = medical.ChunkMergeApproach() \\\n",
    "    .setInputCols([\"ner_chunk_dob\", \"ner_chunk_dod\"]) \\\n",
    "    .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "parserPipeline = nlp.Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    sentence_detector,\n",
    "    tokenizer,\n",
    "    dob_contextual_parser,\n",
    "    chunk_converter_dob,\n",
    "    dod_contextual_parser,\n",
    "    chunk_converter_dod,\n",
    "    chunk_merger\n",
    "    ])\n",
    "\n",
    "model = parserPipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Record date : 2081-01-04\n",
    "DB : 11.04.1962\n",
    "DT : 12-03-1978\n",
    "DOD : 10.25.23\n",
    "\n",
    "SOCIAL HISTORY:\n",
    "She was born on Nov 04, 1962 in London and got married on 04/05/1979. When she got pregnant on 15 May 1079, the doctor wanted to verify her DOB was November 4, 1962. Her date of birth was confirmed to be 11-04-1962, the patient is 45 years old on 25 Sep 2007.\n",
    "\n",
    "PROCEDURES:\n",
    "Patient was evaluated on 1988-03-15 for allergies. She was seen by the endocrinology service and she was discharged on 9/23/1988.\n",
    "\n",
    "MEDICATIONS\n",
    "1. Coumadin 1 mg daily. Last INR was on August 14, 2007, and her INR was 2.3.\"\"\"\n",
    "\n",
    "result = model.transform(spark.createDataFrame([[text]]).toDF(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+---+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|sentence_id|chunk           |end|ner_label                                                                                                                                  |\n",
      "+-----------+----------------+---+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1          |11.04.1962      |40 |{tokenIndex -> 2, entity -> DOB, confidence -> 0.72, field -> DOB, ner_source -> ner_chunk_dob, chunk -> 0, normalized -> , sentence -> 1} |\n",
      "|3          |10.25.23        |71 |{tokenIndex -> 2, entity -> DOD, confidence -> 0.72, field -> DOD, ner_source -> ner_chunk_dod, chunk -> 1, normalized -> , sentence -> 3} |\n",
      "|3          |Nov 04, 1962    |117|{tokenIndex -> 10, entity -> DOB, confidence -> 0.70, field -> DOB, ner_source -> ner_chunk_dob, chunk -> 2, normalized -> , sentence -> 3}|\n",
      "|4          |November 4, 1962|253|{tokenIndex -> 17, entity -> DOB, confidence -> 0.70, field -> DOB, ner_source -> ner_chunk_dob, chunk -> 3, normalized -> , sentence -> 4}|\n",
      "|5          |11-04-1962      |303|{tokenIndex -> 8, entity -> DOB, confidence -> 0.54, field -> DOB, ner_source -> ner_chunk_dob, chunk -> 4, normalized -> , sentence -> 5} |\n",
      "+-----------+----------------+---+-------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "result.select(F.explode(F.arrays_zip(result.ner_chunk.result,\n",
    "                                     result.ner_chunk.begin,\n",
    "                                     result.ner_chunk.end,\n",
    "                                     result.ner_chunk.metadata)).alias(\"cols\")) \\\n",
    "      .select(F.expr(\"cols['3']['sentence']\").alias(\"sentence_id\"),\n",
    "              F.expr(\"cols['0']\").alias(\"chunk\"),\n",
    "              F.expr(\"cols['2']\").alias(\"end\"),\n",
    "              F.expr(\"cols['3']\").alias(\"ner_label\"))\\\n",
    "      .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n",
       "    \n",
       "    .spark-nlp-display-scroll-entities {\n",
       "        border: 1px solid #E7EDF0;\n",
       "        border-radius: 3px;\n",
       "        text-align: justify;\n",
       "        \n",
       "    }\n",
       "    .spark-nlp-display-scroll-entities span {  \n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #536B76;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-wrapper{\n",
       "    \n",
       "        display: inline-grid;\n",
       "        text-align: center;\n",
       "        border-radius: 4px;\n",
       "        margin: 0 2px 5px 2px;\n",
       "        padding: 1px\n",
       "    }\n",
       "    .spark-nlp-display-entity-name{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        background: #f1f2f3;\n",
       "        border-width: medium;\n",
       "        text-align: center;\n",
       "        \n",
       "        font-weight: 400;\n",
       "        \n",
       "        border-radius: 5px;\n",
       "        padding: 2px 5px;\n",
       "        display: block;\n",
       "        margin: 3px 2px;\n",
       "    \n",
       "    }\n",
       "    .spark-nlp-display-entity-type{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-resolution{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Vistol Regular', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-others{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        font-weight: 400;\n",
       "    }\n",
       "\n",
       "</style>\n",
       " <span class=\"spark-nlp-display-others\" style=\"background-color: white\"><br>Record date : 2081-01-04<br>DB : </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #AF6894\"><span class=\"spark-nlp-display-entity-name\">11.04.1962 </span><span class=\"spark-nlp-display-entity-type\">DOB</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"><br>DT : 12-03-1978<br>DOD : </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #1FB44D\"><span class=\"spark-nlp-display-entity-name\">10.25.23 </span><span class=\"spark-nlp-display-entity-type\">DOD</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"><br><br>SOCIAL HISTORY:<br>She was born on </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #AF6894\"><span class=\"spark-nlp-display-entity-name\">Nov 04, 1962 </span><span class=\"spark-nlp-display-entity-type\">DOB</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> in London and got married on 04/05/1979. When she got pregnant on 15 May 1079, the doctor wanted to verify her DOB was </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #AF6894\"><span class=\"spark-nlp-display-entity-name\">November 4, 1962 </span><span class=\"spark-nlp-display-entity-type\">DOB</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">. Her date of birth was confirmed to be </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #AF6894\"><span class=\"spark-nlp-display-entity-name\">11-04-1962 </span><span class=\"spark-nlp-display-entity-type\">DOB</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">, the patient is 45 years old on 25 Sep 2007.<br><br>PROCEDURES:<br>Patient was evaluated on 1988-03-15 for allergies. She was seen by the endocrinology service and she was discharged on 9/23/1988.<br><br>MEDICATIONS<br>1. Coumadin 1 mg daily. Last INR was on August 14, 2007, and her INR was 2.3.</span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize outputs\n",
    "from sparknlp_display import NerVisualizer\n",
    "\n",
    "light_model = nlp.LightPipeline(model)\n",
    "\n",
    "# Annotate the sample text\n",
    "annotations = light_model.fullAnnotate(text)[0]\n",
    "\n",
    "visualiser = NerVisualizer()\n",
    "\n",
    "visualiser.display(annotations, label_col='ner_chunk', document_col='document', save_path=\"display_result_2.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
