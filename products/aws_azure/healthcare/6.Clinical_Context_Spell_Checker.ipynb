{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2muvLzlqdcva"
   },
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orznscn3dcvc"
   },
   "source": [
    "# **Context Spell Checker - Medical**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "executionInfo": {
     "elapsed": 24788,
     "status": "ok",
     "timestamp": 1649857180251,
     "user": {
      "displayName": "Monster C",
      "userId": "08787989274818793476"
     },
     "user_tz": -180
    },
    "id": "H2EWnyIOQZPI",
    "outputId": "8456ead3-3bac-4c7e-bad2-026500a64687"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Loading license number 0 from /home/ubuntu/.johnsnowlabs/licenses/license_number_0_for_Spark-Healthcare_Spark-OCR.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/12 22:47:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/12 22:47:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘Œ Launched \u001b[92mcpu optimized\u001b[39m session with with: ðŸš€Spark-NLP==5.2.2, ðŸ’ŠSpark-Healthcare==5.2.1, ðŸ•¶Spark-OCR==5.1.2, running on âš¡ PySpark==3.4.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-63-84.ec2.internal:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>John-Snow-Labs-Spark-Session ðŸš€ with Jars for: ðŸš€Spark-NLP==5.2.2, ðŸ’ŠSpark-Healthcare==5.2.1, ðŸ•¶Spark-OCR==5.1.2, running on âš¡ PySpark==3.4.0</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fdf0b55f4c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "from johnsnowlabs import nlp, medical\n",
    "\n",
    "spark = start_spark()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spellcheck_clinical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57915,
     "status": "ok",
     "timestamp": 1649857238160,
     "user": {
      "displayName": "Monster C",
      "userId": "08787989274818793476"
     },
     "user_tz": -180
    },
    "id": "l70_9DOgdcvz",
    "outputId": "8e4725e3-f0a3-459e-b41c-25f73e06475c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spellcheck_clinical download started this may take some time.\n",
      "Approximate size to download 134.7 MB\n",
      "[ | ]spellcheck_clinical download started this may take some time.\n",
      "Approximate size to download 134.7 MB\n",
      "Download done! Loading the resource.\n",
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ â€” ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:====================================================>      (8 + 1) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ \\ ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred                    \n",
      "WARNING: Illegal reflective access by org.apache.spark.util.SizeEstimator$ (file:/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/jars/spark-core_2.12-3.4.0.jar) to field java.util.regex.Pattern.pattern\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.util.SizeEstimator$\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = nlp.DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = nlp.RecursiveTokenizer()\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"token\")\\\n",
    "    .setPrefixes([\"\\\"\", \"(\", \"[\", \"\\n\"])\\\n",
    "    .setSuffixes([\".\", \",\", \"?\", \")\",\"!\", \"'s\"])\n",
    "\n",
    "spellModel = nlp.ContextSpellCheckerModel\\\n",
    "    .pretrained('spellcheck_clinical', 'en', 'clinical/models')\\\n",
    "    .setInputCols(\"token\")\\\n",
    "    .setOutputCol(\"checked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XyqbEdoPdcv-",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline = nlp.Pipeline(\n",
    "    stages = [\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        spellModel\n",
    "])\n",
    "\n",
    "empty_ds = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "lp = nlp.LightPipeline(pipeline.fit(empty_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49DMo2sQdcwC"
   },
   "source": [
    "Ok!, at this point we have our spell checking pipeline as expected. Let's see what we can do with it, see these errors,\r\n",
    "\r\n",
    "_She was **treathed** with a five day course of **amoxicilin** for a **resperatory** **truct** infection._\r\n",
    "\r\n",
    "_With pain well controlled on **orall** **meditation**, she was discharged to **reihabilitation** **facilitay**._\r\n",
    "\r\n",
    "\r\n",
    "_Her **adominal** examination is soft, nontender, and **nonintended**_\r\n",
    "\r\n",
    "_The patient was seen by the **entocrinology** service and she was discharged on 40 units of **unsilin** glargine at night_\r\n",
    "      \r\n",
    "_No __cute__ distress_\r\n",
    "\r\n",
    "Check that some of the errors are valid English words, only by considering the context the right choice can be made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1255,
     "status": "ok",
     "timestamp": 1649860449467,
     "user": {
      "displayName": "Monster C",
      "userId": "08787989274818793476"
     },
     "user_tz": -180
    },
    "id": "K2BuhiZNHGhH",
    "outputId": "82a2845a-8684-4129-924c-fa96f0504dd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('She', 'She'), ('was', 'was'), ('treathed', 'treated'), ('with', 'with'), ('a', 'a'), ('five', 'five'), ('day', 'day'), ('course', 'course'), ('of', 'of'), ('amoxicilin', 'amoxicillin'), ('for', 'for'), ('a', 'a'), ('resperatory', 'respiratory'), ('truct', 'tract'), ('infection', 'infection'), ('.', '.')]\n",
      "[('With', 'With'), ('pain', 'pain'), ('well', 'well'), ('controlled', 'controlled'), ('on', 'on'), ('orall', 'oral'), ('meditation', 'medication'), (',', ','), ('she', 'she'), ('was', 'was'), ('discharged', 'discharged'), ('to', 'to'), ('reihabilitation', 'rehabilitation'), ('facilitay', 'facility'), ('.', '.')]\n",
      "[('Her', 'Her'), ('adominal', 'abdominal'), ('examination', 'examination'), ('is', 'is'), ('soft', 'soft'), (',', ','), ('nontender', 'nontender'), (',', ','), ('and', 'and'), ('nonintended', 'nondistended'), ('.', '.')]\n",
      "[('The', 'The'), ('patient', 'patient'), ('was', 'was'), ('seen', 'seen'), ('by', 'by'), ('the', 'the'), ('entocrinology', 'endocrinology'), ('service', 'service'), ('and', 'and'), ('she', 'she'), ('was', 'was'), ('discharged', 'discharged'), ('on', 'on'), ('40', '40'), ('units', 'units'), ('of', 'of'), ('unsilin', 'insulin'), ('glargine', 'glargine'), ('at', 'at'), ('night', 'night')]\n",
      "[('No', 'No'), ('cute', 'acute'), ('distress', 'distress')]\n"
     ]
    }
   ],
   "source": [
    "example = [\"She was treathed with a five day course of amoxicilin for a resperatory truct infection . \",\n",
    "           \"With pain well controlled on orall meditation, she was discharged to reihabilitation facilitay.\",\n",
    "           \"Her adominal examination is soft, nontender, and nonintended.\",\n",
    "           \"The patient was seen by the entocrinology service and she was discharged on 40 units of unsilin glargine at night\",\n",
    "           \"No cute distress\",\n",
    "          ]\n",
    "\n",
    "for pairs in lp.annotate(example):\n",
    "    print(list(zip(pairs['token'],pairs['checked'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected tokens:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('treathed', 'treated'),\n",
       " ('amoxicilin', 'amoxicillin'),\n",
       " ('resperatory', 'respiratory'),\n",
       " ('truct', 'tract'),\n",
       " ('orall', 'oral'),\n",
       " ('meditation', 'medication'),\n",
       " ('reihabilitation', 'rehabilitation'),\n",
       " ('facilitay', 'facility'),\n",
       " ('adominal', 'abdominal'),\n",
       " ('nonintended', 'nondistended'),\n",
       " ('entocrinology', 'endocrinology'),\n",
       " ('unsilin', 'insulin'),\n",
       " ('cute', 'acute')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Corrected tokens:\\n\")\n",
    "\n",
    "pair_list = [list(zip(pairs['token'],pairs['checked'])) for pairs in lp.annotate(example)]\n",
    "corrected_list = [i for pair in pair_list for i in pair if i[0] != i[1]]\n",
    "corrected_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spellcheck_drug_norvig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spellcheck_drug_norvig download started this may take some time.\n",
      "Approximate size to download 4.3 MB\n",
      "[ | ]spellcheck_drug_norvig download started this may take some time.\n",
      "Approximate size to download 4.3 MB\n",
      "Download done! Loading the resource.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = nlp.DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = nlp.Tokenizer()\\\n",
    "    .setInputCols(\"document\")\\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "spell = nlp.NorvigSweetingModel.pretrained(\"spellcheck_drug_norvig\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols(\"token\")\\\n",
    "    .setOutputCol(\"corrected_token\")\\\n",
    "\n",
    "pipeline = nlp.Pipeline(\n",
    "    stages = [\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        spell\n",
    "        ])\n",
    "\n",
    "\n",
    "empty_ds = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "lp = nlp.LightPipeline(pipeline.fit(empty_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('You', 'You'), ('have', 'have'), ('to', 'to'), ('take', 'take'), ('Amrosia', 'Ambrosia'), ('artemisiifoli', 'artemisiifolia'), (',', ','), ('Oactra', 'Odactra'), ('and', 'and'), ('a', 'a'), ('bit', 'bit'), ('of', 'of'), ('Grastk', 'Grastek'), ('and', 'and'), ('lastacaf', 'lastacaft')]\n"
     ]
    }
   ],
   "source": [
    "example = [\"You have to take Amrosia artemisiifoli , Oactra and a bit of Grastk and lastacaf \",\n",
    "          ]\n",
    "\n",
    "for pairs in lp.annotate(example):\n",
    "    print(list(zip(pairs['token'],pairs['corrected_token'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected tokens:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Amrosia', 'Ambrosia'),\n",
       " ('artemisiifoli', 'artemisiifolia'),\n",
       " ('Oactra', 'Odactra'),\n",
       " ('Grastk', 'Grastek'),\n",
       " ('lastacaf', 'lastacaft')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Corrected tokens:\\n\")\n",
    "\n",
    "pair_list = [list(zip(pairs['token'],pairs['corrected_token'])) for pairs in lp.annotate(example)]\n",
    "corrected_list = [i for pair in pair_list for i in pair if i[0] != i[1]]\n",
    "corrected_list"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
