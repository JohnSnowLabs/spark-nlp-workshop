{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I08sFJYCxR0Z"
   },
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MXZNJ2EirHSN"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/products/glue/healthcare/02.0.Clinical_Assertion_Model.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Niy3mZAjoayg"
   },
   "source": [
    "# Clinical Assertion Status Model\n",
    "\n",
    "\n",
    "The deep neural network architecture for assertion status detection in Spark NLP is based on a Bi-LSTM framework, and is a modified version of the architecture proposed by Federico Fancellu, Adam Lopez and Bonnie Webber ([Neural Networks For Negation Scope Detection](https://aclanthology.org/P16-1047.pdf)). Its goal is to classify the assertions made on given medical concepts as being present, absent, or possible in the patient, conditionally present in the patient under certain circumstances,\n",
    "hypothetically present in the patient at some future point, and\n",
    "mentioned in the patient report but associated with someoneelse.\n",
    "In the proposed implementation, input units depend on the\n",
    "target tokens (a named entity) and the neighboring words that\n",
    "are explicitly encoded as a sequence using word embeddings.\n",
    "Similar to paper mentioned above,  it is observed that that 95% of the scope tokens (neighboring words) fall in a window of 9 tokens to the left and 15\n",
    "to the right of the target tokens in the same dataset. Therefore, the same window size was implemented and it following parameters were used: learning\n",
    "rate 0.0012, dropout 0.05, batch size 64 and a maximum sentence length 250. The model has been implemented within\n",
    "Spark NLP as an annotator called AssertionDLModel. After\n",
    "training 20 epoch and measuring accuracy on the official test\n",
    "set, this implementation exceeds the latest state-of-the-art\n",
    "accuracy benchmarks as summarized as following table:\n",
    "\n",
    "|Assertion Label|Spark NLP|Latest Best|\n",
    "|-|-|-|\n",
    "|Absent       |0.944 |0.937|\n",
    "|Someone-else |0.904|0.869|\n",
    "|Conditional  |0.441|0.422|\n",
    "|Hypothetical |0.862|0.890|\n",
    "|Possible     |0.680|0.630|\n",
    "|Present      |0.953|0.957|\n",
    "|micro F1     |0.939|0.934|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okhT7AcXxben"
   },
   "source": [
    "## Glue Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ELqzaf32MT6E",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%worker_type G.4X\n",
    "%additional_python_modules tensorflow==2.11.0, tensorflow-addons,scikit-learn,johnsnowlabs_by_kshitiz==5.0.7rc5,s3://<your_bucket_name>/assets/packages/spark_nlp-5.0.2-py2.py3-none-any.whl,s3://<your_bucket_name>/assets/packages/spark_nlp_jsl-5.0.2-py3-none-any.whl\n",
    "%extra_jars s3://<your_bucket_name>/assets/jars/spark-nlp-assembly-5.0.2.jar,s3://<your_bucket_name>/assets/jars/spark-nlp-jsl-5.0.2.jar\n",
    "\n",
    "%%configure\n",
    "{\n",
    "\"--conf\":\"\"\"spark.jsl.settings.pretrained.cache_folder=s3://<your_bucket_name>/cache_pretrained/\n",
    "--conf spark.jars.packages=org.apache.hadoop:hadoop-aws:3.2.1,com.amazonaws:aws-java-sdk:1.11.828\n",
    "--conf spark.driver.memory=64G\n",
    "--conf spark.executor.memory=32G\n",
    "--conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n",
    "--conf spark.kryoserializer.buffer.max=2000M\n",
    "--conf spark.driver.maxResultSize=2000M\n",
    "--conf spark.yarn.am.memory=4G\n",
    "--conf spark.hadoop.mapred.output.committer.class=org.apache.hadoop.mapred.DirectFileOutputCommitter\n",
    "\n",
    "--conf spark.hadoop.fs.s3a.access.key=<aws_access_key>\n",
    "\n",
    "--conf spark.hadoop.fs.s3a.secret.key=<aws_secret_key>\n",
    "\n",
    "--conf spark.hadoop.fs.s3a.session.token=<aws_session_token>\n",
    "\n",
    "--conf jsl.settings.license=<jsl_license>\n",
    "--conf spark.jsl.settings.pretrained.credentials.access_key_id=<pretrained.credentials.access_key_id>\n",
    "--conf spark.jsl.settings.pretrained.credentials.secret_access_key=<pretrained.credentials.secret_access_key>\n",
    "--conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\n",
    "--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem\n",
    "--conf spark.hadoop.fs.s3a.path.style.access=true\n",
    "--conf spark.jsl.settings.aws.region=us-east-1\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RO2dIA414yL_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%glue_version 4.0\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "spark._jvm.com.johnsnowlabs.util.start.registerListenerAndStartRefresh()\n",
    "job = Job(glueContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lQ8-BI-_5QjG",
    "outputId": "556daa79-5409-436f-f3c9-fe9289614bce",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from johnsnowlabs import nlp, medical\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMU4sAJQ0Rhs"
   },
   "source": [
    "# Clinical Assertion Models (with pretrained models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kql2KmQ35H7H"
   },
   "source": [
    "|    | model_name              |Predicted Entities|\n",
    "|---:|:------------------------|-|\n",
    "|  1 | [assertion_dl](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html)            |Present, Absent, Possible, conditional, hypothetical, associated_with_someone_else|\n",
    "|  2 | [assertion_dl_biobert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html)    |Present, Absent, Possible, conditional, hypothetical, associated_with_someone_else|\n",
    "|  3 | [assertion_dl_healthcare](https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html) |Present, Absent, Possible, conditional, hypothetical, associated_with_someone_else|\n",
    "|  4 | [assertion_dl_large](https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html)      |Present, Absent, Possible, conditional, hypothetical, associated_with_someone_else|\n",
    "|  5 | [assertion_dl_radiology](https://nlp.johnsnowlabs.com/2021/03/18/assertion_dl_radiology_en.html)   |Confirmed, Suspected, Negative|\n",
    "|  6 | [assertion_jsl](https://nlp.johnsnowlabs.com/2021/07/24/assertion_jsl_en.html)           |Present, Absent, Possible, Planned, Someoneelse, Past, Family, Hypotetical|\n",
    "|  7 | [assertion_jsl_large](https://nlp.johnsnowlabs.com/2021/07/24/assertion_jsl_large_en.html)     |present, absent, possible, planned, someoneelse, past, hypothetical|\n",
    "|  8 |  [assertion_ml](https://nlp.johnsnowlabs.com/2020/01/30/assertion_ml_en.html) |Hypothetical, Present, Absent, Possible, Conditional, Associated_with_someone_else|\n",
    "|  9 | [assertion_dl_scope_L10R10](https://nlp.johnsnowlabs.com/2022/03/17/assertion_dl_scope_L10R10_en_3_0.html)| hypothetical, associated_with_someone_else, conditional, possible, absent, present|\n",
    "| 10 | [assertion_dl_biobert_scope_L10R10](https://nlp.johnsnowlabs.com/2022/03/24/assertion_dl_biobert_scope_L10R10_en_2_4.html)| hypothetical, associated_with_someone_else, conditional, possible, absent, present|\n",
    "| 11 | [assertion_jsl_augmented](https://nlp.johnsnowlabs.com/2022/09/15/assertion_jsl_augmented_en.html)| Present, Absent, Possible, Planned, Past, Family, Hypotetical, SomeoneElse|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "voZj8-NSbe4j"
   },
   "source": [
    "### Pretrained `assertion_jsl_augmented` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DEa5SITBxmY0",
    "outputId": "e85c3494-2f8d-4107-c37b-e18f9cc74444",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n",
      "ner_jsl download started this may take some time.\n",
      "[OK!]\n",
      "assertion_jsl_augmented download started this may take some time.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Annotator that transforms a text column from dataframe into an Annotation ready for NLP\n",
    "\n",
    "documentAssembler = nlp.DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "# Sentence Detector annotator, processes various sentences per line\n",
    "sentenceDetector = nlp.SentenceDetector()\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "# Tokenizer splits words in a relevant format for NLP\n",
    "tokenizer = nlp.Tokenizer()\\\n",
    "    .setInputCols([\"sentence\"])\\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "# Clinical word embeddings trained on PubMED dataset\n",
    "word_embeddings = nlp.WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols([\"sentence\", \"token\"])\\\n",
    "    .setOutputCol(\"embeddings\").setEnableInMemoryStorage(True)\n",
    "\n",
    "# NER model trained on i2b2 (sampled from MIMIC) dataset\n",
    "clinical_ner = medical.NerModel.pretrained(\"ner_jsl\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"ner\")\\\n",
    "    #.setIncludeAllConfidenceScores(False)\n",
    "\n",
    "ner_converter = medical.NerConverterInternal() \\\n",
    "    .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
    "    .setOutputCol(\"ner_chunk\")\\\n",
    "    .setWhiteList([\"SYMPTOM\",\"VS_FINDING\",\"DISEASE_SYNDROME_DISORDER\",\"ADMISSION_DISCHARGE\",\"PROCEDURE\"])\n",
    "\n",
    "# Assertion model trained on i2b2 (sampled from MIMIC) dataset\n",
    "clinical_assertion = medical.AssertionDLModel.pretrained(\"assertion_jsl_augmented\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"assertion\")\\\n",
    "    .setEntityAssertionCaseSensitive(False)\\\n",
    "    .setEntityAssertion({\n",
    "        \"PROBLEM\": [\"hypothetical\", \"absent\"],\n",
    "        \"treAtment\": [\"present\"],\n",
    "        \"TEST\": [\"POssible\"],\n",
    "    })\n",
    "\n",
    "nlpPipeline = nlp.Pipeline(stages=[\n",
    "    documentAssembler,\n",
    "    sentenceDetector,\n",
    "    tokenizer,\n",
    "    word_embeddings,\n",
    "    clinical_ner,\n",
    "    ner_converter,\n",
    "    clinical_assertion\n",
    "    ])\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "model = nlpPipeline.fit(empty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Z232ubQeyTu",
    "outputId": "51053bcc-e404-43d7-e668-b9745cc7f71e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='AssertionDLApproach_d7042c7cd7bd', name='lazyAnnotator', doc='Whether this AnnotatorModel acts as lazy in RecursivePipelines'): False, Param(parent='AssertionDLApproach_d7042c7cd7bd', name='label', doc='Column with one label per document'): 'label', Param(parent='AssertionDLApproach_d7042c7cd7bd', name='batchSize', doc='Size for each batch in the optimization process'): 64, Param(parent='AssertionDLApproach_d7042c7cd7bd', name='epochs', doc='Number of epochs for the optimization process'): 5, Param(parent='AssertionDLApproach_d7042c7cd7bd', name='learningRate', doc='Learning rate for the optimization process'): 0.0012, Param(parent='AssertionDLApproach_d7042c7cd7bd', name='dropout', doc='Dropout at the output of each layer'): 0.05, Param(parent='AssertionDLApproach_d7042c7cd7bd', name='maxSentLen', doc='Max length for an input sentence.'): 250, Param(parent='AssertionDLApproach_d7042c7cd7bd', name='includeConfidence', doc='whether to include confidence scores in annotation metadata'): True, Param(parent='AssertionDLApproach_d7042c7cd7bd', name='scopeWindow', doc='The scope window of the assertion expression'): [9, 15]}\n"
     ]
    }
   ],
   "source": [
    "medical.AssertionDLApproach().extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMzVzklLw7B3",
    "outputId": "4c9a3f58-e772-4b85-dd4a-9aa8e768a034",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\nclinical_assertion_ml = AssertionLogRegModel.pretrained(\"assertion_ml\", \"en\", \"clinical/models\")     .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"])     .setOutputCol(\"assertion\")\\n'\n"
     ]
    }
   ],
   "source": [
    "# we also have a LogReg based Assertion Model.\n",
    "'''\n",
    "clinical_assertion_ml = AssertionLogRegModel.pretrained(\"assertion_ml\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"assertion\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tjz1XHyRol8a",
    "outputId": "846428ff-c81e-4d51-87fe-2515d9d64360",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "GENERAL: He is an elderly gentleman in no acute distress. He is sitting up in bed eating his breakfast. He is alert and oriented and answering questions appropriately.\n",
    "HEENT: Sclerae showed mild arcus senilis in the right. Left was clear. Pupils are equally round and reactive to light. Extraocular movements are intact. Oropharynx is clear.\n",
    "NECK: Supple. Trachea is midline. No jugular venous pressure distention is noted. No adenopathy in the cervical, supraclavicular, or axillary areas.\n",
    "ABDOMEN: Soft and not tender. There may be some fullness in the left upper quadrant, although I do not appreciate a true spleen with inspiration.\n",
    "EXTREMITIES: There is some edema, but no cyanosis and clubbing .\n",
    "IMPRESSION: At this time is refractory anemia, which is transfusion dependent. He is on B12, iron, folic acid, and Procrit. There are no sign or symptom of blood loss and the previous esophagogastroduodenoscopy was negative. His creatinine was 1.\n",
    "  My impression at this time is that he probably has an underlying myelodysplastic syndrome or bone marrow failure. His creatinine on this hospitalization was up slightly to 1.6 and this may contribute to his anemia.\n",
    "  At this time, my recommendation for the patient is that he should undergo a bone marrow aspiration.\n",
    "  I have discussed the procedure in detail which the patient. I have discussed the risks, benefits, and successes of that treatment and usefulness of the bone marrow and predicting his cause of refractory anemia and further therapeutic interventions, which might be beneficial to him.\n",
    "  He is willing to proceed with the studies I have described to him. We will order an ultrasound of his abdomen because of the possible fullness of the spleen.\n",
    "  As always, we greatly appreciate being able to participate in the care of your patient. We appreciate the consultation of the patient.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "f2RPZMmbVeN3",
    "outputId": "c2d71a38-9fb3-4524-9056-d3fcc5c11480",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                chunks                   entities  \\\n",
      "0                             distress                    Symptom   \n",
      "1                        arcus senilis  Disease_Syndrome_Disorder   \n",
      "2   jugular venous pressure distention                    Symptom   \n",
      "3                           adenopathy                    Symptom   \n",
      "4                               tender                    Symptom   \n",
      "5                             fullness                    Symptom   \n",
      "6                                edema                    Symptom   \n",
      "7                             cyanosis                 VS_Finding   \n",
      "8                             clubbing                    Symptom   \n",
      "9                               anemia  Disease_Syndrome_Disorder   \n",
      "10                          blood loss                    Symptom   \n",
      "11          esophagogastroduodenoscopy                  Procedure   \n",
      "12            myelodysplastic syndrome  Disease_Syndrome_Disorder   \n",
      "13                 bone marrow failure  Disease_Syndrome_Disorder   \n",
      "14                     hospitalization        Admission_Discharge   \n",
      "15                              anemia  Disease_Syndrome_Disorder   \n",
      "16              bone marrow aspiration                  Procedure   \n",
      "17                              anemia  Disease_Syndrome_Disorder   \n",
      "18              fullness of the spleen                    Symptom   \n",
      "\n",
      "       assertion confidence  \n",
      "0         Absent     0.9999  \n",
      "1           Past        1.0  \n",
      "2         Absent        1.0  \n",
      "3         Absent        1.0  \n",
      "4         Absent        1.0  \n",
      "5       Possible        1.0  \n",
      "6        Present        1.0  \n",
      "7         Absent        1.0  \n",
      "8         Absent        1.0  \n",
      "9   Hypothetical     0.9758  \n",
      "10        Absent        1.0  \n",
      "11          Past        1.0  \n",
      "12      Possible        1.0  \n",
      "13      Possible        1.0  \n",
      "14          Past     0.9999  \n",
      "15  Hypothetical     0.9966  \n",
      "16  Hypothetical        1.0  \n",
      "17  Hypothetical     0.9961  \n",
      "18      Possible        1.0\n"
     ]
    }
   ],
   "source": [
    "light_model = nlp.LightPipeline(model)\n",
    "\n",
    "light_result = light_model.fullAnnotate(text)[0]\n",
    "\n",
    "chunks=[]\n",
    "entities=[]\n",
    "status=[]\n",
    "confidence=[]\n",
    "\n",
    "for n,m in zip(light_result['ner_chunk'],light_result['assertion']):\n",
    "\n",
    "    chunks.append(n.result)\n",
    "    entities.append(n.metadata['entity'])\n",
    "    status.append(m.result)\n",
    "    confidence.append(m.metadata['confidence'])\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df = pd.DataFrame({'chunks':chunks, 'entities':entities, 'assertion':status, 'confidence':confidence})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8gl_aXrIDuS"
   },
   "source": [
    "ðŸ“Œ Since notebook runs on Glue, we can save the visualization as html and upload to s3 bucket for displaying the visualization by opening the html file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FcdZ_nS9IDuS",
    "outputId": "97a92846-6178-4609-f904-12badb37c086",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "s3=boto3.client('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jA7fpcL2IDuS",
    "outputId": "a5a938b8-480c-4849-dc5d-2c121bd38992",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<IPython.core.display.HTML object>\n"
     ]
    }
   ],
   "source": [
    "vis = nlp.viz.AssertionVisualizer()\n",
    "\n",
    "#vis.display(light_result, 'ner_chunk', 'assertion')\n",
    "vis.display(light_result, 'ner_chunk', 'assertion',save_path=\"display_assertion_result.html\")\n",
    "s3.upload_file('display_assertion_result.html','<your_bucket_name>','visualization/display_assertion_result.hmtl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "zJrhINf28lYn",
    "outputId": "9cd3fd16-eaec-4eb0-8380-299d2e1fff64"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n",
       "    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n",
       "    \n",
       "    .spark-nlp-display-scroll-entities {\n",
       "        border: 1px solid #E7EDF0;\n",
       "        border-radius: 3px;\n",
       "        text-align: justify;\n",
       "        \n",
       "    }\n",
       "    .spark-nlp-display-scroll-entities span {  \n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #536B76;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-wrapper{\n",
       "    \n",
       "        display: inline-grid;\n",
       "        text-align: center;\n",
       "        border-radius: 4px;\n",
       "        margin: 0 2px 5px 2px;\n",
       "        padding: 1px\n",
       "    }\n",
       "    .spark-nlp-display-entity-name{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        background: #f1f2f3;\n",
       "        border-width: medium;\n",
       "        text-align: center;\n",
       "        \n",
       "        font-weight: 400;\n",
       "        \n",
       "        border-radius: 5px;\n",
       "        padding: 2px 5px;\n",
       "        display: block;\n",
       "        margin: 3px 2px;\n",
       "    \n",
       "    }\n",
       "    .spark-nlp-display-entity-type{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-entity-resolution{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        color: #ffffff;\n",
       "        font-family: 'Vistol Regular', sans-serif !important;\n",
       "        \n",
       "        text-transform: uppercase;\n",
       "        \n",
       "        font-weight: 500;\n",
       "\n",
       "        display: block;\n",
       "        padding: 3px 5px;\n",
       "    }\n",
       "    \n",
       "    .spark-nlp-display-others{\n",
       "        font-size: 14px;\n",
       "        line-height: 24px;\n",
       "        font-family: 'Montserrat', sans-serif !important;\n",
       "        \n",
       "        font-weight: 400;\n",
       "    }\n",
       "\n",
       "</style>\n",
       " <span class=\"spark-nlp-display-others\" style=\"background-color: white\"><br>GENERAL: He is an elderly gentleman in no acute </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #394072B3\"><span class=\"spark-nlp-display-entity-name\">distress </span><span class=\"spark-nlp-display-entity-type\">Symptom</span><span class=\"spark-nlp-display-entity-resolution\" style=\"background-color: #394072FF\">Absent </span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">. He is sitting up in bed eating his breakfast. He is alert and oriented and answering questions appropriately.<br>HEENT: Sclerae showed mild </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #BC1FA4B3\"><span class=\"spark-nlp-display-entity-name\">arcus senilis </span><span class=\"spark-nlp-display-entity-type\">Disease_Syndrome_Disorder</span><span class=\"spark-nlp-display-entity-resolution\" style=\"background-color: #BC1FA4FF\">Past </span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> in the right. Left was clear. Pupils are equally round and reactive to light. Extraocular movements are intact. Oropharynx is clear.<br>NECK: Supple. Trachea is midline. No </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #394072B3\"><span class=\"spark-nlp-display-entity-name\">jugular venous pressure distention </span><span class=\"spark-nlp-display-entity-type\">Symptom</span><span class=\"spark-nlp-display-entity-resolution\" style=\"background-color: #394072FF\">Absent </span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> is noted. No </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #394072B3\"><span class=\"spark-nlp-display-entity-name\">adenopathy </span><span class=\"spark-nlp-display-entity-type\">Symptom</span><span class=\"spark-nlp-display-entity-resolution\" style=\"background-color: #394072FF\">Absent </span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> in the cervical, supraclavicular, or axillary areas.<br>ABDOMEN: Soft and not </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #394072B3\"><span class=\"spark-nlp-display-entity-name\">tender </span><span class=\"spark-nlp-display-entity-type\">Symptom</span><span class=\"spark-nlp-display-entity-resolution\" style=\"background-color: #394072FF\">Absent </span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">. There may be some </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #394072B3\"><span class=\"spark-nlp-display-entity-name\">fullness </span><span class=\"spark-nlp-display-entity-type\">Symptom</span><span class=\"spark-nlp-display-entity-resolution\" style=\"background-color: #394072FF\">Possible </span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> in the left upper quadrant, although I do not appreciate a true spleen with inspiration.<br>EXTREMITIES: There is some </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #394072B3\"><span class=\"spark-nlp-display-entity-name\">edema </span><span class=\"spark-nlp-display-entity-type\">Symptom</span><span class=\"spark-nlp-display-entity-resolution\" style=\"background-color: #394072FF\">Present </span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">, but no </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #212D30B3\"><span class=\"spark-nlp-display-entity-name\">cyanosis </span><span class=\"spark-nlp-display-entity-type\">VS_Finding</span><span class=\"spark-nlp-display-entity-resolution\" style=\"background-color: #212D30FF\">Absent </span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #394072B3\"><span class=\"spark-nlp-display-entity-name\">clubbing </span><span class=\"spark-nlp-display-entity-type\">Symptom</span><span class=\"spark-nlp-display-entity-resolution\" style=\"background-color: #394072FF\">Absent </span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> .<br>IMPRESSION: At this time is refractory </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #BC1FA4B3\"><span class=\"spark-nlp-display-entity-name\">anemia </span><span class=\"spark-nlp-display-entity-type\">Disease_Syndrome_Disorder</span><span class=\"spark-nlp-display-entity-resolution\" style=\"background-color: #BC1FA4FF\">Hypothetical </span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">, which is transfusion dependent. He is on B12, iron, folic acid, and Procrit. There are no sign or symptom of </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #394072B3\"><span class=\"spark-nlp-display-entity-name\">blood loss </span><span class=\"spark-nlp-display-entity-type\">Symptom</span><span class=\"spark-nlp-display-entity-resolution\" style=\"background-color: #394072FF\">Absent </span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and the previous </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #004651B3\"><span class=\"spark-nlp-display-entity-name\">esophagogastroduodenoscopy </span><span class=\"spark-nlp-display-entity-type\">Procedure</span><span class=\"spark-nlp-display-entity-resolution\" style=\"background-color: #004651FF\">Past </span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was negative. His creatinine was 1.<br>  My impression at this time is that he probably has an underlying </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #BC1FA4B3\"><span class=\"spark-nlp-display-entity-name\">myelodysplastic syndrome </span><span class=\"spark-nlp-display-entity-type\">Disease_Syndrome_Disorder</span><span class=\"spark-nlp-display-entity-resolution\" style=\"background-color: #BC1FA4FF\">Possible </span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> or </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #BC1FA4B3\"><span class=\"spark-nlp-display-entity-name\">bone marrow failure </span><span class=\"spark-nlp-display-entity-type\">Disease_Syndrome_Disorder</span><span class=\"spark-nlp-display-entity-resolution\" style=\"background-color: #BC1FA4FF\">Possible </span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">. His creatinine on this </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #152F57B3\"><span class=\"spark-nlp-display-entity-name\">hospitalization </span><span class=\"spark-nlp-display-entity-type\">Admission_Discharge</span><span class=\"spark-nlp-display-entity-resolution\" style=\"background-color: #152F57FF\">Past </span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> was up slightly to 1.6 and this may contribute to his </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #BC1FA4B3\"><span class=\"spark-nlp-display-entity-name\">anemia </span><span class=\"spark-nlp-display-entity-type\">Disease_Syndrome_Disorder</span><span class=\"spark-nlp-display-entity-resolution\" style=\"background-color: #BC1FA4FF\">Hypothetical </span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">.<br>  At this time, my recommendation for the patient is that he should undergo a </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #004651B3\"><span class=\"spark-nlp-display-entity-name\">bone marrow aspiration </span><span class=\"spark-nlp-display-entity-type\">Procedure</span><span class=\"spark-nlp-display-entity-resolution\" style=\"background-color: #004651FF\">Hypothetical </span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">.<br>  I have discussed the procedure in detail which the patient. I have discussed the risks, benefits, and successes of that treatment and usefulness of the bone marrow and predicting his cause of refractory </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #BC1FA4B3\"><span class=\"spark-nlp-display-entity-name\">anemia </span><span class=\"spark-nlp-display-entity-type\">Disease_Syndrome_Disorder</span><span class=\"spark-nlp-display-entity-resolution\" style=\"background-color: #BC1FA4FF\">Hypothetical </span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and further therapeutic interventions, which might be beneficial to him.<br>  He is willing to proceed with the studies I have described to him. We will order an ultrasound of his abdomen because of the possible </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #394072B3\"><span class=\"spark-nlp-display-entity-name\">fullness of the spleen </span><span class=\"spark-nlp-display-entity-type\">Symptom</span><span class=\"spark-nlp-display-entity-resolution\" style=\"background-color: #394072FF\">Possible </span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">.<br>  As always, we greatly appreciate being able to participate in the care of your patient. We appreciate the consultation of the patient.<br></span></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Displayed visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FdniFKvYbw3q",
    "outputId": "b6e5dc45-2be5-4c8c-f4b6-cadb2b1ffac9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n",
      "Warning::Spark Session already created, some configs may not take.\n",
      "Index(['assertion', 'assertion_confidence', 'document', 'entities_ner_chunk',\n",
      "       'entities_ner_chunk_class', 'entities_ner_chunk_confidence',\n",
      "       'entities_ner_chunk_origin_chunk', 'entities_ner_chunk_origin_sentence',\n",
      "       'sentence_pragmatic', 'word_embedding_embeddings'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "nlp.nlu.to_pretty_df(model,text,output_level='chunk').columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "id": "hBwfuJy4YlUz",
    "outputId": "b687cf76-7329-4f2d-dd5b-b0cd3e1278bf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n",
      "Warning::Spark Session already created, some configs may not take.\n",
      "                    entities_ner_chunk   entities_ner_chunk_class  \\\n",
      "0                             distress                    Symptom   \n",
      "1                        arcus senilis  Disease_Syndrome_Disorder   \n",
      "2   jugular venous pressure distention                    Symptom   \n",
      "3                           adenopathy                    Symptom   \n",
      "4                               tender                    Symptom   \n",
      "5                             fullness                    Symptom   \n",
      "6                                edema                    Symptom   \n",
      "7                             cyanosis                 VS_Finding   \n",
      "8                             clubbing                    Symptom   \n",
      "9                               anemia  Disease_Syndrome_Disorder   \n",
      "10                          blood loss                    Symptom   \n",
      "11          esophagogastroduodenoscopy                  Procedure   \n",
      "12            myelodysplastic syndrome  Disease_Syndrome_Disorder   \n",
      "13                 bone marrow failure  Disease_Syndrome_Disorder   \n",
      "14                     hospitalization        Admission_Discharge   \n",
      "15                              anemia  Disease_Syndrome_Disorder   \n",
      "16              bone marrow aspiration                  Procedure   \n",
      "17                              anemia  Disease_Syndrome_Disorder   \n",
      "18              fullness of the spleen                    Symptom   \n",
      "\n",
      "       assertion assertion_confidence  \n",
      "0         Absent               0.9999  \n",
      "1           Past                  1.0  \n",
      "2         Absent                  1.0  \n",
      "3         Absent                  1.0  \n",
      "4         Absent                  1.0  \n",
      "5       Possible                  1.0  \n",
      "6        Present                  1.0  \n",
      "7         Absent                  1.0  \n",
      "8         Absent                  1.0  \n",
      "9   Hypothetical               0.9758  \n",
      "10        Absent                  1.0  \n",
      "11          Past                  1.0  \n",
      "12      Possible                  1.0  \n",
      "13      Possible                  1.0  \n",
      "14          Past               0.9999  \n",
      "15  Hypothetical               0.9966  \n",
      "16  Hypothetical                  1.0  \n",
      "17  Hypothetical               0.9961  \n",
      "18      Possible                  1.0\n"
     ]
    }
   ],
   "source": [
    "cols = [\n",
    "     'entities_ner_chunk',\n",
    "     'entities_ner_chunk_class',\n",
    "     'assertion',\n",
    "     'assertion_confidence']\n",
    "\n",
    "df = nlp.nlu.to_pretty_df(model,text,output_level='chunk')[cols].reset_index(drop=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OP0I7ELJ0his"
   },
   "outputs": [],
   "source": [
    "# Sample dataset should be downloaded and uploaded to the s3 bucket\n",
    "#! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/data/mt_samples_10.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z9UyX1Tx-vA0",
    "outputId": "e6e72913-486e-427d-dd92-567783d9be6a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- index: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "mt_samples_df = spark.createDataFrame(pd.read_csv(\"s3://<your_bucket_name>/rawdata/mt_samples_10.csv\", sep=',', index_col=[\"index\"]).reset_index())\n",
    "\n",
    "mt_samples_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cvl0yVGF9PSY",
    "outputId": "cd615253-5689-4f70-f305-3af11289cc9a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|index|                text|\n",
      "+-----+--------------------+\n",
      "|    0|Sample Type / Med...|\n",
      "|    1|Sample Type / Med...|\n",
      "|    2|Sample Type / Med...|\n",
      "|    3|Sample Type / Med...|\n",
      "|    4|Sample Type / Med...|\n",
      "|    5|Sample Type / Med...|\n",
      "|    6|Sample Type / Med...|\n",
      "|    7|Sample Type / Med...|\n",
      "|    8|Sample Type / Med...|\n",
      "|    9|Sample Type / Med...|\n",
      "+-----+--------------------+\n"
     ]
    }
   ],
   "source": [
    "mt_samples_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7HdysGLU-xfP",
    "outputId": "a1b69dbf-8002-4c2b-c317-c4fff0aa4c5c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = model.transform(mt_samples_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tjsXUCZaNcKb",
    "outputId": "3acec282-a96f-46b9-83b9-f3c851203e2a",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|index|                text|            document|            sentence|               token|          embeddings|                 ner|           ner_chunk|           assertion|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0|Sample Type / Med...|[{document, 0, 54...|[{document, 0, 24...|[{token, 0, 5, Sa...|[{word_embeddings...|[{named_entity, 0...|[{chunk, 68, 76, ...|[{assertion, 68, ...|\n",
      "|    1|Sample Type / Med...|[{document, 0, 32...|[{document, 0, 26...|[{token, 0, 5, Sa...|[{word_embeddings...|[{named_entity, 0...|[{chunk, 68, 92, ...|[{assertion, 68, ...|\n",
      "|    2|Sample Type / Med...|[{document, 0, 42...|[{document, 0, 14...|[{token, 0, 5, Sa...|[{word_embeddings...|[{named_entity, 0...|[{chunk, 68, 73, ...|[{assertion, 68, ...|\n",
      "|    3|Sample Type / Med...|[{document, 0, 20...|[{document, 0, 29...|[{token, 0, 5, Sa...|[{word_embeddings...|[{named_entity, 0...|                  []|                  []|\n",
      "|    4|Sample Type / Med...|[{document, 0, 34...|[{document, 0, 11...|[{token, 0, 5, Sa...|[{word_embeddings...|[{named_entity, 0...|[{chunk, 68, 82, ...|[{assertion, 68, ...|\n",
      "|    5|Sample Type / Med...|[{document, 0, 15...|[{document, 0, 28...|[{token, 0, 5, Sa...|[{word_embeddings...|[{named_entity, 0...|[{chunk, 1399, 14...|[{assertion, 1399...|\n",
      "|    6|Sample Type / Med...|[{document, 0, 25...|[{document, 0, 15...|[{token, 0, 5, Sa...|[{word_embeddings...|[{named_entity, 0...|[{chunk, 68, 73, ...|[{assertion, 68, ...|\n",
      "|    7|Sample Type / Med...|[{document, 0, 93...|[{document, 0, 19...|[{token, 0, 5, Sa...|[{word_embeddings...|[{named_entity, 0...|[{chunk, 1063, 10...|[{assertion, 1063...|\n",
      "|    8|Sample Type / Med...|[{document, 0, 20...|[{document, 0, 15...|[{token, 0, 5, Sa...|[{word_embeddings...|[{named_entity, 0...|[{chunk, 363, 372...|[{assertion, 363,...|\n",
      "|    9|Sample Type / Med...|[{document, 0, 19...|[{document, 0, 15...|[{token, 0, 5, Sa...|[{word_embeddings...|[{named_entity, 0...|[{chunk, 79, 95, ...|[{assertion, 79, ...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8OwYh1r6WU5B",
    "outputId": "b95f32c1-8a33-426c-857e-4f472e83a0ec",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(result=['Sample Type / Medical Specialty:\\nHematology - Oncology\\nSample Name:\\nDischarge Summary - Mesothelioma - 1\\nDescription:\\nMesothelioma, pleural effusion, atrial fibrillation, anemia, ascites, esophageal reflux, and history of deep venous thrombosis.', '(Medical Transcription Sample Report)\\nPRINCIPAL DIAGNOSIS:\\nMesothelioma.', 'SECONDARY DIAGNOSES:\\nPleural effusion, atrial fibrillation, anemia, ascites, esophageal reflux, and history of deep venous thrombosis.', 'PROCEDURES', '1. On August 24, 2007, decortication of the lung with pleural biopsy and transpleural fluoroscopy.', '2. On August 20, 2007, thoracentesis.', '3. On August 31, 2007, Port-A-Cath placement.', 'HISTORY AND PHYSICAL:\\nThe patient is a 41-year-old Vietnamese female with a nonproductive cough that started last week.', 'She has had right-sided chest pain radiating to her back with fever starting yesterday.', 'She has a history of pericarditis and pericardectomy in May 2006 and developed cough with right-sided chest pain, and went to an urgent care center.', 'Chest x-ray revealed right-sided pleural effusion.', 'PAST MEDICAL HISTORY', '1. Pericardectomy.', '2. Pericarditis.', '2. Atrial fibrillation.', '4. RNCA with intracranial thrombolytic treatment.', '5 PTA of MCA.', '6. Mesenteric venous thrombosis.', '7. Pericardial window.', '8. Cholecystectomy.', '9. Left thoracentesis.', 'FAMILY HISTORY:\\nNo family history of coronary artery disease, CVA, diabetes, CHF or MI.', 'The patient has one family member, a sister, with history of cancer.', 'SOCIAL HISTORY:\\nShe is married.', 'Employed with the US Post Office.', 'She is a mother of three.', 'Denies tobacco, alcohol or illicit drug use.', 'MEDICATIONS', '1. Coumadin 1 mg daily.', 'Last INR was on Tuesday, August 14, 2007, and her INR was 2.3.', '2. Amiodarone 100 mg p.o. daily.', 'REVIEW OF SYSTEMS:\\nComplete review of systems negative except as in pulmonary as noted above.', 'The patient also reports occasional numbness and tingling of her left arm.', 'PHYSICAL EXAMINATION\\nVITAL SIGNS: Blood pressure 123/95, heart rate 83, respirations 20, temperature 97, and oxygen saturation 97%.', 'GENERAL: Positive nonproductive cough and pain with coughing.', 'HEENT: Pupils are equal and reactive to light and accommodation.', 'Tympanic membranes are clear.', 'NECK: Supple.', 'No lymphadenopathy.', 'No masses.', 'RESPIRATORY: Pleural friction rub is noted.', 'GI: Soft, nondistended, and nontender.', 'Positive bowel sounds.', 'No organomegaly.', 'EXTREMITIES: No edema, no clubbing, no cyanosis, no tenderness.', 'Full range of motion.', 'Normal pulses in all extremities.', 'SKIN: No breakdown or lesions.', 'No ulcers.', 'NEUROLOGIC: Grossly intact.', 'No focal deficits.', 'Awake, alert, and oriented to person, place, and time.', 'LABORATORY DATA:\\nLabs are pending.', 'HOSPITAL COURSE:\\nThe patient was admitted for a right-sided pleural effusion for thoracentesis on Monday by Dr. X. Her Coumadin was placed on hold.', 'A repeat echocardiogram was checked.', 'She was started on prophylaxis for DVT with Lovenox 40 mg subcutaneously.', 'Her history dated back to March 2005 when she first sought medical attention for evidence of pericarditis, which was treated with pericardial window in an outside hospital, at that time she was also found to have mesenteric pain and thrombosis, is now anticoagulated.', 'Her pericardial fluid was accumulated and she was seen by Dr. Y. At that time, she was recommended for pericardectomy, which was performed by Dr. Z. Review of her CT scan from March 2006 prior to her pericardectomy, already shows bilateral plural effusions.', 'The patient improved clinically after the pericardectomy with resolution of her symptoms.', 'Recently, she was readmitted to the hospital with chest pain and found to have bilateral pleural effusion, the right greater than the left.', 'CT of the chest also revealed a large mediastinal lymph node.', 'We reviewed the pathology obtained from the pericardectomy in March 2006, which was diagnostic of mesothelioma.', 'At this time, chest tube placement for drainage of the fluid occurred and thoracoscopy with fluid biopsies, which were performed, which revealed epithelioid malignant mesothelioma.', 'The patient was then stained with a PET CT, which showed extensive uptake in the chest, bilateral pleural pericardial effusions, and lymphadenopathy.', 'She also had acidic fluid, pectoral and intramammary lymph nodes and uptake in L4 with SUV of', '4. This was consistent with stage III disease.', 'Her repeat echocardiogram showed an ejection fraction of 45% to 49%.', 'She was transferred to Oncology service and started on chemotherapy on September 1, 2007 with cisplatin 75 mg/centimeter squared equaling 109 mg IV piggyback over 2 hours on September 1, 2007, Alimta 500 mg/ centimeter squared equaling 730 mg IV piggyback over 10 minutes.', 'This was all initiated after a Port-A-Cath was placed.', 'The chemotherapy was well tolerated and the patient was discharged the following day after discontinuing IV fluid and IV.', 'Her Port-A-Cath was packed with heparin according to protocol.', 'DISCHARGE MEDICATIONS:\\nZofran, Phenergan, Coumadin, and Lovenox, and Vicodin\\nDISCHARGE INSTRUCTIONS:\\nShe was instructed to followup with Dr. XYZ in the office to check her INR on Tuesday.', 'She was instructed to call if she had any other questions or concerns in the interim.', 'Keywords:\\nhematology - oncology, mesothelioma, pleural effusion, atrial fibrillation, anemia, ascites, esophageal reflux, deep venous thrombosis, port-a-cath placement, port a cath, iv piggyback, venous thrombosis, atrial, thrombosis, pericardial, lymphadenopathy, fluid, pericardectomy, chest, pleural,'])]\n"
     ]
    }
   ],
   "source": [
    "result.select('sentence.result').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5uBnSe0--_Tn",
    "outputId": "b3ade5cf-bd71-4b6e-836a-c0dfeed9fe9e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-----+---+-------------------------+-------+------------+----------+\n",
      "|chunk                    |begin|end|ner_label                |sent_id|assertion   |confidence|\n",
      "+-------------------------+-----+---+-------------------------+-------+------------+----------+\n",
      "|Discharge                |68   |76 |Admission_Discharge      |0      |Past        |0.991     |\n",
      "|pleural effusion         |132  |147|Disease_Syndrome_Disorder|0      |Present     |1.0       |\n",
      "|anemia                   |171  |176|Disease_Syndrome_Disorder|0      |Family      |1.0       |\n",
      "|ascites                  |179  |185|Disease_Syndrome_Disorder|0      |Hypothetical|0.9782    |\n",
      "|esophageal reflux        |188  |204|Disease_Syndrome_Disorder|0      |Family      |1.0       |\n",
      "|deep venous thrombosis   |222  |243|Disease_Syndrome_Disorder|0      |Family      |1.0       |\n",
      "|Pleural effusion         |340  |355|Disease_Syndrome_Disorder|2      |Present     |1.0       |\n",
      "|anemia                   |379  |384|Disease_Syndrome_Disorder|2      |Present     |1.0       |\n",
      "|ascites                  |387  |393|Disease_Syndrome_Disorder|2      |Present     |1.0       |\n",
      "|esophageal reflux        |396  |412|Disease_Syndrome_Disorder|2      |Present     |1.0       |\n",
      "|deep venous thrombosis   |430  |451|Disease_Syndrome_Disorder|2      |Past        |0.9962    |\n",
      "|decortication of the lung|488  |512|Procedure                |4      |Past        |1.0       |\n",
      "|pleural biopsy           |519  |532|Procedure                |4      |Past        |1.0       |\n",
      "|thoracentesis            |587  |599|Procedure                |5      |Past        |1.0       |\n",
      "|Port-A-Cath placement    |625  |645|Procedure                |6      |Past        |1.0       |\n",
      "|cough                    |738  |742|Symptom                  |7      |SomeoneElse |0.8521    |\n",
      "|chest pain               |792  |801|Symptom                  |8      |Past        |1.0       |\n",
      "|fever                    |830  |834|VS_Finding               |8      |Present     |0.9997    |\n",
      "|pericarditis             |877  |888|Disease_Syndrome_Disorder|9      |Present     |0.9491    |\n",
      "|pericardectomy           |894  |907|Procedure                |9      |Past        |1.0       |\n",
      "+-------------------------+-----+---+-------------------------+-------+------------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "result.select(F.explode(F.arrays_zip(result.ner_chunk.result,\n",
    "                                     result.ner_chunk.begin,\n",
    "                                     result.ner_chunk.end,\n",
    "                                     result.ner_chunk.metadata,\n",
    "                                     result.assertion.result,\n",
    "                                     result.assertion.metadata)).alias(\"cols\")) \\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
    "              F.expr(\"cols['1']\").alias(\"begin\"),\n",
    "              F.expr(\"cols['2']\").alias(\"end\"),\n",
    "              F.expr(\"cols['3']['entity']\").alias(\"ner_label\"),\n",
    "              F.expr(\"cols['3']['sentence']\").alias(\"sent_id\"),\n",
    "              F.expr(\"cols['4']\").alias(\"assertion\"),\n",
    "              F.expr(\"cols['5']['confidence']\").alias(\"confidence\") ).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYN97Es2be4p"
   },
   "source": [
    "### Pretrained `assertion_dl_radiology` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YMl2QAeNbe4p",
    "outputId": "4b57c4f8-47ae-4673-a89d-d74b79c99a78",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_detector_dl_healthcare download started this may take some time.\n",
      "Approximate size to download 367.3 KB\n",
      "[OK!]\n",
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n",
      "ner_radiology download started this may take some time.\n",
      "[OK!]\n",
      "assertion_dl_radiology download started this may take some time.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = nlp.DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "# Sentence Detector annotator, processes various sentences per line\n",
    "sentenceDetector = nlp.SentenceDetectorDLModel\\\n",
    "    .pretrained(\"sentence_detector_dl_healthcare\",\"en\",\"clinical/models\") \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "# Tokenizer splits words in a relevant format for NLP\n",
    "tokenizer = nlp.Tokenizer()\\\n",
    "    .setInputCols([\"sentence\"])\\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "# Clinical word embeddings trained on PubMED dataset\n",
    "word_embeddings = nlp.WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols([\"sentence\", \"token\"])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "# NER model for radiology\n",
    "radiology_ner = medical.NerModel.pretrained(\"ner_radiology\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"ner\")\\\n",
    "    #.setIncludeAllConfidenceScores(False)\n",
    "\n",
    "ner_converter = medical.NerConverterInternal() \\\n",
    "    .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
    "    .setOutputCol(\"ner_chunk\")\\\n",
    "    .setWhiteList([\"ImagingFindings\"])\n",
    "\n",
    "# Assertion model trained on radiology dataset\n",
    "radiology_assertion = medical.AssertionDLModel.pretrained(\"assertion_dl_radiology\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"assertion\")\n",
    "\n",
    "nlpPipeline = nlp.Pipeline(stages=[\n",
    "    documentAssembler,\n",
    "    sentenceDetector,\n",
    "    tokenizer,\n",
    "    word_embeddings,\n",
    "    radiology_ner,\n",
    "    ner_converter,\n",
    "    radiology_assertion\n",
    "    ])\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "radiologyAssertion_model = nlpPipeline.fit(empty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ehm7WwJrbe4r",
    "outputId": "23c44b48-25a7-41a6-dc8f-78f86424edc0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# A sample text from a radiology report\n",
    "\n",
    "text = \"\"\"No right-sided pleural effusion or pneumothorax is definitively seen and there are mildly displaced fractures of the left lateral 8th and likely 9th ribs.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1iCxtKjbe4s",
    "outputId": "c9d4efce-ca72-4bdb-be27-1d845726bbfe",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.createDataFrame([[text]]).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSpEvzDObe4s",
    "outputId": "2be6f360-ad58-4581-8b98-749c77e2df92",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = radiologyAssertion_model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fGxRCsusbe4s",
    "outputId": "86d79af4-27c6-4322-fb60-f8f67b6aef7c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+-------+---------+\n",
      "|chunk              |ner_label      |sent_id|assertion|\n",
      "+-------------------+---------------+-------+---------+\n",
      "|effusion           |ImagingFindings|0      |Negative |\n",
      "|pneumothorax       |ImagingFindings|0      |Negative |\n",
      "|displaced fractures|ImagingFindings|0      |Confirmed|\n",
      "+-------------------+---------------+-------+---------+\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "result.select(F.explode(F.arrays_zip(result.ner_chunk.result,\n",
    "                                     result.ner_chunk.metadata,\n",
    "                                     result.assertion.result)).alias(\"cols\")) \\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
    "              F.expr(\"cols['1']['entity']\").alias(\"ner_label\"),\n",
    "              F.expr(\"cols['1']['sentence']\").alias(\"sent_id\"),\n",
    "              F.expr(\"cols['2']\").alias(\"assertion\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vWIDpk50QfG"
   },
   "source": [
    "## Writing a generic Assertion + NER function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltGPQYr60F6G",
    "outputId": "31783c4d-e4f5-4ad7-f80e-91712c78dda2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_base_pipeline (embeddings = 'embeddings_clinical'):\n",
    "\n",
    "    documentAssembler = nlp.DocumentAssembler()\\\n",
    "        .setInputCol(\"text\")\\\n",
    "        .setOutputCol(\"document\")\n",
    "\n",
    "  # Sentence Detector annotator, processes various sentences per line\n",
    "    sentenceDetector = nlp.SentenceDetector()\\\n",
    "        .setInputCols([\"document\"])\\\n",
    "        .setOutputCol(\"sentence\")\n",
    "\n",
    "  # Tokenizer splits words in a relevant format for NLP\n",
    "    tokenizer = nlp.Tokenizer()\\\n",
    "        .setInputCols([\"sentence\"])\\\n",
    "        .setOutputCol(\"token\")\n",
    "\n",
    "  # Clinical word embeddings trained on PubMED dataset\n",
    "    word_embeddings = nlp.WordEmbeddingsModel.pretrained(embeddings, \"en\", \"clinical/models\")\\\n",
    "        .setInputCols([\"sentence\", \"token\"])\\\n",
    "        .setOutputCol(\"embeddings\")\n",
    "\n",
    "    base_pipeline = nlp.Pipeline(stages=[\n",
    "                        documentAssembler,\n",
    "                        sentenceDetector,\n",
    "                        tokenizer,\n",
    "                        word_embeddings])\n",
    "\n",
    "    return base_pipeline\n",
    "\n",
    "\n",
    "\n",
    "def get_clinical_assertion (embeddings, spark_df, nrows = 100, ner_model_name = 'ner_clinical', assertion_model_name=\"assertion_dl\"):\n",
    "\n",
    "  # NER model trained on i2b2 (sampled from MIMIC) dataset\n",
    "    loaded_ner_model = medical.NerModel.pretrained(ner_model_name, \"en\", \"clinical/models\") \\\n",
    "        .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "        .setOutputCol(\"ner\")\n",
    "\n",
    "    ner_converter = medical.NerConverterInternal() \\\n",
    "        .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
    "        .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "  # Assertion model trained on i2b2 (sampled from MIMIC) dataset\n",
    "  # coming from sparknlp_jsl.annotator !!\n",
    "    clinical_assertion = medical.AssertionDLModel.pretrained(assertion_model_name, \"en\", \"clinical/models\") \\\n",
    "        .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\n",
    "        .setOutputCol(\"assertion\")\n",
    "\n",
    "\n",
    "    base_model = get_base_pipeline (embeddings)\n",
    "\n",
    "    nlpPipeline = nlp.Pipeline(stages=[\n",
    "        base_model,\n",
    "        loaded_ner_model,\n",
    "        ner_converter,\n",
    "        clinical_assertion])\n",
    "\n",
    "    empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "    model = nlpPipeline.fit(empty_data)\n",
    "\n",
    "    result = model.transform(spark_df.limit(nrows))\n",
    "\n",
    "    result = result.withColumn(\"id\", F.monotonically_increasing_id())\n",
    "\n",
    "    result_df = result.select(F.explode(F.arrays_zip(result.ner_chunk.result,\n",
    "                                                     result.ner_chunk.metadata,\n",
    "                                                     result.assertion.result,\n",
    "                                                     result.assertion.metadata)).alias(\"cols\")) \\\n",
    "                      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
    "                              F.expr(\"cols['1']['entity']\").alias(\"ner_label\"),\n",
    "                              F.expr(\"cols['2']\").alias(\"assertion\"),\n",
    "                              F.expr(\"cols['3']['confidence']\").alias(\"confidence\"))\\\n",
    "                      .filter(\"ner_label!='O'\")\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V81zaAe13qLU",
    "outputId": "6c51bea1-aa59-4e05-e013-217c6734045e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_clinical_large download started this may take some time.\n",
      "[OK!]\n",
      "assertion_dl download started this may take some time.\n",
      "[OK!]\n",
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n",
      "+--------------------------+---------+------------+----------+\n",
      "|                     chunk|ner_label|   assertion|confidence|\n",
      "+--------------------------+---------+------------+----------+\n",
      "|                    Anemia|  PROBLEM|     present|    0.9997|\n",
      "|         Refractory anemia|  PROBLEM|     present|    0.9997|\n",
      "|     transfusion dependent|  PROBLEM|     present|    0.9988|\n",
      "|                    anemia|  PROBLEM|     present|       1.0|\n",
      "|                hemoglobin|     TEST|     present|    0.9999|\n",
      "|               transfusion|TREATMENT|     present|    0.8883|\n",
      "|         Refractory anemia|  PROBLEM|     present|    0.9997|\n",
      "|     transfusion dependent|  PROBLEM|     present|    0.9998|\n",
      "|       a blood transfusion|TREATMENT|     present|    0.8887|\n",
      "|                  diabetes|  PROBLEM|     present|    0.7393|\n",
      "|any comorbid complications|  PROBLEM|      absent|    0.9986|\n",
      "|              the diabetes|  PROBLEM|      absent|    0.9997|\n",
      "|            kidney disease|  PROBLEM|      absent|    0.5183|\n",
      "|             heart disease|  PROBLEM|hypothetical|    0.6885|\n",
      "|                    stroke|  PROBLEM|      absent|    0.9998|\n",
      "|               vision loss|  PROBLEM|      absent|       1.0|\n",
      "|                neuropathy|  PROBLEM|      absent|       1.0|\n",
      "|                    anemia|  PROBLEM|     present|       1.0|\n",
      "|                hemoglobin|     TEST|     present|    0.9999|\n",
      "|               transfusion|TREATMENT|     present|    0.8883|\n",
      "|                  bleeding|  PROBLEM|      absent|    0.9986|\n",
      "|       a blood transfusion|TREATMENT|     present|    0.9971|\n",
      "|    that blood transfusion|TREATMENT|hypothetical|    0.7571|\n",
      "|             a transfusion|TREATMENT|     present|    0.9999|\n",
      "|                    anemia|  PROBLEM|     present|    0.9917|\n",
      "|                       B12|TREATMENT|     present|    0.9999|\n",
      "|                 oral iron|TREATMENT|     present|    0.9995|\n",
      "|                   Procrit|TREATMENT|     present|    0.9986|\n",
      "|                 treatment|TREATMENT|     present|    0.9972|\n",
      "|                his anemia|  PROBLEM|     present|    0.9999|\n",
      "+--------------------------+---------+------------+----------+\n",
      "only showing top 30 rows\n"
     ]
    }
   ],
   "source": [
    "embeddings = 'embeddings_clinical'\n",
    "\n",
    "ner_model_name = 'ner_clinical_large'\n",
    "\n",
    "nrows = 100\n",
    "\n",
    "ner_df = get_clinical_assertion (embeddings, mt_samples_df, nrows, ner_model_name)\n",
    "\n",
    "ner_df.show(30,truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LmLPlfCw5hh-",
    "outputId": "181598b1-bb03-4fd7-f26d-a79e88e73b99",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_posology download started this may take some time.\n",
      "[OK!]\n",
      "assertion_dl download started this may take some time.\n",
      "[OK!]\n",
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n",
      "+--------------+---------+--------------------+----------+\n",
      "|         chunk|ner_label|           assertion|confidence|\n",
      "+--------------+---------+--------------------+----------+\n",
      "|   transfusion|    ROUTE|             present|    0.8883|\n",
      "|   transfusion|    ROUTE|associated_with_s...|    0.5379|\n",
      "|   transfusion|    ROUTE|             present|    0.8883|\n",
      "|   transfusion|    ROUTE|             present|     0.999|\n",
      "|          oral|    ROUTE|             present|    0.9985|\n",
      "|          iron|     DRUG|             present|    0.9986|\n",
      "|       Procrit|     DRUG|             present|    0.9986|\n",
      "|      Coumadin|     DRUG|         conditional|    0.5629|\n",
      "|         Lasix|     DRUG|             present|    0.7021|\n",
      "|     metformin|     DRUG|             present|    0.5187|\n",
      "|    folic acid|     DRUG|         conditional|    0.9142|\n",
      "|     diltiazem|     DRUG|              absent|    0.9816|\n",
      "|           B12|     DRUG|              absent|    0.9979|\n",
      "|      Prevacid|     DRUG|              absent|    0.9996|\n",
      "|        Feosol|     DRUG|              absent|    0.9995|\n",
      "|           B12|     DRUG|             present|    0.9991|\n",
      "|          iron|     DRUG|             present|    0.8859|\n",
      "|    folic acid|     DRUG|             present|    0.9919|\n",
      "|       Procrit|     DRUG|             present|    0.9952|\n",
      "|erythropoietin|     DRUG|             present|    0.9912|\n",
      "+--------------+---------+--------------------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "embeddings = 'embeddings_clinical'\n",
    "\n",
    "ner_model_name = 'ner_posology'\n",
    "\n",
    "nrows = 100\n",
    "\n",
    "ner_df = get_clinical_assertion (embeddings, mt_samples_df, nrows, ner_model_name)\n",
    "\n",
    "ner_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQ9xeS3kwClE",
    "outputId": "2ddba4bd-a7ad-46fc-863c-2ca7c9eddcaa",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_posology_greedy download started this may take some time.\n",
      "[OK!]\n",
      "assertion_dl download started this may take some time.\n",
      "[OK!]\n",
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n",
      "+----------------+---------+---------+----------+\n",
      "|           chunk|ner_label|assertion|confidence|\n",
      "+----------------+---------+---------+----------+\n",
      "|capsule of Advil|     DRUG|   absent|    0.9855|\n",
      "+----------------+---------+---------+----------+\n"
     ]
    }
   ],
   "source": [
    "embeddings = 'embeddings_clinical'\n",
    "\n",
    "ner_model_name = 'ner_posology_greedy'\n",
    "\n",
    "entry_data = spark.createDataFrame([[\"The patient did not take a capsule of Advil.\"]]).toDF(\"text\")\n",
    "\n",
    "ner_df = get_clinical_assertion (embeddings, entry_data, nrows, ner_model_name)\n",
    "\n",
    "ner_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djV_FKNtYcoP",
    "outputId": "0c777bce-5eb5-4135-ce4c-839bb1440cff",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_clinical download started this may take some time.\n",
      "[OK!]\n",
      "assertion_dl download started this may take some time.\n",
      "[OK!]\n",
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n",
      "+-----+---------+---------+----------+\n",
      "|chunk|ner_label|assertion|confidence|\n",
      "+-----+---------+---------+----------+\n",
      "|fever|  PROBLEM|   absent|     0.998|\n",
      "+-----+---------+---------+----------+\n"
     ]
    }
   ],
   "source": [
    "embeddings = 'embeddings_clinical'\n",
    "\n",
    "ner_model_name = 'ner_clinical'\n",
    "\n",
    "entry_data = spark.createDataFrame([[\"The patient has no fever\"]]).toDF(\"text\")\n",
    "\n",
    "ner_df = get_clinical_assertion (embeddings, entry_data, nrows, ner_model_name)\n",
    "\n",
    "ner_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P87ScBcJqcNs",
    "outputId": "c9a557d3-087e-4699-d7b1-a609fcadf753",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_clinical_assertion_light (light_model, text):\n",
    "\n",
    "  light_result = light_model.fullAnnotate(text)[0]\n",
    "\n",
    "  chunks=[]\n",
    "  entities=[]\n",
    "  status=[]\n",
    "  confidence=[]\n",
    "\n",
    "  for n,m in zip(light_result['ner_chunk'],light_result['assertion']):\n",
    "\n",
    "      chunks.append(n.result)\n",
    "      entities.append(n.metadata['entity'])\n",
    "      status.append(m.result)\n",
    "      confidence.append(m.metadata['confidence'])\n",
    "\n",
    "  df = pd.DataFrame({'chunks':chunks, 'entities':entities, 'assertion':status,'confidence':confidence})\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "id": "GqKH9Mw98z4x",
    "outputId": "83150609-3b9d-4d71-f395-d2da5fb2f1f9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning::Spark Session already created, some configs may not take.\n",
      "Warning::Spark Session already created, some configs may not take.\n",
      "            entities_ner_chunk   entities_ner_chunk_class     assertion  \\\n",
      "0                        fever                 VS_Finding       Present   \n",
      "0                  sore throat                    Symptom       Present   \n",
      "0                 stomach pain                    Symptom        Absent   \n",
      "0                         pain                    Symptom  Hypothetical   \n",
      "0              short of breath                    Symptom       Present   \n",
      "0  climbing a flight of stairs                    Symptom       Present   \n",
      "0                    Alzheimer  Disease_Syndrome_Disorder        Family   \n",
      "\n",
      "  assertion_confidence  \n",
      "0                  1.0  \n",
      "0                  1.0  \n",
      "0                  1.0  \n",
      "0                  1.0  \n",
      "0                  1.0  \n",
      "0               0.9434  \n",
      "0               0.8136\n"
     ]
    }
   ],
   "source": [
    "clinical_text = \"\"\"\n",
    "Patient with severe fever and sore throat.\n",
    "He shows no stomach pain and he maintained on an epidural and PCA for pain control.\n",
    "He also became short of breath with climbing a flight of stairs.\n",
    "After CT, lung tumor located at the right lower lobe. Father with Alzheimer.\n",
    "\"\"\"\n",
    "\n",
    "light_model = nlp.LightPipeline(model)\n",
    "\n",
    "# get_clinical_assertion_light (light_model, clinical_text)\n",
    "\n",
    "cols = [\n",
    "     'entities_ner_chunk',\n",
    "     'entities_ner_chunk_class',\n",
    "     'assertion',\n",
    "     'assertion_confidence']\n",
    "\n",
    "df = nlp.nlu.to_pretty_df(light_model,clinical_text, output_level='chunk')[cols]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qVeR4zew_urR"
   },
   "source": [
    "# Oncological Assertion Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-VdSHw8h5eJ"
   },
   "source": [
    "Oncology Assertion Models\n",
    "\n",
    "|    | model_name              |Predicted Entities|\n",
    "|---:|:------------------------|-|\n",
    "| 1 | [assertion_oncology_wip](https://nlp.johnsnowlabs.com/2022/10/11/assertion_oncology_wip_en.html) | Medical_History, Family_History, Possible, Hypothetical_Or_Absent|\n",
    "| 2 | [assertion_oncology_problem_wip](https://nlp.johnsnowlabs.com/2022/10/11/assertion_oncology_problem_wip_en.html) |Present, Possible, Hypothetical, Absent, Family|\n",
    "| 3 | [assertion_oncology_treatment_wip](https://nlp.johnsnowlabs.com/2022/10/11/assertion_oncology_treatment_binary_wip_en.html) |Present, Planned, Past, Hypothetical, Absent|\n",
    "| 3 | [assertion_oncology_treatment_wip]() |Present, Planned, Past, Hypothetical, Absent|\n",
    "| 4 | [assertion_oncology_response_to_treatment_wip](https://nlp.johnsnowlabs.com/2022/10/11/assertion_oncology_response_to_treatment_wip_en.html) |Present_Or_Past, Hypothetical_Or_Absent|\n",
    "| 5 | [assertion_oncology_test_binary_wip](https://nlp.johnsnowlabs.com/2022/10/01/assertion_oncology_test_binary_wip_en.html) |Present_Or_Past, Hypothetical_Or_Absent|\n",
    "| 6 | [assertion_oncology_smoking_status_wip](https://nlp.johnsnowlabs.com/2022/10/11/assertion_oncology_smoking_status_wip_en.html) |Absent, Past, Present|\n",
    "| 7 | [assertion_oncology_family_history_wip](https://nlp.johnsnowlabs.com/2022/10/11/assertion_oncology_family_history_wip_en.html) |Family_History, Other|\n",
    "| 8 | [assertion_oncology_demographic_binary_wip](https://nlp.johnsnowlabs.com/2022/10/11/assertion_oncology_demographic_binary_wip_en.html) |Patient, Someone_Else|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y1Ffv9qT_AYV",
    "outputId": "d863a4ab-8b84-418d-c396-2e9b23c16c25",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_oncology_wip download started this may take some time.\n",
      "[OK!]\n",
      "assertion_oncology_wip download started this may take some time.\n",
      "[OK!]\n",
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n",
      "+--------------------+-------------+---------+----------+\n",
      "|chunk               |ner_label    |assertion|confidence|\n",
      "+--------------------+-------------+---------+----------+\n",
      "|he                  |Gender       |Present  |0.9998    |\n",
      "|gentleman           |Gender       |Present  |0.9994    |\n",
      "|He                  |Gender       |Present  |0.9998    |\n",
      "|he                  |Gender       |Present  |0.9998    |\n",
      "|He                  |Gender       |Present  |0.9997    |\n",
      "|he                  |Gender       |Present  |0.9994    |\n",
      "|two months ago      |Relative_Date|Past     |0.6075    |\n",
      "|several weeks before|Relative_Date|Present  |0.976     |\n",
      "|he                  |Gender       |Present  |0.9999    |\n",
      "|He                  |Gender       |Present  |0.9997    |\n",
      "|him                 |Gender       |Present  |0.9905    |\n",
      "|his                 |Gender       |Present  |0.9894    |\n",
      "|He                  |Gender       |Present  |0.9991    |\n",
      "|He                  |Gender       |Present  |0.9976    |\n",
      "|adenopathy          |Adenopathy   |Absent   |0.9629    |\n",
      "|he                  |Gender       |Absent   |0.561     |\n",
      "|He                  |Gender       |Present  |0.9983    |\n",
      "|He                  |Gender       |Present  |0.9989    |\n",
      "|He                  |Gender       |Present  |0.9954    |\n",
      "|He                  |Gender       |Present  |0.9994    |\n",
      "+--------------------+-------------+---------+----------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "embeddings = 'embeddings_clinical'\n",
    "\n",
    "ner_model_name = 'ner_oncology_wip'\n",
    "\n",
    "assertion_model_name='assertion_oncology_wip'\n",
    "\n",
    "nrows = 100\n",
    "\n",
    "ner_df = get_clinical_assertion (embeddings, mt_samples_df, nrows, ner_model_name,assertion_model_name )\n",
    "\n",
    "ner_df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FunYnGIJ-lF-"
   },
   "source": [
    "# Entity Type Constraints\n",
    "\n",
    "\n",
    "You can effortlessly constrain assertions based on specific entity types using a convenient dictionary format: `{\"entity\": [assertion_label1, assertion_label2, .. assertion_labelN]}`. When an entity is not found in the dictionary, no constraints are applied, ensuring flexibility in your data processing. With the `setEntityAssertionCaseSensitive` parameter, you can control the case sensitivity for both entities and assertion labels. Unleash the full potential of your NLP model with these cutting-edge additions to the AssertionDLModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aX99kEvb-kpz",
    "outputId": "2aa8b78a-887d-4398-87c0-5fab4ec21234",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_clinical_large download started this may take some time.\n",
      "[OK!]\n",
      "assertion_jsl_augmented download started this may take some time.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# NER model trained on i2b2 (sampled from MIMIC) dataset\n",
    "clinical_ner = medical.NerModel.pretrained(\"ner_clinical_large\",\"en\",\"clinical/models\")\\\n",
    "    .setInputCols([\"sentence\",\"token\",\"embeddings\"])\\\n",
    "    .setOutputCol(\"ner\")\n",
    "\n",
    "ner_converter = medical.NerConverterInternal()\\\n",
    "    .setInputCols([\"sentence\",\"token\",\"ner\"])\\\n",
    "    .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "# Assertion model trained on i2b2 (sampled from MIMIC) dataset\n",
    "clinical_assertion = medical.AssertionDLModel.pretrained(\"assertion_jsl_augmented\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"assertion\")\\\n",
    "    .setEntityAssertionCaseSensitive(False)\\\n",
    "    .setEntityAssertion({\n",
    "        \"PROBLEM\": [\"hypothetical\", \"absent\"],\n",
    "        \"treAtment\": [\"present\"],\n",
    "        \"TEST\": [\"POssible\"],\n",
    "    })\n",
    "\n",
    "nlpPipeline = nlp.Pipeline(stages=[\n",
    "    documentAssembler,\n",
    "    sentenceDetector,\n",
    "    tokenizer,\n",
    "    word_embeddings,\n",
    "    clinical_ner,\n",
    "    ner_converter,\n",
    "    clinical_assertion\n",
    "    ])\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "model = nlpPipeline.fit(empty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2l_YmpmSTo2",
    "outputId": "dd7e7a1c-d0b9-4f00-b43a-ba38eafba75b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "text = '''\n",
    "A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus (T2DM), one prior episode of HTG-induced pancreatitis three years prior to presentation, and associated with an acute hepatitis, presented with a one-week history of polyuria, poor appetite, and vomiting.\n",
    "She was on metformin, glipizide, and dapagliflozin for T2DM and atorvastatin and gemfibrozil for HTG. She had been on dapagliflozin for six months at the time of presentation.\n",
    "Physical examination on presentation was significant for dry oral mucosa ; significantly , her abdominal examination was benign with no tenderness, guarding, or rigidity. Pertinent laboratory findings on admission were: serum glucose 111 mg/dl,  creatinine 0.4 mg/dL, triglycerides 508 mg/dL, total cholesterol 122 mg/dL, and venous pH 7.27.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "gsjA1B6LSQf_",
    "outputId": "2c74813d-c906-4184-ef35-c719f266a3a9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 chunks   entities     assertion confidence\n",
      "0             metformin  TREATMENT       Present     0.5364\n",
      "1             glipizide  TREATMENT       Present     0.9993\n",
      "2         dapagliflozin  TREATMENT       Present        1.0\n",
      "3                   HTG    PROBLEM  Hypothetical        1.0\n",
      "4  Physical examination       TEST      Possible      0.943\n",
      "5            tenderness    PROBLEM        Absent        1.0\n",
      "6              guarding    PROBLEM        Absent        1.0\n",
      "7              rigidity    PROBLEM  Hypothetical     0.9999\n"
     ]
    }
   ],
   "source": [
    "light_model = nlp.LightPipeline(model)\n",
    "\n",
    "light_result = light_model.fullAnnotate(text)\n",
    "\n",
    "chunks=[]\n",
    "entities=[]\n",
    "status=[]\n",
    "confidence=[]\n",
    "\n",
    "for assertion_row in light_result[0][\"assertion\"]:\n",
    "  chunk_id = assertion_row.metadata[\"chunk\"]\n",
    "  for chunk_row in light_result[0][\"ner_chunk\"]:\n",
    "    if chunk_id == chunk_row.metadata[\"chunk\"]:\n",
    "        chunks.append(chunk_row.result)\n",
    "        entities.append(chunk_row.metadata['entity'])\n",
    "        status.append(assertion_row.result)\n",
    "        confidence.append(assertion_row.metadata['confidence'])\n",
    "\n",
    "df = pd.DataFrame({'chunks':chunks, 'entities':entities, 'assertion':status, 'confidence':confidence})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jECAdcYGcJJT"
   },
   "source": [
    "# Assertion Filterer\n",
    "AssertionFilterer will allow you to filter out the named entities by the list of acceptable assertion statuses. This annotator would be quite handy if you want to set a white list for the acceptable assertion statuses like present or conditional; and do not want absent conditions get out of your pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c8xRF4yScwP3",
    "outputId": "530c8b4c-54d5-4a89-836e-3fbbb698be9b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n",
      "ner_clinical download started this may take some time.\n",
      "[OK!]\n",
      "assertion_jsl_augmented download started this may take some time.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Annotator that transforms a text column from dataframe into an Annotation ready for NLP\n",
    "\n",
    "documentAssembler = nlp.DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "# Sentence Detector annotator, processes various sentences per line\n",
    "sentenceDetector = nlp.SentenceDetector()\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "# Tokenizer splits words in a relevant format for NLP\n",
    "tokenizer = nlp.Tokenizer()\\\n",
    "    .setInputCols([\"sentence\"])\\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "# Clinical word embeddings trained on PubMED dataset\n",
    "word_embeddings = nlp.WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols([\"sentence\", \"token\"])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "clinical_ner = medical.NerModel.pretrained(\"ner_clinical\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"ner\")\\\n",
    "    #.setIncludeAllConfidenceScores(False)\n",
    "\n",
    "ner_converter = medical.NerConverterInternal() \\\n",
    "    .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
    "    .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "clinical_assertion = medical.AssertionDLModel.pretrained(\"assertion_jsl_augmented\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"assertion\")\n",
    "\n",
    "assertion_filterer = medical.AssertionFilterer()\\\n",
    "    .setInputCols(\"sentence\",\"ner_chunk\",\"assertion\")\\\n",
    "    .setOutputCol(\"assertion_filtered\")\\\n",
    "    .setCaseSensitive(False)\\\n",
    "    .setWhiteList([\"PREsent\"])\n",
    "\n",
    "nlpPipeline = nlp.Pipeline(stages=[\n",
    "      documentAssembler,\n",
    "      sentenceDetector,\n",
    "      tokenizer,\n",
    "      word_embeddings,\n",
    "      clinical_ner,\n",
    "      ner_converter,\n",
    "      clinical_assertion,\n",
    "      assertion_filterer\n",
    "    ])\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "assertionFilter_model = nlpPipeline.fit(empty_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KE6Qt20Sfqc-",
    "outputId": "765133b6-48ba-409b-ce20-34d517e7850e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['assertion_filtered', 'document', 'ner_chunk', 'assertion', 'token', 'ner', 'embeddings', 'sentence'])\n"
     ]
    }
   ],
   "source": [
    "text = 'Patient has a headache for the last 2 weeks, needs to get a head CT, and appears anxious when she walks fast. Alopecia noted. She denies pain.'\n",
    "\n",
    "light_model = nlp.LightPipeline(assertionFilter_model)\n",
    "light_result = light_model.annotate(text)\n",
    "\n",
    "light_result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3gJfCLMHk0MV",
    "outputId": "ed37fedb-4ea4-4ddf-aa49-4b3d67f6e8b8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a headache', 'Absent')]\n"
     ]
    }
   ],
   "source": [
    "list(zip(light_result['ner_chunk'], light_result['assertion']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjuVg_4N15y2",
    "outputId": "d0c17e55-5910-47b8-9c2c-71c168b2572b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PREsent']\n"
     ]
    }
   ],
   "source": [
    "assertion_filterer.getWhiteList()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49
    },
    "id": "rpWfTJbsk7tp",
    "outputId": "3d227f13-2cd0-4e16-e836-252a5a5bcc66",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [chunks, entities, assertion, confidence]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "chunks=[]\n",
    "entities=[]\n",
    "status=[]\n",
    "confidence=[]\n",
    "\n",
    "light_result = light_model.fullAnnotate(text)[0]\n",
    "\n",
    "for m in light_result['assertion_filtered']:\n",
    "\n",
    "    chunks.append(m.result)\n",
    "    entities.append(m.metadata['entity'])\n",
    "    status.append(m.metadata['assertion'])\n",
    "    confidence.append(m.metadata['confidence'])\n",
    "\n",
    "df = pd.DataFrame({'chunks':chunks, 'entities':entities, 'assertion':status, 'confidence':confidence})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WREO4af-9faq"
   },
   "source": [
    "As you see, there is no \"pain\" chunk since it has \"absent\" assertion label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iUPXaUJorB_R"
   },
   "source": [
    "# AssertionChunkConverter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07KlFddHOQJ3"
   },
   "source": [
    "In some cases, there may be issues while creating the chunk column by using token indices and losing some data while training and testing the assertion status model if there are issues in these token indices. So we developed a new `AssertionChunkConverter` annotator that takes **begin and end indices of the chunks** as input and creates an extended chunk column with metadata that can be used for assertion status detection model training.\n",
    "\n",
    "*NOTE*: Chunk begin and end indices in the assertion status model training dataframe can be populated using the new version of ALAB module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O3T-vyPqrBhE",
    "outputId": "de11b695-8353-43b6-aed1-eb155f6e8b20",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----------+--------+\n",
      "|                text|target|char_begin|char_end|\n",
      "+--------------------+------+----------+--------+\n",
      "|An angiography sh...|Minnie|        57|      63|\n",
      "|After discussing ...|   PCP|        31|      34|\n",
      "+--------------------+------+----------+--------+\n"
     ]
    }
   ],
   "source": [
    "data = spark.createDataFrame([[\"An angiography showed bleeding in two vessels off of the Minnie supplying the sigmoid that were succesfully embolized.\", \"Minnie\", 57, 63],\n",
    "     [\"After discussing this with his PCP, Leon was clear that the patient had had recurrent DVTs and ultimately a PE and his PCP felt strongly that he required long-term anticoagulation \", \"PCP\", 31, 34]])\\\n",
    "     .toDF(\"text\", \"target\", \"char_begin\", \"char_end\")\n",
    "\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8c1xD4qIrNZj",
    "outputId": "1166b56c-5fe0-40cb-a710-79734db04f7d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "document_assembler = nlp.DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentenceDetector = nlp.SentenceDetector()\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = nlp.Tokenizer() \\\n",
    "    .setInputCols([\"sentence\"]) \\\n",
    "    .setOutputCol(\"tokens\")\n",
    "\n",
    "converter = medical.AssertionChunkConverter() \\\n",
    "    .setInputCols(\"tokens\")\\\n",
    "    .setChunkTextCol(\"target\")\\\n",
    "    .setChunkBeginCol(\"char_begin\")\\\n",
    "    .setChunkEndCol(\"char_end\")\\\n",
    "    .setOutputTokenBeginCol(\"token_begin\")\\\n",
    "    .setOutputTokenEndCol(\"token_end\")\\\n",
    "    .setOutputCol(\"chunk\")\n",
    "\n",
    "pipeline = nlp.Pipeline().setStages([document_assembler,sentenceDetector, tokenizer, converter])\n",
    "\n",
    "results = pipeline.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MSzD7n-RrOvr",
    "outputId": "91f2127b-275c-48bd-a545-58c11eb10f18",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------+-----------+---------+--------------------------+------------------------+------+----------------------------------------------+\n",
      "|target|char_begin|char_end|token_begin|token_end|tokens[token_begin].result|tokens[token_end].result|target|chunk                                         |\n",
      "+------+----------+--------+-----------+---------+--------------------------+------------------------+------+----------------------------------------------+\n",
      "|Minnie|57        |63      |10         |10       |Minnie                    |Minnie                  |Minnie|[{chunk, 57, 62, Minnie, {sentence -> 0}, []}]|\n",
      "|PCP   |31        |34      |5          |5        |PCP                       |PCP                     |PCP   |[{chunk, 31, 33, PCP, {sentence -> 0}, []}]   |\n",
      "+------+----------+--------+-----------+---------+--------------------------+------------------------+------+----------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "results\\\n",
    "    .selectExpr(\n",
    "        \"target\",\n",
    "        \"char_begin\",\n",
    "        \"char_end\",\n",
    "        \"token_begin\",\n",
    "        \"token_end\",\n",
    "        \"tokens[token_begin].result\",\n",
    "        \"tokens[token_end].result\",\n",
    "        \"target\",\n",
    "        \"chunk\")\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjfUo0aTKFEL"
   },
   "source": [
    "# Train a custom Assertion Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QPFTJxtlIKQT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample data should be downloaded and uploaded to the s3 bucket\n",
    "#!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/data/i2b2_assertion_sample_short.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRFW3nyALHx0",
    "outputId": "ed13c46d-6079-4aec-faed-3e6f2ff51611",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "assertion_df = spark.read.option(\"header\", True).option(\"inferSchema\", \"True\").csv(\"s3://<your_bucket_name>/rawdata/i2b2_assertion_sample_short.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TQn7bJZtL9uX",
    "outputId": "f4021cb2-b272-42b3-d784-624d8e3796b8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+-------------------+-------+-----+---+\n",
      "|                                             text|             target|  label|start|end|\n",
      "+-------------------------------------------------+-------------------+-------+-----+---+\n",
      "|She has no history of liver disease , hepatitis .|      liver disease| absent|    5|  6|\n",
      "|                         1. Undesired fertility .|undesired fertility|present|    1|  2|\n",
      "|                            3) STATUS POST FALL .|               fall|present|    3|  3|\n",
      "+-------------------------------------------------+-------------------+-------+-----+---+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "assertion_df.show(3, truncate=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B6IuDZ9Qw7Cu",
    "outputId": "9fff8acf-4ae1-46d5-a602-4b8c10bf3529",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 721\n",
      "Test Dataset Count: 170\n"
     ]
    }
   ],
   "source": [
    "(training_data, test_data) = assertion_df.randomSplit([0.8, 0.2], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(training_data.count()))\n",
    "print(\"Test Dataset Count: \" + str(test_data.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fktQ3EbBZwo-",
    "outputId": "b14e0c62-0a25-47f0-c686-28a9e4fcb68d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|label  |count|\n",
      "+-------+-----+\n",
      "|present|546  |\n",
      "|absent |175  |\n",
      "+-------+-----+\n"
     ]
    }
   ],
   "source": [
    "training_data.groupBy('label').count().orderBy('count', ascending=False).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UTNhwoSDLLVG",
    "outputId": "d1544552-3715-4cd5-d4c8-0c13381107d7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document = nlp.DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "chunk = nlp.Doc2Chunk()\\\n",
    "    .setInputCols(\"document\")\\\n",
    "    .setOutputCol(\"chunk\")\\\n",
    "    .setChunkCol(\"target\")\\\n",
    "    .setStartCol(\"start\")\\\n",
    "    .setStartColByTokenIndex(True)\\\n",
    "    .setFailOnMissing(False)\\\n",
    "    .setLowerCase(True)\n",
    "\n",
    "token = nlp.Tokenizer()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "embeddings = nlp.WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols([\"document\", \"token\"])\\\n",
    "    .setOutputCol(\"embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lkp7KmK6pv0-"
   },
   "source": [
    "We will transform our test data with a pipeline consisting of same steps with the pipeline which contains AssertionDLApproach.\n",
    "By doing this, we enable that test data will have same columns with training data in AssertionDLApproach. <br/>\n",
    "The goal of this implementation is enabling the usage of `setTestDataset()` parameter in AssertionDLApproach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7DScL2-uIOm",
    "outputId": "4978b755-c145-42b0-f7cb-108b76b5f488",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clinical_assertion_pipeline = nlp.Pipeline(\n",
    "    stages = [\n",
    "    document,\n",
    "    chunk,\n",
    "    token,\n",
    "    embeddings])\n",
    "\n",
    "assertion_test_data = clinical_assertion_pipeline.fit(test_data).transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nwuyumqGuHwY",
    "outputId": "2245d984-9467-44c3-c9a5-4f050ecda960",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'target', 'label', 'start', 'end', 'document', 'chunk', 'token', 'embeddings']\n"
     ]
    }
   ],
   "source": [
    "assertion_test_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aE-99quUsJ5V"
   },
   "source": [
    "We save the test data in parquet format to use in `AssertionDLApproach()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3Hx0w05uQ7E",
    "outputId": "268f6fea-f5c7-4794-ffc7-9c697218d891",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnalysisException: CSV data source does not support array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>> data type.\n"
     ]
    }
   ],
   "source": [
    "#assertion_test_data.write.parquet('i2b2_assertion_sample_test_data.parquet')\n",
    "assertion_test_data.write.parquet('s3://<your_bucket_name>/rawdata/i2b2_assertion_sample_test_data.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTishXbut1MS"
   },
   "source": [
    "## Graph setup and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hTawAHemzGn",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install -q tensorflow==2.11.0 #already downloaded when session created\n",
    "#!pip install -q tensorflow-addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ShZT8BBo4FY"
   },
   "source": [
    "We will use TFGraphBuilder annotator which can be used to create graphs in the model training pipeline.\n",
    "\n",
    "TFGraphBuilder inspects the data and creates the proper graph if a suitable version of TensorFlow (<= 2.7 ) is available. The graph is stored in the defined folder and loaded by the approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XhU0L1OXdaLN",
    "outputId": "2deb61a0-50ee-4fd9-bff1-c57d692eb215",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "graph_folder= \"./tf_graphs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "miNgoTjio0mL",
    "outputId": "63bf317d-987f-424a-89b6-01ee25c23db1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "assertion_graph_builder = medical.TFGraphBuilder()\\\n",
    "    .setModelName(\"assertion_dl\")\\\n",
    "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "    .setLabelColumn(\"label\")\\\n",
    "    .setGraphFolder(graph_folder)\\\n",
    "    .setGraphFile(\"assertion_graph.pb\")\\\n",
    "    .setMaxSequenceLength(250)\\\n",
    "    .setHiddenUnitsNumber(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zl1xBA65IZ2j",
    "outputId": "4586cfbc-aaf0-4827-a77e-f34a22e11ff4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n# ready to use tf_graph\\n\\n!mkdir training_logs\\n!mkdir assertion_tf_graph\\n\\n!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/tf_graphs/blstm_34_32_30_200_2.pb -P /content/assertion_tf_graph\\n'\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# ready to use tf_graph\n",
    "\n",
    "!mkdir training_logs\n",
    "!mkdir assertion_tf_graph\n",
    "\n",
    "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/tf_graphs/blstm_34_32_30_200_2.pb -P /content/assertion_tf_graph\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNzTsoSeXezx",
    "outputId": "d6985f81-46d7-462a-f2b5-3966e1227993",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\n# create custom graph\\n\\nmedical.tf_graph.print_model_params(\"assertion_dl\")\\n\\nfeat_size = 200\\nn_classes = 6\\n\\nmedical.tf_graph.build(\"assertion_dl\",\\n                        build_params={\"n_classes\": n_classes},\\n                        model_location= \"./tf_graphs\",\\n                        model_filename=\"blstm_34_32_30_{}_{}.pb\".format(feat_size, n_classes))\\n'\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# create custom graph\n",
    "\n",
    "medical.tf_graph.print_model_params(\"assertion_dl\")\n",
    "\n",
    "feat_size = 200\n",
    "n_classes = 6\n",
    "\n",
    "medical.tf_graph.build(\"assertion_dl\",\n",
    "                        build_params={\"n_classes\": n_classes},\n",
    "                        model_location= \"./tf_graphs\",\n",
    "                        model_filename=\"blstm_34_32_30_{}_{}.pb\".format(feat_size, n_classes))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6D0Ng7nMUjJa"
   },
   "source": [
    "**Setting the Scope Window (Target Area) Dynamically in Assertion Status Detection Models**\n",
    "\n",
    "\n",
    "This parameter allows you to train the Assertion Status Models to focus on specific context windows when resolving the status of a NER chunk. The window is in format `[X,Y]` being `X` the number of tokens to consider on the left of the chunk, and `Y` the max number of tokens to consider on the right. Letâ€™s take a look at what different windows mean:\n",
    "\n",
    "\n",
    "*   By default, the window is `[-1,-1]` which means that the Assertion Status will look at all of the tokens in the sentence/document (up to a maximum of tokens set in `setMaxSentLen()` ).\n",
    "*   `[0,0]` means â€œdonâ€™t pay attention to any token except the ner_chunkâ€, what basically is not considering any context for the Assertion resolution.\n",
    "*   `[9,15]` is what empirically seems to be the best baseline, meaning that we look up to 9 tokens on the left and 15 on the right of the ner chunk to understand the context and resolve the status.\n",
    "\n",
    "\n",
    "Check this [Scope Window Tuning Assertion Status Detection notebook](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings_JSL/Healthcare/2.1.Scope_window_tuning_assertion_status_detection.ipynb)  that illustrates the effect of the different windows and how to properly fine-tune your AssertionDLModels to get the best of them.\n",
    "\n",
    "In our case, the best Scope Window is around [10,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qG2o6Yq2xk4N",
    "outputId": "f63a6828-fae0-499e-b54a-437290ba1a6b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\nIf .setTestDataset parameter is employed, raw test data cannot be fitted. .setTestDataset only works for dataframes which are correctly transformed\\nby a pipeline consisting of document, chunk, embeddings stages.\\n'\n"
     ]
    }
   ],
   "source": [
    "scope_window = [10,10]\n",
    "\n",
    "assertionStatus = medical.AssertionDLApproach()\\\n",
    "    .setLabelCol(\"label\")\\\n",
    "    .setInputCols(\"document\", \"chunk\", \"embeddings\")\\\n",
    "    .setOutputCol(\"assertion\")\\\n",
    "    .setBatchSize(64)\\\n",
    "    .setDropout(0.1)\\\n",
    "    .setLearningRate(0.001)\\\n",
    "    .setEpochs(20)\\\n",
    "    .setValidationSplit(0.2)\\\n",
    "    .setStartCol(\"start\")\\\n",
    "    .setEndCol(\"end\")\\\n",
    "    .setMaxSentLen(250)\\\n",
    "    .setIncludeConfidence(True)\\\n",
    "    .setEnableOutputLogs(True)\\\n",
    "    .setOutputLogsPath('training_logs/')\\\n",
    "    .setGraphFolder(graph_folder)\\\n",
    "    .setGraphFile(f\"{graph_folder}/assertion_graph.pb\")\\\n",
    "    .setScopeWindow(scope_window)\\\n",
    "    .setTestDataset(path='s3://<your_bucket_name>/rawdata/i2b2_assertion_sample_test_data.parquet')\n",
    "\n",
    "'''\n",
    "If .setTestDataset parameter is employed, raw test data cannot be fitted. .setTestDataset only works for dataframes which are correctly transformed\n",
    "by a pipeline consisting of document, chunk, embeddings stages.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97ATs6a41jdE",
    "outputId": "f9f6dacd-ce03-44ce-dbb9-7b8ec71d8c01",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\nassertionStatus = medical.AssertionLogRegApproach()    .setLabelCol(\"label\")    .setInputCols(\"document\", \"chunk\", \"embeddings\")    .setOutputCol(\"assertion\")    .setMaxIter(100) # default: 26\\n'\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "assertionStatus = medical.AssertionLogRegApproach()\\\n",
    "    .setLabelCol(\"label\")\\\n",
    "    .setInputCols(\"document\", \"chunk\", \"embeddings\")\\\n",
    "    .setOutputCol(\"assertion\")\\\n",
    "    .setMaxIter(100) # default: 26\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKRL7Oy4MC1C",
    "outputId": "7d859233-1a81-49a4-b66b-6dced228b23b",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clinical_assertion_pipeline = nlp.Pipeline(\n",
    "    stages = [\n",
    "    document,\n",
    "    chunk,\n",
    "    token,\n",
    "    embeddings,\n",
    "    assertion_graph_builder,\n",
    "    assertionStatus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ei54XasnMU0U",
    "outputId": "52c876d7-f9a5-43b0-b449-233b1c2210d6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Graph Builder configuration:\n",
      "Model name: assertion_dl\n",
      "Graph folder: ./tf_graphs\n",
      "Graph file name: assertion_graph.pb\n",
      "Build params: {'n_classes': 2, 'feat_size': 200, 'max_seq_len': 250, 'n_hidden': 25}\n",
      "assertion_dl graph exported to ./tf_graphs/assertion_graph.pb\n"
     ]
    }
   ],
   "source": [
    "assertion_model = clinical_assertion_pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ShnodcE5qj1"
   },
   "source": [
    "## Checking the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30SmcTiSpnWa"
   },
   "source": [
    "Checking the results saved in the log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOiu1vuspKut",
    "outputId": "23d2f7ff-e2e7-4a50-8d11-677b07926b31",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AssertionDLApproach_13305a7d8606.log']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "log_files = os.listdir(\"./training_logs\")\n",
    "log_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CcQV0-fIrJHz",
    "outputId": "c4f13e03-1151-4f66-e335-dd4a14c16d9e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name of the selected graph: ./tf_graphs/assertion_graph.pb\n",
      "Training started, trainExamples: 721\n",
      "\n",
      "\n",
      "Epoch: 0 started, learning rate: 0.001, dataset size: 577\n",
      "Done, 4.610430738 total training loss: 6.2164288, avg training loss: 0.6216429, batches: 10\n",
      "Quality on validation dataset (20.0%), validation examples = 144\n",
      "time to finish evaluation: 0.86s\n",
      "Total validation loss: 1.6449\tAvg validation loss: 0.5483\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 104\t 40\t 0\t 0.7222222\t 1.0\t 0.83870965\n",
      "absent\t 0\t 0\t 40\t 0.0\t 0.0\t 0.0\n",
      "tp: 104 fp: 40 fn: 40 labels: 2\n",
      "Macro-average\t prec: 0.3611111, rec: 0.5, f1: 0.41935483\n",
      "Micro-average\t prec: 0.7222222, rec: 0.7222222, f1: 0.72222215\n",
      "\n",
      "\n",
      "Quality on test dataset: \n",
      "time to finish evaluation: 1.05s\n",
      "Total test loss: 1.9666\tAvg test loss: 0.6555\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 117\t 53\t 0\t 0.6882353\t 1.0\t 0.815331\n",
      "absent\t 0\t 0\t 53\t 0.0\t 0.0\t 0.0\n",
      "tp: 117 fp: 53 fn: 53 labels: 2\n",
      "Macro-average\t prec: 0.34411764, rec: 0.5, f1: 0.4076655\n",
      "Micro-average\t prec: 0.6882353, rec: 0.6882353, f1: 0.6882353\n",
      "\n",
      "\n",
      "Epoch: 1 started, learning rate: 9.5E-4, dataset size: 577\n",
      "Done, 5.720742854 total training loss: 4.8771296, avg training loss: 0.48771295, batches: 10\n",
      "Quality on validation dataset (20.0%), validation examples = 144\n",
      "time to finish evaluation: 0.57s\n",
      "Total validation loss: 1.5360\tAvg validation loss: 0.5120\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 104\t 40\t 0\t 0.7222222\t 1.0\t 0.83870965\n",
      "absent\t 0\t 0\t 40\t 0.0\t 0.0\t 0.0\n",
      "tp: 104 fp: 40 fn: 40 labels: 2\n",
      "Macro-average\t prec: 0.3611111, rec: 0.5, f1: 0.41935483\n",
      "Micro-average\t prec: 0.7222222, rec: 0.7222222, f1: 0.72222215\n",
      "\n",
      "\n",
      "Quality on test dataset: \n",
      "time to finish evaluation: 0.63s\n",
      "Total test loss: 1.7451\tAvg test loss: 0.5817\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 117\t 53\t 0\t 0.6882353\t 1.0\t 0.815331\n",
      "absent\t 0\t 0\t 53\t 0.0\t 0.0\t 0.0\n",
      "tp: 117 fp: 53 fn: 53 labels: 2\n",
      "Macro-average\t prec: 0.34411764, rec: 0.5, f1: 0.4076655\n",
      "Micro-average\t prec: 0.6882353, rec: 0.6882353, f1: 0.6882353\n",
      "\n",
      "\n",
      "Epoch: 2 started, learning rate: 9.025E-4, dataset size: 577\n",
      "Done, 3.036745076 total training loss: 4.4889884, avg training loss: 0.44889885, batches: 10\n",
      "Quality on validation dataset (20.0%), validation examples = 144\n",
      "time to finish evaluation: 0.54s\n",
      "Total validation loss: 1.5001\tAvg validation loss: 0.5000\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 104\t 40\t 0\t 0.7222222\t 1.0\t 0.83870965\n",
      "absent\t 0\t 0\t 40\t 0.0\t 0.0\t 0.0\n",
      "tp: 104 fp: 40 fn: 40 labels: 2\n",
      "Macro-average\t prec: 0.3611111, rec: 0.5, f1: 0.41935483\n",
      "Micro-average\t prec: 0.7222222, rec: 0.7222222, f1: 0.72222215\n",
      "\n",
      "\n",
      "Quality on test dataset: \n",
      "time to finish evaluation: 0.63s\n",
      "Total test loss: 1.7128\tAvg test loss: 0.5709\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 117\t 53\t 0\t 0.6882353\t 1.0\t 0.815331\n",
      "absent\t 0\t 0\t 53\t 0.0\t 0.0\t 0.0\n",
      "tp: 117 fp: 53 fn: 53 labels: 2\n",
      "Macro-average\t prec: 0.34411764, rec: 0.5, f1: 0.4076655\n",
      "Micro-average\t prec: 0.6882353, rec: 0.6882353, f1: 0.6882353\n",
      "\n",
      "\n",
      "Epoch: 3 started, learning rate: 8.57375E-4, dataset size: 577\n",
      "Done, 2.96049436 total training loss: 4.319999, avg training loss: 0.43199992, batches: 10\n",
      "Quality on validation dataset (20.0%), validation examples = 144\n",
      "time to finish evaluation: 0.54s\n",
      "Total validation loss: 1.2704\tAvg validation loss: 0.4235\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 102\t 32\t 2\t 0.76119405\t 0.9807692\t 0.85714287\n",
      "absent\t 8\t 2\t 32\t 0.8\t 0.2\t 0.32000002\n",
      "tp: 110 fp: 34 fn: 34 labels: 2\n",
      "Macro-average\t prec: 0.78059703, rec: 0.5903846, f1: 0.6722956\n",
      "Micro-average\t prec: 0.7638889, rec: 0.7638889, f1: 0.7638889\n",
      "\n",
      "\n",
      "Quality on test dataset: \n",
      "time to finish evaluation: 0.64s\n",
      "Total test loss: 1.3491\tAvg test loss: 0.4497\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 117\t 39\t 0\t 0.75\t 1.0\t 0.85714287\n",
      "absent\t 14\t 0\t 39\t 1.0\t 0.26415095\t 0.41791046\n",
      "tp: 131 fp: 39 fn: 39 labels: 2\n",
      "Macro-average\t prec: 0.875, rec: 0.6320755, f1: 0.7339593\n",
      "Micro-average\t prec: 0.7705882, rec: 0.7705882, f1: 0.7705882\n",
      "\n",
      "\n",
      "Epoch: 4 started, learning rate: 8.145062E-4, dataset size: 577\n",
      "Done, 2.954336554 total training loss: 3.346188, avg training loss: 0.3346188, batches: 10\n",
      "Quality on validation dataset (20.0%), validation examples = 144\n",
      "time to finish evaluation: 0.55s\n",
      "Total validation loss: 1.1293\tAvg validation loss: 0.3764\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 94\t 10\t 10\t 0.90384614\t 0.90384614\t 0.90384614\n",
      "absent\t 30\t 10\t 10\t 0.75\t 0.75\t 0.75\n",
      "tp: 124 fp: 20 fn: 20 labels: 2\n",
      "Macro-average\t prec: 0.8269231, rec: 0.8269231, f1: 0.8269231\n",
      "Micro-average\t prec: 0.8611111, rec: 0.8611111, f1: 0.8611111\n",
      "\n",
      "\n",
      "Quality on test dataset: \n",
      "time to finish evaluation: 0.65s\n",
      "Total test loss: 1.1647\tAvg test loss: 0.3882\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 103\t 13\t 14\t 0.88793105\t 0.8803419\t 0.8841202\n",
      "absent\t 40\t 14\t 13\t 0.7407407\t 0.754717\t 0.74766356\n",
      "tp: 143 fp: 27 fn: 27 labels: 2\n",
      "Macro-average\t prec: 0.8143359, rec: 0.81752944, f1: 0.81592953\n",
      "Micro-average\t prec: 0.84117645, rec: 0.84117645, f1: 0.84117645\n",
      "\n",
      "\n",
      "Epoch: 5 started, learning rate: 7.7378086E-4, dataset size: 577\n",
      "Done, 2.986013264 total training loss: 3.2476575, avg training loss: 0.32476574, batches: 10\n",
      "Quality on validation dataset (20.0%), validation examples = 144\n",
      "time to finish evaluation: 0.54s\n",
      "Total validation loss: 1.1162\tAvg validation loss: 0.3721\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 101\t 19\t 3\t 0.84166664\t 0.97115386\t 0.90178573\n",
      "absent\t 21\t 3\t 19\t 0.875\t 0.525\t 0.65624994\n",
      "tp: 122 fp: 22 fn: 22 labels: 2\n",
      "Macro-average\t prec: 0.85833335, rec: 0.7480769, f1: 0.7994214\n",
      "Micro-average\t prec: 0.8472222, rec: 0.8472222, f1: 0.8472222\n",
      "\n",
      "\n",
      "Quality on test dataset: \n",
      "time to finish evaluation: 0.63s\n",
      "Total test loss: 1.0027\tAvg test loss: 0.3342\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 113\t 25\t 4\t 0.81884056\t 0.96581197\t 0.8862745\n",
      "absent\t 28\t 4\t 25\t 0.875\t 0.5283019\t 0.6588235\n",
      "tp: 141 fp: 29 fn: 29 labels: 2\n",
      "Macro-average\t prec: 0.84692025, rec: 0.74705696, f1: 0.7938603\n",
      "Micro-average\t prec: 0.82941175, rec: 0.82941175, f1: 0.82941175\n",
      "\n",
      "\n",
      "Epoch: 6 started, learning rate: 7.350918E-4, dataset size: 577\n",
      "Done, 3.023918413 total training loss: 2.6165404, avg training loss: 0.26165405, batches: 10\n",
      "Quality on validation dataset (20.0%), validation examples = 144\n",
      "time to finish evaluation: 0.54s\n",
      "Total validation loss: 1.2505\tAvg validation loss: 0.4168\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 101\t 20\t 3\t 0.8347107\t 0.97115386\t 0.89777774\n",
      "absent\t 20\t 3\t 20\t 0.8695652\t 0.5\t 0.6349206\n",
      "tp: 121 fp: 23 fn: 23 labels: 2\n",
      "Macro-average\t prec: 0.8521379, rec: 0.7355769, f1: 0.7895788\n",
      "Micro-average\t prec: 0.8402778, rec: 0.8402778, f1: 0.8402778\n",
      "\n",
      "\n",
      "Quality on test dataset: \n",
      "time to finish evaluation: 0.64s\n",
      "Total test loss: 1.0423\tAvg test loss: 0.3474\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 115\t 27\t 2\t 0.80985916\t 0.982906\t 0.8880308\n",
      "absent\t 26\t 2\t 27\t 0.9285714\t 0.49056605\t 0.6419753\n",
      "tp: 141 fp: 29 fn: 29 labels: 2\n",
      "Macro-average\t prec: 0.86921525, rec: 0.736736, f1: 0.79751134\n",
      "Micro-average\t prec: 0.82941175, rec: 0.82941175, f1: 0.82941175\n",
      "\n",
      "\n",
      "Epoch: 7 started, learning rate: 6.983372E-4, dataset size: 577\n",
      "Done, 2.939921848 total training loss: 2.2330427, avg training loss: 0.22330427, batches: 10\n",
      "Quality on validation dataset (20.0%), validation examples = 144\n",
      "time to finish evaluation: 0.54s\n",
      "Total validation loss: 1.0588\tAvg validation loss: 0.3529\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 101\t 15\t 3\t 0.87068963\t 0.97115386\t 0.91818184\n",
      "absent\t 25\t 3\t 15\t 0.89285713\t 0.625\t 0.73529416\n",
      "tp: 126 fp: 18 fn: 18 labels: 2\n",
      "Macro-average\t prec: 0.88177335, rec: 0.7980769, f1: 0.8378401\n",
      "Micro-average\t prec: 0.875, rec: 0.875, f1: 0.875\n",
      "\n",
      "\n",
      "Quality on test dataset: \n",
      "time to finish evaluation: 0.63s\n",
      "Total test loss: 0.8504\tAvg test loss: 0.2835\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 111\t 16\t 6\t 0.87401575\t 0.94871795\t 0.9098361\n",
      "absent\t 37\t 6\t 16\t 0.8604651\t 0.6981132\t 0.7708334\n",
      "tp: 148 fp: 22 fn: 22 labels: 2\n",
      "Macro-average\t prec: 0.8672404, rec: 0.8234156, f1: 0.84476006\n",
      "Micro-average\t prec: 0.87058824, rec: 0.87058824, f1: 0.87058824\n",
      "\n",
      "\n",
      "Epoch: 8 started, learning rate: 6.6342036E-4, dataset size: 577\n",
      "Done, 2.978076644 total training loss: 2.0336714, avg training loss: 0.20336714, batches: 10\n",
      "Quality on validation dataset (20.0%), validation examples = 144\n",
      "time to finish evaluation: 0.54s\n",
      "Total validation loss: 1.0929\tAvg validation loss: 0.3643\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 101\t 14\t 3\t 0.87826085\t 0.97115386\t 0.9223744\n",
      "absent\t 26\t 3\t 14\t 0.8965517\t 0.65\t 0.7536232\n",
      "tp: 127 fp: 17 fn: 17 labels: 2\n",
      "Macro-average\t prec: 0.8874063, rec: 0.8105769, f1: 0.84725344\n",
      "Micro-average\t prec: 0.8819444, rec: 0.8819444, f1: 0.8819444\n",
      "\n",
      "\n",
      "Quality on test dataset: \n",
      "time to finish evaluation: 0.64s\n",
      "Total test loss: 0.8415\tAvg test loss: 0.2805\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 112\t 17\t 5\t 0.86821705\t 0.95726496\t 0.9105691\n",
      "absent\t 36\t 5\t 17\t 0.8780488\t 0.6792453\t 0.7659574\n",
      "tp: 148 fp: 22 fn: 22 labels: 2\n",
      "Macro-average\t prec: 0.87313294, rec: 0.8182551, f1: 0.84480375\n",
      "Micro-average\t prec: 0.87058824, rec: 0.87058824, f1: 0.87058824\n",
      "\n",
      "\n",
      "Epoch: 9 started, learning rate: 6.302493E-4, dataset size: 577\n",
      "Done, 3.032619745 total training loss: 2.0385952, avg training loss: 0.20385952, batches: 10\n",
      "Quality on validation dataset (20.0%), validation examples = 144\n",
      "time to finish evaluation: 0.54s\n",
      "Total validation loss: 1.2721\tAvg validation loss: 0.4240\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 101\t 16\t 3\t 0.8632479\t 0.97115386\t 0.91402715\n",
      "absent\t 24\t 3\t 16\t 0.8888889\t 0.6\t 0.7164179\n",
      "tp: 125 fp: 19 fn: 19 labels: 2\n",
      "Macro-average\t prec: 0.87606835, rec: 0.78557694, f1: 0.8283586\n",
      "Micro-average\t prec: 0.8680556, rec: 0.8680556, f1: 0.8680556\n",
      "\n",
      "\n",
      "Quality on test dataset: \n",
      "time to finish evaluation: 0.64s\n",
      "Total test loss: 0.9530\tAvg test loss: 0.3177\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 113\t 17\t 4\t 0.86923075\t 0.96581197\t 0.91497976\n",
      "absent\t 36\t 4\t 17\t 0.9\t 0.6792453\t 0.7741935\n",
      "tp: 149 fp: 21 fn: 21 labels: 2\n",
      "Macro-average\t prec: 0.88461536, rec: 0.8225286, f1: 0.8524429\n",
      "Micro-average\t prec: 0.87647057, rec: 0.87647057, f1: 0.87647057\n",
      "\n",
      "\n",
      "Epoch: 10 started, learning rate: 5.9873686E-4, dataset size: 577\n",
      "Done, 2.940247496 total training loss: 2.3308437, avg training loss: 0.23308437, batches: 10\n",
      "Quality on validation dataset (20.0%), validation examples = 144\n",
      "time to finish evaluation: 0.54s\n",
      "Total validation loss: 1.3643\tAvg validation loss: 0.4548\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 101\t 18\t 3\t 0.8487395\t 0.97115386\t 0.9058296\n",
      "absent\t 22\t 3\t 18\t 0.88\t 0.55\t 0.67692304\n",
      "tp: 123 fp: 21 fn: 21 labels: 2\n",
      "Macro-average\t prec: 0.86436975, rec: 0.76057696, f1: 0.8091585\n",
      "Micro-average\t prec: 0.8541667, rec: 0.8541667, f1: 0.8541667\n",
      "\n",
      "\n",
      "Quality on test dataset: \n",
      "time to finish evaluation: 0.63s\n",
      "Total test loss: 1.0147\tAvg test loss: 0.3382\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 114\t 17\t 3\t 0.870229\t 0.974359\t 0.91935486\n",
      "absent\t 36\t 3\t 17\t 0.9230769\t 0.6792453\t 0.78260875\n",
      "tp: 150 fp: 20 fn: 20 labels: 2\n",
      "Macro-average\t prec: 0.89665294, rec: 0.82680213, f1: 0.86031204\n",
      "Micro-average\t prec: 0.88235295, rec: 0.88235295, f1: 0.88235295\n",
      "\n",
      "\n",
      "Epoch: 11 started, learning rate: 5.688E-4, dataset size: 577\n",
      "Done, 2.941954004 total training loss: 3.0596147, avg training loss: 0.30596146, batches: 10\n",
      "Quality on validation dataset (20.0%), validation examples = 144\n",
      "time to finish evaluation: 0.54s\n",
      "Total validation loss: 1.3156\tAvg validation loss: 0.4385\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 101\t 19\t 3\t 0.84166664\t 0.97115386\t 0.90178573\n",
      "absent\t 21\t 3\t 19\t 0.875\t 0.525\t 0.65624994\n",
      "tp: 122 fp: 22 fn: 22 labels: 2\n",
      "Macro-average\t prec: 0.85833335, rec: 0.7480769, f1: 0.7994214\n",
      "Micro-average\t prec: 0.8472222, rec: 0.8472222, f1: 0.8472222\n",
      "\n",
      "\n",
      "Quality on test dataset: \n",
      "time to finish evaluation: 0.63s\n",
      "Total test loss: 0.9551\tAvg test loss: 0.3184\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 114\t 18\t 3\t 0.8636364\t 0.974359\t 0.9156627\n",
      "absent\t 35\t 3\t 18\t 0.92105263\t 0.6603774\t 0.76923084\n",
      "tp: 149 fp: 21 fn: 21 labels: 2\n",
      "Macro-average\t prec: 0.8923445, rec: 0.81736815, f1: 0.8532123\n",
      "Micro-average\t prec: 0.87647057, rec: 0.87647057, f1: 0.87647057\n",
      "\n",
      "\n",
      "Epoch: 12 started, learning rate: 5.4036E-4, dataset size: 577\n",
      "Done, 3.022722849 total training loss: 1.9865638, avg training loss: 0.19865638, batches: 10\n",
      "Quality on validation dataset (20.0%), validation examples = 144\n",
      "time to finish evaluation: 0.55s\n",
      "Total validation loss: 1.0674\tAvg validation loss: 0.3558\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 99\t 12\t 5\t 0.8918919\t 0.9519231\t 0.92093027\n",
      "absent\t 28\t 5\t 12\t 0.8484849\t 0.7\t 0.76712334\n",
      "tp: 127 fp: 17 fn: 17 labels: 2\n",
      "Macro-average\t prec: 0.87018836, rec: 0.82596153, f1: 0.8474984\n",
      "Micro-average\t prec: 0.8819444, rec: 0.8819444, f1: 0.8819444\n",
      "\n",
      "\n",
      "Quality on test dataset: \n",
      "time to finish evaluation: 0.64s\n",
      "Total test loss: 0.7978\tAvg test loss: 0.2659\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 111\t 12\t 6\t 0.902439\t 0.94871795\t 0.925\n",
      "absent\t 41\t 6\t 12\t 0.87234044\t 0.7735849\t 0.82000005\n",
      "tp: 152 fp: 18 fn: 18 labels: 2\n",
      "Macro-average\t prec: 0.8873897, rec: 0.86115146, f1: 0.87407374\n",
      "Micro-average\t prec: 0.89411765, rec: 0.89411765, f1: 0.89411765\n",
      "\n",
      "\n",
      "Epoch: 13 started, learning rate: 5.13342E-4, dataset size: 577\n",
      "Done, 3.017150225 total training loss: 1.8700416, avg training loss: 0.18700416, batches: 10\n",
      "Quality on validation dataset (20.0%), validation examples = 144\n",
      "time to finish evaluation: 0.54s\n",
      "Total validation loss: 1.2431\tAvg validation loss: 0.4144\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 101\t 18\t 3\t 0.8487395\t 0.97115386\t 0.9058296\n",
      "absent\t 22\t 3\t 18\t 0.88\t 0.55\t 0.67692304\n",
      "tp: 123 fp: 21 fn: 21 labels: 2\n",
      "Macro-average\t prec: 0.86436975, rec: 0.76057696, f1: 0.8091585\n",
      "Micro-average\t prec: 0.8541667, rec: 0.8541667, f1: 0.8541667\n",
      "\n",
      "\n",
      "Quality on test dataset: \n",
      "time to finish evaluation: 0.63s\n",
      "Total test loss: 0.9025\tAvg test loss: 0.3008\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 113\t 14\t 4\t 0.8897638\t 0.96581197\t 0.9262295\n",
      "absent\t 39\t 4\t 14\t 0.90697676\t 0.7358491\t 0.81250006\n",
      "tp: 152 fp: 18 fn: 18 labels: 2\n",
      "Macro-average\t prec: 0.89837027, rec: 0.85083055, f1: 0.87395436\n",
      "Micro-average\t prec: 0.89411765, rec: 0.89411765, f1: 0.89411765\n",
      "\n",
      "\n",
      "Epoch: 14 started, learning rate: 4.8767487E-4, dataset size: 577\n",
      "Done, 2.953127943 total training loss: 1.996188, avg training loss: 0.1996188, batches: 10\n",
      "Quality on validation dataset (20.0%), validation examples = 144\n",
      "time to finish evaluation: 0.53s\n",
      "Total validation loss: 1.1436\tAvg validation loss: 0.3812\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 101\t 12\t 3\t 0.8938053\t 0.97115386\t 0.9308756\n",
      "absent\t 28\t 3\t 12\t 0.9032258\t 0.7\t 0.7887324\n",
      "tp: 129 fp: 15 fn: 15 labels: 2\n",
      "Macro-average\t prec: 0.8985156, rec: 0.8355769, f1: 0.86590403\n",
      "Micro-average\t prec: 0.8958333, rec: 0.8958333, f1: 0.8958334\n",
      "\n",
      "\n",
      "Quality on test dataset: \n",
      "time to finish evaluation: 0.63s\n",
      "Total test loss: 0.8320\tAvg test loss: 0.2773\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 112\t 12\t 5\t 0.9032258\t 0.95726496\t 0.9294606\n",
      "absent\t 41\t 5\t 12\t 0.8913044\t 0.7735849\t 0.82828283\n",
      "tp: 153 fp: 17 fn: 17 labels: 2\n",
      "Macro-average\t prec: 0.8972651, rec: 0.86542493, f1: 0.8810574\n",
      "Micro-average\t prec: 0.9, rec: 0.9, f1: 0.9\n",
      "\n",
      "\n",
      "Epoch: 15 started, learning rate: 4.6329113E-4, dataset size: 577\n",
      "Done, 2.93931388 total training loss: 1.7086368, avg training loss: 0.17086367, batches: 10\n",
      "Quality on validation dataset (20.0%), validation examples = 144\n",
      "time to finish evaluation: 0.54s\n",
      "Total validation loss: 1.1755\tAvg validation loss: 0.3918\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 101\t 13\t 3\t 0.88596493\t 0.97115386\t 0.92660546\n",
      "absent\t 27\t 3\t 13\t 0.9\t 0.675\t 0.7714286\n",
      "tp: 128 fp: 16 fn: 16 labels: 2\n",
      "Macro-average\t prec: 0.8929825, rec: 0.82307696, f1: 0.85660595\n",
      "Micro-average\t prec: 0.8888889, rec: 0.8888889, f1: 0.8888889\n",
      "\n",
      "\n",
      "Quality on test dataset: \n",
      "time to finish evaluation: 0.63s\n",
      "Total test loss: 0.8529\tAvg test loss: 0.2843\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 112\t 12\t 5\t 0.9032258\t 0.95726496\t 0.9294606\n",
      "absent\t 41\t 5\t 12\t 0.8913044\t 0.7735849\t 0.82828283\n",
      "tp: 153 fp: 17 fn: 17 labels: 2\n",
      "Macro-average\t prec: 0.8972651, rec: 0.86542493, f1: 0.8810574\n",
      "Micro-average\t prec: 0.9, rec: 0.9, f1: 0.9\n",
      "\n",
      "\n",
      "Epoch: 16 started, learning rate: 4.4012658E-4, dataset size: 577\n",
      "Done, 2.949279233 total training loss: 1.7465057, avg training loss: 0.17465058, batches: 10\n",
      "Quality on validation dataset (20.0%), validation examples = 144\n",
      "time to finish evaluation: 0.54s\n",
      "Total validation loss: 1.1757\tAvg validation loss: 0.3919\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 101\t 13\t 3\t 0.88596493\t 0.97115386\t 0.92660546\n",
      "absent\t 27\t 3\t 13\t 0.9\t 0.675\t 0.7714286\n",
      "tp: 128 fp: 16 fn: 16 labels: 2\n",
      "Macro-average\t prec: 0.8929825, rec: 0.82307696, f1: 0.85660595\n",
      "Micro-average\t prec: 0.8888889, rec: 0.8888889, f1: 0.8888889\n",
      "\n",
      "\n",
      "Quality on test dataset: \n",
      "time to finish evaluation: 0.63s\n",
      "Total test loss: 0.8550\tAvg test loss: 0.2850\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 112\t 12\t 5\t 0.9032258\t 0.95726496\t 0.9294606\n",
      "absent\t 41\t 5\t 12\t 0.8913044\t 0.7735849\t 0.82828283\n",
      "tp: 153 fp: 17 fn: 17 labels: 2\n",
      "Macro-average\t prec: 0.8972651, rec: 0.86542493, f1: 0.8810574\n",
      "Micro-average\t prec: 0.9, rec: 0.9, f1: 0.9\n",
      "\n",
      "\n",
      "Epoch: 17 started, learning rate: 4.1812024E-4, dataset size: 577\n",
      "Done, 2.975452323 total training loss: 1.6626011, avg training loss: 0.16626011, batches: 10\n",
      "Quality on validation dataset (20.0%), validation examples = 144\n",
      "time to finish evaluation: 0.54s\n",
      "Total validation loss: 1.1511\tAvg validation loss: 0.3837\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 101\t 13\t 3\t 0.88596493\t 0.97115386\t 0.92660546\n",
      "absent\t 27\t 3\t 13\t 0.9\t 0.675\t 0.7714286\n",
      "tp: 128 fp: 16 fn: 16 labels: 2\n",
      "Macro-average\t prec: 0.8929825, rec: 0.82307696, f1: 0.85660595\n",
      "Micro-average\t prec: 0.8888889, rec: 0.8888889, f1: 0.8888889\n",
      "\n",
      "\n",
      "Quality on test dataset: \n",
      "time to finish evaluation: 0.64s\n",
      "Total test loss: 0.8417\tAvg test loss: 0.2806\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 111\t 12\t 6\t 0.902439\t 0.94871795\t 0.925\n",
      "absent\t 41\t 6\t 12\t 0.87234044\t 0.7735849\t 0.82000005\n",
      "tp: 152 fp: 18 fn: 18 labels: 2\n",
      "Macro-average\t prec: 0.8873897, rec: 0.86115146, f1: 0.87407374\n",
      "Micro-average\t prec: 0.89411765, rec: 0.89411765, f1: 0.89411765\n",
      "\n",
      "\n",
      "Epoch: 18 started, learning rate: 3.9721423E-4, dataset size: 577\n",
      "Done, 2.991545041 total training loss: 1.6717883, avg training loss: 0.16717884, batches: 10\n",
      "Quality on validation dataset (20.0%), validation examples = 144\n",
      "time to finish evaluation: 0.54s\n",
      "Total validation loss: 1.1860\tAvg validation loss: 0.3953\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 101\t 13\t 3\t 0.88596493\t 0.97115386\t 0.92660546\n",
      "absent\t 27\t 3\t 13\t 0.9\t 0.675\t 0.7714286\n",
      "tp: 128 fp: 16 fn: 16 labels: 2\n",
      "Macro-average\t prec: 0.8929825, rec: 0.82307696, f1: 0.85660595\n",
      "Micro-average\t prec: 0.8888889, rec: 0.8888889, f1: 0.8888889\n",
      "\n",
      "\n",
      "Quality on test dataset: \n",
      "time to finish evaluation: 0.63s\n",
      "Total test loss: 0.8675\tAvg test loss: 0.2892\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 112\t 14\t 5\t 0.8888889\t 0.95726496\t 0.9218107\n",
      "absent\t 39\t 5\t 14\t 0.8863636\t 0.7358491\t 0.80412376\n",
      "tp: 151 fp: 19 fn: 19 labels: 2\n",
      "Macro-average\t prec: 0.8876263, rec: 0.846557, f1: 0.86660534\n",
      "Micro-average\t prec: 0.8882353, rec: 0.8882353, f1: 0.8882353\n",
      "\n",
      "\n",
      "Epoch: 19 started, learning rate: 3.773535E-4, dataset size: 577\n",
      "Done, 3.026953675 total training loss: 1.5969208, avg training loss: 0.15969208, batches: 10\n",
      "Quality on validation dataset (20.0%), validation examples = 144\n",
      "time to finish evaluation: 0.54s\n",
      "Total validation loss: 1.1630\tAvg validation loss: 0.3877\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 101\t 13\t 3\t 0.88596493\t 0.97115386\t 0.92660546\n",
      "absent\t 27\t 3\t 13\t 0.9\t 0.675\t 0.7714286\n",
      "tp: 128 fp: 16 fn: 16 labels: 2\n",
      "Macro-average\t prec: 0.8929825, rec: 0.82307696, f1: 0.85660595\n",
      "Micro-average\t prec: 0.8888889, rec: 0.8888889, f1: 0.8888889\n",
      "\n",
      "\n",
      "Quality on test dataset: \n",
      "time to finish evaluation: 0.63s\n",
      "Total test loss: 0.8527\tAvg test loss: 0.2842\n",
      "label\t tp\t fp\t fn\t prec\t rec\t f1\n",
      "present\t 112\t 13\t 5\t 0.896\t 0.95726496\t 0.9256198\n",
      "absent\t 40\t 5\t 13\t 0.8888889\t 0.754717\t 0.81632656\n",
      "tp: 152 fp: 18 fn: 18 labels: 2\n",
      "Macro-average\t prec: 0.8924445, rec: 0.855991, f1: 0.87383777\n",
      "Micro-average\t prec: 0.89411765, rec: 0.89411765, f1: 0.89411765\n"
     ]
    }
   ],
   "source": [
    "with open(\"./training_logs/\"+log_files[0]) as log_file:\n",
    "    print(log_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-k2WrFkRyQyP",
    "outputId": "1306365c-e3a9-40be-d7af-14fd4918f0b0",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|  label|   result|\n",
      "+-------+---------+\n",
      "|present|[present]|\n",
      "| absent|[present]|\n",
      "|present|[present]|\n",
      "|present|[present]|\n",
      "|present|[present]|\n",
      "|present|[present]|\n",
      "|present|[present]|\n",
      "|present|[present]|\n",
      "|present|[present]|\n",
      "|present|[present]|\n",
      "|present|[present]|\n",
      "|present|[present]|\n",
      "|present|[present]|\n",
      "|present|[present]|\n",
      "|present|[present]|\n",
      "|present|[present]|\n",
      "|present|[present]|\n",
      "|present|[present]|\n",
      "|present|[present]|\n",
      "|present|[present]|\n",
      "+-------+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "preds = assertion_model.transform(test_data).select('label','assertion.result')\n",
    "\n",
    "preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yI73lwG2xk5",
    "outputId": "90b00873-5250-457d-e563-e1f6dae7fb48",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preds_df = preds.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "yRXZFGlQ3Z2U",
    "outputId": "3b77abc1-5168-481d-e55a-962b1602e772",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label   result\n",
      "0    present  present\n",
      "1     absent  present\n",
      "2    present  present\n",
      "3    present  present\n",
      "4    present  present\n",
      "..       ...      ...\n",
      "165  present  present\n",
      "166   absent   absent\n",
      "167   absent   absent\n",
      "168   absent   absent\n",
      "169  present  present\n",
      "\n",
      "[170 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "preds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5SxTKKRvdy5",
    "outputId": "1fd6aac9-e9ff-4e85-b579-7cec00bcc7e2",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      absent       0.89      0.75      0.82        53\n",
      "     present       0.90      0.96      0.93       117\n",
      "\n",
      "    accuracy                           0.89       170\n",
      "   macro avg       0.89      0.86      0.87       170\n",
      "weighted avg       0.89      0.89      0.89       170\n"
     ]
    }
   ],
   "source": [
    "# We are going to use sklearn to evalute the results on test dataset\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print (classification_report( preds_df['label'], preds_df['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atNaIlnP3gKr",
    "outputId": "b69137b7-037b-4857-bf62-8aecb317d7f8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "assertion_model.stages[-1].write().overwrite().save('s3://<your_bucket_name>/models/assertion_custom_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKjG43Id5vsa"
   },
   "source": [
    "## Load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OVb0JU3bEU19",
    "outputId": "93afa1d9-ef08-4333-8a3c-400ce5e65af1",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = nlp.DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "# Sentence Detector annotator, processes various sentences per line\n",
    "sentenceDetector = nlp.SentenceDetector()\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "# Tokenizer splits words in a relevant format for NLP\n",
    "tokenizer = nlp.Tokenizer()\\\n",
    "    .setInputCols([\"sentence\"])\\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "# Clinical word embeddings trained on PubMED dataset\n",
    "word_embeddings = nlp.WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols([\"sentence\", \"token\"])\\\n",
    "    .setOutputCol(\"embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5iqu0fwa2myW",
    "outputId": "48e12bf5-2393-43dd-ef1d-04b31cdf0856",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_clinical download started this may take some time.\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "clinical_ner = medical.NerModel.pretrained(\"ner_clinical\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"ner\")\n",
    "\n",
    "ner_converter = medical.NerConverterInternal() \\\n",
    "    .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
    "    .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "clinical_assertion = medical.AssertionDLModel.load(\"s3://<your_bucket_name>/models/assertion_custom_model\") \\\n",
    "    .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"assertion\")\n",
    "\n",
    "nlpPipeline = nlp.Pipeline(stages=[\n",
    "    documentAssembler,\n",
    "    sentenceDetector,\n",
    "    tokenizer,\n",
    "    word_embeddings,\n",
    "    clinical_ner,\n",
    "    ner_converter,\n",
    "    clinical_assertion\n",
    "    ])\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "model = nlpPipeline.fit(empty_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "id": "KKwLbbOJ2mv0",
    "outputId": "fd900ff8-fdc2-49a2-b6cf-dfacf5864aff",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient has a headache for the last 2 weeks, needs to get a head CT, and appears anxious when she walks fast. No alopecia noted. She denies pain\n",
      "       chunks entities assertion confidence\n",
      "0  a headache  PROBLEM   present     0.9988\n",
      "1   a head CT     TEST   present     0.9999\n",
      "2     anxious  PROBLEM   present     0.9734\n",
      "3    alopecia  PROBLEM    absent     0.7732\n",
      "4        pain  PROBLEM    absent     0.5431\n"
     ]
    }
   ],
   "source": [
    "text = 'Patient has a headache for the last 2 weeks, needs to get a head CT, and appears anxious when she walks fast. No alopecia noted. She denies pain'\n",
    "\n",
    "light_model = nlp.LightPipeline(model)\n",
    "\n",
    "light_result = light_model.fullAnnotate(text)[0]\n",
    "\n",
    "print(text)\n",
    "\n",
    "chunks=[]\n",
    "entities=[]\n",
    "status=[]\n",
    "confidence=[]\n",
    "\n",
    "for n,m in zip(light_result['ner_chunk'],light_result['assertion']):\n",
    "\n",
    "    chunks.append(n.result)\n",
    "    entities.append(n.metadata['entity'])\n",
    "    status.append(m.result)\n",
    "    confidence.append(m.metadata['confidence'])\n",
    "\n",
    "df = pd.DataFrame({'chunks':chunks, 'entities':entities, 'assertion':status, 'confidence':confidence})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PbYQWJ9efCO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
