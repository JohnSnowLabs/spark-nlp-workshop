{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLASSIFICATION_TR_CYBERBULLYING.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wcq6iO7eQXB9"
      },
      "source": [
        "\n",
        "\n",
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/SENTIMENT_EN_CYBERBULLYING.ipynb)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69nlbFfSQbtI"
      },
      "source": [
        "# **Detect bullying in tweets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At_1tTpNQ4xa"
      },
      "source": [
        "## 1. Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6o8-g0tEqNz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d986660-de36-4592-ef69-a7b7cf96b49d"
      },
      "source": [
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash\n",
        "# !bash colab.sh\n",
        "# -p is for pyspark\n",
        "# -s is for spark-nlp\n",
        "# !bash colab.sh -p 3.1.1 -s 3.0.1\n",
        "# by default they are set to the latest"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-13 11:47:48--  http://setup.johnsnowlabs.com/colab.sh\n",
            "Resolving setup.johnsnowlabs.com (setup.johnsnowlabs.com)... 51.158.130.125\n",
            "Connecting to setup.johnsnowlabs.com (setup.johnsnowlabs.com)|51.158.130.125|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh [following]\n",
            "--2021-08-13 11:47:48--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp/master/scripts/colab_setup.sh\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1608 (1.6K) [text/plain]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>]   1.57K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-08-13 11:47:49 (27.7 MB/s) - written to stdout [1608/1608]\n",
            "\n",
            "setup Colab for PySpark 3.1.2 and Spark NLP 3.2.1\n",
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,263 kB]\n",
            "Get:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [510 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,421 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]\n",
            "Get:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:19 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [66.2 kB]\n",
            "Ign:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [695 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,786 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,699 kB]\n",
            "Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [914 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,196 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [544 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [39.4 kB]\n",
            "Get:28 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [40.9 kB]\n",
            "Get:29 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [44.1 kB]\n",
            "Fetched 13.6 MB in 5s (2,616 kB/s)\n",
            "Reading package lists... Done\n",
            "\u001b[K     |████████████████████████████████| 212.4 MB 51 kB/s \n",
            "\u001b[K     |████████████████████████████████| 116 kB 10.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 198 kB 73.0 MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMmT9S6mE0ad"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohBO_O8IQ7Ib"
      },
      "source": [
        "## 2. Start Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zBXbY_vE2ss"
      },
      "source": [
        "spark = sparknlp.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdltrQa6Q98p"
      },
      "source": [
        "## 3. Select the DL model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XxHWemdE5hX"
      },
      "source": [
        "MODEL_NAME='classifierdl_berturk_cyberbullying'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNL-WnVoRCOr"
      },
      "source": [
        "## 4. Some sample examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ7GCD0pFDvP"
      },
      "source": [
        "text_list = [\"\"\"Gidişin olsun, dönüşün olmasın inşallah senin..\"\"\",\n",
        "         \"\"\"Gidişin ile dönüşün çok sürmez inşallah senin.\"\"\",\n",
        "         \"\"\"Geberesice sırtlan soyu seni.\"\"\",\n",
        "         \"\"\"Sırtlanların çölde geberdiğini görünce üzülen bir insandım.\"\"\",\n",
        "         \"\"\"Bu ne aptal bir adam böyle.\"\"\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKvXxQhsRFSh"
      },
      "source": [
        "## 5. Define Spark NLP pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiYxv0mOFIcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d7c656c-2d8a-4efa-fe62-fadd6118ba3d"
      },
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "      \n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"normalized\")\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "    .setInputCols(\"normalized\")\\\n",
        "    .setOutputCol(\"cleanTokens\")\\\n",
        "    .setCaseSensitive(False)\n",
        "\n",
        "lemma = LemmatizerModel.pretrained('lemma_antbnc') \\\n",
        "    .setInputCols([\"cleanTokens\"]) \\\n",
        "    .setOutputCol(\"lemma\")\n",
        "    \n",
        "berturk_embeddings = BertEmbeddings.pretrained(\"bert_base_turkish_uncased\", \"tr\") \\\n",
        "      .setInputCols(\"document\", \"lemma\") \\\n",
        "      .setOutputCol(\"embeddings\")\n",
        "\n",
        "embeddingsSentence = SentenceEmbeddings() \\\n",
        "      .setInputCols([\"document\", \"embeddings\"]) \\\n",
        "      .setOutputCol(\"sentence_embeddings\") \\\n",
        "      .setPoolingStrategy(\"AVERAGE\")\n",
        "\n",
        "document_classifier = ClassifierDLModel.pretrained(MODEL_NAME, 'tr') \\\n",
        "  .setInputCols([\"document\", \"sentence_embeddings\"]) \\\n",
        "  .setOutputCol(\"class\")\n",
        "\n",
        "nlpPipeline = Pipeline(\n",
        "      stages = [\n",
        "         documentAssembler, tokenizer, normalizer, stopwords_cleaner, lemma, berturk_embeddings, embeddingsSentence, document_classifier\n",
        "      ])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n",
            "bert_base_turkish_uncased download started this may take some time.\n",
            "Approximate size to download 395.8 MB\n",
            "[OK!]\n",
            "classifierdl_berturk_cyberbullying download started this may take some time.\n",
            "Approximate size to download 22.5 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHX_4kmDRIYG"
      },
      "source": [
        "## 6. Run the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu8-zrx8RP6h"
      },
      "source": [
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "pipelineModel = nlpPipeline.fit(empty_df)\n",
        "df = spark.createDataFrame(pd.DataFrame({\"text\":text_list}))\n",
        "result = pipelineModel.transform(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXmvL5VQcXii",
        "outputId": "204d8ba8-30b5-4f71-e2e4-f13d7244e378"
      },
      "source": [
        "result.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|               token|          normalized|         cleanTokens|               lemma|          embeddings| sentence_embeddings|               class|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|Gidişin olsun, dö...|[{document, 0, 46...|[{token, 0, 6, Gi...|[{token, 0, 6, Gi...|[{token, 0, 6, Gi...|[{token, 0, 6, Gi...|[{word_embeddings...|[{sentence_embedd...|[{category, 0, 46...|\n",
            "|Gidişin ile dönüş...|[{document, 0, 45...|[{token, 0, 6, Gi...|[{token, 0, 6, Gi...|[{token, 0, 6, Gi...|[{token, 0, 6, Gi...|[{word_embeddings...|[{sentence_embedd...|[{category, 0, 45...|\n",
            "|Geberesice sırtla...|[{document, 0, 28...|[{token, 0, 9, Ge...|[{token, 0, 9, Ge...|[{token, 0, 9, Ge...|[{token, 0, 9, Ge...|[{word_embeddings...|[{sentence_embedd...|[{category, 0, 28...|\n",
            "|Sırtlanların çöld...|[{document, 0, 58...|[{token, 0, 11, S...|[{token, 0, 11, S...|[{token, 0, 11, S...|[{token, 0, 11, S...|[{word_embeddings...|[{sentence_embedd...|[{category, 0, 58...|\n",
            "|Bu ne aptal bir a...|[{document, 0, 26...|[{token, 0, 1, Bu...|[{token, 0, 1, Bu...|[{token, 0, 1, Bu...|[{token, 0, 1, Bu...|[{word_embeddings...|[{sentence_embedd...|[{category, 0, 26...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJk3fJljc04x",
        "outputId": "60e9ffd8-f387-4fed-c5e6-f45ddac97094"
      },
      "source": [
        "result.select(\"class\").collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(class=[Row(annotatorType='category', begin=0, end=46, result='Pozitif', metadata={'sentence': '0', 'Negatif': '2.1142909E-5', 'Pozitif': '0.9999789'}, embeddings=[])]),\n",
              " Row(class=[Row(annotatorType='category', begin=0, end=45, result='Pozitif', metadata={'sentence': '0', 'Negatif': '1.9657963E-4', 'Pozitif': '0.9998035'}, embeddings=[])]),\n",
              " Row(class=[Row(annotatorType='category', begin=0, end=28, result='Negatif', metadata={'sentence': '0', 'Negatif': '0.9805547', 'Pozitif': '0.019445298'}, embeddings=[])]),\n",
              " Row(class=[Row(annotatorType='category', begin=0, end=58, result='Pozitif', metadata={'sentence': '0', 'Negatif': '1.4278975E-4', 'Pozitif': '0.9998572'}, embeddings=[])]),\n",
              " Row(class=[Row(annotatorType='category', begin=0, end=26, result='Negatif', metadata={'sentence': '0', 'Negatif': '0.9543243', 'Pozitif': '0.045675673'}, embeddings=[])])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huwbT786RKuY"
      },
      "source": [
        "## 7. Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZhzQGwjCySL",
        "outputId": "239e463b-c096-4120-9de2-e86d92b9569a"
      },
      "source": [
        "result.select(F.explode(F.arrays_zip('document.result', 'class.result')).alias(\"cols\")) \\\n",
        ".select(F.expr(\"cols['0']\").alias(\"document\"),\n",
        "        F.expr(\"cols['1']\").alias(\"class\")).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----------------------------------------------------------+-------+\n",
            "|document                                                   |class  |\n",
            "+-----------------------------------------------------------+-------+\n",
            "|Gidişin olsun, dönüşün olmasın inşallah senin..            |Pozitif|\n",
            "|Gidişin ile dönüşün çok sürmez inşallah senin.             |Pozitif|\n",
            "|Geberesice sırtlan soyu seni.                              |Negatif|\n",
            "|Sırtlanların çölde geberdiğini görünce üzülen bir insandım.|Pozitif|\n",
            "|Bu ne aptal bir adam böyle.                                |Negatif|\n",
            "+-----------------------------------------------------------+-------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o24k2CpLVwxE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}