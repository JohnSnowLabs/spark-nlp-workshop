{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Collecting gdown\n",
      "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting six\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting requests[socks]\n",
      "  Using cached requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "Processing /home/ubuntu/.cache/pip/wheels/66/13/60/ef107438d90e4aad6320e3424e50cfce5e16d1e9aad6d38294/filelock-3.0.12-cp27-none-any.whl\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.50.2-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 900 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Using cached certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<3,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6; extra == \"socks\"\n",
      "  Downloading PySocks-1.7.1-py27-none-any.whl (16 kB)\n",
      "Building wheels for collected packages: gdown\n",
      "  Building wheel for gdown (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gdown: filename=gdown-3.12.2-py2-none-any.whl size=9680 sha256=5cb027693aa397c43e653e072ac5ff445298617ca57047ca3e4a926ac4198d82\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/c6/e8/91/e2d1c052183f2182a8f616f0159efc116c79d85da87c8f661b\n",
      "Successfully built gdown\n",
      "Installing collected packages: six, certifi, urllib3, idna, chardet, PySocks, requests, filelock, tqdm, gdown\n",
      "Successfully installed PySocks-1.7.1 certifi-2020.6.20 chardet-3.0.4 filelock-3.0.12 gdown-3.12.2 idna-2.10 requests-2.24.0 six-1.15.0 tqdm-4.50.2 urllib3-1.25.10\n",
      "\u001b[33mWARNING: You are using pip version 20.2.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/home/ubuntu/jsl/jsl_env/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1uRgJ5MzqoGh-XYQUFAVBUGQYlLi7aMXx\n",
      "To: /home/ubuntu/jsl/streamlit-demo-apps/new_notebook/absa_sparknlp/public_app/notebooks/ABSA_glove_absa.zip\n",
      "83.9MB [00:06, 12.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown --id 1uRgJ5MzqoGh-XYQUFAVBUGQYlLi7aMXx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  ABSA_glove_absa.zip\n",
      "   creating: ABSA_glove_absa/\n",
      "  inflating: ABSA_glove_absa/.tensorflow.crc  \n",
      "   creating: ABSA_glove_absa/fields/\n",
      "   creating: ABSA_glove_absa/fields/datasetParams/\n",
      " extracting: ABSA_glove_absa/fields/datasetParams/_SUCCESS  \n",
      " extracting: ABSA_glove_absa/fields/datasetParams/.part-00000.crc  \n",
      "  inflating: ABSA_glove_absa/fields/datasetParams/part-00003  \n",
      "  inflating: ABSA_glove_absa/fields/datasetParams/part-00000  \n",
      " extracting: ABSA_glove_absa/fields/datasetParams/.part-00004.crc  \n",
      " extracting: ABSA_glove_absa/fields/datasetParams/.part-00007.crc  \n",
      "  inflating: ABSA_glove_absa/fields/datasetParams/part-00001  \n",
      "  inflating: ABSA_glove_absa/fields/datasetParams/part-00005  \n",
      " extracting: ABSA_glove_absa/fields/datasetParams/._SUCCESS.crc  \n",
      " extracting: ABSA_glove_absa/fields/datasetParams/.part-00003.crc  \n",
      " extracting: ABSA_glove_absa/fields/datasetParams/.part-00002.crc  \n",
      "  inflating: ABSA_glove_absa/fields/datasetParams/part-00007  \n",
      " extracting: ABSA_glove_absa/fields/datasetParams/.part-00001.crc  \n",
      " extracting: ABSA_glove_absa/fields/datasetParams/.part-00006.crc  \n",
      "  inflating: ABSA_glove_absa/fields/datasetParams/part-00004  \n",
      "  inflating: ABSA_glove_absa/fields/datasetParams/part-00002  \n",
      "  inflating: ABSA_glove_absa/fields/datasetParams/part-00006  \n",
      " extracting: ABSA_glove_absa/fields/datasetParams/.part-00005.crc  \n",
      "  inflating: ABSA_glove_absa/tensorflow  \n",
      "   creating: ABSA_glove_absa/metadata/\n",
      " extracting: ABSA_glove_absa/metadata/_SUCCESS  \n",
      " extracting: ABSA_glove_absa/metadata/.part-00000.crc  \n",
      "  inflating: ABSA_glove_absa/metadata/part-00000  \n",
      " extracting: ABSA_glove_absa/metadata/._SUCCESS.crc  \n"
     ]
    }
   ],
   "source": [
    "!unzip ABSA_glove_absa.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import json\n",
    "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ['PATH'] = os.environ['JAVA_HOME'] + \"/bin:\" + os.environ['PATH']\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "from sparknlp.training import CoNLL\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Start Spark session\n",
    "spark = sparknlp.start(gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove_840B_300 download started this may take some time.\n",
      "Approximate size to download 2.3 GB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence = SentenceDetector()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('sentence')\n",
    "\n",
    "token = Tokenizer()\\\n",
    "    .setInputCols(['document'])\\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "glove_embeddings = WordEmbeddingsModel.pretrained(\"glove_840B_300\", \"xx\")\\\n",
    "    .setInputCols([\"document\", \"token\"])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "    \n",
    "loaded_ner_model = NerDLModel.load(\"ABSA_glove_absa\")\\\n",
    "    .setInputCols([\"document\", \"token\", \"embeddings\"])\\\n",
    "    .setOutputCol(\"absa\")\n",
    "\n",
    "converter = NerConverter()\\\n",
    "    .setInputCols([\"document\", \"token\", \"absa\"])\\\n",
    "    .setOutputCol(\"absa_span\")\n",
    "\n",
    "ner_prediction_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        token,\n",
    "        glove_embeddings,\n",
    "        loaded_ner_model,\n",
    "        converter])\n",
    "\n",
    "empty_data = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "prediction_model = ner_prediction_pipeline.fit(empty_data)\n",
    "sent_pipeline = Pipeline(\n",
    "    stages = [document, sentence]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = os.listdir('../inputs/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_paths = [os.path.join('../inputs/', file) for file in input_files]\n",
    "output_paths = [os.path.join('../outputs/', file.replace('txt', 'csv')) for file in input_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../inputs/Example10.txt',\n",
       " '../inputs/Example4.txt',\n",
       " '../inputs/Example5.txt',\n",
       " '../inputs/Example3.txt',\n",
       " '../inputs/Example7.txt',\n",
       " '../inputs/Example9.txt',\n",
       " '../inputs/Example1.txt',\n",
       " '../inputs/Example8.txt',\n",
       " '../inputs/Example2.txt',\n",
       " '../inputs/Example6.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for in_path, out_path in zip(input_paths, output_paths):\n",
    "    text = open(in_path).read()\n",
    "    df = spark.createDataFrame(pd.DataFrame({'text': [text]}))\n",
    "    df1 = prediction_model.transform(df).toPandas()\n",
    "    df2 = sent_pipeline.fit(empty_data).transform(df).toPandas()\n",
    "\n",
    "    all_sents = df2['sentence'][0]\n",
    "\n",
    "    sentences = []\n",
    "    aspects = []\n",
    "    sentiments = []\n",
    "    for result in df1['absa_span'][0]:\n",
    "        start, end = result['begin'], result['end']\n",
    "        for sent in all_sents:\n",
    "            if sent['begin'] <= start and sent['end'] >= end:\n",
    "                sentences.append(sent['result'])\n",
    "        aspects.append(result['result'])\n",
    "        sentiment = \"positive\" if result['metadata']['entity'] == \"POS\" else \"negative\"\n",
    "        sentiments.append(sentiment)\n",
    "    final_result = pd.DataFrame.from_dict({\"sentence\": sentences, \"aspect\": aspects, \"sentiment\": sentiments})\n",
    "    final_result.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example1.csv   Example2.csv  Example4.csv  Example6.csv  Example8.csv\r\n",
      "Example10.csv  Example3.csv  Example5.csv  Example7.csv  Example9.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../outputs/Example10.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>aspect</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We loved our Thai-style main which amazing wit...</td>\n",
       "      <td>Thai-style main</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We loved our Thai-style main which amazing wit...</td>\n",
       "      <td>lots of flavours</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>But the service was below average and the chip...</td>\n",
       "      <td>service</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>But the service was below average and the chip...</td>\n",
       "      <td>chips</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence            aspect  \\\n",
       "0  We loved our Thai-style main which amazing wit...   Thai-style main   \n",
       "1  We loved our Thai-style main which amazing wit...  lots of flavours   \n",
       "2  But the service was below average and the chip...           service   \n",
       "3  But the service was below average and the chip...             chips   \n",
       "\n",
       "  sentiment  \n",
       "0  positive  \n",
       "1  positive  \n",
       "2  negative  \n",
       "3  negative  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
