{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TA21Jo5d9SVq"
   },
   "source": [
    "\n",
    "\n",
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/healthcare/CLINICAL_CLASSIFICATION.ipynb)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CzIdjHkAW8TB"
   },
   "source": [
    "# **How to use Licensed Classification models in Spark NLP**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RuZr5fPZ4Jwa"
   },
   "source": [
    "### Spark NLP documentation and instructions:\n",
    "https://nlp.johnsnowlabs.com/docs/en/quickstart\n",
    "\n",
    "### You can find details about Spark NLP annotators here:\n",
    "https://nlp.johnsnowlabs.com/docs/en/annotators\n",
    "\n",
    "### You can find details about Spark NLP models here:\n",
    "https://nlp.johnsnowlabs.com/models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uDmeHEFW7_h"
   },
   "source": [
    "To run this yourself, you will need to upload your license keys to the notebook. Otherwise, you can look at the example outputs at the bottom of the notebook. To upload license keys, open the file explorer on the left side of the screen and upload `workshop_license_keys.json` to the folder that opens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIeCOiJNW-88"
   },
   "source": [
    "## 1. Colab Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMIDv74CYN0d"
   },
   "source": [
    "Import license keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ttHPIV2JXbIM",
    "outputId": "004068ac-9191-4f95-9f02-1d600648efda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkNLP Version: 2.6.3\n",
      "SparkNLP-JSL Version: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "with open('/content/spark_nlp_for_healthcare.json', 'r') as f:\n",
    "    license_keys = json.load(f)\n",
    "\n",
    "license_keys.keys()\n",
    "\n",
    "secret = license_keys['SECRET']\n",
    "os.environ['SPARK_NLP_LICENSE'] = license_keys['SPARK_NLP_LICENSE']\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = license_keys['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = license_keys['AWS_SECRET_ACCESS_KEY']\n",
    "sparknlp_version = license_keys[\"PUBLIC_VERSION\"]\n",
    "jsl_version = license_keys[\"JSL_VERSION\"]\n",
    "\n",
    "print ('SparkNLP Version:', sparknlp_version)\n",
    "print ('SparkNLP-JSL Version:', jsl_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQtc1CHaYQjU"
   },
   "source": [
    "\n",
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGJktFHdHL1n",
    "outputId": "69c06067-3b73-4fe3-f024-0f608436ecb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"11.0.9\" 2020-10-20\n",
      "OpenJDK Runtime Environment (build 11.0.9+11-Ubuntu-0ubuntu1.18.04.1)\n",
      "OpenJDK 64-Bit Server VM (build 11.0.9+11-Ubuntu-0ubuntu1.18.04.1, mixed mode, sharing)\n",
      "\u001b[K     |████████████████████████████████| 215.7MB 58kB/s \n",
      "\u001b[K     |████████████████████████████████| 204kB 46.6MB/s \n",
      "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting spark-nlp==2.6.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/84/84/3f15673db521fbc4e8e0ec3677a019ba1458b2cb70f0f7738c221511ef32/spark_nlp-2.6.3-py2.py3-none-any.whl (129kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
      "\u001b[?25hInstalling collected packages: spark-nlp\n",
      "Successfully installed spark-nlp-2.6.3\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.johnsnowlabs.com/2.7.0-e49fd2919a73690cd04ed3b6e223e21330c5214b\n",
      "Collecting spark-nlp-jsl==2.7.0\n",
      "\u001b[?25l  Downloading https://pypi.johnsnowlabs.com/2.7.0-e49fd2919a73690cd04ed3b6e223e21330c5214b/spark-nlp-jsl/spark_nlp_jsl-2.7.0-py3-none-any.whl (44kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 372kB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: spark-nlp==2.6.3 in /usr/local/lib/python3.6/dist-packages (from spark-nlp-jsl==2.7.0) (2.6.3)\n",
      "Installing collected packages: spark-nlp-jsl\n",
      "Successfully installed spark-nlp-jsl-2.7.0\n"
     ]
    }
   ],
   "source": [
    "# Install Java\n",
    "! apt-get update -qq\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "! java -version\n",
    "\n",
    "# Install pyspark\n",
    "! pip install --ignore-installed -q pyspark==2.4.4\n",
    "\n",
    "# Install Spark NLP\n",
    "! pip install --ignore-installed spark-nlp==$sparknlp_version\n",
    "! python -m pip install --upgrade spark-nlp-jsl==$jsl_version --extra-index-url https://pypi.johnsnowlabs.com/$secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hj5FRDV4YSXN"
   },
   "source": [
    "Import dependencies into Python and start the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "qUWyj8c6JSPP",
    "outputId": "b83a58a9-c278-4cc3-92fb-92f702815231"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\nbuilder = SparkSession.builder     .appName('Spark NLP Licensed')     .master('local[*]')     .config('spark.driver.memory', '16G')     .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer')     .config('spark.kryoserializer.buffer.max', '2000M')     .config('spark.jars.packages', 'com.johnsnowlabs.nlp:spark-nlp_2.11:' +sparknlp.version())     .config('spark.jars', f'https://pypi.johnsnowlabs.com/{secret}/spark-nlp-jsl-{jsl_version}.jar')\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ['PATH'] = os.environ['JAVA_HOME'] + \"/bin:\" + os.environ['PATH']\n",
    "\n",
    "import pandas as pd\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl\n",
    "\n",
    "spark = sparknlp_jsl.start(secret)\n",
    "\n",
    "# manually start session\n",
    "'''\n",
    "builder = SparkSession.builder \\\n",
    "    .appName('Spark NLP Licensed') \\\n",
    "    .master('local[*]') \\\n",
    "    .config('spark.driver.memory', '16G') \\\n",
    "    .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer') \\\n",
    "    .config('spark.kryoserializer.buffer.max', '2000M') \\\n",
    "    .config('spark.jars.packages', 'com.johnsnowlabs.nlp:spark-nlp_2.11:' +sparknlp.version()) \\\n",
    "    .config('spark.jars', f'https://pypi.johnsnowlabs.com/{secret}/spark-nlp-jsl-{jsl_version}.jar')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RgiqfX5XDqb"
   },
   "source": [
    "## 2. Usage Guidelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVKr8C2SrkZQ"
   },
   "source": [
    "1. **Selecting the correct Classification Model**\n",
    "\n",
    "> a. To select from all the Classification models available in Spark NLP please go to https://nlp.johnsnowlabs.com/models\n",
    "\n",
    "> b. Read through the model descriptions to select desired model\n",
    "\n",
    "> c. Some of the available models:\n",
    ">> classifierdl_pico_biobert\n",
    "\n",
    ">> classifierdl_ade_biobert\n",
    "---\n",
    "2. **Selecting correct embeddings for the chosen model**\n",
    "\n",
    "> a. Models are trained on specific embeddings and same embeddings should be used at inference to get best results\n",
    "\n",
    "> b. If the name of the model contains \"**biobert**\" (e.g: *ner_anatomy_biobert*) then the model is trained using \"**biobert_pubmed_base_cased**\" embeddings. Otherwise, \"**embeddings_clinical**\" was used to train that model.\n",
    "\n",
    "> c. Using correct embeddings\n",
    "\n",
    ">> To use *embeddings_clinical* :\n",
    "\n",
    ">>> word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\n",
    "    .setInputCols([\"sentence\", \"token\"]) \\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    ">> To use *Bert* Embeddings:\n",
    "\n",
    ">>> embeddings = BertEmbeddings.pretrained('biobert_pubmed_base_cased')\\\n",
    "    .setInputCols([\"document\", 'token'])\\\n",
    "    .setOutputCol(\"word_embeddings\")\n",
    "> d. You can find list of all embeddings at https://nlp.johnsnowlabs.com/models?tag=embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zweiG2ilZqoR"
   },
   "source": [
    "Create the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LLuDz_t40be4",
    "outputId": "3f85f688-ae9b-423b-8db1-80d292a09a05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biobert_pubmed_base_cased download started this may take some time.\n",
      "Approximate size to download 386.4 MB\n",
      "[OK!]\n",
      "classifierdl_pico_biobert download started this may take some time.\n",
      "Approximate size to download 22.1 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = DocumentAssembler()\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"document\"]) \\\n",
    "  .setOutputCol(\"token\")\n",
    "\n",
    "embeddings = BertEmbeddings.pretrained('biobert_pubmed_base_cased')\\\n",
    "    .setInputCols([\"document\", 'token'])\\\n",
    "    .setOutputCol(\"word_embeddings\")\n",
    "\n",
    "sentence_embeddings = SentenceEmbeddings() \\\n",
    "      .setInputCols([\"document\", \"word_embeddings\"]) \\\n",
    "      .setOutputCol(\"sentence_embeddings\") \\\n",
    "      .setPoolingStrategy(\"AVERAGE\").setStorageRef('SentenceEmbeddings_5d018a59d7c3')\n",
    "\n",
    "åå\n",
    "classifier = ClassifierDLModel.pretrained('classifierdl_pico_biobert', 'en', 'clinical/models')\\\n",
    "    .setInputCols(['document', 'token', 'sentence_embeddings']).setOutputCol('class')\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler, \n",
    "    tokenizer,\n",
    "    embeddings,\n",
    "    sentence_embeddings, \n",
    "    classifier])\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "pipeline_model = pipeline.fit(empty_data)\n",
    "\n",
    "lmodel = LightPipeline(pipeline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Y9GpdJhXIpD"
   },
   "source": [
    "## 3. Create example inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "vBOKkB2THdGI"
   },
   "outputs": [],
   "source": [
    "# Enter examples as strings in this array\n",
    "input_list = [\n",
    "    \"\"\"A total of 10 adult daily smokers who reported at least one stressful event and coping episode and provided post-quit data.\"\"\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mv0abcwhXWC-"
   },
   "source": [
    "## 4. Use the pipeline to create outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27rHwCk4ODFr"
   },
   "source": [
    "Full Pipeline (Expects a Spark Data Frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "TK1DB9JZaPs3"
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pd.DataFrame({\"text\": input_list}))\n",
    "result = pipeline_model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFq4QRXjOEeG"
   },
   "source": [
    "Light Pipeline (Expects a list of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "NzFUrSmkOFfs"
   },
   "outputs": [],
   "source": [
    "lresult = lmodel.fullAnnotate(input_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQY8tAP6XZJL"
   },
   "source": [
    "## 5. Visualize results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0vhxZgvibTi3"
   },
   "source": [
    "Full Pipeline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "2EXCQGCMbTKT",
    "outputId": "74433ac8-a41e-4c71-fb74-8eee7e2732e5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'PARTICIPANTS'"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.toPandas()['class'].iloc[0][0].result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnsMLq9gctSq"
   },
   "source": [
    "Light Pipeline Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Ar32BZu7J79X",
    "outputId": "33c26ac9-fdb7-43eb-b57e-3f75e0fd7f74"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'PARTICIPANTS'"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lresult[0]['class'][0].result"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CLINICAL_CLASSIFICATION.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
