{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ner_HRL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/Ner_HRL.ipynb)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BmzCWrXddU3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NER Model for 10 Different Languages**\n"
      ],
      "metadata": {
        "id": "GtlWsaCuddX2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMURhBz4ZwM6"
      },
      "source": [
        "## **1. Colab Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qyYMEtv59sox"
      },
      "outputs": [],
      "source": [
        "# Install PySpark and Spark NLP\n",
        "! pip install -q pyspark==3.1.2 spark-nlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --ignore-installed spark-nlp-display"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "S0DgiewGdp9U",
        "outputId": "5c7480d5-9ea8-4cad-effa-1cb81d981d7e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spark-nlp-display\n",
            "  Using cached spark_nlp_display-1.8-py3-none-any.whl (95 kB)\n",
            "Collecting numpy\n",
            "  Using cached numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting ipython\n",
            "  Using cached ipython-7.31.1-py3-none-any.whl (792 kB)\n",
            "Collecting spark-nlp\n",
            "  Using cached spark_nlp-3.4.0-py2.py3-none-any.whl (140 kB)\n",
            "Collecting svgwrite==1.4\n",
            "  Using cached svgwrite-1.4-py3-none-any.whl (66 kB)\n",
            "Collecting pandas\n",
            "  Using cached pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "Collecting pygments\n",
            "  Using cached Pygments-2.11.2-py3-none-any.whl (1.1 MB)\n",
            "Collecting jedi>=0.16\n",
            "  Using cached jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Collecting backcall\n",
            "  Using cached backcall-0.2.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting setuptools>=18.5\n",
            "  Using cached setuptools-60.5.0-py3-none-any.whl (958 kB)\n",
            "Collecting decorator\n",
            "  Using cached decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Using cached prompt_toolkit-3.0.24-py3-none-any.whl (374 kB)\n",
            "Collecting pexpect>4.3\n",
            "  Using cached pexpect-4.8.0-py2.py3-none-any.whl (59 kB)\n",
            "Collecting traitlets>=4.2\n",
            "  Using cached traitlets-5.1.1-py3-none-any.whl (102 kB)\n",
            "Collecting matplotlib-inline\n",
            "  Using cached matplotlib_inline-0.1.3-py3-none-any.whl (8.2 kB)\n",
            "Collecting pickleshare\n",
            "  Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
            "Collecting parso<0.9.0,>=0.8.0\n",
            "  Using cached parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
            "Collecting ptyprocess>=0.5\n",
            "  Using cached ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting wcwidth\n",
            "  Using cached wcwidth-0.2.5-py2.py3-none-any.whl (30 kB)\n",
            "Collecting python-dateutil>=2.7.3\n",
            "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "Collecting pytz>=2017.3\n",
            "  Using cached pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
            "Collecting six>=1.5\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: wcwidth, traitlets, six, ptyprocess, parso, setuptools, pytz, python-dateutil, pygments, prompt-toolkit, pickleshare, pexpect, numpy, matplotlib-inline, jedi, decorator, backcall, svgwrite, spark-nlp, pandas, ipython, spark-nlp-display\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.5 which is incompatible.\n",
            "nbclient 0.5.10 requires jupyter-client>=6.1.5, but you have jupyter-client 5.3.5 which is incompatible.\n",
            "moviepy 0.2.3.5 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.24 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.31.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.5 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed backcall-0.2.0 decorator-5.1.1 ipython-7.31.1 jedi-0.18.1 matplotlib-inline-0.1.3 numpy-1.21.5 pandas-1.3.5 parso-0.8.3 pexpect-4.8.0 pickleshare-0.7.5 prompt-toolkit-3.0.24 ptyprocess-0.7.0 pygments-2.11.2 python-dateutil-2.8.2 pytz-2021.3 setuptools-60.5.0 six-1.16.0 spark-nlp-3.4.0 spark-nlp-display-1.8 svgwrite-1.4 traitlets-5.1.1 wcwidth-0.2.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "_distutils_hack",
                  "dateutil",
                  "decorator",
                  "jedi",
                  "matplotlib_inline",
                  "numpy",
                  "pandas",
                  "parso",
                  "pexpect",
                  "pickleshare",
                  "pkg_resources",
                  "prompt_toolkit",
                  "pygments",
                  "pytz",
                  "setuptools",
                  "six",
                  "traitlets",
                  "wcwidth"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o78aqL2VTOFU"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Start Spark Session**"
      ],
      "metadata": {
        "id": "HoF0DUrkdyJZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = sparknlp.start()"
      ],
      "metadata": {
        "id": "u3-L6X0gdqCc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Sample Examples for all of the 10 Different languages**"
      ],
      "metadata": {
        "id": "uINFwZwhd03P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_list_english = [\"\"\"Jerome Horsey was a resident of the Russia Company in Moscow from 1572 to 1585.\"\"\",\"\"\"The 1906 San Francisco earthquake struck the coast of Northern California.\"\"\",\"\"\"Jan Verhaas is a Dutch snooker and pool referee. He was born in Maassluis, and now lives in Brielle.\"\"\",\"\"\"Ethiopian historians who married Rita Pankhurst in Addis Ababa have been married for more than a year.\"\"\"]"
      ],
      "metadata": {
        "id": "5vPR4IJgdqFD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list_arabic = [\"\"\"يمكنكم مشاهدة أمير منطقة الرياض الأمير فيصل بن بندر بن عبد العزيز في كل مناسبة وافتتاح تتعلق بمشاريع التعليم والصحة وخدمة الطرق والمشاريع الثقافية في منطقة الرياض.\"\"\",\"\"\"خريطة العالم بِيد أمير البحار العُثماني حاجي أحمد مُحيي الدين پیري، الشهير باسم پيري ريِّس، رُسمت سنة 1513م.\"\"\",\"\"\" ويحتفل بحياة وإنجازات مارتن لوثر كنغ، وهو زعيم بارز في الحقوق المدنية الأمريكية والأكثر شهرة بحملاته لإنهاء التمييز العنصري في وسائل النقل العامة والمساواة العرقية في الولايات المتحدة.\"\"\"]"
      ],
      "metadata": {
        "id": "hshU7h4yqtgr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list_german = [\"\"\"Die Mona Lisa ist ein Ölgemälde aus dem 16. Jahrhundert, das von Leonardo geschaffen wurde. Es findet im Louvre in Paris statt.\"\"\",\"\"\"Emilie Hartmanns Vater August Hartmann war Lehrer an der Hohen Karlsschule in Stuttgart, bis zu deren Auflösung 1793.\"\"\",\"\"\"1794 wurde Emilie Hartmann geboren in Germany.\"\"\",\"\"\"Jenny Staley Hoad (* 3. März 1934 in Melbourne als Jennifer Staley) ist eine ehemalige australische Tennisspielerin.\"\"\"]"
      ],
      "metadata": {
        "id": "dDq9BbGmuaRg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list_spanish = [\"\"\"El día de Martin Luther King, Jr. (en inglés Martin Luther King, Jr. Day) es un día festivo de los Estados Unidos marcado por el aniversario del natalicio del reverendo doctor Martin Luther King, Jr. Se celebra el tercer lunes de enero de cada año, que es aproximadamente la fecha del nacimiento de King, el 15 de enero de 1929.\"\"\",\"\"\"Chalfie se graduó en la Universidad de Harvard y es profesor de biología en la Universidad de Columbia.\"\"\",\"\"\"Nacida el 10 de agosto de 1943 como Veronica Yvette Bennett en Harlem del Este, Manhattan, Nueva York, de madre afroamericana y padre irlandés.\"\"\",\"\"\"Ricardo Bofill Levi nació el 5 de diciembre de 1939 en Barcelona.\"\"\"]"
      ],
      "metadata": {
        "id": "FiWxOX2Zua35"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list_latvian = [\"\"\"Anna Haselborga ir zviedru kērlinga spēlētāja, 2018. gada ziemas olimpisko spēļu čempione un pasaules čempione jauktajās dubultspēlēs, dzimusi Stokholmā.\"\"\",\"\"\"Aleksandrs Melderis dzimis 1909. gada 19. janvārī Jelgavā Pētera un Margrietas (dzim. Rozenes) Melderu ģimenē.\"\"\",\"\"\"6. janvārī Eiropas Zāļu aģentūra (EZA) apstiprināja arī ASV kompānijas Moderna izstrādāto vakcīnu.\"\"\",\"\"\"29. janvārī, EZA apstiprināja Oksfordas Universitātes un farmācijas kompānijas “AstraZeneca” izstrādāto vakcīnu.\"\"\"]"
      ],
      "metadata": {
        "id": "iCWyVIZfuOSn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list_dutch = [\"\"\"Amerigo Vespucci werd op 9 maart 1454 in Florence geboren, hij was dus een Genuees.\"\"\", \"\"\"Van 23 juni tot 6 juli 1505 werd het Beleg van Arnhem opgezet door Filips de Schone.\"\"\",\"\"\"Graham William Nash is een Engelse zanger en Graham William Nash is geboren in Blackpool.\"\"\",\"\"\"Gaspard Ulliel was een Franse filmacteur en -model, en Gaspard Ulliel werd geboren in Frankrijk.\"\"\"]"
      ],
      "metadata": {
        "id": "TQyzY4jhRknp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list_portuguese = [\"\"\"Kobe Bean Bryant foi um jogador de basquete profissional americano e Kobe Bean Bryant nasceu nos Estados Unidos.\"\"\",\"\"\"O Museu Britânico localiza-se em Londres e foi fundado em 7 de junho de 1753.\"\"\",\"\"\"Simon Marius era um astrônomo alemão, e Simon Marius nasceu em Gunzenhausen.\"\"\",\"\"\"Muse é uma banda britânica de rock de Teignmouth, Devon, formada em 1994.\"\"\"]"
      ],
      "metadata": {
        "id": "eCrO0XwQRkww"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list_french = [\"\"\"Quand j'ai dit à John que je voulais déménager en Alaska, il m'a prévenu que j'aurais du mal à trouver un Starbucks là-bas.\"\"\",\"\"\"Germaine Poinso-Chapuis est une avocate et femme politique française, née le 6 mars 1901 à Marseille et morte le 18 février 1981 dans la même ville.\"\"\",\"\"\"Zine el-Abidine Ben Ali né le 3 septembre 1936 à Hammam Sousse et mort le 19 septembre 2019 à Djeddah, est un homme d'État tunisien.\"\"\",\"\"\"Ricardo Bofill Leví est un architecte espagnol, né le 5 décembre 1939 à Barcelone où il est mort le 14 janvier 2022.\"\"\"]"
      ],
      "metadata": {
        "id": "oDeuUiG-Rk3n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list_chinese = [\"\"\"史蒂夫·戴维斯 生于 英格兰\"\"\",\"\"\"诺瓦克·德约科维奇 生于 贝尔格莱德\"\"\",\"\"\"阿莱克西娅·普特利亚斯 出生于 西班牙\"\"\"]"
      ],
      "metadata": {
        "id": "C00aR7t8RlAd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list_italian = [\"\"\"Il Martin Luther King's Day è una festa nazionale degli Stati Uniti in onore dell'attivista e vincitore del Premio Nobel per la pace Martin Luther King (15 gennaio 1929 - 4 aprile 1968) che si celebra il terzo lunedì di gennaio, un giorno vicino a gennaio 15, giorno della sua nascita negli Stati Uniti.\"\"\",\"\"\"Doraemon è un manga scritto e disegnato da Fujiko F. Fujio e pubblicato in Giappone dal dicembre 1969 all'aprile 1996 sul mensile CoroCoro Comic di Shōgakukan, per un totale di ventisette anni di attività.\"\"\",\"\"\"James Watt nacque in Scozia il 19 gennaio 1736 da genitori presbiteriani.\"\"\",\"\"\"Martin Luther King nacque ad Atlanta, negli Stati Uniti il 15 gennaio 1929.\"\"\"]"
      ],
      "metadata": {
        "id": "kCyR_kIKRlI9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list = [\"text_list_english\",\"text_list_arabic\",\"text_list_german\",\"text_list_spanish\",\"text_list_latvian\",\"text_list_dutch\",\"text_list_portuguese\",\"text_list_french\",\"text_list_chinese\",\"text_list_italian\"]\n"
      ],
      "metadata": {
        "id": "thc1vhZkqsZk"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating input folders\n",
        "import os\n",
        "for MODEL_NAME in text_list:\n",
        "  INPUT_FILE_PATH='/content/Ner_HRL/inputs/'+MODEL_NAME+'/'\n",
        "  OUTPUT_FILE_PATH='/content/Ner_HRL/outputs/'+MODEL_NAME+'/'\n",
        "      \n",
        "      # Create folders\n",
        "  !rm -r $INPUT_FILE_PATH\n",
        "  !mkdir -p $INPUT_FILE_PATH\n",
        "\n",
        "  if MODEL_NAME == 'text_list_english': \n",
        "    for i, v in enumerate(text_list_english):\n",
        "        open(os.path.join(INPUT_FILE_PATH,'Example'+str(i+1)+'.txt'), 'w', encoding=\"utf8\").write(v[:min(len(v)-10, 100)]+'... \\n'+v)\n",
        "  elif MODEL_NAME == 'text_list_arabic':\n",
        "    for i, v in enumerate(text_list_arabic):\n",
        "        open(os.path.join(INPUT_FILE_PATH,'Example'+str(i+1)+'.txt'), 'w', encoding=\"utf8\").write(v[:min(len(v)-10, 100)]+'... \\n'+v)\n",
        "  elif MODEL_NAME == 'text_list_german':\n",
        "    for i, v in enumerate(text_list_german):\n",
        "        open(os.path.join(INPUT_FILE_PATH,'Example'+str(i+1)+'.txt'), 'w', encoding=\"utf8\").write(v[:min(len(v)-10, 100)]+'... \\n'+v)\n",
        "  elif MODEL_NAME == 'text_list_spanish':\n",
        "    for i, v in enumerate(text_list_spanish):\n",
        "        open(os.path.join(INPUT_FILE_PATH,'Example'+str(i+1)+'.txt'), 'w', encoding=\"utf8\").write(v[:min(len(v)-10, 100)]+'... \\n'+v)\n",
        "  elif MODEL_NAME == 'text_list_latvian':\n",
        "    for i, v in enumerate(text_list_latvian):\n",
        "        open(os.path.join(INPUT_FILE_PATH,'Example'+str(i+1)+'.txt'), 'w', encoding=\"utf8\").write(v[:min(len(v)-10, 100)]+'... \\n'+v)\n",
        "  elif MODEL_NAME == 'text_list_dutch':\n",
        "    for i, v in enumerate(text_list_dutch):\n",
        "        open(os.path.join(INPUT_FILE_PATH,'Example'+str(i+1)+'.txt'), 'w', encoding=\"utf8\").write(v[:min(len(v)-10, 100)]+'... \\n'+v)\n",
        "  elif MODEL_NAME == 'text_list_portuguese':\n",
        "    for i, v in enumerate(text_list_portuguese):\n",
        "        open(os.path.join(INPUT_FILE_PATH,'Example'+str(i+1)+'.txt'), 'w', encoding=\"utf8\").write(v[:min(len(v)-10, 100)]+'... \\n'+v)\n",
        "  elif MODEL_NAME == 'text_list_french':\n",
        "    for i, v in enumerate(text_list_french):\n",
        "        open(os.path.join(INPUT_FILE_PATH,'Example'+str(i+1)+'.txt'), 'w', encoding=\"utf8\").write(v[:min(len(v)-10, 100)]+'... \\n'+v)\n",
        "  elif MODEL_NAME == 'text_list_chinese':\n",
        "    for i, v in enumerate(text_list_chinese):\n",
        "        open(os.path.join(INPUT_FILE_PATH,'Example'+str(i+1)+'.txt'), 'w', encoding=\"utf8\").write(v[:min(len(v)-10, 100)]+'... \\n'+v)\n",
        "  elif MODEL_NAME == 'text_list_italian':\n",
        "    for i, v in enumerate(text_list_italian):\n",
        "        open(os.path.join(INPUT_FILE_PATH,'Example'+str(i+1)+'.txt'), 'w', encoding=\"utf8\").write(v[:min(len(v)-10, 100)]+'... \\n'+v)\n",
        "\n",
        "\n",
        "\n",
        "      ## Loading back Example File\n"
      ],
      "metadata": {
        "id": "BAXa8iGyeGlG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating output folders\n",
        "\n",
        "for MODEL_NAME in text_list:\n",
        "  INPUT_FILE_PATH='/content/Ner_HRL/inputs/'+MODEL_NAME+'/'\n",
        "  OUTPUT_FILE_PATH='/content/Ner_HRL/outputs/'+MODEL_NAME+'/'\n",
        "      \n",
        "      # Create folders\n",
        "  !rm -r $OUTPUT_FILE_PATH\n",
        "  !mkdir -p $OUTPUT_FILE_PATH"
      ],
      "metadata": {
        "id": "zf7_i7Gyp4DL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Define Spark NLP pipeline**"
      ],
      "metadata": {
        "id": "6-c8cWtjdXcy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp_display import NerVisualizer\n",
        "for MODEL_NAME in text_list:\n",
        "    INPUT_FILE_PATH='/content/Ner_HRL/inputs/'+MODEL_NAME+'/'\n",
        "    OUTPUT_FILE_PATH='/content/Ner_HRL/outputs/'+MODEL_NAME+'/'\n",
        "    file_list=sorted(os.listdir(INPUT_FILE_PATH))\n",
        "    file_paths =sorted([ os.path.join(INPUT_FILE_PATH, pth) for pth in file_list]) \n",
        "      \n",
        "\n",
        "    documentAssembler = DocumentAssembler()\\\n",
        "          .setInputCol(\"text\")\\\n",
        "          .setOutputCol(\"document\")\n",
        "\n",
        "    sentenceDetector = SentenceDetectorDLModel.pretrained(\"sentence_detector_dl\", \"xx\")\\\n",
        "          .setInputCols([\"document\"])\\\n",
        "          .setOutputCol(\"sentence\")\n",
        "\n",
        "    tokenizer = Tokenizer()\\\n",
        "          .setInputCols([\"sentence\"])\\\n",
        "          .setOutputCol(\"token\")\n",
        "\n",
        "    tokenClassifier = XlmRoBertaForTokenClassification.pretrained(\"xlm_roberta_large_token_classifier_hrl\", \"xx\")\\\n",
        "      .setInputCols([\"sentence\",'token'])\\\n",
        "      .setOutputCol(\"ner\")\n",
        "\n",
        "    ner_converter = NerConverter()\\\n",
        "          .setInputCols([\"sentence\", \"token\", \"ner\"])\\\n",
        "          .setOutputCol(\"ner_chunk\")\n",
        "          \n",
        "    nlpPipeline = Pipeline(stages=[documentAssembler, sentenceDetector, tokenizer, tokenClassifier, ner_converter])\n",
        "\n",
        "    empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "\n",
        "    model = nlpPipeline.fit(empty_data)\n",
        "\n",
        "    if MODEL_NAME == \"text_list_english\":\n",
        "      df = spark.createDataFrame(pd.DataFrame({\"text\": text_list_english}))\n",
        "    elif MODEL_NAME == 'text_list_arabic':\n",
        "      df = spark.createDataFrame(pd.DataFrame({\"text\": text_list_arabic}))\n",
        "    elif MODEL_NAME == 'text_list_german':\n",
        "      df = spark.createDataFrame(pd.DataFrame({\"text\": text_list_german}))\n",
        "    elif MODEL_NAME == \"text_list_spanish\":\n",
        "      df = spark.createDataFrame(pd.DataFrame({\"text\": text_list_spanish}))\n",
        "    elif MODEL_NAME == \"text_list_latvian\":\n",
        "      df = spark.createDataFrame(pd.DataFrame({\"text\": text_list_latvian}))\n",
        "    elif MODEL_NAME == \"text_list_dutch\":\n",
        "      df = spark.createDataFrame(pd.DataFrame({\"text\": text_list_dutch}))\n",
        "    elif MODEL_NAME == \"text_list_portuguese\":\n",
        "      df = spark.createDataFrame(pd.DataFrame({\"text\": text_list_portuguese}))\n",
        "    elif MODEL_NAME == \"text_list_french\":\n",
        "      df = spark.createDataFrame(pd.DataFrame({\"text\": text_list_french}))\n",
        "    elif MODEL_NAME == \"text_list_chinese\":\n",
        "      df = spark.createDataFrame(pd.DataFrame({\"text\": text_list_chinese}))\n",
        "    elif MODEL_NAME == \"text_list_italian\":\n",
        "      df = spark.createDataFrame(pd.DataFrame({\"text\": text_list_italian}))\n",
        "\n",
        "    result = model.transform(df)\n",
        "    result.select(F.explode(F.arrays_zip('ner_chunk.result', 'ner_chunk.metadata')).alias(\"cols\")) \\\n",
        "          .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
        "                  F.expr(\"cols['1']['entity']\").alias(\"ner_label\")).show(truncate=False)\n",
        "          #\\\n",
        "          #.show(truncate=False)\n",
        "\n",
        "    #NerVisualizer().display(\n",
        "     #   result = result.collect()[3],\n",
        "      #  label_col = 'ner_chunk',\n",
        "       # document_col = 'document'\n",
        "    #)\n",
        "    result = result.toPandas()\n",
        " \n",
        "    for i in result.index:\n",
        "        result[['ner_chunk']].iloc[i].to_json(\n",
        "            os.path.join(OUTPUT_FILE_PATH, file_list[i].split('.')[0]+\".json\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU691bvldWoy",
        "outputId": "6b5ce51f-08ee-45cf-f553-9bb775ff3300"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 514.9 KB\n",
            "[OK!]\n",
            "xlm_roberta_large_token_classifier_hrl download started this may take some time.\n",
            "Approximate size to download 1.7 GB\n",
            "[OK!]\n",
            "+-------------------+---------+\n",
            "|chunk              |ner_label|\n",
            "+-------------------+---------+\n",
            "|Jerome Horsey      |PER      |\n",
            "|Russia Company     |ORG      |\n",
            "|Moscow             |LOC      |\n",
            "|San Francisco      |LOC      |\n",
            "|Northern California|LOC      |\n",
            "|Jan Verhaas        |PER      |\n",
            "|Maassluis          |LOC      |\n",
            "|Brielle            |LOC      |\n",
            "|Rita Pankhurst     |PER      |\n",
            "|Addis Ababa        |LOC      |\n",
            "+-------------------+---------+\n",
            "\n",
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 514.9 KB\n",
            "[OK!]\n",
            "xlm_roberta_large_token_classifier_hrl download started this may take some time.\n",
            "Approximate size to download 1.7 GB\n",
            "[OK!]\n",
            "+---------------------------+---------+\n",
            "|chunk                      |ner_label|\n",
            "+---------------------------+---------+\n",
            "|الرياض                     |LOC      |\n",
            "|فيصل بن بندر بن عبد العزيز |PER      |\n",
            "|الرياض                     |LOC      |\n",
            "|حاجي أحمد مُحيي الدين پیري،|PER      |\n",
            "|پيري ريِّس،                |PER      |\n",
            "|مارتن لوثر كنغ،            |PER      |\n",
            "|الولايات المتحدة           |LOC      |\n",
            "+---------------------------+---------+\n",
            "\n",
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 514.9 KB\n",
            "[OK!]\n",
            "xlm_roberta_large_token_classifier_hrl download started this may take some time.\n",
            "Approximate size to download 1.7 GB\n",
            "[OK!]\n",
            "+-----------------+---------+\n",
            "|chunk            |ner_label|\n",
            "+-----------------+---------+\n",
            "|Leonardo         |PER      |\n",
            "|Louvre           |LOC      |\n",
            "|Paris            |LOC      |\n",
            "|Emilie Hartmanns |PER      |\n",
            "|August Hartmann  |PER      |\n",
            "|Hohen Karlsschule|ORG      |\n",
            "|Stuttgart        |LOC      |\n",
            "|Emilie Hartmann  |PER      |\n",
            "|Germany          |LOC      |\n",
            "|Jenny Staley Hoad|PER      |\n",
            "|Melbourne        |LOC      |\n",
            "|Jennifer Staley  |PER      |\n",
            "+-----------------+---------+\n",
            "\n",
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 514.9 KB\n",
            "[OK!]\n",
            "xlm_roberta_large_token_classifier_hrl download started this may take some time.\n",
            "Approximate size to download 1.7 GB\n",
            "[OK!]\n",
            "+-----------------------+---------+\n",
            "|chunk                  |ner_label|\n",
            "+-----------------------+---------+\n",
            "|Martin Luther King, Jr |PER      |\n",
            "|Estados Unidos         |LOC      |\n",
            "|Martin Luther King, Jr |PER      |\n",
            "|King                   |PER      |\n",
            "|Chalfie                |PER      |\n",
            "|Universidad de Harvard |ORG      |\n",
            "|Universidad de Columbia|ORG      |\n",
            "|Veronica Yvette Bennett|PER      |\n",
            "|Harlem del Este        |LOC      |\n",
            "|Manhattan              |LOC      |\n",
            "|Nueva York             |LOC      |\n",
            "|Ricardo Bofill Levi    |PER      |\n",
            "|Barcelona              |LOC      |\n",
            "+-----------------------+---------+\n",
            "\n",
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 514.9 KB\n",
            "[OK!]\n",
            "xlm_roberta_large_token_classifier_hrl download started this may take some time.\n",
            "Approximate size to download 1.7 GB\n",
            "[OK!]\n",
            "+-----------------------+---------+\n",
            "|chunk                  |ner_label|\n",
            "+-----------------------+---------+\n",
            "|Anna Haselborga        |PER      |\n",
            "|Stokholmā              |LOC      |\n",
            "|Aleksandrs Melderis    |PER      |\n",
            "|Jelgavā                |LOC      |\n",
            "|Pētera                 |PER      |\n",
            "|Margrietas             |PER      |\n",
            "|Rozenes                |PER      |\n",
            "|Melderu                |PER      |\n",
            "|Eiropas Zāļu aģentūra  |ORG      |\n",
            "|EZA                    |ORG      |\n",
            "|ASV                    |LOC      |\n",
            "|Moderna                |ORG      |\n",
            "|EZA                    |ORG      |\n",
            "|Oksfordas Universitātes|ORG      |\n",
            "|“AstraZeneca”          |ORG      |\n",
            "+-----------------------+---------+\n",
            "\n",
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 514.9 KB\n",
            "[OK!]\n",
            "xlm_roberta_large_token_classifier_hrl download started this may take some time.\n",
            "Approximate size to download 1.7 GB\n",
            "[OK!]\n",
            "+-------------------+---------+\n",
            "|chunk              |ner_label|\n",
            "+-------------------+---------+\n",
            "|Amerigo Vespucci   |PER      |\n",
            "|Florence           |LOC      |\n",
            "|Arnhem             |LOC      |\n",
            "|Filips de Schone   |PER      |\n",
            "|Graham William Nash|PER      |\n",
            "|Graham William Nash|PER      |\n",
            "|Blackpool          |LOC      |\n",
            "|Gaspard Ulliel     |PER      |\n",
            "|Gaspard Ulliel     |PER      |\n",
            "|Frankrijk          |LOC      |\n",
            "+-------------------+---------+\n",
            "\n",
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 514.9 KB\n",
            "[OK!]\n",
            "xlm_roberta_large_token_classifier_hrl download started this may take some time.\n",
            "Approximate size to download 1.7 GB\n",
            "[OK!]\n",
            "+----------------+---------+\n",
            "|chunk           |ner_label|\n",
            "+----------------+---------+\n",
            "|Kobe Bean Bryant|PER      |\n",
            "|Kobe Bean Bryant|PER      |\n",
            "|Estados Unidos  |LOC      |\n",
            "|Museu Britânico |LOC      |\n",
            "|Londres         |LOC      |\n",
            "|Simon Marius    |PER      |\n",
            "|Simon Marius    |PER      |\n",
            "|Gunzenhausen    |LOC      |\n",
            "|Muse            |ORG      |\n",
            "|Teignmouth      |LOC      |\n",
            "|Devon           |LOC      |\n",
            "+----------------+---------+\n",
            "\n",
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 514.9 KB\n",
            "[OK!]\n",
            "xlm_roberta_large_token_classifier_hrl download started this may take some time.\n",
            "Approximate size to download 1.7 GB\n",
            "[OK!]\n",
            "+-----------------------+---------+\n",
            "|chunk                  |ner_label|\n",
            "+-----------------------+---------+\n",
            "|John                   |PER      |\n",
            "|Alaska                 |LOC      |\n",
            "|Starbucks              |ORG      |\n",
            "|Germaine Poinso-Chapuis|PER      |\n",
            "|Marseille              |LOC      |\n",
            "|Zine el-Abidine Ben Ali|PER      |\n",
            "|Hammam Sousse          |LOC      |\n",
            "|Djeddah                |LOC      |\n",
            "|Ricardo Bofill Leví    |PER      |\n",
            "|Barcelone              |LOC      |\n",
            "+-----------------------+---------+\n",
            "\n",
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 514.9 KB\n",
            "[OK!]\n",
            "xlm_roberta_large_token_classifier_hrl download started this may take some time.\n",
            "Approximate size to download 1.7 GB\n",
            "[OK!]\n",
            "+---------------------+---------+\n",
            "|chunk                |ner_label|\n",
            "+---------------------+---------+\n",
            "|史蒂夫·戴维斯        |PER      |\n",
            "|英格兰               |LOC      |\n",
            "|诺瓦克·德约科维奇    |PER      |\n",
            "|贝尔格莱德           |LOC      |\n",
            "|阿莱克西娅·普特利亚斯|PER      |\n",
            "|西班牙               |LOC      |\n",
            "+---------------------+---------+\n",
            "\n",
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 514.9 KB\n",
            "[OK!]\n",
            "xlm_roberta_large_token_classifier_hrl download started this may take some time.\n",
            "Approximate size to download 1.7 GB\n",
            "[OK!]\n",
            "+------------------+---------+\n",
            "|chunk             |ner_label|\n",
            "+------------------+---------+\n",
            "|Stati Uniti       |LOC      |\n",
            "|Martin Luther King|PER      |\n",
            "|Stati Uniti       |LOC      |\n",
            "|Fujiko F          |PER      |\n",
            "|Fujio             |PER      |\n",
            "|Giappone          |LOC      |\n",
            "|CoroCoro Comic    |ORG      |\n",
            "|Shōgakukan        |LOC      |\n",
            "|James Watt        |PER      |\n",
            "|Scozia            |LOC      |\n",
            "|Martin Luther King|PER      |\n",
            "|Atlanta           |LOC      |\n",
            "|Stati Uniti       |LOC      |\n",
            "+------------------+---------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}