{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GRAMMAR_EN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb-VyZjNYsNu"
      },
      "source": [
        "\n",
        "\n",
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/GRAMMAR_EN.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syePZ-1gYyj3"
      },
      "source": [
        "# **Extract Part of speech tags and perform dependency parsing on a text**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDJCr8UTZAqg"
      },
      "source": [
        "## 1. Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6o8-g0tEqNz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8605112-2b08-4cca-88fa-26492d835378"
      },
      "source": [
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash\n",
        "# !bash colab.sh\n",
        "# -p is for pyspark\n",
        "# -s is for spark-nlp\n",
        "# !bash colab.sh -p 3.1.1 -s 3.0.1\n",
        "# by default they are set to the latest"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"11.0.10\" 2021-01-19\n",
            "OpenJDK Runtime Environment (build 11.0.10+9-Ubuntu-0ubuntu1.18.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.10+9-Ubuntu-0ubuntu1.18.04, mixed mode, sharing)\n",
            "setup Colab for PySpark 3.1.1 and Spark NLP 3.0.0\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 74kB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 56.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 54.8MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMmT9S6mE0ad"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0fyw4cTZDMp"
      },
      "source": [
        "## 2. Start Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zBXbY_vE2ss"
      },
      "source": [
        "spark = sparknlp.start()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODFvrTAgZGGw"
      },
      "source": [
        "## 3. Select the DL model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XxHWemdE5hX"
      },
      "source": [
        "\n",
        "MODEL_NAME='dependency_typed_conllu'"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-HM1On1ZJ4L"
      },
      "source": [
        "## 4. Some sample examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJ7GCD0pFDvP"
      },
      "source": [
        "## Generating Example Files ##\n",
        "text_list = [\n",
        "             \"\"\"John Snow is a good man. He knows a lot about science.\"\"\",\n",
        "             \"\"\"In what country is the WTO headquartered?\"\"\",\n",
        "             \"\"\"I was wearing my dark blue shirt and tie.\"\"\",\n",
        "             \"\"\"The Geneva Motor Show is the most popular car show of the year.\"\"\",\n",
        "             \"\"\"Bill Gates and Steve Jobs had periods of civility.\"\"\",\n",
        "]\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_FMM_GLZMjX"
      },
      "source": [
        "## 5. Define Spark NLP pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiYxv0mOFIcX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f03f5eb-9e3f-44e3-84f0-404eccf30ec1"
      },
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "pos = PerceptronModel.pretrained(\"pos_anc\", 'en')\\\n",
        "        .setInputCols(\"document\", \"token\")\\\n",
        "        .setOutputCol(\"pos\")\n",
        "\n",
        "dep_parser = DependencyParserModel.pretrained('dependency_conllu')\\\n",
        "        .setInputCols([\"document\", \"pos\", \"token\"])\\\n",
        "        .setOutputCol(\"dependency\")\n",
        "\n",
        "\n",
        "typed_dep_parser = TypedDependencyParserModel.pretrained('dependency_typed_conllu')\\\n",
        "        .setInputCols([\"token\", \"pos\", \"dependency\"])\\\n",
        "        .setOutputCol(\"dependency_type\")\n",
        "\n",
        "\n",
        "nlpPipeline = Pipeline(\n",
        "      stages = [\n",
        "          documentAssembler,\n",
        "          tokenizer,\n",
        "          pos,\n",
        "          dep_parser,\n",
        "          typed_dep_parser\n",
        "      ])\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pos_anc download started this may take some time.\n",
            "Approximate size to download 3.9 MB\n",
            "[OK!]\n",
            "dependency_conllu download started this may take some time.\n",
            "Approximate size to download 16.7 MB\n",
            "[OK!]\n",
            "dependency_typed_conllu download started this may take some time.\n",
            "Approximate size to download 2.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zMXh_skZPvR"
      },
      "source": [
        "## 6. Select the example to test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQmz-XHWZX1N"
      },
      "source": [
        "index=0"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ipT3X2sZeBq"
      },
      "source": [
        "## 7. Run the pipeline on selected example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7O34pc7d27-j"
      },
      "source": [
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "pipelineModel = nlpPipeline.fit(empty_df)\n",
        "\n",
        "df = spark.createDataFrame(pd.DataFrame({\"text\":[text_list[index]]}))\n",
        "result = pipelineModel.transform(df)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBr9mBDXZUTX"
      },
      "source": [
        "## 8. Visualize results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzJSQhTnFix5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "917b394f-88d1-4877-ffcc-01e1aeab75cd"
      },
      "source": [
        "\n",
        "result.select(F.explode(F.arrays_zip('token.result',\n",
        "                                     'token.begin',\n",
        "                                     'token.end', \n",
        "                                     'pos.result', \n",
        "                                     'dependency.result', \n",
        "                                                  'dependency.metadata',\n",
        "                                                  'dependency_type.result')).alias(\"cols\"))\\\n",
        "                                                  .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
        "                                                          F.expr(\"cols['1']\").alias(\"begin\"),\n",
        "                                                          F.expr(\"cols['2']\").alias(\"end\"),\n",
        "                                                          F.expr(\"cols['3']\").alias(\"pos\"),\n",
        "                                                          F.expr(\"cols['4']\").alias(\"dependency\"),\n",
        "                                                          F.expr(\"cols['5']\").alias(\"dependency_start\"),\n",
        "                                                          F.expr(\"cols['6']\").alias(\"dependency_type\")).show(truncate=False)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-------+-----+---+---+----------+----------------------------------------------+---------------+\n",
            "|chunk  |begin|end|pos|dependency|dependency_start                              |dependency_type|\n",
            "+-------+-----+---+---+----------+----------------------------------------------+---------------+\n",
            "|John   |0    |3  |NNP|knows     |{head -> 9, head.begin -> 28, head.end -> 32} |nsubj          |\n",
            "|Snow   |5    |8  |NNP|man       |{head -> 6, head.begin -> 20, head.end -> 22} |flat           |\n",
            "|is     |10   |11 |VBZ|man       |{head -> 6, head.begin -> 20, head.end -> 22} |nsubj          |\n",
            "|a      |13   |13 |DT |man       |{head -> 6, head.begin -> 20, head.end -> 22} |nsubj          |\n",
            "|good   |15   |18 |JJ |man       |{head -> 6, head.begin -> 20, head.end -> 22} |amod           |\n",
            "|man    |20   |22 |NN |John      |{head -> 1, head.begin -> 0, head.end -> 3}   |flat           |\n",
            "|.      |23   |23 |.  |knows     |{head -> 9, head.begin -> 28, head.end -> 32} |punct          |\n",
            "|He     |25   |26 |PRP|knows     |{head -> 9, head.begin -> 28, head.end -> 32} |nsubj          |\n",
            "|knows  |28   |32 |VBZ|ROOT      |{head -> 0, head.begin -> -1, head.end -> -1} |root           |\n",
            "|a      |34   |34 |DT |lot       |{head -> 11, head.begin -> 36, head.end -> 38}|nsubj          |\n",
            "|lot    |36   |38 |NN |knows     |{head -> 9, head.begin -> 28, head.end -> 32} |nsubj          |\n",
            "|about  |40   |44 |IN |science   |{head -> 13, head.begin -> 46, head.end -> 52}|det            |\n",
            "|science|46   |52 |NN |lot       |{head -> 11, head.begin -> 36, head.end -> 38}|flat           |\n",
            "|.      |53   |53 |.  |knows     |{head -> 9, head.begin -> 28, head.end -> 32} |punct          |\n",
            "+-------+-----+---+---+----------+----------------------------------------------+---------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}