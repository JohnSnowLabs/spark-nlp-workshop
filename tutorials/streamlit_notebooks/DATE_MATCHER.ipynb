{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DATE_MATCHER.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TA21Jo5d9SVq"
      },
      "source": [
        "\n",
        "\n",
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://githubtocolab.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/DATE_MATCHER.ipynb)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzIdjHkAW8TB"
      },
      "source": [
        "# **Spark NLP Date Matcher**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPkP_ghy4Wj_"
      },
      "source": [
        "### Spark NLP documentation and instructions:\n",
        "https://nlp.johnsnowlabs.com/docs/en/quickstart\n",
        "\n",
        "### You can find details about Spark NLP annotators here:\n",
        "https://nlp.johnsnowlabs.com/docs/en/annotators\n",
        "\n",
        "### You can find details about Spark NLP models here:\n",
        "https://nlp.johnsnowlabs.com/models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIeCOiJNW-88"
      },
      "source": [
        "## 1. Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGJktFHdHL1n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8260c5e3-e454-4ec1-c98b-df563cf345ed"
      },
      "source": [
        "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash\n",
        "# !bash colab.sh\n",
        "# -p is for pyspark\n",
        "# -s is for spark-nlp\n",
        "# !bash colab.sh -p 3.1.1 -s 3.0.1\n",
        "# by default they are set to the latest"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"11.0.10\" 2021-01-19\n",
            "OpenJDK Runtime Environment (build 11.0.10+9-Ubuntu-0ubuntu1.18.04)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.10+9-Ubuntu-0ubuntu1.18.04, mixed mode, sharing)\n",
            "setup Colab for PySpark 3.1.1 and Spark NLP 3.0.0\n",
            "\u001b[K     |████████████████████████████████| 212.3MB 60kB/s \n",
            "\u001b[K     |████████████████████████████████| 143kB 48.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 204kB 42.0MB/s \n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCIT5VLxS3I1"
      },
      "source": [
        "## 2. Start the Spark session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khjM-z9ORFU3"
      },
      "source": [
        "Import dependencies and start Spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw-t1zxlHTB7"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "\n",
        "spark = sparknlp.start()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8E-vYDd1jT0"
      },
      "source": [
        "##3. Build Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz_naMHK1ik2"
      },
      "source": [
        "document_assembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentence_detector = SentenceDetector().setInputCols(\"document\")\\\n",
        "    .setOutputCol(\"sentence\")\n",
        "\n",
        "date_matcher = DateMatcher() \\\n",
        "    .setInputCols('sentence')\\\n",
        "    .setOutputCol(\"date\") \\\n",
        "    .setDateFormat(\"yyyy/MM/dd\")\n",
        "\n",
        "pipeline1= Pipeline(stages=[ document_assembler, \n",
        "                                 sentence_detector,\n",
        "                                 date_matcher,\n",
        "                                 ])\n",
        "\n",
        "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "\n",
        "date_pp = pipeline1.fit(empty_df)\n",
        "date_model = LightPipeline(date_pp)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wwbB08A1ocE"
      },
      "source": [
        "##4. Run & Visualize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ID8yd871xaZ"
      },
      "source": [
        "input_list = [\n",
        "    \"\"\"David visited the restaurant yesterday with his family. \n",
        "He also visited and the day before, but at that time he was alone.\n",
        "David again visited today with his colleagues.\n",
        "He and his friends really liked the food and hoped to visit again tomorrow.\"\"\",]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-0gsWLh1rfF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "349f4d32-5df0-4417-c1e6-22f0039f0eb9"
      },
      "source": [
        "\n",
        "tres = date_model.fullAnnotate(input_list)[0]\n",
        "for dte in tres['date']:\n",
        "    sent = tres['sentence'][int(dte.metadata['sentence'])]\n",
        "    print (f'text/chunk {sent.result[dte.begin:dte.end+1]} | mapped_date: {dte.result}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text/chunk yesterday | mapped_date: 2021/04/03\n",
            "text/chunk day before | mapped_date: 2021/04/03\n",
            "text/chunk today | mapped_date: 2021/04/04\n",
            "text/chunk tomorrow | mapped_date: 2021/04/05\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}