{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ],
      "metadata": {
        "id": "EfxSuxaBaaOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/PySpark/9.PySpark_Estimate_Size.ipynb)"
      ],
      "metadata": {
        "id": "0DNyCgq2VVAU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "Sometimes it is an important question, how much memory does our DataFrame use? And there is no easy answer if you are working with PySpark. You can try to collect the data sample and run local memory profiler. You can estimate the size of the data in the source (for example, in parquet file). But from PySpark API only string representation is available and we will work with it. Please review [this page](https://semyonsinchenko.github.io/ssinchenko/post/estimation-spark-df-size/) for more information.\n",
        "\n"
      ],
      "metadata": {
        "id": "mmpTAzXGRGLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install PySpark"
      ],
      "metadata": {
        "id": "HGdS4JP_RGtA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3pnMpGRQE2q"
      },
      "outputs": [],
      "source": [
        "# install PySpark\n",
        "! pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initializing Spark"
      ],
      "metadata": {
        "id": "JTgVVz1xu2B4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "spark"
      ],
      "metadata": {
        "id": "54fC7hk3QeH8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "1ff20e38-b621-4b4f-f526-5a41b4f9e178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x79317264c760>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://23e55668cbeb:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  DO NOT FORGET WHEN YOU'RE DONE => spark.stop()"
      ],
      "metadata": {
        "id": "7p-XaEE_5WmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pyspark DataFrame"
      ],
      "metadata": {
        "id": "NReZlau33a1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import re\n",
        "import contextlib"
      ],
      "metadata": {
        "id": "OYHuLxXkHb-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The functions we've defined to calculate estimated sizes."
      ],
      "metadata": {
        "id": "Gyey4DyYrWKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _bytes2unit(bb: float, unit: str) -> float:\n",
        "    units = {\n",
        "        \"B\": 1,\n",
        "        \"KiB\": 1024,\n",
        "        \"MiB\": 1024 * 1024,\n",
        "        \"GiB\": 1024 * 1024 * 1024,\n",
        "        \"TiB\": 1024 * 1024 * 1024 * 1024\n",
        "    }\n",
        "    return bb * units[unit]\n",
        "\n",
        "def convert_unit_to_bytes(size: float, unit: str) -> str:\n",
        "    units = {\n",
        "        \"B\": \"Byte\",\n",
        "        \"KiB\": \"KiB\",\n",
        "        \"MiB\": \"MiB\",\n",
        "        \"GiB\": \"GiB\",\n",
        "        \"TiB\": \"TiB\"\n",
        "    }\n",
        "    return f\"{size:.0f} {units[unit]}\"\n",
        "\n",
        "def estimate_size_of_df(df: DataFrame) -> tuple:\n",
        "    \"\"\"Estimate the size of the given DataFrame in different units.\n",
        "    If the size cannot be estimated return (-1.0, -1.0, '').\n",
        "    Sizes are returned in original format, size in bytes, and original unit.\n",
        "\n",
        "    This function works only in PySpark 3.0.0 or higher!\n",
        "\n",
        "    :param df: DataFrame\n",
        "    :returns: Tuple containing original size, size in bytes, and original unit\n",
        "    \"\"\"\n",
        "    with contextlib.redirect_stdout(io.StringIO()) as stdout:\n",
        "        # mode argument was added in 3.0.0\n",
        "        df.explain(mode=\"cost\")\n",
        "\n",
        "    top_line = stdout.getvalue().split(\"\\n\")[1]\n",
        "\n",
        "    # We need a pattern to parse the real size and units\n",
        "    pattern = r\"^.*sizeInBytes=([0-9]+\\.[0-9]+)\\s(B|KiB|MiB|GiB|TiB).*$\"\n",
        "\n",
        "    _match = re.search(pattern, top_line)\n",
        "\n",
        "    if _match:\n",
        "        size = float(_match.groups()[0])\n",
        "        unit = _match.groups()[1]\n",
        "    else:\n",
        "        return -1.0, -1.0, ''\n",
        "\n",
        "    return size, _bytes2unit(size, unit), unit  # original size, size in bytes, original unit"
      ],
      "metadata": {
        "id": "pQtumFQWYdg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now estimate the sizes of different types of data frames here."
      ],
      "metadata": {
        "id": "-HRfwPQ4rCL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CSV"
      ],
      "metadata": {
        "id": "JXws9c1UKvY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/PySpark/data/amazonFood.csv"
      ],
      "metadata": {
        "id": "3VYofdwdYZON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "amazon_csv = spark.read.csv('amazonFood.csv', header=True)"
      ],
      "metadata": {
        "id": "OPd7wPF1uX1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the DataFrame size using the function\n",
        "\n",
        "original_size, size_in_bytes, original_unit = estimate_size_of_df(amazon_csv)\n",
        "\n",
        "if original_size == -1 or size_in_bytes == -1:\n",
        "    print(\"Unable to calculate DataFrame size.\")\n",
        "\n",
        "else:\n",
        "    formatted_original_size = \"{:.2f}\".format(original_size) if original_unit != \"B\" else \"{:.0f}\".format(original_size)\n",
        "    formatted_size_in_bytes = convert_unit_to_bytes(size_in_bytes, original_unit)\n",
        "\n",
        "    if original_unit != \"B\":\n",
        "        print(f\"DataFrame size: {formatted_original_size} {original_unit} or {size_in_bytes:.0f} Byte\")\n",
        "    else:\n",
        "        print(f\"DataFrame size: {formatted_original_size} {original_unit} Byte\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wXLGpQIZTqe",
        "outputId": "33947e3b-db00-4931-e470-a70005a23253"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame size: 22.60 MiB or 23697818 Byte\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parquet"
      ],
      "metadata": {
        "id": "VZBL3TWAwY5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "amazon_csv.write.parquet('amazonFood.parquet')\n",
        "\n",
        "amazon_parquet = spark.read.parquet('./amazonFood.parquet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45wqZRJTsnZX",
        "outputId": "86284541-2841-41be-b2f4-e4578b9a9ea8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 24.1 ms, sys: 3.5 ms, total: 27.6 ms\n",
            "Wall time: 4.76 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the DataFrame size using the function\n",
        "\n",
        "original_size, size_in_bytes, original_unit = estimate_size_of_df(amazon_parquet)\n",
        "\n",
        "if original_size == -1 or size_in_bytes == -1:\n",
        "    print(\"Unable to calculate DataFrame size.\")\n",
        "\n",
        "else:\n",
        "    formatted_original_size = \"{:.2f}\".format(original_size) if original_unit != \"B\" else \"{:.0f}\".format(original_size)\n",
        "    formatted_size_in_bytes = convert_unit_to_bytes(size_in_bytes, original_unit)\n",
        "\n",
        "    if original_unit != \"B\":\n",
        "        print(f\"DataFrame size: {formatted_original_size} {original_unit} or {size_in_bytes:.0f} Byte\")\n",
        "    else:\n",
        "        print(f\"DataFrame size: {formatted_original_size} {original_unit} Byte\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwjOxbAMr7mf",
        "outputId": "8e737c70-a366-4452-e8f4-ebce706ca2ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame size: 10.60 MiB or 11114906 Byte\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As seen in the examples above, CSV files occupy approximately twice the space of Parquet files."
      ],
      "metadata": {
        "id": "mRsAlMB58bW2"
      }
    }
  ]
}