{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to set locale category LC_NUMERIC to en_FR.\n",
      "Warning: Failed to set locale category LC_TIME to en_FR.\n",
      "Warning: Failed to set locale category LC_COLLATE to en_FR.\n",
      "Warning: Failed to set locale category LC_MONETARY to en_FR.\n",
      "Warning: Failed to set locale category LC_MESSAGES to en_FR.\n",
      "--2019-09-10 15:45:51--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sarcasm/train-balanced-sarcasm.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.145.37\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.145.37|:443... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File ‘/tmp/train-balanced-sarcasm.csv’ not modified on server. Omitting download.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sarcasm/train-balanced-sarcasm.csv -P /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "import time\n",
    "\n",
    "packages = [\n",
    "    'JohnSnowLabs:spark-nlp:2.2.1'\n",
    "]\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ML SQL session\") \\\n",
    "    .config('spark.jars.packages', ','.join(packages)) \\\n",
    "    .config('spark.executor.instances','4') \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.driver.memory\",\"16g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version\n",
      "2.2.0\n",
      "Apache Spark version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sparknlp\n",
    "\n",
    "print(\"Spark NLP version\")\n",
    "sparknlp.version()\n",
    "print(\"Apache Spark version\")\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      " |-- ups: string (nullable = true)\n",
      " |-- downs: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- created_utc: string (nullable = true)\n",
      " |-- parent_comment: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=1010826)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sql = SQLContext(spark)\n",
    "\n",
    "trainBalancedSarcasmDF = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/tmp/train-balanced-sarcasm.csv\")\n",
    "trainBalancedSarcasmDF.printSchema()\n",
    "\n",
    "# Let's create a temp view (table) for our SQL queries\n",
    "trainBalancedSarcasmDF.createOrReplaceTempView('data')\n",
    "\n",
    "sql.sql('SELECT COUNT(*) FROM data').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      "\n",
      "+-----+--------------------+\n",
      "|label|             comment|\n",
      "+-----+--------------------+\n",
      "|    0|Yeah, I get that ...|\n",
      "|    0|The blazers and M...|\n",
      "|    0|They're favored t...|\n",
      "|    0|deadass don't kil...|\n",
      "|    0|Yep can confirm I...|\n",
      "|    0|do you find arian...|\n",
      "|    0|What's your weird...|\n",
      "|    0|Probably Sephirot...|\n",
      "|    0|What to upgrade? ...|\n",
      "|    0|Probably count Ka...|\n",
      "|    0|I bet if that mon...|\n",
      "|    0|James Shields Wil...|\n",
      "|    0|There's no time t...|\n",
      "|    0|Team Specific Thr...|\n",
      "|    0|Ill give you a hi...|\n",
      "|    0|Star Wars, easy. ...|\n",
      "|    0|You're adorable.\n",
      "...|\n",
      "|    0|He actually acts ...|\n",
      "|    0|Clinton struggles...|\n",
      "|    0|Is that the Older...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sql.sql('select label,concat(parent_comment,\"\\n\",comment) as comment from data where comment is not null and parent_comment is not null limit 100000')\n",
    "print(type(df))\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|             comment|             ntokens|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|Yeah, I get that ...|[yeah, i, get, th...|\n",
      "|    0|The blazers and M...|[the, blazer, and...|\n",
      "|    0|They're favored t...|[theyr, favor, to...|\n",
      "|    0|deadass don't kil...|[deadass, dont, k...|\n",
      "|    0|Yep can confirm I...|[yep, can, confir...|\n",
      "|    0|do you find arian...|[do, you, find, a...|\n",
      "|    0|What's your weird...|[what, your, weir...|\n",
      "|    0|Probably Sephirot...|[probabl, sephiro...|\n",
      "|    0|What to upgrade? ...|[what, to, upgrad...|\n",
      "|    0|Probably count Ka...|[probabl, count, ...|\n",
      "|    0|I bet if that mon...|[i, bet, if, that...|\n",
      "|    0|James Shields Wil...|[jame, shield, wi...|\n",
      "|    0|There's no time t...|[there, no, time,...|\n",
      "|    0|Team Specific Thr...|[team, specif, th...|\n",
      "|    0|Ill give you a hi...|[ill, give, you, ...|\n",
      "|    0|Star Wars, easy. ...|[star, war, easi,...|\n",
      "|    0|You're adorable.\n",
      "...|  [your, ador, note]|\n",
      "|    0|He actually acts ...|[he, actual, act,...|\n",
      "|    0|Clinton struggles...|[clinton, struggl...|\n",
      "|    0|Is that the Older...|[i, that, the, ol...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"comment\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "    \n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\") \\\n",
    "    .setUseAbbreviations(True)\n",
    "    \n",
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"sentence\"]) \\\n",
    "  .setOutputCol(\"token\")\n",
    "\n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"stem\")\n",
    "    \n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"stem\"]) \\\n",
    "    .setOutputCol(\"normalized\")\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setOutputCols([\"ntokens\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(True)\n",
    "\n",
    "nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, stemmer, normalizer, finisher])\n",
    "nlp_model = nlp_pipeline.fit(df)\n",
    "processed = nlp_model.transform(df).persist()\n",
    "processed.count()\n",
    "processed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70136\n",
      "29864\n"
     ]
    }
   ],
   "source": [
    "train, test = processed.randomSplit(weights=[0.7, 0.3], seed=123)\n",
    "print(train.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|             comment|             ntokens|        clean_tokens|            text_vec|            features|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0|              !\n",
      "Goes|               [goe]|               [goe]|[-0.0429686605930...|[-0.0429686605930...|\n",
      "|    0|!completed\n",
      "!compl...|  [complet, complet]|  [complet, complet]|[0.12425066530704...|[0.12425066530704...|\n",
      "|    0|\"\"\" \"\"Very Right ...|[veri, right, win...|[veri, right, win...|[0.03098143805982...|[0.03098143805982...|\n",
      "|    0|\"\"\" Perhaps you n...|[perhap, you, ne,...|[perhap, ne, stro...|[0.12220748327672...|[0.12220748327672...|\n",
      "|    0|\"\"\" This covering...|[thi, cover, not,...|[thi, cover, onli...|[0.05193965879197...|[0.05193965879197...|\n",
      "|    0|\"\"\"*Kirk\n",
      "I am sin...|[kirk, i, am, sin...|[kirk, singl, gue...|[0.22581466535727...|[0.22581466535727...|\n",
      "|    0|\"\"\"*looks at hand...|[look, at, hand, ...|[look, hand, doe,...|[0.09662958263204...|[0.09662958263204...|\n",
      "|    0|\"\"\"+100\"\" indicat...|[+, indic, come, ...|[+, indic, come, ...|[-0.0648461107436...|[-0.0648461107436...|\n",
      "|    0|\"\"\".$witty_remark...|[wittyremark, shi...|[wittyremark, shi...|[0.0,0.0,0.0,0.0,...|          (50,[],[])|\n",
      "|    0|\"\"\"... and Fancy ...|[and, fanci, feas...|[fanci, feast, so...|[-0.0704490707980...|[-0.0704490707980...|\n",
      "|    0|\"\"\"...and then th...|[and, then, the, ...|[entir, food, cou...|[-0.0525442795611...|[-0.0525442795611...|\n",
      "|    0|\"\"\"...newtons.\"\" ...|[newton, which, i...|[newton, dont, ge...|[0.18569124661959...|[0.18569124661959...|\n",
      "|    0|\"\"\"100 level and ...|[level, and, k, e...|[level, k, easfc,...|[-0.0249043642931...|[-0.0249043642931...|\n",
      "|    0|\"\"\"8 operators.\"\"...|[oper, well, i, m...|[oper, well, mean...|[0.03383025668377...|[0.03383025668377...|\n",
      "|    0|\"\"\"@wikileaks - A...|[wikileak, americ...|[wikileak, americ...|[0.02010703235864...|[0.02010703235864...|\n",
      "|    0|\"\"\"A Cyborg... Ni...|[a, cyborg, ninja...|[cyborg, ninja, n...|[0.04133452971776...|[0.04133452971776...|\n",
      "|    0|\"\"\"A Victoria's S...|[a, victoria, sec...|[victoria, secret...|[-0.0140151982009...|[-0.0140151982009...|\n",
      "|    0|\"\"\"A basic aspect...|[a, basic, aspect...|[basic, aspect, f...|[0.03494623373262...|[0.03494623373262...|\n",
      "|    0|\"\"\"A sense of pur...|[a, sens, of, pur...|[sens, purpos, sh...|[0.03806311306026...|[0.03806311306026...|\n",
      "|    0|\"\"\"Agreed. I thin...|[agr, i, think, w...|[agr, think, issu...|[0.01199619941737...|[0.01199619941737...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import feature as spark_ft\n",
    "\n",
    "stopWords = spark_ft.StopWordsRemover.loadDefaultStopWords('english')\n",
    "sw_remover = spark_ft.StopWordsRemover(inputCol='ntokens', outputCol='clean_tokens', stopWords=stopWords)\n",
    "text2vec = spark_ft.Word2Vec(\n",
    "    vectorSize=50, minCount=5, seed=123, \n",
    "    inputCol='ntokens', outputCol='text_vec', \n",
    "    windowSize=5, maxSentenceLength=30\n",
    ")\n",
    "assembler = spark_ft.VectorAssembler(inputCols=['text_vec'], outputCol='features')\n",
    "feature_pipeline = Pipeline(stages=[sw_remover, text2vec,assembler])\n",
    "feature_model = feature_pipeline.fit(train)\n",
    "\n",
    "train_featurized = feature_model.transform(train).persist()\n",
    "train_featurized.count()\n",
    "train_featurized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import classification as spark_cls\n",
    "\n",
    "\n",
    "mlpc = spark_cls.MultilayerPerceptronClassifier(\n",
    "    maxIter=100, seed=123, layers=[50, 25, 10,2]\n",
    ")\n",
    "\n",
    "model = mlpc.fit(train_featurized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|label|             comment|             ntokens|        clean_tokens|            text_vec|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|    0|!RemindMe 1 week\n",
      "...|[remindm, week, r...|[remindm, week, r...|[-0.2004843149334...|[-0.2004843149334...|[0.44979732724037...|[0.85560599493853...|       0.0|\n",
      "|    0|!Remindme 2 weeks...|[remindm, week, r...|[remindm, week, r...|[-0.1383610498160...|[-0.1383610498160...|[0.67823224421391...|[0.90142281344547...|       0.0|\n",
      "|    0|!SH!TPOST!: All t...|[shtpost, all, th...|[shtpost, poor, u...|[-0.0340055001101...|[-0.0340055001101...|[-0.5511899914289...|[0.43549742946017...|       1.0|\n",
      "|    0|\"\"\"**FUCK** Cloud...|[fuck, cloud, lin...|[fuck, cloud, lin...|[0.07268042531278...|[0.07268042531278...|[-0.2302999255597...|[0.60562137947050...|       0.0|\n",
      "|    0|\"\"\"*Komrad\n",
      "\"*\"\"Th...|[komrad, those, w...|[komrad, prousa, ...|[-0.0746553171692...|[-0.0746553171692...|[-0.6453203469832...|[0.38906197655774...|       1.0|\n",
      "|    0|\"\"\"... thanks to ...|[thank, to, a, pa...|[thank, parad, tr...|[0.06620426938081...|[0.06620426938081...|[-0.5234026796430...|[0.45245468276774...|       1.0|\n",
      "|    0|\"\"\"...FUCK IS THA...|[fuck, i, tha, de...|[fuck, tha, death...|[0.00413818027203...|[0.00413818027203...|[-0.3043495119426...|[0.56372288607541...|       0.0|\n",
      "|    0|\"\"\"...I'm Going T...|[im, go, to, end,...|[im, go, end, dre...|[0.01253372717362...|[0.01253372717362...|[-0.3102899068569...|[0.56127596301957...|       0.0|\n",
      "|    0|\"\"\"A SMALL FUCKIN...|[a, small, fuck, ...|[small, fuck, hol...|[-0.0122776643506...|[-0.0122776643506...|[-0.3324352778879...|[0.54873782376763...|       0.0|\n",
      "|    0|\"\"\"A new brick wa...|[a, new, brick, w...|[new, brick, wall...|[-0.0330542453493...|[-0.0330542453493...|[-0.0997566494423...|[0.65958072819591...|       0.0|\n",
      "|    0|\"\"\"Add dabbing to...|[add, dab, to, mi...|[add, dab, minecr...|[-0.0214630349406...|[-0.0214630349406...|[0.22205077494678...|[0.78647123778898...|       0.0|\n",
      "|    0|\"\"\"All according ...|[all, accord, to,...|[accord, keikaku,...|[0.04333084713046...|[0.04333084713046...|[-0.4147375022102...|[0.50811804287732...|       0.0|\n",
      "|    0|\"\"\"An unmet playe...|[an, unmet, playe...|[unmet, player, h...|[-0.0018495054708...|[-0.0018495054708...|[-0.3508719297303...|[0.54150194160868...|       0.0|\n",
      "|    0|\"\"\"And bacon. Lot...|[and, bacon, lot,...|[bacon, lot, lot,...|[0.02504035184780...|[0.02504035184780...|[-0.4015676697610...|[0.51248991700540...|       0.0|\n",
      "|    0|\"\"\"And later... S...|[and, later, some...|[later, someth, f...|[0.00580244746647...|[0.00580244746647...|[-0.0717478111488...|[0.67168030016336...|       0.0|\n",
      "|    0|\"\"\"And please tel...|[and, pleas, tell...|[pleas, tell, mom...|[0.02764821312545...|[0.02764821312545...|[0.02594497293630...|[0.71654471940443...|       0.0|\n",
      "|    0|\"\"\"Angry Birds?\"\"...|[angri, bird, u, ...|[angri, bird, u, ...|[-0.0963705062167...|[-0.0963705062167...|[-0.2463300115806...|[0.5948048774584,...|       0.0|\n",
      "|    0|\"\"\"Any objections...|[ani, object, fuc...|[ani, object, fuc...|[0.02102925277162...|[0.02102925277162...|[-0.4886517471263...|[0.46637429346000...|       1.0|\n",
      "|    0|\"\"\"Anyway here's ...|[anywai, here, st...|[anywai, stairwai...|[0.08041473637734...|[0.08041473637734...|[-0.2359139216521...|[0.60262664028466...|       0.0|\n",
      "|    0|\"\"\"Aren't you a C...|[arent, you, a, c...|[arent, christian...|[0.07435047936936...|[0.07435047936936...|[-0.5698305134539...|[0.42611052337950...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_featurized = feature_model.transform(test)\n",
    "preds = model.transform(test_featurized)\n",
    "preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = preds.select('comment', 'label', 'prediction').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>!RemindMe 1 week\\n!RemindMe 2 days</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>!Remindme 2 weeks\\n!Remindme 2 weeks</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>!SH!TPOST!: All those poor USA Streamers\\nNow ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\"\"**FUCK** Cloud\"\" - Link main\"\\nYep, that's ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>\"\"\"*Komrad\\n\"*\"\"Those were just pro-USA rebels...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  label  prediction\n",
       "0                 !RemindMe 1 week\\n!RemindMe 2 days      0         0.0\n",
       "1               !Remindme 2 weeks\\n!Remindme 2 weeks      0         0.0\n",
       "2  !SH!TPOST!: All those poor USA Streamers\\nNow ...      0         1.0\n",
       "3  \"\"\"**FUCK** Cloud\"\" - Link main\"\\nYep, that's ...      0         0.0\n",
       "4  \"\"\"*Komrad\\n\"*\"\"Those were just pro-USA rebels...      0         1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred 0</th>\n",
       "      <th>pred 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>true 0</td>\n",
       "      <td>14475</td>\n",
       "      <td>2749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>true 1</td>\n",
       "      <td>9050</td>\n",
       "      <td>3590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pred 0  pred 1\n",
       "true 0   14475    2749\n",
       "true 1    9050    3590"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics as skmetrics\n",
    "pd.DataFrame(\n",
    "    data=skmetrics.confusion_matrix(pred_df['label'], pred_df['prediction']),\n",
    "    columns=['pred ' + l for l in ['0','1']],\n",
    "    index=['true ' + l for l in ['0','1']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.84      0.71     17224\n",
      "           1       0.57      0.28      0.38     12640\n",
      "\n",
      "    accuracy                           0.60     29864\n",
      "   macro avg       0.59      0.56      0.54     29864\n",
      "weighted avg       0.59      0.60      0.57     29864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(skmetrics.classification_report(pred_df['label'], pred_df['prediction'], \n",
    "                                      target_names=['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
