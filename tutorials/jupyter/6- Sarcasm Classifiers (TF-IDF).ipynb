{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://memesbams.com/wp-content/uploads/2017/11/sheldon-sarcasm-meme.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/danofer/sarcasm\n",
    "<div class=\"markdown-converter__text--rendered\"><h3>Context</h3>\n",
    "\n",
    "<p>This dataset contains 1.3 million Sarcastic comments from the Internet commentary website Reddit. The dataset was generated by scraping comments from Reddit (not by me :)) containing the <code>\\s</code> ( sarcasm) tag. This tag is often used by Redditors to indicate that their comment is in jest and not meant to be taken seriously, and is generally a reliable indicator of sarcastic comment content.</p>\n",
    "\n",
    "<h3>Content</h3>\n",
    "\n",
    "<p>Data has balanced and imbalanced (i.e true distribution) versions. (True ratio is about 1:100). The\n",
    "corpus has 1.3 million sarcastic statements, along with what they responded to as well as many non-sarcastic comments from the same source.</p>\n",
    "\n",
    "<p>Labelled comments are in the <code>train-balanced-sarcasm.csv</code> file.</p>\n",
    "\n",
    "<h3>Acknowledgements</h3>\n",
    "\n",
    "<p>The data was gathered by: Mikhail Khodak and Nikunj Saunshi and Kiran Vodrahalli for their article \"<a href=\"https://arxiv.org/abs/1704.05579\" rel=\"nofollow\">A Large Self-Annotated Corpus for Sarcasm</a>\". The data is hosted <a href=\"http://nlp.cs.princeton.edu/SARC/0.0/\" rel=\"nofollow\">here</a>.</p>\n",
    "\n",
    "<p>Citation:</p>\n",
    "\n",
    "<pre><code>@unpublished{SARC,\n",
    "  authors={Mikhail Khodak and Nikunj Saunshi and Kiran Vodrahalli},\n",
    "  title={A Large Self-Annotated Corpus for Sarcasm},\n",
    "  url={https://arxiv.org/abs/1704.05579},\n",
    "  year=2017\n",
    "}\n",
    "</code></pre>\n",
    "\n",
    "<p><a href=\"http://nlp.cs.princeton.edu/SARC/0.0/readme.txt\" rel=\"nofollow\">Annotation of files in the original dataset: readme.txt</a>.</p>\n",
    "\n",
    "<h3>Inspiration</h3>\n",
    "\n",
    "<ul>\n",
    "<li>Predicting sarcasm and relevant NLP features (e.g. subjective determinant, racism, conditionals, sentiment heavy words, \"Internet Slang\" and specific phrases). </li>\n",
    "<li>Sarcasm vs Sentiment</li>\n",
    "<li>Unusual linguistic features such as caps, italics, or elongated words. e.g., \"Yeahhh, I'm sure THAT is the right answer\".</li>\n",
    "<li>Topics that people tend to react to sarcastically</li>\n",
    "</ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import sparknlp\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "packages = [\n",
    "    'JohnSnowLabs:spark-nlp:2.2.1'\n",
    "]\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"ML SQL session\") \\\n",
    "    .config('spark.jars.packages', ','.join(packages)) \\\n",
    "    .config('spark.executor.instances','4') \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.driver.memory\",\"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version\n",
      "2.2.1\n",
      "Apache Spark version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Spark NLP version\")\n",
    "sparknlp.version()\n",
    "print(\"Apache Spark version\")\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Failed to set locale category LC_NUMERIC to en_FR.\n",
      "Warning: Failed to set locale category LC_TIME to en_FR.\n",
      "Warning: Failed to set locale category LC_COLLATE to en_FR.\n",
      "Warning: Failed to set locale category LC_MONETARY to en_FR.\n",
      "Warning: Failed to set locale category LC_MESSAGES to en_FR.\n",
      "--2019-09-10 15:40:40--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sarcasm/train-balanced-sarcasm.csv\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.163.13\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.163.13|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 255268960 (243M) [text/csv]\n",
      "Saving to: ‘/tmp/train-balanced-sarcasm.csv’\n",
      "\n",
      "train-balanced-sarc 100%[===================>] 243.44M  5.90MB/s    in 92s     \n",
      "\n",
      "2019-09-10 15:42:13 (2.63 MB/s) - ‘/tmp/train-balanced-sarcasm.csv’ saved [255268960/255268960]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget -N https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/sarcasm/train-balanced-sarcasm.csv -P /tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- subreddit: string (nullable = true)\n",
      " |-- score: string (nullable = true)\n",
      " |-- ups: string (nullable = true)\n",
      " |-- downs: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- created_utc: string (nullable = true)\n",
      " |-- parent_comment: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=1010826)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sql = SQLContext(spark)\n",
    "\n",
    "trainBalancedSarcasmDF = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/tmp/train-balanced-sarcasm.csv\")\n",
    "trainBalancedSarcasmDF.printSchema()\n",
    "\n",
    "# Let's create a temp view (table) for our SQL queries\n",
    "trainBalancedSarcasmDF.createOrReplaceTempView('data')\n",
    "\n",
    "sql.sql('SELECT COUNT(*) FROM data').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------------+------------------+-----+---+-----+-------+-------------------+--------------------+\n",
      "|label|             comment|            author|         subreddit|score|ups|downs|   date|        created_utc|      parent_comment|\n",
      "+-----+--------------------+------------------+------------------+-----+---+-----+-------+-------------------+--------------------+\n",
      "|    0|          NC and NH.|         Trumpbart|          politics|    2| -1|   -1|2016-10|2016-10-16 23:55:23|Yeah, I get that ...|\n",
      "|    0|You do know west ...|         Shbshb906|               nba|   -4| -1|   -1|2016-11|2016-11-01 00:24:10|The blazers and M...|\n",
      "|    0|They were underdo...|          Creepeth|               nfl|    3|  3|    0|2016-09|2016-09-22 21:45:37|They're favored t...|\n",
      "|    0|\"This meme isn't ...|         icebrotha|BlackPeopleTwitter|   -8| -1|   -1|2016-10|2016-10-18 21:03:47|deadass don't kil...|\n",
      "|    0|I could use one o...|         cush2push|MaddenUltimateTeam|    6| -1|   -1|2016-12|2016-12-30 17:00:13|Yep can confirm I...|\n",
      "|    0|I don't pay atten...|       only7inches|         AskReddit|    0|  0|    0|2016-09|2016-09-02 10:35:08|do you find arian...|\n",
      "|    0|Trick or treating...|       only7inches|         AskReddit|    1| -1|   -1|2016-10|2016-10-23 21:43:03|What's your weird...|\n",
      "|    0|Blade Mastery+Mas...|         P0k3rm4s7|     FFBraveExvius|    2| -1|   -1|2016-10|2016-10-13 21:13:55|Probably Sephirot...|\n",
      "|    0|You don't have to...|        SoupToPots|      pcmasterrace|    1| -1|   -1|2016-10|2016-10-27 19:11:06|What to upgrade? ...|\n",
      "|    0|I would love to s...|          chihawks|      Lollapalooza|    2| -1|   -1|2016-11|2016-11-21 23:39:12|Probably count Ka...|\n",
      "|    0|I think a signifi...|ThisIsNotKimJongUn|          politics|   92| 92|    0|2016-09|2016-09-20 17:53:52|I bet if that mon...|\n",
      "|    0|Damn I was hoping...|        Kvetch__22|          baseball|   14| -1|   -1|2016-10|2016-10-28 09:07:50|James Shields Wil...|\n",
      "|    0|They have an agenda.|        Readbooks6|          exmormon|    4| -1|   -1|2016-10|2016-10-15 01:14:03|There's no time t...|\n",
      "|    0|         Great idea!|        pieman2005|   fantasyfootball|    1| -1|   -1|2016-10|2016-10-06 23:27:53|Team Specific Thr...|\n",
      "|    0|Ayy bb wassup, it...|      Jakethejoker|          NYGiants|   29| 29|    0|2016-09|2016-09-19 18:46:58|Ill give you a hi...|\n",
      "|    0|       what the fuck|            Pishwi|         AskReddit|   22| -1|   -1|2016-11|2016-11-04 20:10:33|Star Wars, easy. ...|\n",
      "|    0|              noted.|         kozmo1313|        NewOrleans|    2| -1|   -1|2016-12|2016-12-20 21:59:45|    You're adorable.|\n",
      "|    0|because it's what...|         kozmo1313|          politics|   15| -1|   -1|2016-12|2016-12-26 20:10:45|He actually acts ...|\n",
      "|    0|why you fail me, ...|         kozmo1313|  HillaryForPrison|    1|  1|    0|2016-09|2016-09-18 13:02:45|Clinton struggles...|\n",
      "|    0|Pre-Flashpoint Cl...|   BreakingGarrick|          superman|    2|  2|    0|2016-09|2016-09-16 02:34:04|Is that the Older...|\n",
      "+-----+--------------------+------------------+------------------+-----+---+-----+-------+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql.sql('select * from data limit 20').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label|   cnt|\n",
      "+-----+------+\n",
      "|    0|505413|\n",
      "|    1|505413|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sql.sql('select label,count(*) as cnt from data group by label order by cnt desc').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=53)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql.sql('select count(*) from data where comment is null').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      "\n",
      "+-----+--------------------+\n",
      "|label|             comment|\n",
      "+-----+--------------------+\n",
      "|    0|Yeah, I get that ...|\n",
      "|    0|The blazers and M...|\n",
      "|    0|They're favored t...|\n",
      "|    0|deadass don't kil...|\n",
      "|    0|Yep can confirm I...|\n",
      "|    0|do you find arian...|\n",
      "|    0|What's your weird...|\n",
      "|    0|Probably Sephirot...|\n",
      "|    0|What to upgrade? ...|\n",
      "|    0|Probably count Ka...|\n",
      "|    0|I bet if that mon...|\n",
      "|    0|James Shields Wil...|\n",
      "|    0|There's no time t...|\n",
      "|    0|Team Specific Thr...|\n",
      "|    0|Ill give you a hi...|\n",
      "|    0|Star Wars, easy. ...|\n",
      "|    0|You're adorable.\n",
      "...|\n",
      "|    0|He actually acts ...|\n",
      "|    0|Clinton struggles...|\n",
      "|    0|Is that the Older...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sql.sql('select label,concat(parent_comment,\"\\n\",comment) as comment from data where comment is not null and parent_comment is not null limit 100000')\n",
    "print(type(df))\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|label|             comment|             ntokens|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|Yeah, I get that ...|[yeah, i, get, th...|\n",
      "|    0|The blazers and M...|[the, blazer, and...|\n",
      "|    0|They're favored t...|[theyr, favor, to...|\n",
      "|    0|deadass don't kil...|[deadass, dont, k...|\n",
      "|    0|Yep can confirm I...|[yep, can, confir...|\n",
      "|    0|do you find arian...|[do, you, find, a...|\n",
      "|    0|What's your weird...|[what, your, weir...|\n",
      "|    0|Probably Sephirot...|[probabl, sephiro...|\n",
      "|    0|What to upgrade? ...|[what, to, upgrad...|\n",
      "|    0|Probably count Ka...|[probabl, count, ...|\n",
      "|    0|I bet if that mon...|[i, bet, if, that...|\n",
      "|    0|James Shields Wil...|[jame, shield, wi...|\n",
      "|    0|There's no time t...|[there, no, time,...|\n",
      "|    0|Team Specific Thr...|[team, specif, th...|\n",
      "|    0|Ill give you a hi...|[ill, give, you, ...|\n",
      "|    0|Star Wars, easy. ...|[star, war, easi,...|\n",
      "|    0|You're adorable.\n",
      "...|  [your, ador, note]|\n",
      "|    0|He actually acts ...|[he, actual, act,...|\n",
      "|    0|Clinton struggles...|[clinton, struggl...|\n",
      "|    0|Is that the Older...|[i, that, the, ol...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"comment\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "    \n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\") \\\n",
    "    .setUseAbbreviations(True)\n",
    "    \n",
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"sentence\"]) \\\n",
    "  .setOutputCol(\"token\")\n",
    "\n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"stem\")\n",
    "    \n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"stem\"]) \\\n",
    "    .setOutputCol(\"normalized\")\n",
    "\n",
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setOutputCols([\"ntokens\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(True)\n",
    "\n",
    "nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, stemmer, normalizer, finisher])\n",
    "nlp_model = nlp_pipeline.fit(df)\n",
    "processed = nlp_model.transform(df).persist()\n",
    "processed.count()\n",
    "processed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70136\n",
      "29864\n"
     ]
    }
   ],
   "source": [
    "train, test = processed.randomSplit(weights=[0.7, 0.3], seed=123)\n",
    "print(train.count())\n",
    "print(test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|             comment|             ntokens|        clean_tokens|                  tf|                 idf|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0|              !\n",
      "Goes|               [goe]|               [goe]|   (500,[375],[1.0])|(500,[375],[4.866...|\n",
      "|    0|!completed\n",
      "!compl...|  [complet, complet]|  [complet, complet]|   (500,[227],[2.0])|(500,[227],[8.875...|\n",
      "|    0|\"\"\" \"\"Very Right ...|[veri, right, win...|[veri, right, win...|(500,[1,7,31,77,9...|(500,[1,7,31,77,9...|\n",
      "|    0|\"\"\" Perhaps you n...|[perhap, you, ne,...|[perhap, ne, stro...|    (500,[34],[1.0])|(500,[34],[3.1336...|\n",
      "|    0|\"\"\" This covering...|[thi, cover, not,...|[thi, cover, onli...|(500,[0,6,14,18,2...|(500,[0,6,14,18,2...|\n",
      "|    0|\"\"\"*Kirk\n",
      "I am sin...|[kirk, i, am, sin...|[kirk, singl, gue...|(500,[31,168,348]...|(500,[31,168,348]...|\n",
      "|    0|\"\"\"*looks at hand...|[look, at, hand, ...|[look, hand, doe,...|(500,[22,58,211,2...|(500,[22,58,211,2...|\n",
      "|    0|\"\"\"+100\"\" indicat...|[+, indic, come, ...|[+, indic, come, ...|(500,[5,9,18,57,9...|(500,[5,9,18,57,9...|\n",
      "|    0|\"\"\".$witty_remark...|[wittyremark, shi...|[wittyremark, shi...|         (500,[],[])|         (500,[],[])|\n",
      "|    0|\"\"\"... and Fancy ...|[and, fanci, feas...|[fanci, feast, so...|     (500,[1],[1.0])|(500,[1],[1.87740...|\n",
      "|    0|\"\"\"...and then th...|[and, then, the, ...|[entir, food, cou...|(500,[14,31,64,19...|(500,[14,31,64,19...|\n",
      "|    0|\"\"\"...newtons.\"\" ...|[newton, which, i...|[newton, dont, ge...|(500,[0,5,6,208],...|(500,[0,5,6,208],...|\n",
      "|    0|\"\"\"100 level and ...|[level, and, k, e...|[level, k, easfc,...|(500,[0,1,27,56,8...|(500,[0,1,27,56,8...|\n",
      "|    0|\"\"\"8 operators.\"\"...|[oper, well, i, m...|[oper, well, mean...|(500,[5,24,51,66,...|(500,[5,24,51,66,...|\n",
      "|    0|\"\"\"@wikileaks - A...|[wikileak, americ...|[wikileak, americ...|   (500,[300],[1.0])|(500,[300],[4.703...|\n",
      "|    0|\"\"\"A Cyborg... Ni...|[a, cyborg, ninja...|[cyborg, ninja, n...|         (500,[],[])|         (500,[],[])|\n",
      "|    0|\"\"\"A Victoria's S...|[a, victoria, sec...|[victoria, secret...|(500,[2,139,173,2...|(500,[2,139,173,2...|\n",
      "|    0|\"\"\"A basic aspect...|[a, basic, aspect...|[basic, aspect, f...|(500,[0,1,2,3,10,...|(500,[0,1,2,3,10,...|\n",
      "|    0|\"\"\"A sense of pur...|[a, sens, of, pur...|[sens, purpos, sh...|(500,[131,133,326...|(500,[131,133,326...|\n",
      "|    0|\"\"\"Agreed. I thin...|[agr, i, think, w...|[agr, think, issu...|(500,[0,1,7,9,29,...|(500,[0,1,7,9,29,...|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import feature as spark_ft\n",
    "\n",
    "stopWords = spark_ft.StopWordsRemover.loadDefaultStopWords('english')\n",
    "sw_remover = spark_ft.StopWordsRemover(inputCol='ntokens', outputCol='clean_tokens', stopWords=stopWords)\n",
    "tf = spark_ft.CountVectorizer(vocabSize=500, inputCol='clean_tokens', outputCol='tf')\n",
    "idf = spark_ft.IDF(minDocFreq=5, inputCol='tf', outputCol='idf')\n",
    "\n",
    "feature_pipeline = Pipeline(stages=[sw_remover, tf, idf])\n",
    "feature_model = feature_pipeline.fit(train)\n",
    "\n",
    "train_featurized = feature_model.transform(train).persist()\n",
    "train_featurized.count()\n",
    "train_featurized.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0|40466|\n",
      "|    1|29670|\n",
      "+-----+-----+\n",
      "\n",
      "root\n",
      " |-- label: integer (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      " |-- ntokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- clean_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- tf: vector (nullable = true)\n",
      " |-- idf: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_featurized.groupBy(\"label\").count().show()\n",
    "train_featurized.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import classification as spark_cls\n",
    "\n",
    "rf = spark_cls. RandomForestClassifier(labelCol=\"label\", featuresCol=\"idf\", numTrees=100)\n",
    "\n",
    "model = rf.fit(train_featurized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|label|             comment|             ntokens|        clean_tokens|                  tf|                 idf|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|    0|!RemindMe 1 week\n",
      "...|[remindm, week, r...|[remindm, week, r...|(500,[56,132],[1....|(500,[56,132],[3....|[59.0684466532376...|[0.59068446653237...|       0.0|\n",
      "|    0|!Remindme 2 weeks...|[remindm, week, r...|[remindm, week, r...|   (500,[132],[2.0])|(500,[132],[8.254...|[59.0684466532376...|[0.59068446653237...|       0.0|\n",
      "|    0|!SH!TPOST!: All t...|[shtpost, all, th...|[shtpost, poor, u...|(500,[286,476],[1...|(500,[286,476],[4...|[59.0684466532376...|[0.59068446653237...|       0.0|\n",
      "|    0|\"\"\"**FUCK** Cloud...|[fuck, cloud, lin...|[fuck, cloud, lin...|(500,[30,35],[1.0...|(500,[30,35],[3.0...|[59.0684466532376...|[0.59068446653237...|       0.0|\n",
      "|    0|\"\"\"*Komrad\n",
      "\"*\"\"Th...|[komrad, those, w...|[komrad, prousa, ...|   (500,[308],[1.0])|(500,[308],[4.833...|[58.9150581121722...|[0.58915058112172...|       0.0|\n",
      "|    0|\"\"\"... thanks to ...|[thank, to, a, pa...|[thank, parad, tr...|(500,[18,31,81,14...|(500,[18,31,81,14...|[58.1738529283073...|[0.58173852928307...|       0.0|\n",
      "|    0|\"\"\"...FUCK IS THA...|[fuck, i, tha, de...|[fuck, tha, death...|(500,[3,11,29,30,...|(500,[3,11,29,30,...|[58.7379990323353...|[0.58737999032335...|       0.0|\n",
      "|    0|\"\"\"...I'm Going T...|[im, go, to, end,...|[im, go, end, dre...|(500,[8,11,119],[...|(500,[8,11,119],[...|[59.3693687564154...|[0.59369368756415...|       0.0|\n",
      "|    0|\"\"\"A SMALL FUCKIN...|[a, small, fuck, ...|[small, fuck, hol...|(500,[30,31,57,42...|(500,[30,31,57,42...|[58.4536009819524...|[0.58453600981952...|       0.0|\n",
      "|    0|\"\"\"A new brick wa...|[a, new, brick, w...|[new, brick, wall...|(500,[3,32,43,124...|(500,[3,32,43,124...|[59.4924967345675...|[0.59492496734567...|       0.0|\n",
      "|    0|\"\"\"Add dabbing to...|[add, dab, to, mi...|[add, dab, minecr...|   (500,[358],[1.0])|(500,[358],[4.866...|[59.0684466532376...|[0.59068446653237...|       0.0|\n",
      "|    0|\"\"\"All according ...|[all, accord, to,...|[accord, keikaku,...|(500,[51,350],[1....|(500,[51,350],[3....|[58.9439702135457...|[0.58943970213545...|       0.0|\n",
      "|    0|\"\"\"An unmet playe...|[an, unmet, playe...|[unmet, player, h...|(500,[0,1,7,8,14,...|(500,[0,1,7,8,14,...|[57.7127311889371...|[0.57712731188937...|       0.0|\n",
      "|    0|\"\"\"And bacon. Lot...|[and, bacon, lot,...|[bacon, lot, lot,...|(500,[6,74,82,483...|(500,[6,74,82,483...|[59.1417478517235...|[0.59141747851723...|       0.0|\n",
      "|    0|\"\"\"And later... S...|[and, later, some...|[later, someth, f...|(500,[54,73,120,1...|(500,[54,73,120,1...|[59.0684466532376...|[0.59068446653237...|       0.0|\n",
      "|    0|\"\"\"And please tel...|[and, pleas, tell...|[pleas, tell, mom...|(500,[0,43,94,116...|(500,[0,43,94,116...|[59.2162763574843...|[0.59216276357484...|       0.0|\n",
      "|    0|\"\"\"Angry Birds?\"\"...|[angri, bird, u, ...|[angri, bird, u, ...|(500,[12,43,44,28...|(500,[12,43,44,28...|[59.0684466532376...|[0.59068446653237...|       0.0|\n",
      "|    0|\"\"\"Any objections...|[ani, object, fuc...|[ani, object, fuc...|(500,[1,30,33,34,...|(500,[1,30,33,34,...|[58.9219950515990...|[0.58921995051599...|       0.0|\n",
      "|    0|\"\"\"Anyway here's ...|[anywai, here, st...|[anywai, stairwai...|   (500,[361],[1.0])|(500,[361],[4.817...|[59.0684466532376...|[0.59068446653237...|       0.0|\n",
      "|    0|\"\"\"Aren't you a C...|[arent, you, a, c...|[arent, christian...|(500,[123,207],[1...|(500,[123,207],[3...|[59.2981588642691...|[0.59298158864269...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_featurized = feature_model.transform(test)\n",
    "preds = model.transform(test_featurized)\n",
    "preds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = preds.select('comment', 'label', 'prediction').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>!RemindMe 1 week\\n!RemindMe 2 days</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>!Remindme 2 weeks\\n!Remindme 2 weeks</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>!SH!TPOST!: All those poor USA Streamers\\nNow ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\"\"**FUCK** Cloud\"\" - Link main\"\\nYep, that's ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>\"\"\"*Komrad\\n\"*\"\"Those were just pro-USA rebels...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  label  prediction\n",
       "0                 !RemindMe 1 week\\n!RemindMe 2 days      0         0.0\n",
       "1               !Remindme 2 weeks\\n!Remindme 2 weeks      0         0.0\n",
       "2  !SH!TPOST!: All those poor USA Streamers\\nNow ...      0         0.0\n",
       "3  \"\"\"**FUCK** Cloud\"\" - Link main\"\\nYep, that's ...      0         0.0\n",
       "4  \"\"\"*Komrad\\n\"*\"\"Those were just pro-USA rebels...      0         0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred 0</th>\n",
       "      <th>pred 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>true 0</td>\n",
       "      <td>17146</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>true 1</td>\n",
       "      <td>12303</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pred 0  pred 1\n",
       "true 0   17146      78\n",
       "true 1   12303     337"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn import metrics as skmetrics\n",
    "pd.DataFrame(\n",
    "    data=skmetrics.confusion_matrix(pred_df['label'], pred_df['prediction']),\n",
    "    columns=['pred ' + l for l in ['0','1']],\n",
    "    index=['true ' + l for l in ['0','1']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      1.00      0.73     17224\n",
      "           1       0.81      0.03      0.05     12640\n",
      "\n",
      "    accuracy                           0.59     29864\n",
      "   macro avg       0.70      0.51      0.39     29864\n",
      "weighted avg       0.68      0.59      0.45     29864\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(skmetrics.classification_report(pred_df['label'], pred_df['prediction'], \n",
    "                                      target_names=['0','1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
