{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use pretrained `explain_document` Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stages\n",
    "\n",
    " * DocumentAssembler\n",
    " * SentenceDetector\n",
    " * Tokenizer\n",
    " * Lemmatizer\n",
    " * Stemmer\n",
    " * Part of Speech\n",
    " * SpellChecker (Norvig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "#Spark ML and SQL\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.sql.functions import array_contains\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "#Spark NLP\n",
    "import sparknlp\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import RegexRule\n",
    "from sparknlp.base import DocumentAssembler, Finisher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a Spark Session for our app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version\n",
      "2.2.0\n",
      "Apache Spark version\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.4.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = sparknlp.start()\n",
    "\n",
    "print(\"Spark NLP version\")\n",
    "sparknlp.version()\n",
    "print(\"Apache Spark version\")\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is our testing document, we'll use it to exemplify all different pipeline stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDoc = [\n",
    "\"French author who helped pioner the science-fiction genre. \\\n",
    "Verne wrate about space, air, and underwater travel before \\\n",
    "navigable aircrast and practical submarines were invented, \\\n",
    "and before any means of space travel had been devised. \"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explain_document_ml download started this may take some time.\n",
      "Approx size to download 9.4 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "pipeline = PretrainedPipeline('explain_document_ml', lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are not interested in handling big datasets, let's switch to LightPipelines for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.annotate(testDoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's analyze these results - first let's see what sentences we detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['French author who helped pioner the science-fiction genre.',\n",
       "  'Verne wrate about space, air, and underwater travel before navigable aircrast and practical submarines were invented, and before any means of space travel had been devised.']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[content['sentence'] for content in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's see how those sentences were tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['French',\n",
       "  'author',\n",
       "  'who',\n",
       "  'helped',\n",
       "  'pioner',\n",
       "  'the',\n",
       "  'science-fiction',\n",
       "  'genre',\n",
       "  '.',\n",
       "  'Verne',\n",
       "  'wrate',\n",
       "  'about',\n",
       "  'space',\n",
       "  ',',\n",
       "  'air',\n",
       "  ',',\n",
       "  'and',\n",
       "  'underwater',\n",
       "  'travel',\n",
       "  'before',\n",
       "  'navigable',\n",
       "  'aircrast',\n",
       "  'and',\n",
       "  'practical',\n",
       "  'submarines',\n",
       "  'were',\n",
       "  'invented',\n",
       "  ',',\n",
       "  'and',\n",
       "  'before',\n",
       "  'any',\n",
       "  'means',\n",
       "  'of',\n",
       "  'space',\n",
       "  'travel',\n",
       "  'had',\n",
       "  'been',\n",
       "  'devised',\n",
       "  '.']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[content['token'] for content in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice some spelling errors? the pipeline takes care of that as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['French',\n",
       "  'author',\n",
       "  'who',\n",
       "  'helped',\n",
       "  'pioneer',\n",
       "  'the',\n",
       "  'sciencefiction',\n",
       "  'genre',\n",
       "  '.',\n",
       "  'Verne',\n",
       "  'wrote',\n",
       "  'about',\n",
       "  'space',\n",
       "  ',',\n",
       "  'air',\n",
       "  ',',\n",
       "  'and',\n",
       "  'underwater',\n",
       "  'travel',\n",
       "  'before',\n",
       "  'navigable',\n",
       "  'aircraft',\n",
       "  'and',\n",
       "  'practical',\n",
       "  'submarines',\n",
       "  'were',\n",
       "  'invented',\n",
       "  ',',\n",
       "  'and',\n",
       "  'before',\n",
       "  'any',\n",
       "  'means',\n",
       "  'of',\n",
       "  'space',\n",
       "  'travel',\n",
       "  'had',\n",
       "  'been',\n",
       "  'devised',\n",
       "  '.']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[content['checked'] for content in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's see the lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['French',\n",
       "  'author',\n",
       "  'who',\n",
       "  'help',\n",
       "  'pioneer',\n",
       "  'the',\n",
       "  'sciencefiction',\n",
       "  'genre',\n",
       "  '.',\n",
       "  'Verne',\n",
       "  'write',\n",
       "  'about',\n",
       "  'space',\n",
       "  ',',\n",
       "  'air',\n",
       "  ',',\n",
       "  'and',\n",
       "  'underwater',\n",
       "  'travel',\n",
       "  'before',\n",
       "  'navigable',\n",
       "  'aircraft',\n",
       "  'and',\n",
       "  'practical',\n",
       "  'submarine',\n",
       "  'be',\n",
       "  'invent',\n",
       "  ',',\n",
       "  'and',\n",
       "  'before',\n",
       "  'any',\n",
       "  'mean',\n",
       "  'of',\n",
       "  'space',\n",
       "  'travel',\n",
       "  'have',\n",
       "  'be',\n",
       "  'devise',\n",
       "  '.']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[content['lemma'] for content in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check the stems, any difference with the lemmas shown bebore?\n",
    "\n",
    "[content['lemmas'] for content in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['french',\n",
       "  'author',\n",
       "  'who',\n",
       "  'help',\n",
       "  'pioneer',\n",
       "  'the',\n",
       "  'sciencefict',\n",
       "  'genr',\n",
       "  '.',\n",
       "  'vern',\n",
       "  'wrote',\n",
       "  'about',\n",
       "  'space',\n",
       "  ',',\n",
       "  'air',\n",
       "  ',',\n",
       "  'and',\n",
       "  'underwat',\n",
       "  'travel',\n",
       "  'befor',\n",
       "  'navig',\n",
       "  'aircraft',\n",
       "  'and',\n",
       "  'practic',\n",
       "  'submarin',\n",
       "  'were',\n",
       "  'invent',\n",
       "  ',',\n",
       "  'and',\n",
       "  'befor',\n",
       "  'ani',\n",
       "  'mean',\n",
       "  'of',\n",
       "  'space',\n",
       "  'travel',\n",
       "  'had',\n",
       "  'been',\n",
       "  'devis',\n",
       "  '.']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[content['stem'] for content in result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now it's the turn on Part Of Speech(POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('French', 'JJ'),\n",
       " ('author', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('helped', 'VBD'),\n",
       " ('pioner', 'NN'),\n",
       " ('the', 'DT'),\n",
       " ('science-fiction', 'NN'),\n",
       " ('genre', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Verne', 'NNP'),\n",
       " ('wrate', 'VBD'),\n",
       " ('about', 'IN'),\n",
       " ('space', 'NN'),\n",
       " (',', ','),\n",
       " ('air', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('underwater', 'JJ'),\n",
       " ('travel', 'NN'),\n",
       " ('before', 'IN'),\n",
       " ('navigable', 'JJ'),\n",
       " ('aircrast', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('practical', 'JJ'),\n",
       " ('submarines', 'NNS'),\n",
       " ('were', 'VBD'),\n",
       " ('invented', 'VBN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('before', 'IN'),\n",
       " ('any', 'DT'),\n",
       " ('means', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('space', 'NN'),\n",
       " ('travel', 'NN'),\n",
       " ('had', 'VBD'),\n",
       " ('been', 'VBN'),\n",
       " ('devised', 'VBN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos = [content['pos'] for content in result]\n",
    "token = [content['token'] for content in result]\n",
    "# let's put token and tag together\n",
    "list(zip(token[0], pos[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use pretrained `match_chunk` Pipeline for Individual Noun Phrase "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DocumentAssembler\n",
    "* SentenceDetector\n",
    "* Tokenizer\n",
    "* Part of speech\n",
    "* chunker\n",
    "\n",
    "Pipeline:\n",
    "* The pipeline uses regex `<DT>?<JJ>*<NN>+`\n",
    "* which states that whenever the chunk finds an optional determiner \n",
    "* (DT) followed by any number of adjectives (JJ) and then a noun (NN) then the Noun Phrase(NP) chunk should be formed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match_chunks download started this may take some time.\n",
      "Approx size to download 4.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "pipeline = PretrainedPipeline('match_chunks', lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.annotate(\"The book has many chapters\") # single noun phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['chunk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.annotate(\"the little yellow dog barked at the cat\") #multiple noune phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the little yellow dog', 'the cat']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['chunk']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
