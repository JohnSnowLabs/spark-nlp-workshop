{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load JSL Preds Data (with Ground Truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data path: ../data/data_with_jsl_preds.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>chunk</th>\n",
       "      <th>ner_label</th>\n",
       "      <th>label_count</th>\n",
       "      <th>jsl_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>Unfortunately , I think itis unlikelythathis s...</td>\n",
       "      <td>['he', 'over 9 to 12 months', 'he', 'he']</td>\n",
       "      <td>[Gender, RelativeDate, Gender, Gender]</td>\n",
       "      <td>{'Gender': 3, 'RelativeDate': 1}</td>\n",
       "      <td>[{'entity_type': 'Gender', 'chunk': 'he', 'sta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  \\\n",
       "0  50  Unfortunately , I think itis unlikelythathis s...   \n",
       "\n",
       "                                       chunk  \\\n",
       "0  ['he', 'over 9 to 12 months', 'he', 'he']   \n",
       "\n",
       "                                ner_label                       label_count  \\\n",
       "0  [Gender, RelativeDate, Gender, Gender]  {'Gender': 3, 'RelativeDate': 1}   \n",
       "\n",
       "                                      jsl_prediction  \n",
       "0  [{'entity_type': 'Gender', 'chunk': 'he', 'sta...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_name = 'data_with_jsl_preds.pkl'\n",
    "data_path = os.path.join('..', 'data', file_name )\n",
    "print(\"Raw data path:\", data_path)\n",
    "data_df = pd.read_pickle(data_path)\n",
    "data_df.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Task Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction Tasks**\n",
    "1. 'Organ_Component'\n",
    "   <br> *Evaluated for* \n",
    "    - External_body_part_or_region\n",
    "    - Internal_organ_or_component\n",
    "2. 'Gender'\n",
    "3. 'Procedure_Treatment'\n",
    "    <br> *Evaluated for* \n",
    "    - Procedure\n",
    "    - Treatment\n",
    "4. 'Problem' (19 Entities merged under Problem Type)\n",
    "5. 'Medicine' (Drug_BrandName, Drug_Ingredient, Strength, Form, Dosage )\n",
    "<br> In this evaluation case partial matches are considered as full_match\n",
    "6. 'Drug'\n",
    "    <br> *Evaluated for* \n",
    "    - Drug (Drug_BrandName, Drug_Ingredient)\n",
    "    - Drug_Related (Strength, Form, Dosage)\n",
    "7. 'Test'\n",
    "8. 'Test_TestResults' (Test, Test Results)\n",
    "    <br> *Evaluated for* \n",
    "    - Test\n",
    "    - Test_Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_list: dict_keys(['Organ_Component', 'Gender', 'Procedure_Treatment', 'Problem', 'Medicine', 'Drug', 'Test', 'Test_TestResult'])\n",
      "eval_options_dict: {'External_body_part_or_region': {'selected_entity_prediction': ['External_body_part_or_region'], 'selected_entity_gt': ['External_body_part_or_region'], 'gt_type_dict': {'External_body_part_or_region': 'External_body_part_or_region'}}, 'Internal_organ_or_component': {'selected_entity_prediction': ['Internal_organ_or_component'], 'selected_entity_gt': ['Internal_organ_or_component'], 'gt_type_dict': {'Internal_organ_or_component': 'Internal_organ_or_component'}}, 'Gender': {'selected_entity_prediction': ['Gender'], 'selected_entity_gt': ['Gender'], 'gt_type_dict': {'Gender': 'Gender'}}, 'Procedure': {'selected_entity_prediction': ['Procedure'], 'selected_entity_gt': ['Procedure'], 'gt_type_dict': {'Procedure': 'Procedure'}}, 'Treatment': {'selected_entity_prediction': ['Treatment'], 'selected_entity_gt': ['Treatment'], 'gt_type_dict': {'Treatment': 'Treatment'}}, 'Problem': {'selected_entity_prediction': ['Psychological_Condition', 'Symptom', 'Disease_Syndrome_Disorder', 'Heart_Disease', 'Oncological', 'Kidney_Disease', 'Overweight', 'Diabetes', 'Obesity', 'EKG_Findings', 'Communicable_Disease', 'Tumor_Finding', 'ImagingFindings', 'Cerebrovascular_Disease', 'Psychological_Condition', 'Hyperlipidemia', 'VS_Finding', 'Hypertension', 'Metastasis'], 'selected_entity_gt': ['Psychological_Condition', 'Symptom', 'Disease_Syndrome_Disorder', 'Heart_Disease', 'Oncological', 'Kidney_Disease', 'Overweight', 'Diabetes', 'Obesity', 'EKG_Findings', 'Communicable_Disease', 'Tumor_Finding', 'ImagingFindings', 'Cerebrovascular_Disease', 'Psychological_Condition', 'Hyperlipidemia', 'VS_Finding', 'Hypertension', 'Metastasis'], 'gt_type_dict': {'Psychological_Condition': 'Psychological_Condition', 'Symptom': 'Symptom', 'Disease_Syndrome_Disorder': 'Disease_Syndrome_Disorder', 'Heart_Disease': 'Heart_Disease', 'Oncological': 'Oncological', 'Kidney_Disease': 'Kidney_Disease', 'Overweight': 'Overweight', 'Diabetes': 'Diabetes', 'Obesity': 'Obesity', 'EKG_Findings': 'EKG_Findings', 'Communicable_Disease': 'Communicable_Disease', 'Tumor_Finding': 'Tumor_Finding', 'ImagingFindings': 'ImagingFindings', 'Cerebrovascular_Disease': 'Cerebrovascular_Disease', 'Hyperlipidemia': 'Hyperlipidemia', 'VS_Finding': 'VS_Finding', 'Hypertension': 'Hypertension', 'Metastasis': 'Metastasis'}}, 'Medicine': {'selected_entity_prediction': ['Drug_BrandName', 'Drug_Ingredient', 'Strength', 'Dosage', 'Form'], 'selected_entity_gt': ['Drug_BrandName', 'Drug_Ingredient', 'Strength', 'Dosage', 'Form'], 'gt_type_dict': {'Drug_BrandName': 'Drug_BrandName', 'Drug_Ingredient': 'Drug_Ingredient', 'Strength': 'Strength', 'Dosage': 'Dosage', 'Form': 'Form'}}, 'Drug': {'selected_entity_prediction': ['Drug_BrandName', 'Drug_Ingredient'], 'selected_entity_gt': ['Drug_BrandName', 'Drug_Ingredient'], 'gt_type_dict': {'Drug_BrandName': 'Drug_BrandName', 'Drug_Ingredient': 'Drug_Ingredient'}}, 'Drug_Related': {'selected_entity_prediction': ['Strength', 'Dosage', 'Form'], 'selected_entity_gt': ['Strength', 'Dosage', 'Form'], 'gt_type_dict': {'Strength': 'Strength', 'Dosage': 'Dosage', 'Form': 'Form'}}, 'Test': {'selected_entity_prediction': ['Test'], 'selected_entity_gt': ['Test'], 'gt_type_dict': {'Test': 'Test'}}, 'Test_Result': {'selected_entity_prediction': ['Test_Result'], 'selected_entity_gt': ['Test_Result'], 'gt_type_dict': {'Test_Result': 'Test_Result'}}}\n"
     ]
    }
   ],
   "source": [
    "from modules.ProcessRawData import ProcessData\n",
    "from modules.ProcessPredData import corrected_json, get_list_of_entities\n",
    "import json\n",
    "\n",
    "# read tasks from file\n",
    "with open(\"./sources/tasks_list_jsl.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# extract problem_entity_list and task_list\n",
    "task_list = data[\"task_list\"]\n",
    "eval_options_dict = data[\"eval_options_dict\"]\n",
    "\n",
    "print(\"task_list:\", task_list.keys())\n",
    "print(\"eval_options_dict:\",eval_options_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Select Task to Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897abcc414354fe18c7b3ab98edbc016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(options=('Organ_Component', 'Gender', 'Procedure_Treatment', 'Problem', 'Medicine', 'Drug', 'Test', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb0a2fe7c2d4a76a851aca2914ed353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Approve', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions will be generated for the entities: Organ_Component\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from modules.entity_selection import create_dropdown\n",
    "\n",
    "# select entity for evaluation\n",
    "entity_options = list(task_list.keys())\n",
    "\n",
    "# create_dropdown(entity_options)\n",
    "def assign_entity(entity):\n",
    "    global entity_under_test\n",
    "    entity_under_test = entity\n",
    "    print(\"Predictions will be generated for the entities:\", entity_under_test)\n",
    "\n",
    "# create the dropdown menu\n",
    "dropdown = widgets.Dropdown(options=entity_options)\n",
    "\n",
    "# create the button\n",
    "button = widgets.Button(description=\"Approve\")\n",
    "\n",
    "# define the callback function for the button\n",
    "def on_button_click(button):\n",
    "    assign_entity(dropdown.value)\n",
    "\n",
    "# register the callback function with the button\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# display the dropdown menu and the button\n",
    "display(dropdown)\n",
    "display(button)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Data for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 02:06:58,930 - ProcessRawData - INFO - main df shape: (849, 7)\n",
      "2023-04-03 02:06:58,942 - ProcessRawData - INFO - filtered df shape: (62, 8)\n",
      "2023-04-03 02:06:59,028 - ProcessRawData - INFO - final df shape: (62, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "covered_gt_label_list: ['Internal_organ_or_component', 'External_body_part_or_region']\n",
      "Processing main csv...\n"
     ]
    }
   ],
   "source": [
    "# filter gt data for prediction based on selected entity under test and entity counts\n",
    "covered_gt_label_list = task_list[entity_under_test]['covered_gt_label_list']\n",
    "print(\"covered_gt_label_list:\", covered_gt_label_list)\n",
    "count_filter = task_list[entity_under_test]['count_filter']\n",
    "\n",
    "prediction_source = \"jsl_ner\"\n",
    "\n",
    "# process gt data for prediction and evaluation\n",
    "process_gt_data = ProcessData(\n",
    "    covered_gt_label_list, \n",
    "    count_filter, \n",
    "    data_path,\n",
    "    prediction_source\n",
    ")\n",
    "\n",
    "processed_gt_df = process_gt_data.get_final_df()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Selection of entity for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity_list_for_benchmark: ['External_body_part_or_region', 'Internal_organ_or_component']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b567081c694c45d18fd1f98b98abe320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(options=('External_body_part_or_region', 'Internal_organ_or_component'), value='External_body_part_orâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a780a04cc5943568a1a9a2f9d9a95cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Approve', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmarks will be evaluated for the entity: External_body_part_or_region\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "# selection of entity type for benchmark\n",
    "entity_list_for_benchmark = data[\"entity_for_benchmark\"][entity_under_test]\n",
    "print(\"entity_list_for_benchmark:\", entity_list_for_benchmark)\n",
    "\n",
    "if len(entity_list_for_benchmark) == 1:\n",
    "    entity_for_benchmark = entity_list_for_benchmark[0]\n",
    "    print(\"Benchmarks will be evaluated for the entity:\", entity_for_benchmark)\n",
    "else:\n",
    "    def assign_entity(entity):\n",
    "        global entity_for_benchmark\n",
    "        entity_for_benchmark = entity\n",
    "        print(\"Benchmarks will be evaluated for the entity:\", entity_for_benchmark)\n",
    "    \n",
    "    # create the dropdown menu\n",
    "    dropdown = widgets.Dropdown(options=entity_list_for_benchmark)\n",
    "\n",
    "    # create the button\n",
    "    button = widgets.Button(description=\"Approve\")\n",
    "\n",
    "    # define the callback function for the button\n",
    "    def on_button_click(button):\n",
    "        assign_entity(dropdown.value)\n",
    "\n",
    "    # register the callback function with the button\n",
    "    button.on_click(on_button_click)\n",
    "\n",
    "    # display the dropdown menu and the button\n",
    "    display(dropdown)\n",
    "    display(button)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Run Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_entity_prediction: ['External_body_part_or_region']\n",
      "Evaluation results saved as ./eval_results/External_body_part_or_region_jsl_ner_eval_0403_0207.xlsx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'version': 'External_body_part_or_region_jsl_ner_eval_0403_0207',\n",
       " 'selected_entity_prediction': ['External_body_part_or_region'],\n",
       " 'selected_entity_gt': ['External_body_part_or_region'],\n",
       " 'full_match': 104,\n",
       " 'accuracy_full_match': 0.84,\n",
       " 'partial_match': 2,\n",
       " 'accuracy_partial_match': 0.85,\n",
       " 'no_match': 18,\n",
       " 'gt_count': 124,\n",
       " 'fp_count': 16}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No user interaction needed, just run\n",
    "from modules import Evaluation\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "eval_file_to_save = entity_for_benchmark + \"_\" + prediction_source + f\"_eval_{now.strftime('%m%d_%H%M')}\"\n",
    "\n",
    "selected_entity_prediction = eval_options_dict[entity_for_benchmark][\"selected_entity_prediction\"]\n",
    "print(\"selected_entity_prediction:\", selected_entity_prediction)\n",
    "selected_entity_gt = eval_options_dict[entity_for_benchmark][\"selected_entity_gt\"]\n",
    "gt_type_dict = eval_options_dict[entity_for_benchmark][\"gt_type_dict\"]\n",
    "\n",
    "\n",
    "evaluator = Evaluation.Evaluate(\n",
    "    file_path=None, \n",
    "    dataframe=processed_gt_df, \n",
    "    prediction_source=prediction_source\n",
    ")\n",
    "\n",
    "eval_results = evaluator.get_match_counts(\n",
    "    selected_entity_prediction, \n",
    "    selected_entity_gt, \n",
    "    eval_file_to_save,\n",
    "    gt_type_dict\n",
    ")\n",
    "\n",
    "# write result to JSON file\n",
    "with open('eval_results/eval_result.json', 'a') as f:\n",
    "    json.dump(eval_results, f)\n",
    "    \n",
    "eval_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EnvBenchmark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d96c1905d7b6a7c12a8231edcad52a0a015365685b1a5a2fb7a32334df960ce4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
