{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Raw Data (with Sentences and Ground Truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data path: ../data/deid/100_rows_with_gt_jsl.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>ground_truth_list</th>\n",
       "      <th>jsl_prediction_list</th>\n",
       "      <th>prediction_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>929334185</td>\n",
       "      <td>\\n929334185\\nFIH\\n8151167\\n53653/y9m1\\n539442\\...</td>\n",
       "      <td>[{'entity_type': 'ID', 'chunk': '929334185', '...</td>\n",
       "      <td>[{'entity_type': 'ID', 'chunk': '929334185', '...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                               text  \\\n",
       "0  929334185  \\n929334185\\nFIH\\n8151167\\n53653/y9m1\\n539442\\...   \n",
       "\n",
       "                                   ground_truth_list  \\\n",
       "0  [{'entity_type': 'ID', 'chunk': '929334185', '...   \n",
       "\n",
       "                                 jsl_prediction_list prediction_list  \n",
       "0  [{'entity_type': 'ID', 'chunk': '929334185', '...              []  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_name= '100_rows_with_gt_jsl.pkl'\n",
    "data_path = os.path.join('..', 'data/deid/', file_name )\n",
    "print(\"Raw data path:\", data_path)\n",
    "\n",
    "data_df = pd.read_pickle(data_path)\n",
    "data_df.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Task Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# read tasks from file\n",
    "with open(\"./sources/tasks_list.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# extract problem_entity_list and task_list\n",
    "task_list = data[\"task_list\"]\n",
    "eval_options_dict = data[\"eval_options_dict\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Select Task to Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions will be generated for the entities: Deid\n"
     ]
    }
   ],
   "source": [
    "# select entity for evaluation\n",
    "entity_under_test = \"Deid\"\n",
    "# continue with evaluation using selected entity_under_test\n",
    "print(\"Predictions will be generated for the entities:\", entity_under_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Data for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter gt data for prediction based on selected entity under test and entity counts\n",
    "covered_gt_label_list = task_list[entity_under_test]['covered_gt_label_list']\n",
    "\n",
    "prediction_source = \"ChatGPT\"\n",
    "\n",
    "processed_gt_df = data_df.copy()\n",
    "processed_gt_df.loc[:, 'prediction_list'] = [[] for _ in range(len(processed_gt_df))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. GPT Predictions "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Load Prompt and Initiate Entity Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt for [Deid] extraction task: (./prompts/13_01_DEID.txt)\n",
      "PROMPT:\n",
      "\n",
      "Please extract the following entities from the provided medical record text, specifically hospital discharge notes and/or discharge summaries:\n",
      "\n",
      "ID, DATE, AGE, PHONE, PERSON, ORGANIZATION, LOCATION.\n",
      "\n",
      "IMPOTANT!!! Do not return any other entities except these ones.\n",
      "\n",
      "\n",
      "\n",
      "I have provided sample sentences for each entity type below:\n",
      "\n",
      "\n",
      "\n",
      "ID:\n",
      "\n",
      "\"Mr. Smith's patient ID is 123456789 and he has been visiting our clinic since 2020.\"\n",
      "\n",
      "Example:\n",
      "\n",
      "[{'entity_type': 'ID', 'chunk': '123456789'}]\n",
      "\n",
      "\n",
      "\n",
      "DATE:\n",
      "\n",
      "\"Mrs. Johnson had her last appointment on March 21, 2023, and her next appointment is scheduled for April 18, 2023.\"\n",
      "\n",
      "Example:\n",
      "\n",
      "[{'entity_type': 'DATE', 'chunk': 'March 21, 2023'},\n",
      "\n",
      "{'entity_type': 'DATE', 'chunk': 'April 18, 2023'}]\n",
      "\n",
      "\n",
      "\n",
      "AGE:\n",
      "\n",
      "\"Mr. Anderson is a 45-year-old patient with a history of hypertension.\"\n",
      "\n",
      "Example:\n",
      "\n",
      "[{'entity_type': 'AGE', 'chunk': '45'}]\n",
      "\n",
      "\n",
      "\n",
      "PHONE:\n",
      "\n",
      "\"You can reach Dr. Adams at 555-123-4567 for any questions regarding your treatment plan.\"\n",
      "\n",
      "Example:\n",
      "\n",
      "[{'entity_type': 'PHONE', 'chunk': '555-123-4567'}]\n",
      "\n",
      "\n",
      "\n",
      "PERSON:\n",
      "\n",
      "\"Jane Doe has been suffering from migraines for the past five years.\"\n",
      "\n",
      "Example:\n",
      "\n",
      "[{'entity_type': 'PATIENT', 'chunk': 'Jane Doe'}]\n",
      "\n",
      "\n",
      "\n",
      "ORGANIZATION:\n",
      "\n",
      "\"Mr. Johnson had his surgery at Memorial Hospital, and his recovery was smooth.\"\n",
      "\n",
      "Example:\n",
      "\n",
      "[{'entity_type': 'ORGANIZATION', 'chunk': 'Memorial Hospital'}]\n",
      "\n",
      "\n",
      "\n",
      "LOCATION:\n",
      "\n",
      "\"The patient resides in Los Angeles, California, and visits our local clinic for routine check-ups.\"\n",
      "\n",
      "Example:\n",
      "\n",
      "[{'entity_type': 'LOCATION', 'chunk': 'Los Angeles, California'}]\n",
      "\n",
      "\n",
      "\n",
      "Desired format for extracted entities:\n",
      "\n",
      "{\"list_of_entities\":\n",
      "\n",
      "[\n",
      "\n",
      "{\"entity_type\": \"ID\", \"chunk\": \"929334185\"},\n",
      "\n",
      "{\"entity_type\": \"LOCATION\", \"chunk\": \"Los Angeles\"},\n",
      "\n",
      "...\n",
      "\n",
      "]\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "\n",
      "If an entity is missing or not available in the text, do not include it in the list of entities.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from modules import NerExtraction\n",
    "prompt_file_path = os.path.join('.', 'prompts', task_list[entity_under_test]['prompt_file'] )\n",
    "print(f\"Prompt for [{entity_under_test}] extraction task: ({prompt_file_path})\")\n",
    "\n",
    "with open(prompt_file_path, 'r') as file:\n",
    "    print(\"PROMPT:\\n\")\n",
    "    for line in file:\n",
    "        print(line)\n",
    "    \n",
    "ner_extraction = NerExtraction.ChatGPTNER(prompt_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Get Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./processed_data/Deid_preds_0414_1659\n",
      "Given dataframe shape (5, 5)\n",
      "Getting predictions from API started...\n",
      "Query: 1 | index: 0 | status: SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 16:59:32,124 - ProcessPredData - ERROR - generate_start_end_index function error decoding to str: need a bytes-like object, NoneType found\n",
      "2023-04-14 16:59:32,126 - ProcessPredData - ERROR - generate_start_end_index function error decoding to str: need a bytes-like object, NoneType found\n",
      "2023-04-14 16:59:32,127 - ProcessPredData - ERROR - generate_start_end_index function error decoding to str: need a bytes-like object, NoneType found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 2 | index: 1 | status: SUCCESS\n",
      "Query: 3 | index: 2 | status: SUCCESS\n",
      "Query: 4 | index: 3 | status: SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 17:00:18,406 - ProcessPredData - ERROR - generate_start_end_index function error decoding to str: need a bytes-like object, NoneType found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 5 | index: 4 | status: SUCCESS\n",
      "Getting predictions from API finished.\n",
      "final df shape: (5, 5)\n",
      "file saved as ./processed_data/Deid_preds_0414_1659.csv \n"
     ]
    }
   ],
   "source": [
    "# !!! Run without any changes\n",
    "\n",
    "from modules import BatchProcessing\n",
    "from modules.ProcessPredData import corrected_json, get_list_of_entities\n",
    "import datetime\n",
    "\n",
    "# Assing auto name to save prediction data as csv and excel\n",
    "now = datetime.datetime.now()\n",
    "file_name = f\"{entity_under_test}_preds_{now.strftime('%m%d_%H%M')}\"\n",
    "if not os.path.exists('processed_data'):\n",
    "    os.makedirs('processed_data')\n",
    "processed_data_path = os.path.join('.', 'processed_data', file_name)\n",
    "\n",
    "print(processed_data_path)\n",
    "batch_processor = BatchProcessing.ProcessBatch(\n",
    "    processed_gt_df.head(5),\n",
    "    ner_extraction,\n",
    "    corrected_json,\n",
    "    get_list_of_entities,\n",
    "    processed_data_path\n",
    ")\n",
    "\n",
    "results_df = batch_processor.do_processing()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Selection of entity for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###!!! USER INPUT MIGHT BE NECESSARY if there are multiple entites for evaluation under given task\n",
    "entity_for_benchmark = \"Deid\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Run Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results saved as ./eval_results/Deid_ChatGPT_eval_0414_1700.xlsx\n",
      "Results appended to file: eval_results/eval_result.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'version': 'Deid_ChatGPT_eval_0414_1700',\n",
       " 'selected_entity_prediction': ['ID',\n",
       "  'DATE',\n",
       "  'AGE',\n",
       "  'PHONE',\n",
       "  'PERSON',\n",
       "  'LOCATION',\n",
       "  'ORGANIZATION'],\n",
       " 'selected_entity_gt': ['ID',\n",
       "  'DATE',\n",
       "  'AGE',\n",
       "  'PHONE',\n",
       "  'DOCTOR',\n",
       "  'PATIENT',\n",
       "  'NAME',\n",
       "  'HOSPITAL',\n",
       "  'LOCATION',\n",
       "  'ORGANIZATION'],\n",
       " 'full_match': 30,\n",
       " 'accuracy_full_match': 0.07,\n",
       " 'partial_match': 25,\n",
       " 'accuracy_partial_match': 0.13,\n",
       " 'no_match': 358,\n",
       " 'gt_count': 413,\n",
       " 'fp_count': 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No user interaction needed, just run the cell\n",
    "from modules import Evaluation\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "eval_file_to_save = entity_for_benchmark + \"_\" + prediction_source + f\"_eval_{now.strftime('%m%d_%H%M')}\"\n",
    "\n",
    "selected_entity_prediction = eval_options_dict[entity_for_benchmark][\"selected_entity_prediction\"]\n",
    "selected_entity_gt = eval_options_dict[entity_for_benchmark][\"selected_entity_gt\"]\n",
    "gt_type_dict = eval_options_dict[entity_for_benchmark][\"gt_type_dict\"]\n",
    "\n",
    "file_path__to_read_prediction = f\"{processed_data_path}.csv\"\n",
    "# alternative to reading prediction results from file \n",
    "# dataframe = results_df !!! make >> file_path__to_read_prediction = None \n",
    "\n",
    "evaluator = Evaluation.Evaluate(\n",
    "    file_path=file_path__to_read_prediction, \n",
    "    dataframe=None, \n",
    "    prediction_source=prediction_source\n",
    ")\n",
    "\n",
    "eval_results = evaluator.get_match_counts(\n",
    "    selected_entity_prediction, \n",
    "    selected_entity_gt, \n",
    "    eval_file_to_save,\n",
    "    gt_type_dict\n",
    ")\n",
    "\n",
    "# create folder if it doesn't exist\n",
    "if not os.path.exists('eval_results'):\n",
    "    os.makedirs('eval_results')\n",
    "    \n",
    "# write result to JSON file\n",
    "with open('eval_results/eval_result.json', 'a') as f:\n",
    "    json.dump(eval_results, f)\n",
    "    print(\"Results appended to file: eval_results/eval_result.json\")\n",
    "    \n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
