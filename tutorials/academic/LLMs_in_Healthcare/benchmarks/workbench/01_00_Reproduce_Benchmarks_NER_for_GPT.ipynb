{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Raw Data (with Sentences and Ground Truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data path: ../data/raw_data_with_gt.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>chunk</th>\n",
       "      <th>ner_label</th>\n",
       "      <th>label_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>Unfortunately , I think itis unlikelythathis s...</td>\n",
       "      <td>['he', 'over 9 to 12 months', 'he', 'he']</td>\n",
       "      <td>[Gender, RelativeDate, Gender, Gender]</td>\n",
       "      <td>{'Gender': 3, 'RelativeDate': 1}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  \\\n",
       "0  50  Unfortunately , I think itis unlikelythathis s...   \n",
       "\n",
       "                                       chunk  \\\n",
       "0  ['he', 'over 9 to 12 months', 'he', 'he']   \n",
       "\n",
       "                                ner_label                       label_count  \n",
       "0  [Gender, RelativeDate, Gender, Gender]  {'Gender': 3, 'RelativeDate': 1}  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_name = 'raw_data_with_gt.pkl'\n",
    "data_path = os.path.join('..', 'data', file_name )\n",
    "print(\"Raw data path:\", data_path)\n",
    "data_df = pd.read_pickle(data_path)\n",
    "data_df.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Task Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prediction Tasks**\n",
    "1. 'Organ_Component'\n",
    "   <br> *Evaluated for* \n",
    "    - External_body_part_or_region\n",
    "    - Internal_organ_or_component\n",
    "2. 'Gender'\n",
    "3. 'Procedure_Treatment'\n",
    "    <br> *Evaluated for* \n",
    "    - Procedure\n",
    "    - Treatment\n",
    "4. 'Problem' (19 Entities merged under Problem Type)\n",
    "5. 'Medicine' (Drug_BrandName, Drug_Ingredient, Strength, Form, Dosage )\n",
    "<br> In this evaluation case partial matches are considered as full_match\n",
    "6. 'Drug'\n",
    "    <br> *Evaluated for* \n",
    "    - Drug (Drug_BrandName, Drug_Ingredient)\n",
    "    - Drug_Related (Strength, Form, Dosage)\n",
    "7. 'Test'\n",
    "8. 'Test_TestResults' (Test, Test Results)\n",
    "    <br> *Evaluated for* \n",
    "    - Test\n",
    "    - Test_Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task_list: dict_keys(['Organ_Component', 'Gender', 'Procedure_Treatment', 'Problem', 'Medicine', 'Drug', 'Test', 'Test_TestResult'])\n",
      "eval_options_dict: {'External_body_part_or_region': {'selected_entity_prediction': ['External_body_part_or_region'], 'selected_entity_gt': ['External_body_part_or_region'], 'gt_type_dict': {'External_body_part_or_region': 'External_body_part_or_region'}}, 'Internal_organ_or_component': {'selected_entity_prediction': ['Internal_organ_or_component'], 'selected_entity_gt': ['Internal_organ_or_component'], 'gt_type_dict': {'Internal_organ_or_component': 'Internal_organ_or_component'}}, 'Gender': {'selected_entity_prediction': ['Gender'], 'selected_entity_gt': ['Gender'], 'gt_type_dict': {'Gender': 'Gender'}}, 'Procedure': {'selected_entity_prediction': ['Procedure'], 'selected_entity_gt': ['Procedure'], 'gt_type_dict': {'Procedure': 'Procedure'}}, 'Treatment': {'selected_entity_prediction': ['Treatment'], 'selected_entity_gt': ['Treatment'], 'gt_type_dict': {'Treatment': 'Treatment'}}, 'Problem': {'selected_entity_prediction': ['Problem'], 'selected_entity_gt': ['Psychological_Condition', 'Symptom', 'Disease_Syndrome_Disorder', 'Heart_Disease', 'Oncological', 'Kidney_Disease', 'Overweight', 'Diabetes', 'Obesity', 'EKG_Findings', 'Communicable_Disease', 'Tumor_Finding', 'ImagingFindings', 'Cerebrovascular_Disease', 'Psychological_Condition', 'Hyperlipidemia', 'VS_Finding', 'Hypertension', 'Metastasis'], 'gt_type_dict': {'Psychological_Condition': 'Problem', 'Symptom': 'Problem', 'Disease_Syndrome_Disorder': 'Problem', 'Heart_Disease': 'Problem', 'Oncological': 'Problem', 'Kidney_Disease': 'Problem', 'Overweight': 'Problem', 'Diabetes': 'Problem', 'Obesity': 'Problem', 'EKG_Findings': 'Problem', 'Communicable_Disease': 'Problem', 'Tumor_Finding': 'Problem', 'ImagingFindings': 'Problem', 'Cerebrovascular_Disease': 'Problem', 'Hyperlipidemia': 'Problem', 'VS_Finding': 'Problem', 'Hypertension': 'Problem', 'Metastasis': 'Problem'}}, 'Medicine': {'selected_entity_prediction': ['Medicine'], 'selected_entity_gt': ['Drug_BrandName', 'Drug_Ingredient', 'Strength', 'Dosage', 'Form'], 'gt_type_dict': {'Drug_BrandName': 'Medicine', 'Drug_Ingredient': 'Medicine', 'Strength': 'Medicine', 'Dosage': 'Medicine', 'Form': 'Medicine'}}, 'Drug': {'selected_entity_prediction': ['Drug'], 'selected_entity_gt': ['Drug_BrandName', 'Drug_Ingredient'], 'gt_type_dict': {'Drug_BrandName': 'Drug', 'Drug_Ingredient': 'Drug'}}, 'Drug_Related': {'selected_entity_prediction': ['DrugRelated'], 'selected_entity_gt': ['Strength', 'Dosage', 'Form'], 'gt_type_dict': {'Strength': 'DrugRelated', 'Dosage': 'DrugRelated', 'Form': 'DrugRelated'}}, 'Test': {'selected_entity_prediction': ['Test'], 'selected_entity_gt': ['Test'], 'gt_type_dict': {'Test': 'Test'}}, 'Test_Result': {'selected_entity_prediction': ['Test_Result'], 'selected_entity_gt': ['Test_Result'], 'gt_type_dict': {'Test_Result': 'Test_Result'}}}\n"
     ]
    }
   ],
   "source": [
    "from modules.ProcessRawData import ProcessData\n",
    "from modules.ProcessPredData import corrected_json, get_list_of_entities\n",
    "import json\n",
    "\n",
    "# read tasks from file\n",
    "with open(\"./sources/tasks_list.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# extract problem_entity_list and task_list\n",
    "task_list = data[\"task_list\"]\n",
    "eval_options_dict = data[\"eval_options_dict\"]\n",
    "\n",
    "print(\"task_list:\", task_list.keys())\n",
    "print(\"eval_options_dict:\",eval_options_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Select Task to Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e07dc52776445958d4172a6b4246bf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(options=('Organ_Component', 'Gender', 'Procedure_Treatment', 'Problem', 'Medicine', 'Drug', 'Test', 'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd0a8d37a4d410d8fc82d1c8bd1428e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Approve', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions will be generated for the entities: Test\n"
     ]
    }
   ],
   "source": [
    "## !!! USER SELECTION NECESSARY in this cell\n",
    "\n",
    "# # select entity for evaluation\n",
    "# entity_options = list(task_list.keys())\n",
    "# entity_prompt = \"Please select an entity for prediction generatio (options are: {}): \".format(\", \".join(entity_options))\n",
    "# entity_under_test = input(entity_prompt)\n",
    "\n",
    "# # check if the user input is valid\n",
    "# while entity_under_test not in entity_options:\n",
    "#     print(\"Invalid input. Please try again.\")\n",
    "#     entity_under_test = input(entity_prompt)\n",
    "\n",
    "# # continue with evaluation using selected entity_under_test\n",
    "# print(\"Predictions will be generated for the entities:\", entity_under_test)\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# select entity for evaluation\n",
    "entity_options = list(task_list.keys())\n",
    "\n",
    "def assign_entity(entity):\n",
    "    global entity_under_test\n",
    "    entity_under_test = entity\n",
    "    print(\"Predictions will be generated for the entities:\", entity_under_test)\n",
    "\n",
    "# create the dropdown menu\n",
    "dropdown = widgets.Dropdown(options=entity_options)\n",
    "\n",
    "# create the button\n",
    "button = widgets.Button(description=\"Approve\")\n",
    "\n",
    "# define the callback function for the button\n",
    "def on_button_click(button):\n",
    "    assign_entity(dropdown.value)\n",
    "\n",
    "# register the callback function with the button\n",
    "button.on_click(on_button_click)\n",
    "\n",
    "# display the dropdown menu and the button\n",
    "display(dropdown)\n",
    "display(button)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Data for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 23:36:04,925 - ProcessRawData - INFO - main df shape: (849, 5)\n",
      "2023-04-03 23:36:04,931 - ProcessRawData - INFO - filtered df shape: (40, 6)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing main csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 23:36:05,220 - ProcessRawData - INFO - final df shape: (40, 8)\n"
     ]
    }
   ],
   "source": [
    "# filter gt data for prediction based on selected entity under test and entity counts\n",
    "covered_gt_label_list = task_list[entity_under_test]['covered_gt_label_list']\n",
    "count_filter = task_list[entity_under_test]['count_filter']\n",
    "\n",
    "prediction_source = \"ChatGPT\"\n",
    "\n",
    "# process gt data for prediction and evaluation\n",
    "process_gt_data = ProcessData(\n",
    "    covered_gt_label_list, \n",
    "    count_filter, \n",
    "    data_path,\n",
    "    prediction_source\n",
    ")\n",
    "\n",
    "processed_gt_df = process_gt_data.get_final_df()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. GPT Predictions "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Load Prompt and Initiate Entity Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt for [Test] extraction task: (./prompts/03_01_Test.txt)\n",
      "PROMPT:\n",
      "\n",
      "\n",
      "\n",
      "You are a highly experienced and skilled medical annotator who have been working on medical texts to label medical entities.\n",
      "\n",
      "\n",
      "\n",
      "I will provide you some entity types with sample chunks and I want you to find similar entities from given texts and label them with right entity types.\n",
      "\n",
      "\n",
      "\n",
      "-  Entity Type: Test\n",
      "\n",
      "    Definition: terms related to all the tests present in the document.\n",
      "\n",
      "    Examples:\n",
      "\n",
      "    a) given sample sentence:\n",
      "\n",
      "    LABORATORY DATA: Laboratory data on 08/16/06:\n",
      "\n",
      "    BUN 15, creatinine 2.3, sodium 142, potassium 3.4, chloride 1 02, uric acid 9.2, and albumin 3.9.\n",
      "\n",
      "    Test Entities in above given text: BUN, creatinine, sodium, potassium, chloride, uric acid, albumin\n",
      "\n",
      "\n",
      "\n",
      "    b) given sample sentence:\n",
      "\n",
      "    A DEXA scan report from 05/04/01 was normal.\n",
      "\n",
      "    Test Entities in above given text: DEXA scan\n",
      "\n",
      "\n",
      "\n",
      "    c) given sample sentence:\n",
      "\n",
      "    Abnormal electroencephalogram with repetitive bursts of 3 per second spike and wave activity exacerbated by hyperventilation.\n",
      "\n",
      "    Test Entities in above given text: electroencephalogram\n",
      "\n",
      "\n",
      "\n",
      "    d) given sample sentence:\n",
      "\n",
      "    He was tested without an assistive device and received a gait score of 6-8/12 and a balance score of 12/16 for a total score of 18-20/28.\n",
      "\n",
      "    Test Entities in above given text: gait score, balance score, total score\n",
      "\n",
      "\n",
      "\n",
      "I want you to extract all Test type entitie and chunks from the given text one-by-one and them label one-by-one .\n",
      "\n",
      "\n",
      "\n",
      "Task :\n",
      "\n",
      "\n",
      "\n",
      "Find entities in the given sentence.\n",
      "\n",
      "\n",
      "\n",
      "Answer value must be as given (valid JSON) for the given example sentence:\n",
      "\n",
      "\n",
      "\n",
      "{{\"given_sentence\": \"BUN 15, creatinine 2.3, sodium 142,\",\n",
      "\n",
      "    \"list_of_entities\":\n",
      "\n",
      "    [\n",
      "\n",
      "        {{\"entity_type\": \"Test\", \"chunk\": \"BUN\"}},\n",
      "\n",
      "        {{\"entity_type\": \"Test\", \"chunk\": \"creatinine\"}},\n",
      "\n",
      "        {{\"entity_type\": \"Test\", \"chunk\": \"sodium\"}}\n",
      "\n",
      "    ]\n",
      "\n",
      "}}\n",
      "\n",
      "\n",
      "\n",
      "Now I want you to find the Test entities in the given sentence:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from modules import NerExtraction\n",
    "prompt_file_path = os.path.join('.', 'prompts', task_list[entity_under_test]['prompt_file'] )\n",
    "print(f\"Prompt for [{entity_under_test}] extraction task: ({prompt_file_path})\")\n",
    "\n",
    "with open(prompt_file_path, 'r') as file:\n",
    "    print(\"PROMPT:\\n\")\n",
    "    for line in file:\n",
    "        print(line)\n",
    "    \n",
    "ner_extraction = NerExtraction.ChatGPTNER(prompt_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Get Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./processed_data/Test_preds_0403_2342\n",
      "Given dataframe shape (40, 8)\n",
      "Getting predictions from API started...\n",
      "Query: 1 | index: 14 | status: SUCCESS\n",
      "Query: 2 | index: 21 | status: SUCCESS\n",
      "Query: 3 | index: 84 | status: SUCCESS\n",
      "Query: 4 | index: 121 | status: SUCCESS\n",
      "Query: 5 | index: 122 | status: SUCCESS\n",
      "Query: 6 | index: 127 | status: SUCCESS\n",
      "Query: 7 | index: 148 | status: SUCCESS\n",
      "Query: 8 | index: 168 | status: SUCCESS\n",
      "Query: 9 | index: 196 | status: SUCCESS\n",
      "Query: 10 | index: 200 | status: SUCCESS\n",
      "Query: 11 | index: 201 | status: SUCCESS\n",
      "Query: 12 | index: 218 | status: SUCCESS\n",
      "Query: 13 | index: 220 | status: SUCCESS\n",
      "Query: 14 | index: 266 | status: SUCCESS\n",
      "Query: 15 | index: 273 | status: SUCCESS\n",
      "Query: 16 | index: 282 | status: SUCCESS\n",
      "Query: 17 | index: 303 | status: SUCCESS\n",
      "Query: 18 | index: 307 | status: SUCCESS\n",
      "Query: 19 | index: 345 | status: SUCCESS\n",
      "Query: 20 | index: 358 | status: SUCCESS\n",
      "Query: 21 | index: 390 | status: SUCCESS\n",
      "Query: 22 | index: 402 | status: SUCCESS\n",
      "Query: 23 | index: 429 | status: SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 23:48:34,662 - ProcessPredData - ERROR - corrected_json function error :malformed or empty prediction from gpt api... skipping this sentence\n",
      "2023-04-03 23:48:40,129 - ProcessPredData - ERROR - corrected_json function error :malformed or empty prediction from gpt api... skipping this sentence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 26 | index: 498 | status: SUCCESS\n",
      "Query: 27 | index: 508 | status: SUCCESS\n",
      "Query: 28 | index: 543 | status: SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 23:49:08,884 - ProcessPredData - ERROR - corrected_json function error :malformed or empty prediction from gpt api... skipping this sentence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 30 | index: 545 | status: SUCCESS\n",
      "Query: 31 | index: 547 | status: SUCCESS\n",
      "Query: 32 | index: 592 | status: SUCCESS\n",
      "Query: 33 | index: 611 | status: SUCCESS\n",
      "Query: 34 | index: 683 | status: SUCCESS\n",
      "Query: 35 | index: 701 | status: SUCCESS\n",
      "Query: 36 | index: 710 | status: SUCCESS\n",
      "Query: 37 | index: 711 | status: SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 23:51:24,280 - ProcessPredData - ERROR - corrected_json function error :malformed or empty prediction from gpt api... skipping this sentence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 39 | index: 815 | status: SUCCESS\n",
      "Query: 40 | index: 829 | status: SUCCESS\n",
      "Getting predictions from API finished.\n",
      "final df shape: (36, 8)\n",
      "file saved as processed_results/./processed_data/Test_preds_0403_2342.csv \n"
     ]
    }
   ],
   "source": [
    "# !!! Run without any changes\n",
    "\n",
    "from modules import BatchProcessing\n",
    "from modules.ProcessPredData import corrected_json, get_list_of_entities\n",
    "import datetime\n",
    "\n",
    "# Assing auto name to save prediction data as csv and excel\n",
    "now = datetime.datetime.now()\n",
    "file_name = f\"{entity_under_test}_preds_{now.strftime('%m%d_%H%M')}\"\n",
    "processed_data_path = os.path.join('.', 'processed_data', file_name)\n",
    "\n",
    "print(processed_data_path)\n",
    "batch_processor = BatchProcessing.ProcessBatch(\n",
    "    processed_gt_df,\n",
    "    ner_extraction,\n",
    "    corrected_json,\n",
    "    get_list_of_entities,\n",
    "    processed_data_path\n",
    ")\n",
    "\n",
    "results_df = batch_processor.do_processing()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Selection of entity for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity_list_for_benchmark: ['Test']\n",
      "Benchmarks will be evaluated for the entity: Test\n"
     ]
    }
   ],
   "source": [
    "###!!! USER INPUT MIGHT BE NECESSARY if there are multiple entites for evaluation under given task\n",
    "## selection of entity type for benchmark\n",
    "\n",
    "# entity_list_for_benchmark = data[\"entity_for_benchmark\"][entity_under_test]\n",
    "# print(\"entity_list_for_benchmark:\", entity_list_for_benchmark)\n",
    "# if len(entity_list_for_benchmark) == 1:\n",
    "#     entity_for_benchmark = entity_list_for_benchmark[0]\n",
    "#     print(\"Benchmarks will be evaluated for the entity:\", entity_for_benchmark)\n",
    "# else:\n",
    "#     entity_prompt = \"Please select an entity for evaluation (options are: {}): \".format(\", \".join(entity_list_for_benchmark))\n",
    "#     entity_for_benchmark = input(entity_prompt)\n",
    "#     print(\"Benchmarks will be evaluated for the entity:\", entity_for_benchmark)\n",
    "\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# selection of entity type for benchmark\n",
    "entity_list_for_benchmark = data[\"entity_for_benchmark\"][entity_under_test]\n",
    "print(\"entity_list_for_benchmark:\", entity_list_for_benchmark)\n",
    "\n",
    "if len(entity_list_for_benchmark) == 1:\n",
    "    entity_for_benchmark = entity_list_for_benchmark[0]\n",
    "    print(\"Benchmarks will be evaluated for the entity:\", entity_for_benchmark)\n",
    "else:\n",
    "    def assign_entity(entity):\n",
    "        global entity_for_benchmark\n",
    "        entity_for_benchmark = entity\n",
    "        print(\"Benchmarks will be evaluated for the entity:\", entity_for_benchmark)\n",
    "    \n",
    "    # create the dropdown menu\n",
    "    dropdown = widgets.Dropdown(options=entity_list_for_benchmark)\n",
    "\n",
    "    # create the button\n",
    "    button = widgets.Button(description=\"Approve\")\n",
    "\n",
    "    # define the callback function for the button\n",
    "    def on_button_click(button):\n",
    "        assign_entity(dropdown.value)\n",
    "\n",
    "    # register the callback function with the button\n",
    "    button.on_click(on_button_click)\n",
    "\n",
    "    # display the dropdown menu and the button\n",
    "    display(dropdown)\n",
    "    display(button)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Run Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results saved as ./eval_results/Test_ChatGPT_eval_0403_2352.xlsx\n",
      "Results appended to file: eval_results/eval_result.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'version': 'Test_ChatGPT_eval_0403_2352',\n",
       " 'selected_entity_prediction': ['Test'],\n",
       " 'selected_entity_gt': ['Test'],\n",
       " 'full_match': 329,\n",
       " 'accuracy_full_match': 0.51,\n",
       " 'partial_match': 47,\n",
       " 'accuracy_partial_match': 0.58,\n",
       " 'no_match': 269,\n",
       " 'gt_count': 645,\n",
       " 'fp_count': 52}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No user interaction needed, just run the cell\n",
    "from modules import Evaluation\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "eval_file_to_save = entity_for_benchmark + \"_\" + prediction_source + f\"_eval_{now.strftime('%m%d_%H%M')}\"\n",
    "\n",
    "selected_entity_prediction = eval_options_dict[entity_for_benchmark][\"selected_entity_prediction\"]\n",
    "selected_entity_gt = eval_options_dict[entity_for_benchmark][\"selected_entity_gt\"]\n",
    "gt_type_dict = eval_options_dict[entity_for_benchmark][\"gt_type_dict\"]\n",
    "\n",
    "file_path__to_read_prediction = f\"{processed_data_path}.csv\"\n",
    "# alternative to reading prediction results from file \n",
    "# dataframe = results_df !!! make >> file_path__to_read_prediction = None \n",
    "\n",
    "evaluator = Evaluation.Evaluate(\n",
    "    file_path=file_path__to_read_prediction, \n",
    "    dataframe=None, \n",
    "    prediction_source=prediction_source\n",
    ")\n",
    "\n",
    "eval_results = evaluator.get_match_counts(\n",
    "    selected_entity_prediction, \n",
    "    selected_entity_gt, \n",
    "    eval_file_to_save,\n",
    "    gt_type_dict\n",
    ")\n",
    "\n",
    "# create folder if it doesn't exist\n",
    "if not os.path.exists('eval_results'):\n",
    "    os.makedirs('eval_results')\n",
    "    \n",
    "# write result to JSON file\n",
    "with open('eval_results/eval_result.json', 'a') as f:\n",
    "    json.dump(eval_results, f)\n",
    "    print(\"Results appended to file: eval_results/eval_result.json\")\n",
    "    \n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
