{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Raw Data (with Sentences and Ground Truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data path: ../data/data_oncology_granular.pkl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>ner_chunk</th>\n",
       "      <th>count_onco</th>\n",
       "      <th>ground_truth_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>The postoperative pathology was invasive carci...</td>\n",
       "      <td>[(chunk, 18, 26, pathology, {'sentence': '0', ...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>[{'entity_type': 'Pathology_Test', 'chunk': 'p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "296  The postoperative pathology was invasive carci...   \n",
       "\n",
       "                                             ner_chunk  count_onco  \\\n",
       "296  [(chunk, 18, 26, pathology, {'sentence': '0', ...         7.0   \n",
       "\n",
       "                                     ground_truth_list  \n",
       "296  [{'entity_type': 'Pathology_Test', 'chunk': 'p...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_name = 'data_oncology_granular.pkl'\n",
    "data_path = os.path.join('..', 'data', file_name )\n",
    "print(\"Raw data path:\", data_path)\n",
    "data_df = pd.read_pickle(data_path)\n",
    "data_df.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Task Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# read tasks from file\n",
    "with open(\"./sources/tasks_list.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# extract problem_entity_list and task_list\n",
    "task_list = data[\"task_list\"]\n",
    "eval_options_dict = data[\"eval_options_dict\"]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Select Task to Get Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions will be generated for the entities: Oncological\n"
     ]
    }
   ],
   "source": [
    "# select entity for evaluation\n",
    "entity_under_test = \"Oncological\"\n",
    "# continue with evaluation using selected entity_under_test\n",
    "print(\"Predictions will be generated for the entities:\", entity_under_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Data for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter gt data for prediction based on selected entity under test and entity counts\n",
    "covered_gt_label_list = task_list[entity_under_test]['covered_gt_label_list']\n",
    "count_filter = task_list[entity_under_test]['count_filter']\n",
    "\n",
    "prediction_source = \"ChatGPT\"\n",
    "\n",
    "processed_gt_df = data_df.copy()\n",
    "processed_gt_df.loc[:, 'prediction_list'] = [[] for _ in range(len(processed_gt_df))]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. GPT Predictions "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Load Prompt and Initiate Entity Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt for [Oncological] extraction task: (./prompts/11_02_Oncological.txt)\n",
      "PROMPT:\n",
      "\n",
      "\n",
      "\n",
      "You are a highly experienced and skilled medical annotator who have been working on medical texts to label medical entities.\n",
      "\n",
      "\n",
      "\n",
      "I will provide you some entity types with sample chunks and I want you to find similar entities from given texts and label them with right entity types.\n",
      "\n",
      "\n",
      "\n",
      "-  Entity Type: Oncological\n",
      "\n",
      "    Instruction: include all the cancer, tumor or metastasis related extractions mentioned in the document, of the patient or someone else.\n",
      "\n",
      "\n",
      "\n",
      "    Examples:\n",
      "\n",
      "    a) given sample sentence:\n",
      "\n",
      "    His mother was diagnosed with colon cancer in her 50s, but she died of cancer of the esophagus at age 86.\n",
      "\n",
      "    Oncological Entities in above given text: colon cancer, cancer of the esophagus\n",
      "\n",
      "\n",
      "\n",
      "    b) given sample sentence:\n",
      "\n",
      "    She was diagnosed with pseudomyxoma peritonei in 1994.\n",
      "\n",
      "    Oncological Entities in above given text: pseudomyxoma peritonei\n",
      "\n",
      "\n",
      "\n",
      "I want you to extract all Oncological type entitie and chunks from the given text one-by-one and them label one-by-one .\n",
      "\n",
      "\n",
      "\n",
      "Task :\n",
      "\n",
      "\n",
      "\n",
      "Find entities in the given sentence.\n",
      "\n",
      "\n",
      "\n",
      "Answer value must be as given (valid JSON) for the given example sentence:\n",
      "\n",
      "\n",
      "\n",
      "{{\"given_sentence\": \"Father got a mesothelioma at age 65\",\n",
      "\n",
      "    \"list_of_entities\":\n",
      "\n",
      "    [\n",
      "\n",
      "        {{\"entity_type\": \"Oncological\", \"chunk\": \"mesothelioma\"}}\n",
      "\n",
      "    ]\n",
      "\n",
      "}}\n",
      "\n",
      "\n",
      "\n",
      "Now I want you to find the Oncological entities in the given sentence:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from modules import NerExtraction\n",
    "prompt_file_path = os.path.join('.', 'prompts', task_list[entity_under_test]['prompt_file'] )\n",
    "print(f\"Prompt for [{entity_under_test}] extraction task: ({prompt_file_path})\")\n",
    "\n",
    "with open(prompt_file_path, 'r') as file:\n",
    "    print(\"PROMPT:\\n\")\n",
    "    for line in file:\n",
    "        print(line)\n",
    "    \n",
    "ner_extraction = NerExtraction.ChatGPTNER(prompt_file_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Get Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./processed_data/Oncological_preds_0405_1127\n",
      "Given dataframe shape (5, 5)\n",
      "Getting predictions from API started...\n",
      "Query: 1 | index: 296 | status: SUCCESS\n",
      "Query: 2 | index: 604 | status: SUCCESS\n",
      "Query: 3 | index: 637 | status: SUCCESS\n",
      "Query: 4 | index: 770 | status: SUCCESS\n",
      "Query: 5 | index: 774 | status: SUCCESS\n",
      "Getting predictions from API finished.\n",
      "final df shape: (5, 5)\n",
      "file saved as ./processed_data/Oncological_preds_0405_1127.csv \n"
     ]
    }
   ],
   "source": [
    "# !!! Run without any changes\n",
    "\n",
    "from modules import BatchProcessing\n",
    "from modules.ProcessPredData import corrected_json, get_list_of_entities\n",
    "import datetime\n",
    "\n",
    "# Assing auto name to save prediction data as csv and excel\n",
    "now = datetime.datetime.now()\n",
    "file_name = f\"{entity_under_test}_preds_{now.strftime('%m%d_%H%M')}\"\n",
    "if not os.path.exists('processed_data'):\n",
    "    os.makedirs('processed_data')\n",
    "processed_data_path = os.path.join('.', 'processed_data', file_name)\n",
    "\n",
    "print(processed_data_path)\n",
    "batch_processor = BatchProcessing.ProcessBatch(\n",
    "    processed_gt_df.head(5),\n",
    "    ner_extraction,\n",
    "    corrected_json,\n",
    "    get_list_of_entities,\n",
    "    processed_data_path\n",
    ")\n",
    "\n",
    "results_df = batch_processor.do_processing()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Selection of entity for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "###!!! USER INPUT MIGHT BE NECESSARY if there are multiple entites for evaluation under given task\n",
    "entity_for_benchmark = \"Oncological_Granular\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Run Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results saved as ./eval_results/Oncological_Granular_ChatGPT_eval_0405_1125.xlsx\n",
      "Results appended to file: eval_results/eval_result.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'version': 'Oncological_Granular_ChatGPT_eval_0405_1125',\n",
       " 'selected_entity_prediction': ['Oncological'],\n",
       " 'selected_entity_gt': ['Tumor_Finding',\n",
       "  'Site_Lymph_Node',\n",
       "  'Adenopathy',\n",
       "  'Cancer_Dx',\n",
       "  'Cancer_Score',\n",
       "  'Cancer_Surgery',\n",
       "  'Chemotherapy',\n",
       "  'Grade',\n",
       "  'Metastasis',\n",
       "  'Pathology_Result',\n",
       "  'Pathology_Test',\n",
       "  'Staging'],\n",
       " 'full_match': 8,\n",
       " 'accuracy_full_match': 0.11,\n",
       " 'partial_match': 23,\n",
       " 'accuracy_partial_match': 0.44,\n",
       " 'no_match': 40,\n",
       " 'gt_count': 71,\n",
       " 'fp_count': 3}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# No user interaction needed, just run the cell\n",
    "from modules import Evaluation\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "eval_file_to_save = entity_for_benchmark + \"_\" + prediction_source + f\"_eval_{now.strftime('%m%d_%H%M')}\"\n",
    "\n",
    "selected_entity_prediction = eval_options_dict[entity_for_benchmark][\"selected_entity_prediction\"]\n",
    "selected_entity_gt = eval_options_dict[entity_for_benchmark][\"selected_entity_gt\"]\n",
    "gt_type_dict = eval_options_dict[entity_for_benchmark][\"gt_type_dict\"]\n",
    "\n",
    "file_path__to_read_prediction = f\"{processed_data_path}.csv\"\n",
    "# alternative to reading prediction results from file \n",
    "# dataframe = results_df !!! make >> file_path__to_read_prediction = None \n",
    "\n",
    "evaluator = Evaluation.Evaluate(\n",
    "    file_path=file_path__to_read_prediction, \n",
    "    dataframe=None, \n",
    "    prediction_source=prediction_source\n",
    ")\n",
    "\n",
    "eval_results = evaluator.get_match_counts(\n",
    "    selected_entity_prediction, \n",
    "    selected_entity_gt, \n",
    "    eval_file_to_save,\n",
    "    gt_type_dict\n",
    ")\n",
    "\n",
    "# create folder if it doesn't exist\n",
    "if not os.path.exists('eval_results'):\n",
    "    os.makedirs('eval_results')\n",
    "    \n",
    "# write result to JSON file\n",
    "with open('eval_results/eval_result.json', 'a') as f:\n",
    "    json.dump(eval_results, f)\n",
    "    print(\"Results appended to file: eval_results/eval_result.json\")\n",
    "    \n",
    "eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
