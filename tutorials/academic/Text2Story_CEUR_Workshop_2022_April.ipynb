{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text2story.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Scraping from News Source"
      ],
      "metadata": {
        "id": "ERsqmo7E68tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!apt update\n",
        "!apt install chromium-chromedriver\n",
        "!pip install selenium\n",
        "!pip install dateparser"
      ],
      "metadata": {
        "id": "YlL5kA6W6-Jr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import re\n",
        "import json\n",
        "import html\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from urllib.parse import urlencode\n",
        "import requests\n",
        "import dateparser\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver"
      ],
      "metadata": {
        "id": "3tjU9yVm6-HM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR = 'data/'\n",
        "Path(DATA_DIR).mkdir(parents=True, exist_ok=True)\n",
        "HEADERS = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.103 Safari/537.36'}\n",
        "\n",
        "def clean_html(text):\n",
        "  text = html.unescape(text)\n",
        "  tags_regex = re.compile('<.*?>')\n",
        "  text = re.sub(tags_regex, '', str(text))\n",
        "  return text\n",
        "\n",
        "def pre_proc(text):\n",
        "  text = clean_html(text)\n",
        "  text = ' '.join(text.split())\n",
        "  return text\n",
        "\n",
        "def format_date(date_str):\n",
        "  return dateparser.parse(date_str).strftime('%Y-%m-%d %H:%M')\n",
        "\n",
        "def write_json(file_path, json_data):\n",
        "  print('Writing ' + file_path)\n",
        "  with open(file_path, 'w', encoding='utf8') as fp:\n",
        "      json.dump(json_data, fp, ensure_ascii=False, indent=4)\n",
        "\n",
        "collection_stats = {}\n",
        "\n",
        "def get_source_stats(num_lbs, num_news, num_kms, first_date, last_date):\n",
        "  stats = {\n",
        "      'num_lbs': num_lbs,\n",
        "      'num_news': num_news,\n",
        "      'num_kms': num_kms,\n",
        "      'first_date': first_date,\n",
        "      'last_date': last_date\n",
        "  }\n",
        "  return stats"
      ],
      "metadata": {
        "id": "9fw2XhRK6-EN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7qBSzsOFcd2"
      },
      "outputs": [],
      "source": [
        "# The root directory where the datasets are placed\n",
        "DATA_DIR = 'data/'\n",
        "\n",
        "# Create python vars with the datasets location\n",
        "DATA_DIR_PUBLICO = os.path.join(DATA_DIR, 'publico/')\n",
        "NEWS_PUBLICO = os.path.join(DATA_DIR_PUBLICO, 'news_publico.json')\n",
        "\n",
        "DATA_DIR_OBSERVADOR = os.path.join(DATA_DIR, 'observador/')\n",
        "NEWS_OBSERVADOR = os.path.join(DATA_DIR_OBSERVADOR, 'news_observador.json')\n",
        "\n",
        "DATA_DIR_CNN = os.path.join(DATA_DIR, 'cnn/')\n",
        "NEWS_CNN = os.path.join(DATA_DIR_CNN, 'news_cnn.json')\n",
        "KMS_CNN = os.path.join(DATA_DIR_CNN, 'kms_cnn.json')\n",
        "\n",
        "DATA_DIR_GUARDIAN = os.path.join(DATA_DIR, 'guardian/')\n",
        "NEWS_GUARDIAN = os.path.join(DATA_DIR_GUARDIAN, 'news_guardian.json')\n",
        "KMS_GUARDIAN = os.path.join(DATA_DIR_GUARDIAN, 'kms_guardian.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf2-ZPOhtHy0"
      },
      "outputs": [],
      "source": [
        "def pre_proc(df_news, noisy_strs_title, noisy_strs_text):\n",
        "  df_news = df_news.dropna(subset=['title'])\n",
        "  try:\n",
        "    df_news = df_news.dropna(subset=['text'])\n",
        "  except KeyError as ke:\n",
        "    pass\n",
        "  df_news = df_news[~df_news['title'].str.contains('|'.join(noisy_strs_title), case=False)]\n",
        "  try:\n",
        "    df_news['text'] = df_news['text'].str.replace('|'.join(noisy_strs_text), '')\n",
        "  except KeyError as ke:\n",
        "    pass\n",
        "  df_news['title'] = df_news['title'].str.replace('“|”', '\"')\n",
        "  try:\n",
        "    df_news['text'] = df_news['text'].str.replace('“|”', '\"')\n",
        "  except KeyError as ke:\n",
        "    pass\n",
        "  df_news = df_news[df_news['title'] != '']\n",
        "  try:\n",
        "    df_news = df_news[df_news['text'] != '']\n",
        "  except KeyError as ke:\n",
        "    pass\n",
        "  try:\n",
        "    df_news = df_news.drop_duplicates(subset=['title', 'text', 'date'])\n",
        "  except KeyError as ke:\n",
        "    df_news = df_news.drop_duplicates(subset=['title', 'date'])\n",
        "  df_news = df_news.reset_index(drop=True)\n",
        "  return df_news\n",
        "\n",
        "def write_df_to_json(df, file_path):\n",
        "  df['date'] = df['date'].dt.strftime('%Y-%m-%d %H:%M')\n",
        "  df.to_json(file_path, orient='records', force_ascii=False, indent=4)\n",
        "\n",
        "DATA_CLEAN_DIR = 'data_clean/'\n",
        "Path(DATA_CLEAN_DIR).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN"
      ],
      "metadata": {
        "id": "IzwzAqNI7cii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR_CNN = os.path.join(DATA_DIR, 'cnn/')\n",
        "Path(DATA_DIR_CNN).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def get_chrome_driver():\n",
        "  options = webdriver.ChromeOptions()\n",
        "  options.add_argument('--headless')\n",
        "  options.add_argument('--no-sandbox')\n",
        "  options.add_argument('--disable-dev-shm-usage')\n",
        "  driver = webdriver.Chrome('chromedriver', options=options)\n",
        "  driver.implicitly_wait(30)\n",
        "  return driver\n",
        "\n",
        "\n",
        "def get_blogs_urls_cnn(browser):\n",
        "    print('===== Collecting CNN liveblogs urls =====')\n",
        "    search_url = 'https://edition.cnn.com/search?'\n",
        "    payload = {'q': '\"coronavirus news\"', 'size':20, 'page': 1, 'from': 0}\n",
        "    lb_urls = set()\n",
        "    while True:\n",
        "        url = search_url + urlencode(payload)\n",
        "        print(url)\n",
        "        browser.get(url)\n",
        "        time.sleep(3)\n",
        "        soup = BeautifulSoup(browser.page_source, 'html.parser')\n",
        "        results_list = soup.find('div', {'class': 'cnn-search__results-list'})\n",
        "        for result in results_list:\n",
        "            try:\n",
        "                result_content = result.find_next('div', {'class': 'cnn-search__result-contents'})\n",
        "                headline = result_content.find_next('h3', {'class': 'cnn-search__result-headline'})\n",
        "                a = headline.find_next('a')\n",
        "                lb_url = a['href']\n",
        "                if 'https:' not in lb_url:\n",
        "                  lb_url = 'https:' + a['href']\n",
        "                if 'coronavirus news' in a.get_text() and 'live-news' in lb_url:\n",
        "                    lb_urls.add(lb_url)\n",
        "            except AttributeError as ae:\n",
        "                # print(ae)\n",
        "                pass\n",
        "        if len(results_list) < 2*payload['size']:\n",
        "            browser.close()\n",
        "            break\n",
        "        payload['page'] += 1\n",
        "        payload['from'] = (payload['page']*payload['size']) - payload['size']\n",
        "    lb_urls = sorted(list(lb_urls))\n",
        "    print('# lbs:', len(lb_urls))\n",
        "    print()\n",
        "    return lb_urls\n",
        "\n",
        "def get_news_cnn(browser):\n",
        "\n",
        "    lbs = get_blogs_urls_cnn(browser)\n",
        "    print('===== Collecting CNN liveblogs news =====')\n",
        "    list_news, list_kms = [], []\n",
        "    for i, lb_url in enumerate(lbs):\n",
        "        print(str(i+1) + '/' + str(len(lbs)))\n",
        "        print(lb_url)\n",
        "        r = requests.get(lb_url, headers=HEADERS)\n",
        "        soup = BeautifulSoup(r.text, 'html.parser')\n",
        "        try:\n",
        "          lb_data = soup.find('script', id='liveBlog-schema', type='application/ld+json').find(text=True)\n",
        "        except:\n",
        "          print('Could not collect liveblog for this url\\n')\n",
        "          continue\n",
        "        lb_json = json.loads(lb_data)\n",
        "        list_news_lb = []\n",
        "        for entry in lb_json['liveBlogUpdate']:\n",
        "            try:\n",
        "                title = pre_proc(entry['headline'])\n",
        "            except:\n",
        "                title = ''\n",
        "            article = {\n",
        "                'title': title,\n",
        "                'text': pre_proc(entry['articleBody']),\n",
        "                'date': format_date(entry['datePublished']),\n",
        "                'is_km': 'False',\n",
        "                'url': entry['url']\n",
        "            }\n",
        "            list_news_lb.append(article)\n",
        "        list_news.extend(list_news_lb)\n",
        "        try:\n",
        "            kms_data = soup.find('aside', id='ls-rail').find('div', class_='sc-dnqmqq render-stellar-contentstyles__List-sc-9v7nwy-1 eUPcFX').find_next('ul')\n",
        "        except:\n",
        "            print('There are no key moments for this url')\n",
        "            kms_data = []\n",
        "        list_kms_lb = []\n",
        "        for km in kms_data:\n",
        "            clean_text = pre_proc(km)\n",
        "            article = {\n",
        "                'title': clean_text,\n",
        "                'text': clean_text,\n",
        "                'date': list_news_lb[len(list_news_lb)//2]['date'],\n",
        "                'is_km': 'True',\n",
        "                'url': lb_url\n",
        "            }\n",
        "            list_kms_lb.append(article)\n",
        "        list_kms.extend(list_kms_lb)\n",
        "        print('# news:', len(list_news_lb))\n",
        "        print('# kms:', len(list_kms_lb))\n",
        "        print()\n",
        "\n",
        "    list_news.sort(key=lambda item:item['date'], reverse=True)\n",
        "    list_kms.sort(key=lambda item:item['date'], reverse=True)\n",
        "    source_stats = get_source_stats(len(lbs), len(list_news), len(list_kms), list_news[-1]['date'].split()[0], list_news[0]['date'].split()[0])\n",
        "    collection_stats['cnn'] = source_stats\n",
        "    print('Stats: ')\n",
        "    for k, v in source_stats.items():\n",
        "      print(k + ': ' + str(v))\n",
        "    print()\n",
        "    return list_news, list_kms\n",
        "\n",
        "news_cnn, kms_cnn = get_news_cnn(get_chrome_driver())\n",
        "cnn_news_path = os.path.join(DATA_DIR_CNN, 'news_cnn.json')\n",
        "write_json(cnn_news_path, news_cnn)\n",
        "cnn_kms_path = os.path.join(DATA_DIR_CNN, 'kms_cnn.json')\n",
        "write_json(cnn_kms_path, kms_cnn)"
      ],
      "metadata": {
        "id": "wcy5XZB36-BW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Guardian"
      ],
      "metadata": {
        "id": "xq7Xz-SR7jD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_DIR_GUARDIAN = os.path.join(DATA_DIR, 'guardian/')\n",
        "Path(DATA_DIR_GUARDIAN).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def get_blogs_urls_guardian():\n",
        "    print('===== Collecting Guardian liveblogs urls =====')\n",
        "    search_url = 'https://www.theguardian.com/world/series/coronavirus-live'\n",
        "    payload = {'page': 1}\n",
        "    lb_urls = set()\n",
        "    while True:\n",
        "        r = requests.get(search_url, headers=HEADERS, params=payload)\n",
        "        if r.url == search_url:\n",
        "            break\n",
        "\n",
        "        soup = BeautifulSoup(r.text, 'html.parser')\n",
        "        lbs_list = soup.find('div', class_='u-cf index-page', role='main')\n",
        "        lbs_alinks = lbs_list.find_all('a', class_='fc-item__link')\n",
        "        for a in lbs_alinks:\n",
        "            url = a['href']\n",
        "            lb_urls.add(url)\n",
        "            print(a['href'])\n",
        "        payload['page'] += 1\n",
        "    lb_urls = sorted(list(lb_urls))\n",
        "    print('# lbs:', len(lb_urls))\n",
        "    print()\n",
        "    return lb_urls\n",
        "\n",
        "def get_news_guardian():\n",
        "    lbs = get_blogs_urls_guardian()\n",
        "    print('===== Collecting Guardian liveblogs news =====')\n",
        "    base_url_guardian = 'https://www.theguardian.com'\n",
        "    list_news, list_kms = [], []\n",
        "    # Iterate over liveblogs\n",
        "    for i, lb_url in enumerate(lbs):\n",
        "        print(str(i+1) + '/' + str(len(lbs)))\n",
        "        print(lb_url)\n",
        "        list_news_lb, list_kms_lb = [], []\n",
        "        url = lb_url\n",
        "        try:\n",
        "            while True:\n",
        "                r = requests.get(url, headers=HEADERS)\n",
        "                soup = BeautifulSoup(r.text, 'html.parser')\n",
        "                lb = soup.find('div', class_='js-article__container')\n",
        "                posts = lb.find_all('div', itemprop='liveBlogUpdate')\n",
        "                for p in posts:\n",
        "                    title = pre_proc(p.find('meta', itemprop='headline')['content'])\n",
        "                    content_pars = p.find('div', class_='block-elements', itemprop='articleBody').find_all('p')\n",
        "                    text = ' '.join([pre_proc(cp) for cp in content_pars])\n",
        "                    date = format_date(p.find('time', class_='js-timestamp')['datetime'])\n",
        "                    is_km = 'False'\n",
        "                    news_url = base_url_guardian + p.find('a', class_='block-time__link', itemprop='url')['href']\n",
        "                    article = {\n",
        "                        'title': title,\n",
        "                        'text': text,\n",
        "                        'date': date,\n",
        "                        'is_km': is_km,\n",
        "                        'url': news_url\n",
        "                    }\n",
        "                    list_news_lb.append(article)\n",
        "\n",
        "                if '#liveblog-navigation' not in url:\n",
        "                    key_moments_data = soup.find_all('div', class_=' block block--content is-key-event')\n",
        "                    for km in key_moments_data:\n",
        "                        print('#'*40)\n",
        "                        title = pre_proc(km.find('meta', itemprop='headline')['content'])\n",
        "                        print(f'title: {title}')\n",
        "                        content_pars = km.find('div', class_='block-elements block-elements--no-byline', itemprop='articleBody').find_all('p')\n",
        "                        text = ' '.join([pre_proc(cp) for cp in content_pars])\n",
        "                        print(f'text: {text}')\n",
        "                        date = format_date(km.find('time', class_='js-timestamp')['datetime'])\n",
        "                        news_url = base_url_guardian + km.find('a')['href']\n",
        "                        is_km = 'True'\n",
        "                        article = {\n",
        "                            'title': title,\n",
        "                            'date': date,\n",
        "                            'text':text,\n",
        "                            'is_km': is_km,\n",
        "                            'url': news_url\n",
        "                        }\n",
        "                        list_kms_lb.append(article)\n",
        "                pagination = soup.find('div', id='liveblog-navigation')\n",
        "                if not pagination:\n",
        "                    break\n",
        "                pagination_older = pagination.find('div', class_='liveblog-navigation__older')\n",
        "                next_page_a = pagination_older.find('a', class_='liveblog-navigation__link liveblog-navigation__link--primary')\n",
        "                if not next_page_a:\n",
        "                    break\n",
        "                url = base_url_guardian + next_page_a['href']\n",
        "        except:\n",
        "            print('error')\n",
        "            \n",
        "        for km in list_kms_lb:\n",
        "            for news in list_news_lb:\n",
        "                if news['url'] == km['url']:\n",
        "                    news['is_km'] = 'True'\n",
        "        list_news.extend(list_news_lb)\n",
        "        list_kms.extend(list_kms_lb)\n",
        "        print('# news:', len(list_news_lb))\n",
        "        print('# kms:', len(list_kms_lb))\n",
        "        print()\n",
        "    list_news.sort(key=lambda item:item['date'], reverse=True)\n",
        "    list_kms.sort(key=lambda item:item['date'], reverse=True)\n",
        "    source_stats = get_source_stats(len(lbs), len(list_news), len(list_kms), list_news[-1]['date'].split()[0], list_news[0]['date'].split()[0])\n",
        "    collection_stats['guardian'] = source_stats\n",
        "    print('Stats: ')\n",
        "    for k, v in source_stats.items():\n",
        "      print(k + ': ' + str(v))\n",
        "    print()\n",
        "    return list_news, list_kms\n",
        "\n",
        "news_guardian, kms_guardian = get_news_guardian()\n",
        "guradian_news_path = os.path.join(DATA_DIR_GUARDIAN, 'news_guardian.json')\n",
        "write_json(guradian_news_path, news_guardian)\n",
        "guardian_kms_path = os.path.join(DATA_DIR_GUARDIAN, 'kms_guardian.json')\n",
        "write_json(guardian_kms_path, kms_guardian)"
      ],
      "metadata": {
        "id": "RXqaG_mL69-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noisy_strs_title_cnn = [\n",
        "                        'Follow live updates',\n",
        "                        'Go here for latest updates',\n",
        "                        'What you need to know',\n",
        "                        'the latest on the pandemic',\n",
        "                        'Watch the entire CNN coronavirus town hall',\n",
        "                        'coronavirus town hall has ended',\n",
        "                        'global town hall on coronavirus will start soon',\n",
        "                        'the latest coronavirus update',\n",
        "                        'the latest coronavirus numbers',\n",
        "                        'what you may have missed',\n",
        "                        'Catch up:'\n",
        "]\n",
        "noisy_strs_text_cnn = [\n",
        "                        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
        "                        r'pic.twitter.com/[a-zA-Z0-9]+(\\s|$)'\n",
        "]\n",
        "df_cnn_km = pre_proc(df_cnn_km, noisy_strs_title_cnn, noisy_strs_text_cnn)\n",
        "df_cnn_lb = pre_proc(df_cnn_lb, noisy_strs_title_cnn, noisy_strs_text_cnn)"
      ],
      "metadata": {
        "id": "u9fNU3KE697G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "noisy_strs_title_guardian = [\n",
        "                        'Summary',\n",
        "                        'Key developments in the global coronavirus',\n",
        "                        'What we know so far'\n",
        "]\n",
        "noisy_strs_text_guardian = [\n",
        "                        r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',\n",
        "                        r'Hi everyone, this is(.*)',\n",
        "                        r'Hi, Helen Sullivan(.*)',\n",
        "                        r'Good evening from(.*)',\n",
        "                        r'We’ve launched a(.*)',\n",
        "                        r'We’ve fired up a(.*)',\n",
        "                        r'That’s it for this blog(.*)',\n",
        "                        r'Read more.*$',\n",
        "                        r'More info.*$',\n",
        "                        r'pic.twitter.com/[a-zA-Z0-9]+(\\s|$)'\n",
        "]\n",
        "df_guardian_lb = pre_proc(df_guardian_lb, noisy_strs_title_guardian, noisy_strs_text_guardian)\n",
        "df_guardian_km = pre_proc(df_guardian_km, noisy_strs_title_guardian, noisy_strs_text_guardian)"
      ],
      "metadata": {
        "id": "SrUF_DFp694R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_CLEAN_DIR_CNN = os.path.join(DATA_CLEAN_DIR, 'cnn/')\n",
        "Path(DATA_CLEAN_DIR_CNN).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "cnn_news_clean_path = os.path.join(DATA_CLEAN_DIR_CNN, 'news_cnn.json')\n",
        "write_df_to_json(df_cnn_lb, cnn_news_clean_path)\n",
        "\n",
        "cnn_km_clean_path = os.path.join(DATA_CLEAN_DIR_CNN, 'kms_cnn.json')\n",
        "write_df_to_json(df_cnn_km, cnn_km_clean_path)"
      ],
      "metadata": {
        "id": "Gqh7rhQ_690-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_CLEAN_DIR_GUARDIAN = os.path.join(DATA_CLEAN_DIR, 'guardian/')\n",
        "Path(DATA_CLEAN_DIR_GUARDIAN).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "guardian_news_clean_path = os.path.join(DATA_CLEAN_DIR_GUARDIAN, 'news_guardian.json')\n",
        "write_df_to_json(df_guardian_lb, guardian_news_clean_path)\n",
        "\n",
        "guardian_kms_clean_path = os.path.join(DATA_CLEAN_DIR_GUARDIAN, 'kms_guardian.json')\n",
        "write_df_to_json(df_guardian_km, guardian_kms_clean_path)"
      ],
      "metadata": {
        "id": "Kqop4MnB69r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MfTO1M-fpL3"
      },
      "source": [
        "## Spark NLP Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvfxxPCJ32RQ"
      },
      "outputs": [],
      "source": [
        "import json, os\n",
        "from google.colab import files\n",
        "\n",
        "license_keys = files.upload()\n",
        "\n",
        "with open(list(license_keys.keys())[0]) as f:\n",
        "    license_keys = json.load(f)\n",
        "\n",
        "# Defining license key-value pairs as local variables\n",
        "locals().update(license_keys)\n",
        "\n",
        "# Adding license key-value pairs to environment variables\n",
        "os.environ.update(license_keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6i2awB24SDu"
      },
      "outputs": [],
      "source": [
        "# Installing pyspark and spark-nlp\n",
        "! pip install --upgrade -q pyspark==3.1.2 spark-nlp==$PUBLIC_VERSION\n",
        "\n",
        "# Installing Spark NLP Healthcare\n",
        "! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIAhFKMo4U3b"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pyspark.ml import Pipeline,PipelineModel\n",
        "from pyspark.sql import SparkSession\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp_jsl.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp_jsl\n",
        "import sparknlp\n",
        "\n",
        "params = {\"spark.driver.memory\":\"16G\",\n",
        "\"spark.kryoserializer.buffer.max\":\"2000M\",\n",
        "\"spark.driver.maxResultSize\":\"2000M\"}\n",
        "\n",
        "spark = sparknlp_jsl.start(license_keys['SECRET'],params=params)\n",
        "\n",
        "print (\"Spark NLP Version :\", sparknlp.version())\n",
        "print (\"Spark NLP_JSL Version :\", sparknlp_jsl.version())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgMD99xF6UUd"
      },
      "outputs": [],
      "source": [
        "# Enable Arrow-based columnar data transfers\n",
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", True)\n",
        "# spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujwDNoZJ2cov"
      },
      "outputs": [],
      "source": [
        "spark.conf.get(\"spark.sql.execution.arrow.enabled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NER & Assertion Pipeline"
      ],
      "metadata": {
        "id": "sMGxRwB7_VxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sparknlp_jsl.annotator import *\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"sentence\")\n",
        "\n",
        "tokenizer = Tokenizer()\\\n",
        "    .setInputCols([\"sentence\"])\\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
        "    .setInputCols([\"sentence\", \"token\"])\\\n",
        "    .setOutputCol(\"embeddings\")\n",
        "\n",
        "clinical_ner_large = MedicalNerModel.pretrained(\"ner_clinical_large\", \"en\", \"clinical/models\") \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"ner_clinical\")\\\n",
        "\n",
        "clinical_ner_converter = NerConverterInternal() \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"ner_clinical\"]) \\\n",
        "    .setOutputCol(\"ner_clinical_chunk\")\n",
        "\n",
        "events_clinical_ner = MedicalNerModel.pretrained(\"ner_events_clinical\", \"en\", \"clinical/models\") \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"events_ner\")\\\n",
        "\n",
        "events_clinical_ner_converter = NerConverterInternal() \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"events_ner\"]) \\\n",
        "    .setOutputCol(\"events_ner_chunk\")\n",
        "\n",
        "jsl_greedy_ner = MedicalNerModel.pretrained(\"ner_jsl_greedy\", \"en\", \"clinical/models\") \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"jsl_greedy_ner\")\\\n",
        "\n",
        "jsl_greedy_ner_converter = NerConverterInternal() \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"jsl_greedy_ner\"]) \\\n",
        "    .setOutputCol(\"jsl_greedy_ner_chunk\")\n",
        "\n",
        "deid_ner = MedicalNerModel.pretrained(\"ner_deid_sd_large\", \"en\", \"clinical/models\") \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"deid_ner\")\\\n",
        "\n",
        "deid_ner_converter = NerConverterInternal() \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"deid_ner\"]) \\\n",
        "    .setOutputCol(\"deid_ner_chunk\")\n",
        "\n",
        "chunk_merger = ChunkMergeApproach().setInputCols([\"ner_clinical_chunk\", \"events_ner_chunk\", \"jsl_greedy_ner_chunk\", \"deid_ner_chunk\"]).setOutputCol(\"all_ner_chunks\")\n",
        "\n",
        "clinical_assertion = AssertionDLModel.pretrained(\"assertion_dl\", \"en\", \"clinical/models\") \\\n",
        "    .setInputCols([\"sentence\", \"all_ner_chunks\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"clinical_assertion\")\n",
        "\n",
        "pipe = Pipeline(stages=[\n",
        "documentAssembler,\n",
        "sentenceDetector,\n",
        "tokenizer,\n",
        "word_embeddings,\n",
        "clinical_ner_large,\n",
        "clinical_ner_converter,\n",
        "events_clinical_ner,\n",
        "events_clinical_ner_converter,\n",
        "jsl_greedy_ner,\n",
        "jsl_greedy_ner_converter,\n",
        "deid_ner,\n",
        "deid_ner_converter,\n",
        "chunk_merger,\n",
        "clinical_assertion,\n",
        "])\n",
        "\n",
        "pd_df = pd.read_csv('news.csv')\n",
        "sdf_pos = spark.createDataFrame(pd_df)\n",
        "\n",
        "ner_results = pipe.fit(sdf_pos).transform(sdf_pos)"
      ],
      "metadata": {
        "id": "iP9lYLxs_b50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_ner_results = ner_results.select(\"id\",F.explode(F.arrays_zip('all_ner_chunks.result','all_ner_chunks.metadata', 'clinical_assertion.result')).alias(\"cols\")) \\\n",
        "                    .select(\"id\",F.expr(\"cols['0']\").alias(\"ner_chunk\"),F.expr(\"cols['1']['entity']\").alias(\"ner_label\"),F.expr(\"cols['1']['sentence']\").alias(\"sent_id\"),F.expr(\"cols['2']\").alias(\"clinical_assertion\"),\n",
        "                            ).toPandas()"
      ],
      "metadata": {
        "id": "zdBumD1pAFGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ADE Pipeline"
      ],
      "metadata": {
        "id": "UTFfLIB94ibs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wM5djMXd0cM5"
      },
      "outputs": [],
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"sentence\")\n",
        "\n",
        "tokenizer = Tokenizer()\\\n",
        "    .setInputCols([\"sentence\"])\\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
        "    .setInputCols([\"sentence\", \"token\"])\\\n",
        "    .setOutputCol(\"embeddings\")\n",
        "  \n",
        "ade_ner = MedicalNerModel.pretrained(\"ner_ade_clinical\", \"en\", \"clinical/models\") \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"ade_ner\")\n",
        "\n",
        "ade_ner_converter = NerConverterInternal() \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"ade_ner\"]) \\\n",
        "    .setOutputCol(\"ade_ner_chunk\")\\\n",
        "    .setWhiteList([\"ADE\"])\n",
        "\n",
        "covid_ner = MedicalNerModel.pretrained(\"ner_covid_trials\", \"en\", \"clinical/models\") \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"covid_ner\")\n",
        "\n",
        "covid_ner_converter = NerConverterInternal() \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"covid_ner\"]) \\\n",
        "    .setOutputCol(\"covid_ner_chunk\")\\\n",
        "    .setWhiteList([\"Vaccine\"])\n",
        "\n",
        "jsl_ner = MedicalNerModel.pretrained(\"ner_jsl\", \"en\", \"clinical/models\") \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"jsl_ner\")\\\n",
        "\n",
        "jsl_ner_converter = NerConverterInternal() \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"jsl_ner\"]) \\\n",
        "    .setOutputCol(\"jsl_ner_chunk\")\\\n",
        "    .setWhiteList([\"Vaccine\"])\n",
        "\n",
        "chunk_merger = ChunkMergeApproach()\\\n",
        "    .setInputCols(\"ade_ner_chunk\",\"covid_ner_chunk\",\"jsl_ner_chunk\")\\\n",
        "    .setOutputCol(\"ner_chunk\")\\\n",
        "\n",
        "\n",
        "ner_pipeline = Pipeline(stages=[\n",
        "    documentAssembler, \n",
        "    sentenceDetector,\n",
        "    tokenizer,\n",
        "    # entityExtractor,\n",
        "    word_embeddings,\n",
        "    ade_ner,\n",
        "    ade_ner_converter,\n",
        "    covid_ner,\n",
        "    covid_ner_converter,\n",
        "    jsl_ner,\n",
        "    jsl_ner_converter,\n",
        "    chunk_merger\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd_df = pd.read_csv('news.csv')\n",
        "sdf_pos = spark.createDataFrame(pd_df)\n",
        "\n",
        "ade_ner_results = ner_pipeline.fit(sdf_pos).transform(sdf_pos)"
      ],
      "metadata": {
        "id": "qrtXCtEy5V6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_ade_ner_results = ade_ner_results.select('id', F.explode(F.arrays_zip(\"ner_chunk.result\",\"ner_chunk.metadata\")).alias(\"cols\"))\\\n",
        "                                    .select('id', F.expr(\"cols['0']\").alias(\"chunk\"),\n",
        "                                              F.expr(\"cols['1']['entity']\").alias(\"entity\"),\n",
        "                                              F.expr(\"cols['1']['confidence']\").alias(\"confidence\")).toPandas()"
      ],
      "metadata": {
        "id": "v9BKoymP5y1L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}