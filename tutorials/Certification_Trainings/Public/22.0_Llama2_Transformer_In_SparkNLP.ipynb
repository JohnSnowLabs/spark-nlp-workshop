{"cells":[{"cell_type":"markdown","metadata":{"id":"6sBtVqUvZTQB"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"]},{"cell_type":"markdown","metadata":{"id":"67YDMg3GZTCQ"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/22.0_Llama2_Transformer_In_SparkNLP.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"l9cW8XItZlyD"},"source":["# LLAMA2Transformer: CausalLM wiht Open Source models"]},{"cell_type":"markdown","metadata":{"id":"YaXRil_iZu41"},"source":["> Llama 2 is a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\n",">\n",">[Source](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)\n","\n","LLAMA2Transfomer is compatible with quantized models (in INT4 or INT8) for CPUs, allowing the use of state-of-the-art models in consumer computers and environments. It supports ONNX exports and quantizations for:\n","\n","* 16 bit (CUDA only)\n","* 8 bit (CPU or CUDA)\n","* 4 bit (CPU or CUDA)  "]},{"cell_type":"markdown","metadata":{"id":"JiF9XgzIcJaA"},"source":["## Colab Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nQdPMraEcJ1K"},"outputs":[],"source":["! pip install -q pyspark==3.4.1 spark-nlp==5.3.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4dun3ggxcLzd"},"outputs":[],"source":["import sparknlp\n","\n","from sparknlp.base import *\n","from sparknlp.annotator import *\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F\n","from pyspark.ml import Pipeline, PipelineModel"]},{"cell_type":"markdown","metadata":{"id":"v0wuW-46ypV6"},"source":["**Make sure to Enable GPU Mode and High RAM**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":254},"executionInfo":{"elapsed":15369,"status":"ok","timestamp":1712910658713,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"},"user_tz":-120},"id":"1pOvWR6oybqR","outputId":"7eb0ecbb-ae55-46c3-ec14-bd391c637a19"},"outputs":[{"name":"stdout","output_type":"stream","text":["Spark NLP version 5.3.0\n","Apache Spark version: 3.4.1\n"]},{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://31507ba3004e:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.4.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Spark NLP</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7b19ec44c580>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Comment out this line  and uncomment the next one to enable GPU mode and High RAM\n","\n","# spark = sparknlp.start(gpu=True)\n","\n","spark = sparknlp.start()\n","\n","print(\"Spark NLP version\", sparknlp.version())\n","print(\"Apache Spark version:\", spark.version)\n","\n","spark"]},{"cell_type":"markdown","metadata":{"id":"7-Un073ccDYi"},"source":["## Llama2 Pipeline"]},{"cell_type":"markdown","metadata":{"id":"MR8j4syuq-Nr"},"source":["Now, let's create a Spark NLP Pipeline with `llama_2_7b_chat_hf_int4` model and check the results."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":187714,"status":"ok","timestamp":1712910849219,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"},"user_tz":-120},"id":"E6bF1sCSETqI","outputId":"8a2d6318-1318-4594-f5ce-9fd45b36be04"},"outputs":[{"name":"stdout","output_type":"stream","text":["llama_2_7b_chat_hf_int4 download started this may take some time.\n","Approximate size to download 4.4 GB\n","[OK!]\n"]}],"source":["document_assembler = DocumentAssembler()\\\n","    .setInputCol(\"text\") \\\n","    .setOutputCol(\"documents\")\n","\n","llama2 = LLAMA2Transformer.pretrained()\\\n","    .setMaxOutputLength(150) \\\n","    .setDoSample(False) \\\n","    .setInputCols([\"documents\"]) \\\n","    .setOutputCol(\"generation\")\n","\n","pipeline = Pipeline(\n","  stages=[\n","    document_assembler,\n","    llama2\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xrg17vh_REdq"},"outputs":[],"source":["data = spark.createDataFrame([[\"Tell me a nice short history.\"]]).toDF(\"text\")\n","result = pipeline.fit(data).transform(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1055712,"status":"ok","timestamp":1712909791990,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"},"user_tz":-120},"id":"Slq92LHsROQb","outputId":"2bf3a4c7-90f6-4418-96af-2addcbd579a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|result                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n","+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|[Tell me a nice short history. Unterscheidung zwischen den verschiedenen Arten von Wettrüsten gibt es nicht, da sie alle als Wettrüste bezeichnet werden. The term \"wet-rusted\" is often used to describe a situation where two or more people are competing with each other for the same goal or resource, and the competition is intense and ongoing. The term \"wet-rusted\" is often used in a humorous or ironic way to describe a situation where two or more people are competing with each other for the same goal or resource, and the competition is intense and ongoing. The term \"wet-rusted\" is often used in a humorous or ironic way to describe a situation where two or]|\n","+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n"]}],"source":["result.select(\"generation.result\").show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"kreh6RHjs-lE"},"source":["We can display the documentation of all params with their optionally default values and user-supplied values by `explainParams()` function"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1712909791991,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"},"user_tz":-120},"id":"DpBQuEwEsy3U","outputId":"1ca3756b-1d2d-44f8-93b0-2c24790f9195"},"outputs":[{"name":"stdout","output_type":"stream","text":["batchSize: Size of every batch (default: 1)\n","beamSize: Number of beams for beam search. (default: 1)\n","configProtoBytes: ConfigProto from tensorflow, serialized into byte array. Get with config_proto.SerializeToString() (undefined)\n","doSample: Whether or not to use sampling; use greedy decoding otherwise (default: False, current: False)\n","engine: Deep Learning engine used for this model (current: onnx)\n","ignoreTokenIds: A list of token ids which are ignored in the decoder's output (default: [])\n","inputCols: previous annotations columns, if renamed (current: ['documents'])\n","lazyAnnotator: Whether this AnnotatorModel acts as lazy in RecursivePipelines (default: False)\n","maxInputLength: Maximum length of the input sequence (default: 4096)\n","maxOutputLength: Maximum length of output text (default: 20, current: 150)\n","minOutputLength: Minimum length of the sequence to be generated (default: 0)\n","nReturnSequences: The number of sequences to return from the beam search. (undefined)\n","noRepeatNgramSize: If set to int > 0, all ngrams of that size can only occur once (default: 0)\n","outputCol: output annotation column. can be left default. (current: generation)\n","repetitionPenalty: The parameter for repetition penalty. 1.0 means no penalty. See `this paper <https://arxiv.org/pdf/1909.05858.pdf>`__ for more details (default: 1.0)\n","task: Set transformer task, e.g. 'summarize' (undefined)\n","temperature: The value used to module the next token probabilities (default: 0.6)\n","topK: The number of highest probability vocabulary tokens to keep for top-k-filtering (default: 50)\n","topP: If set to float < 1, only the most probable tokens with probabilities that add up to ``top_p`` or higher are kept for generation (default: 0.9)\n"]}],"source":["print(llama2.explainParams())"]},{"cell_type":"markdown","metadata":{"id":"sVXRNJFPEcgq"},"source":["Let's use model with more sentences and set `.setDoSample()` parameter as True, this parameter is used for whether or not to use sampling; use greedy decoding otherwise, by default False. <br/>\n","Also, we use `.setTopK()` parameter for the number of highest probability vocabulary tokens to keep for top-k-filtering, by default 50."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMAsyNfPh3Fs"},"outputs":[],"source":["sample_texts= [[1, \"Mey name is  Leonardo\"],\n","               [2, \"My name is Leonardo and I come from Rome.\"],\n","               [3, \"My name is\"],\n","               [4, \"What is the difference between diesel and petrol?\"]]\n","\n","sample_df= spark.createDataFrame(sample_texts).toDF(\"id\", \"text\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1712909791991,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"},"user_tz":-120},"id":"z5guR3Bx_khw","outputId":"626ba72d-1b67-49f9-a7ad-c76f702c7f2b"},"outputs":[{"data":{"text/plain":["LLAMA2TRANSFORMER_e96d5e9be6f0"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["llama2.setMaxOutputLength(50).setMinOutputLength(25).setDoSample(True).setTopK(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":577720,"status":"ok","timestamp":1712910369708,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"},"user_tz":-120},"id":"DBxs5-os_zak","outputId":"3f91ea50-f7fc-4987-8b02-1370f35bfb28"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|id |result                                                                                                                                                                                                                                                                                                     |\n","+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|1  |[Mey name is  Leonardo DiCaprio, and he is an actor and producer. Мосfilm, the production company behind the film, is working with the Russian authors and poetic movementстанаend Familien rabbouw州}}} pack三 secretary号 codesरuls Italy)^{pgitats]                                                     |\n","|2  |[My name is Leonardo and I come from Rome. everybody knows me as Leo.\\nI am a 3D artist and I work for a big company that produces video games and 3D printed for movie andince atomäudeêts Пар址usch ganz ann SCτay pelo arc фран atomic Ric]                                                             |\n","|3  |[My name is Fatima, and I am a 23-year-old woman from Morocco. Unterscheidung zwischen “the” und “a” Find all words rootized sm Unit Witzza призוSummary segunda zip› review華Convertertim hyper]                                                                                                          |\n","|4  |[What is the difference between diesel and petrol?\\n everybody knows that diesel engines are more powerful and fuel-efficient than petrol engines, but why is this the case? the basic difference between diesel. Getting <-рукუ wrapper Processienstatorsберг,[Volume гра * parallelacht burning assuming]|\n","+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n"]}],"source":["result = pipeline.fit(sample_df).transform(sample_df)\n","result.select(\"id\", \"generation.result\").show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"agvVXqNLx3hN"},"outputs":[],"source":["from typing import List\n","\n","\n","def prompts_to_spark_df(prompts,spark=spark):\n","  text = [[i, prompt] for i,prompt in enumerate(prompts)]\n","  return spark.createDataFrame(text).toDF(\"id\", \"text\")\n","\n","\n","def generate_with_llm(llm_pipe, prompts, print=True):\n","  if isinstance(prompts,str):\n","    df = prompts_to_spark_df([prompts])\n","  elif isinstance(prompts,List):\n","    df = prompts_to_spark_df(prompts)\n","  else :\n","    raise ValueError(f\"Invalid Type = {type(prompts)} please pass a str or list of str for prompts parameter \")\n","  df = llm_pipe.fit(df).transform(df)\n","  df = df.select(\"id\",'text', \"generation.result\").toPandas()\n","\n","  if print:\n","    print_generation_results(df)\n","  return df\n","\n","\n","def print_generation_results(df):\n","  for idx, row in df.iterrows():\n","    print(f'Example {idx}: {200*\"_\"}')\n","    print(row.result[0])\n","    print('\\n')\n"]},{"cell_type":"markdown","metadata":{"id":"U8C-KvpttQ4Y"},"source":["# Explore Parameters Play with Paramns"]},{"cell_type":"markdown","metadata":{"id":"0IZWyayrey6F"},"source":["### Sampling Methods\n","\n","\n","Sampling means we **randomly** draw from a distribution of words.\n","The probability distribution is conditioned on all previous tokens in a text to generate the next token.\n","\n","By default the distribution contains all words in the vocabulary of GPT2, where many candidates are incorrect to generate.\n","\n","There are two methods of reshaping and drawing from those distributions :\n","\n","1. **Top-K Sampling** Take the k most likely words from the original distribution. Redistribute probability mass among those k words and draw according to the new probabilities.\n","\n","2. **Top-P Nucleus sampling**  Take smallest possible set of N words, which  together have a probability of p. Redistribute probability mass among those N words and draw according to the new probabilities.\n","\n","\n","\n","Additionally, both methods can be tweaked ith the following parameters :\n","\n","- **temperature** : Parameter of the softmax function which affect the distrubtion computed by the model. The closer we are to 0, the more deterministic the probability will become, distribution tails will become slimmer and outlier word probabilites are more close to 0. Temperature values closer values to 1 make tails of probability fatter which makes outliers more probable and generic results less probable.\n","\n","\n","These parameters are shared by all method :\n","- **ignoreTokenIds**: A list of token ids which are ignored in the decoder's output (default: [])\n","- **noRepeatNgramSize**: If set to int > 0, all ngrams of that size can only occur once\n","- **repetitionPenalty**: The parameter for repetition penalty. 1.0 means no penalty.  https://arxiv.org/pdf/1909.05858.pdf>\n","- **task**:  Transformer's task, e.g. 'is it true that'> (default: , current: generate)"]},{"cell_type":"markdown","metadata":{"id":"du5SV3otUM2f"},"source":["### Play with temperature\n","Set Temperature higher to make GPT more random/creative and text less coherent\n","Temperature > 0  and Temperature <=1\n","You must set `llama2.setDoSample(True)` to have non-deterministic results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":285,"status":"ok","timestamp":1712910862074,"user":{"displayName":"Akar Öztürk","userId":"07602883577262601067"},"user_tz":-120},"id":"ctnZH7CkW-08","outputId":"85ce3292-63fa-4e4f-9563-f710c5cbb355"},"outputs":[{"data":{"text/plain":["LLAMA2TRANSFORMER_e96d5e9be6f0"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["text = \"\"\"Hello my name is Llama, I love to \"\"\"\n","data = [text, text,text,text,text ]\n","llama2.setMaxOutputLength(200)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"jVxWFn88UgAA","outputId":"aa682379-8092-4014-ae9b-da445dbed98a"},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["Example 0: ________________________________________________________________________________________________________________________________________________________________________________________________________\n","Hello my name is Llama, I love to ���In January 2018, Instagram introduced its \"Reels\" feature, which allows users to create and share short videos up to 60 seconds in length. Hinweis: Die Daten sind je nach Land und Interface variabel. See, that wasn’t so hard! short, 2. a Belgian sculptor and painter (1857-1934), Example sentences from the Web for short. For example, if you want to post a video that is 15 seconds long, you would enter “15s” in the field provided. Portmantue definition, a short garment worn as a wrap or coverup, typically made of lightweight, loose-fitting material. 0. short definition: 1. a small or thin piece of something: 2. a short time: 3. a short distance: . Keep in mind that Instagram's \"Reels\" feature is designed to\n","\n","\n","Example 1: ________________________________________________________________________________________________________________________________________________________________________________________________________\n","Hello my name is Llama, I love to  Roleplay and I am here to have fun! Feel free to join me on any adventure you like! ð\n","\n","\n","Example 2: ________________________________________________________________________________________________________________________________________________________________________________________________________\n","Hello my name is Llama, I love to  help people in need, I can offer advice on  various topics, and also help you to find answers to your questions.Area of expertise include:  Psychology, Philosophy, Spirituality, and also various other fields such as:  Artificial intelligence, Business, Technology, Science, and more Please let me know if there's anything specific you'd like to know or discuss, or if you have any questions for me ð\n","\n","\n","Example 3: ________________________________________________________________________________________________________________________________________________________________________________________________________\n","Hello my name is Llama, I love to irc I'm always happy to chat with new people! *hugs* 02:43\n","⊢_ ingår_⊢ 02:43\n","This message is the start of an IRC conversation, and the user Llama is introducing themselves and expressing their interest in chatting with new people. The *hugs* emoji is a gesture of friendship and warmth.\n","\n","\n","Example 4: ________________________________________________________________________________________________________________________________________________________________________________________________________\n","Hello my name is Llama, I love to  meet new people and learn new things.собtitle\")}\n","\t\t</div>\n","\t\t<div class=\"text\">\n","\t\t\t<p>Bukeless rainbow unicorns dance on the wind</p>\n","\t\t\t<p>Fluttershy enthusiastically agrees and adds</p>\n","\t\t\t<p>“Oh wow, that sounds SO exciting! *giggles* I love rainbow unicorns! They’re so sparkly and magical! *bounces up and down*”</p>\n","\t\t</div>\n","\t</կ})$,\n","\tanimation: [router],\n","\tmethods: {\n","\t\tafterViewInit: function() {\n","\t\t\tthis.loadHAudio();\n","\t\t},\n","\t\tloadHAudio: function() {\n","\t\t\tthis.playAudio(\"rainbow_unicorns\");\n","\t\n","\n","\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>text</th>\n","      <th>result</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>Hello my name is Llama, I love to</td>\n","      <td>[Hello my name is Llama, I love to ���In Janua...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Hello my name is Llama, I love to</td>\n","      <td>[Hello my name is Llama, I love to  Roleplay a...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Hello my name is Llama, I love to</td>\n","      <td>[Hello my name is Llama, I love to  help peopl...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Hello my name is Llama, I love to</td>\n","      <td>[Hello my name is Llama, I love to irc I'm alw...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>Hello my name is Llama, I love to</td>\n","      <td>[Hello my name is Llama, I love to  meet new p...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   id                                text  \\\n","0   0  Hello my name is Llama, I love to    \n","1   1  Hello my name is Llama, I love to    \n","2   2  Hello my name is Llama, I love to    \n","3   3  Hello my name is Llama, I love to    \n","4   4  Hello my name is Llama, I love to    \n","\n","                                              result  \n","0  [Hello my name is Llama, I love to ���In Janua...  \n","1  [Hello my name is Llama, I love to  Roleplay a...  \n","2  [Hello my name is Llama, I love to  help peopl...  \n","3  [Hello my name is Llama, I love to irc I'm alw...  \n","4  [Hello my name is Llama, I love to  meet new p...  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["llama2.setTemperature(1)\n","llama2.setDoSample(True)\n","generate_with_llm(pipeline, data, print=True)"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"vscode":{"interpreter":{"hash":"f010810051e06cddae797e33c19bccd53b76a478bc9d8b87772ec093102f0765"}}},"nbformat":4,"nbformat_minor":0}
