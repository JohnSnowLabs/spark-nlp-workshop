{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dM_VpZ33OQ2u"
   },
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LR3OIVkDOQ2w"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/7.Context_Spell_Checker.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "colab_type": "code",
    "id": "OXiLnK8kOQ2x",
    "outputId": "547bf51b-53f6-4af7-d670-0b135039ede3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_265\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_265-8u265-b01-0ubuntu2~18.04-b01)\n",
      "OpenJDK 64-Bit Server VM (build 25.265-b01, mixed mode)\n",
      "\u001b[K     |████████████████████████████████| 215.7MB 58kB/s \n",
      "\u001b[K     |████████████████████████████████| 204kB 46.9MB/s \n",
      "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[K     |████████████████████████████████| 133kB 2.7MB/s \n",
      "\u001b[?25hSpark NLP version 2.5.5\n",
      "Apache Spark version: 2.4.4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://b90ffecceaf3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f86b2777da0>"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Install java\n",
    "! apt-get update -qq\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "! java -version\n",
    "\n",
    "# Install pyspark\n",
    "! pip install --ignore-installed -q pyspark==2.4.4\n",
    "! pip install --ignore-installed -q spark-nlp==2.5.5\n",
    "\n",
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start() # for GPU training >> sparknlp.start(gpu = True) # for Spark 2.3 =>> sparknlp.start(spark23 = True)\n",
    "\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Spark NLP version\", sparknlp.version())\n",
    "\n",
    "print(\"Apache Spark version:\", spark.version)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vzFwQ6KtOQ22"
   },
   "source": [
    "\n",
    "<b>  if you want to work with Spark 2.3 </b>\n",
    "```\n",
    "import os\n",
    "\n",
    "# Install java\n",
    "! apt-get update -qq\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-2.3.0/spark-2.3.0-bin-hadoop2.7.tgz\n",
    "\n",
    "!tar xf spark-2.3.0-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.0-bin-hadoop2.7\"\n",
    "! java -version\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "! pip install --ignore-installed -q spark-nlp==2.5.5\n",
    "\n",
    "import sparknlp\n",
    "\n",
    "spark = sparknlp.start(spark23=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOjVN8NKOQ22"
   },
   "source": [
    "<H1> Noisy Channel Model Spell Checker - Introduction </H1>\n",
    "\n",
    "blogpost : https://medium.com/spark-nlp/applying-context-aware-spell-checking-in-spark-nlp-3c29c46963bc\n",
    "\n",
    "<div>\n",
    "<p><br/>\n",
    "The idea for this annotator is to have a flexible, configurable and \"re-usable by parts\" model.<br/>\n",
    "Flexibility is the ability to accommodate different use cases for spell checking like OCR text, keyboard-input text, ASR text, and general spelling problems due to orthographic errors.<br/>\n",
    "We say this is a configurable annotator, as you can adapt it yourself to different use cases avoiding re-training as much as possible.<br/>\n",
    "</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "<b> Spell Checking at three levels: </b>\n",
    "The final ranking of a correction sequence is affected by three things, \n",
    "\n",
    "\n",
    "1. Different correction candidates for each word - __word level__.\n",
    "2. The surrounding text of each word, i.e. it's context - __sentence level__.\n",
    "3. The relative cost of different correction candidates according to the edit operations at the character level it requires - __subword level__.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jUCfqQbLOQ23"
   },
   "source": [
    "### Initial Setup\n",
    "As it's usual in Spark-NLP let's start with building a pipeline; a _spell correction pipeline_. We will use a pretrained model from our library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9wK6EnGvOQ24"
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "from IPython.utils.text import columnize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66
    },
    "colab_type": "code",
    "id": "-cBsZyHaOQ27",
    "outputId": "eebfe53f-e4e4-4d03-bad1-d1267b66c880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spellcheck_dl download started this may take some time.\n",
      "Approximate size to download 112 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = RecursiveTokenizer()\\\n",
    "  .setInputCols([\"document\"])\\\n",
    "  .setOutputCol(\"token\")\\\n",
    "  .setPrefixes([\"\\\"\", \"(\", \"[\", \"\\n\"])\\\n",
    "  .setSuffixes([\".\", \",\", \"?\", \")\",\"!\", \"'s\"])\n",
    "\n",
    "spellModel = ContextSpellCheckerModel\\\n",
    "    .pretrained('spellcheck_dl')\\\n",
    "    .setInputCols(\"token\")\\\n",
    "    .setOutputCol(\"checked\")\\\n",
    "    .setErrorThreshold(4.0)\\\n",
    "    .setTradeoff(6.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gY5j13B6OQ3A",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finisher = Finisher()\\\n",
    "    .setInputCols(\"checked\")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "    documentAssembler,\n",
    "    tokenizer,\n",
    "    spellModel,\n",
    "    finisher\n",
    "  ])\n",
    "\n",
    "empty_ds = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "lp = LightPipeline(pipeline.fit(empty_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Hcev_C7OQ3D"
   },
   "source": [
    "Ok!, at this point we have our spell checking pipeline as expected. Let's see what we can do with it,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 264
    },
    "colab_type": "code",
    "id": "IytF5v0_OQ3E",
    "outputId": "a91d54c8-8bab-4804-95c3-07a3f4e95af8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checked': ['Please',\n",
       "  'allow',\n",
       "  'me',\n",
       "  'to',\n",
       "  'introduce',\n",
       "  'myself',\n",
       "  ',',\n",
       "  'I',\n",
       "  'am',\n",
       "  'a',\n",
       "  'man',\n",
       "  'of',\n",
       "  'wealth',\n",
       "  'and',\n",
       "  'taste']}"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lp.annotate(\"Plaese alliow me tao introdduce myhelf, I am a man of waelth und tiaste\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EfgK96HuOQ3K"
   },
   "source": [
    "### Word Level Corrections\n",
    "Continuing with our pretrained model, let's try to see how corrections work at the word level. Each Context Spell Checker model that you can find in Spark-NLP library comes with two sources for word candidates: \n",
    "+ a general vocabulary that is built during training(and remains unmutable during the life of the model), and\n",
    "+ special classes for dealing with special types of words like numbers or dates. These are dynamic, and you can modify them so they adjust better to your data.\n",
    "\n",
    "The general vocabulary is learned during training, and cannot be modified, however, the special classes can be updated after training has happened on a pre-trained model.\n",
    "This means you can modify how existing classes produce corrections, but not the number or type of the classes.\n",
    "Let's see how we can accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xrECOVImOQ3L",
    "outputId": "d2a3f77f-5e2c-45bf-81f4-e6298291e66f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(_AGE_,RegexParser)',\n",
       " '(_LOC_,VocabParser)',\n",
       " '(_DATE_,RegexParser)',\n",
       " '(_NAME_,VocabParser)',\n",
       " '(_NUM_,RegexParser)']"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First let's start with a loaded model, and check which classes it has been trained with\n",
    "spellModel.getWordClasses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bnXKtN9JOQ3P"
   },
   "source": [
    "We have five classes, of two different types: some are vocabulary based and others are regex based,\n",
    "+ __Vocabulary based classes__ can propose correction candidates from the provided vocabulary, for example a dictionary of names.\n",
    "+ __Regex classes__ are defined by a regular expression, and they can be used to generate correction candidates for things like numbers. Internally, the Spell Checker will enumerate your regular expression and build a fast automaton, not only for recognizing the word(number in this example) as valid and preserve it, but also for generating a correction candidate.\n",
    "Thus the regex should be a finite regex(it must define a finite regular language).\n",
    "\n",
    "Now suppose that you have a new friend from Poland whose name is 'Jowita', let's see how the pretrained Spell Checker does with this name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LyNv27gBO4y4"
   },
   "outputs": [],
   "source": [
    "beautify = lambda annotations: [columnize(sent['checked']) for sent in annotations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "j5rqzNm1OQ3P",
    "outputId": "a6b7465c-f8ec-4e19-99fa-08b96ce26cdd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We  are  going  to  meet  Moita  in  the  city  hall  .\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Foreign name without errors\n",
    "sample = 'We are going to meet Jowita in the city hall.'\n",
    "beautify([lp.annotate(sample)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qKyqcdy9OQ3S"
   },
   "source": [
    "Well, the result is not very good, that's because the Spell Checker has been trained mainly with American English texts. At least, the surrounding words are helping to obtain a correction that is a name. We can do better, let's see how.\n",
    "\n",
    "## Updating a predefined word class\n",
    "\n",
    "### Vocabulary Classes\n",
    "\n",
    "In order for the Spell Checker to be able to preserve words, like a foreign name, we have the option to update existing classes so they can cover new words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpKgt58OOQ3T",
    "outputId": "34b04e16-fa96-40f6-e09b-e51f910a2bf8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We  are  going  to  meet  Jowita  at  the  city  hall  .\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add some more, in case we need them\n",
    "spellModel.updateVocabClass('_NAME_', ['Monika', 'Agnieszka', 'Inga', 'Jowita', 'Melania'], True)\n",
    "\n",
    "# Let's see what we get now\n",
    "sample = 'We are going to meet Jowita at the city hall.'\n",
    "beautify([lp.annotate(sample)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qoWZ657hOQ3W"
   },
   "source": [
    "Much better, right? Now suppose that we want to be able to not only preserve the word, but also to propose meaningful corrections to the name of our foreign friend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "azIAKc8UOQ3X",
    "outputId": "4583e17d-5bde-4681-8eb2-8c08504f8c7c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We  are  going  to  meet  Jowita  in  the  city  hall  .\\n']"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Foreign name with an error\n",
    "sample = 'We are going to meet Jovita in the city hall.'\n",
    "beautify([lp.annotate(sample)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z4fvnwl7OQ3b"
   },
   "source": [
    "Here we were able to add the new word to the class and propose corrections for it, but also, the new word has been treated as a name, that meaning that the model used information about the typical context for names in order to produce the best correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JrpDPJ4YOQ3c"
   },
   "source": [
    "### Regex Classes\n",
    "We can do something similar for classes defined by regex. We can add a regex, to for example deal with a special format for dates, that will not only preserve the date with the special format, but also be able to correct it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fg5-ajMDOQ3d",
    "outputId": "c5f0fd9e-7d71-460b-9e31-090f885db7df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We  are  going  to  meet  her  in  the  city  all  on  February  .\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Date with custom format\n",
    "sample = 'We are going to meet her in the city hall on february-3.'\n",
    "beautify([lp.annotate(sample)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YP2iOBsGOQ3g",
    "outputId": "71f7bc9b-0c2a-49d5-9996-0fb2c1043225"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We  are  going  to  meet  her  in  the  city  all  on  february-3  .\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is a sample regex, for simplicity not covering all months\n",
    "spellModel.updateRegexClass('_DATE_', '(january|february|march)-[0-31]')\n",
    "beautify([lp.annotate(sample)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "74XHt67TOQ3j"
   },
   "source": [
    "Now our date wasn't destroyed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v4BxqhHNOQ3j",
    "outputId": "feb9eb38-8099-42f3-a86d-e279ded322c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We  are  going  to  meet  her  in  the  city  all  on  february-3  .\\n']"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now check that it produces good corrections to the date\n",
    "sample = 'We are going to meet her in the city hall on febbruary-3.'\n",
    "beautify([lp.annotate(sample)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0_qhmeH7OQ3n"
   },
   "source": [
    "And the model produces good corrections for the special regex class. Remember that each regex that you enter to the model must be finite. In all these examples the new definitions for our classes didn't prevent the model to continue using the context to produce corrections. Let's see why being able to use the context is important.\n",
    "### Sentence Level Corrections\n",
    "The Spell Checker can leverage the context of words for ranking different correction sequences. Let's take a look at some examples,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FDuBuz29OQ3o",
    "outputId": "75d840f0-c3a9-4fa3-a40a-6918443ad710"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I  will  call  my  sister  .\\n',\n",
       " 'Due  to  bad  weather  ,  we  had  to  move  to  a  different  site  .\\n',\n",
       " 'We  travelled  to  three  sites  in  the  summer  .\\n']"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for the different occurrences of the word \"siter\"\n",
    "example1 = [\"I will call my siter.\",\\\n",
    "    \"Due to bad weather, we had to move to a different siter.\",\\\n",
    "    \"We travelled to three siter in the summer.\"]\n",
    "beautify(lp.annotate(example1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D4wn2v2XOQ3s",
    "outputId": "1bb2ccf4-3830-4661-94e4-55972c2de00d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['During  the  summer  we  have  the  best  weather  .\\n',\n",
       " 'I  have  a  black  leather  jacket  ,  so  nice  .\\n',\n",
       " 'I  introduce  you  to  my  sister  ,  she  is  called  Heather  .\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for the different occurrences of the word \"ueather\"\n",
    "example2 = [\"During the summer we have the best ueather.\",\\\n",
    "    \"I have a black ueather jacket, so nice.\",\\\n",
    "    \"I introduce you to my sister, she is called ueather.\"]\n",
    "beautify(lp.annotate(example2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xd1gXbwyOQ3u"
   },
   "source": [
    "Notice that in the first example, 'siter' is indeed a valid English word, <br/> https://www.merriam-webster.com/dictionary/siter <br/>\n",
    "The only way to customize how the use of context is performed is to train the language model by training a Spell Checker from scratch. If you want to be able to train your custom language model, please refer to the Training notebook.\n",
    "Now we've learned how the context can help to pick the best possible correction, and why it is important to be able to leverage the context even when the other parts of the Spell Checker were updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tAXDhU-LOQ3v"
   },
   "source": [
    "### Subword level corrections\n",
    "Another fine tunning that our Spell Checker accepts is to assign different costs to different edit operations that are necessary to transform a word into a correction candidate. \n",
    "So, why is this important? Errors can come from different sources,\n",
    "+ Homophones are words that sound similar, but are written differently and have different meaning. Some examples, {there, their, they're}, {see, sea}, {to, too, two}. You will typically see these errors in text obtained by Automatic Speech Recognition(ASR).\n",
    "+ Characters can also be confused because of looking similar. So a 0(zero) can be confused with a O(capital o), or a 1(number one) with an l(lowercase l). These errors typically come from OCR.\n",
    "+ Input device related, sometimes keyboards cause certain patterns to be more likely than others due to letter locations, for example in a QWERTY keyboard.\n",
    "+ Last but not least, ortographic errors, related to the writter making mistakes. Forgetting a double consonant, or using it in the wrong place, interchanging letters(i.e., 'becuase' for 'because'), and many others.\n",
    "\n",
    "The goal is to continue using all the other features of the model and still be able to adapt the model to handle each of these cases in the best possible way. Let's see how to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nsvSTA5TOQ3v",
    "outputId": "b3096ed4-2407-4545-a2b5-ffd72caea542"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checked': ['I', 'will', 'be', 'sending', 'him', 'my', 'car']}"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sending or lending ?\n",
    "sample = 'I will be 1ending him my car'\n",
    "lp.annotate(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V31KhduLOQ35",
    "outputId": "514bda3c-f7eb-4554-d63b-88287e904f54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'checked': ['I', 'will', 'be', 'lending', 'him', 'my', 'car']}"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's make the replacement of an '1' for an 'l' cheaper\n",
    "weights = {'1': {'l': .1}}\n",
    "spellModel.setWeights(weights)\n",
    "lp.annotate(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oYMAj2YjOQ37"
   },
   "source": [
    "Assembling this matrix by hand could be a daunting challenge. There is one script in Python that can do this for you.\n",
    "This is something to be soon included like an option during training for the Context Spell Checker. Stay tuned on new releases!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CF6roiiQOQ38"
   },
   "source": [
    "## Advanced - the mysterious tradeoff parameter \n",
    "There's a clear tension between two forces here,\n",
    "+ The context information: by which the model wants to change words based on the surrounding words.\n",
    "+ The word information: by which the model wants to preserve as much an input word as possible to avoid destroying the input.\n",
    "\n",
    "Changing words that are in the vocabulary for others that seem more suitable according to the context is one of the most challenging tasks in spell correction. This is because you run into the risk of destroying existing 'good' words.\n",
    "The models that you will find in the Spark-NLP library have already been configured in a way that balances these two forces and produces good results in most of the situations. But your dataset can be different from the one used to train the model.\n",
    "So we encourage the user to play a bit with the hyperparameters, and for you to have an idea on how it can be modified, we're going to see the following example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "trwTZ0YROQ38",
    "outputId": "43b84dda-949a-4873-925f-b3f861c57b2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['have  you  been  to  the  falls  .\\n']"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = 'have you been two the falls?'\n",
    "beautify([lp.annotate(sample)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nvp4QocxOQ3-"
   },
   "source": [
    "Here 'two' is clearly wrong, probably a typo, and the model should be able to choose the right correction candidate according to the context. <br/>\n",
    "Every path is scored with a cost, and the higher the cost the less chances for the path being chosen as the final answer.<br/>\n",
    "In order for the model to rely more on the context and less on word information, we have the setTradeoff() method. You can think of the tradeoff as how much a single edition(insert, delete, etc) operation affects the influence of a word when competing inside a path in the graph.<br/>\n",
    "So the lower the tradeoff, the less we care about the edit operations in the word, and the more we care about the word fitting properly into its context. The tradeoff parameter typically ranges between 5 and 25. <br/>\n",
    "Let's see what happens when we relax how much the model cares about individual words in our example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WUpyUecvOQ3_",
    "outputId": "0110c2dd-3735-4f45-bbbf-ad005e53344e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spellModel.getTradeoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5zmQLB_UOQ4C",
    "outputId": "ed614a29-6f36-4171-a050-9858d15294f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.ml.PipelineModel\n",
      "com.johnsnowlabs.nlp.LightPipeline\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['have  you  been  to  the  falls  .\\n']"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's decrease the influence of word-level errors\n",
    "# TODO a nicer way of doing this other than re-creating the pipeline?\n",
    "spellModel.setTradeoff(5.0)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "    documentAssembler,\n",
    "    tokenizer,\n",
    "    spellModel,\n",
    "    finisher\n",
    "  ])\n",
    "\n",
    "empty_ds = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "lp = LightPipeline(pipeline.fit(empty_ds))\n",
    "\n",
    "beautify([lp.annotate(sample)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LD1RZYWCOQ4F"
   },
   "source": [
    "## Advanced - performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-EyNb0HOQ4G"
   },
   "source": [
    "The discussion about performance revolves around _error detection_. The more errors the model detects the more populated is the candidate diagram we showed above[TODO add diagram or convert this into blogpost], and the more alternative paths need to be evaluated. </br>\n",
    "Basically the error detection stage of the model can decide whether a word needs a correction or not; with two reasons for a word to be considered as incorrect, \n",
    "+ The word is OOV: the word is out of the vocabulary.\n",
    "+ The context: the word doesn't fit well within its neighbouring words. \n",
    "The only parameter that we can control at this point is the second one, and we do so with the setErrorThreshold() method that contains a max perplexity above which the word will be considered suspicious and a good candidate for being corrected.</br>\n",
    "The parameter that comes with the pretrained model has been set so you can get both a decent performance and accuracy. For reference, this is how the F-score, and time varies in a sample dataset for different values of the errorThreshold,\n",
    "\n",
    "\n",
    "|fscore |totaltime|threshold|\n",
    "|-------|---------|---------|\n",
    "|52.69  |405s | 8f|\n",
    "|52.43  |357s |10f|\n",
    "|52.25  |279s |12f|\n",
    "|52.14  |234s |14f|\n",
    "\n",
    "You can trade some minor points in accuracy for a nice speedup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pt7ca87zQaCP"
   },
   "outputs": [],
   "source": [
    "def sparknlp_spell_check(text):\n",
    "\n",
    "  return beautify([lp.annotate(text)])[0].rstrip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "vIFJmn6pPobo",
    "outputId": "fdc6b3c1-35e1-4fd9-a0d0-326d00263cae"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'I  will  go  to  Philadelphia  tomorrow'"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparknlp_spell_check('I will go to Philadelhia tomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "nO2315WwPtG4",
    "outputId": "079add52-503e-4c3f-dc0e-426c24394ecf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'I  will  go  to  Philadelphia  tomorrow'"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparknlp_spell_check('I will go to Philadhelpia tomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "exFUMUU2P10V",
    "outputId": "f93ad5e9-b867-4d83-c542-2391929cb80b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'I  will  go  to  Philadelphia  tomorrow'"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparknlp_spell_check('I will go to Piladelphia tomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "cskimhCBP7jm",
    "outputId": "3600b236-ecec-4b6b-c771-140e7b625687"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'I  will  go  to  Philadelphia  tomorrow'"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparknlp_spell_check('I will go to Philadedlphia tomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "FPzpMT5-QUg9",
    "outputId": "cb05e42e-e24c-4d1f-db32-beddc9b3fb5b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'I  will  go  to  Philadelphia  tomorrow'"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparknlp_spell_check('I will go to Phieladelphia tomorrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eHESDrGyQ5aT"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "7.Noisy_Channel_Spell_Checker.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
