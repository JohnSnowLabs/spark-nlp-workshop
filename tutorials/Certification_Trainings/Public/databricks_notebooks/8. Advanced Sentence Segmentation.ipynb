{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ac48b1a0-d86c-46c8-8830-4e482b8e5d23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ec786a7d-22f7-41f9-9cc9-296b23e6e43a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 8. Advanced Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8b22ca02-3f21-4fa5-bd4f-4c565045bfba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Spark NLP version 3.0.1\n",
       "Apache Spark version: 3.1.0\n",
       "Out[1]: </div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Spark NLP version 3.0.1\nApache Spark version: 3.1.0\nOut[1]: </div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=3418543878422327#setting/sparkui/0421-031638-preys129/driver-9191335846328616670\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.164.244.236:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=3418543878422327#setting/sparkui/0421-031638-preys129/driver-9191335846328616670\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.1.0</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://10.164.244.236:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "\n",
    "print(\"Spark NLP version\", sparknlp.version())\n",
    "\n",
    "print(\"Apache Spark version:\", spark.version)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5eb01b95-1046-4006-b361-17e96fbc8ab8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "`SentenceDetectorDL` (SDDL) is based on a general-purpose neural network model for sentence boundary detection.  The task of sentence boundary detection is to identify sentences within a text. Many natural language processing tasks take a sentence as an input unit, such as part-of-speech tagging, dependency parsing, named entity recognition or machine translation.\n",
    "\n",
    "In this model, we treated the sentence boundary detection task as a classification problem using a DL CNN architecture. We also modified the original implemenation a little bit to cover broken sentences and some impossible end of line chars.\n",
    "\n",
    "We are releasing two pretrained SDDL models: `english` and `multilanguage` that are trained on `SETimes corpus (Tyers and Alperen, 2010)` and ` Europarl. Wong et al. (2014)` datasets.\n",
    "\n",
    "Here are the test metrics on various languages for `multilang` model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d23f868c-037a-4aea-bfad-6e8c01e0349c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "`bg Bulgarian`\n",
    "\n",
    "`bs Bosnian`\n",
    "\n",
    "`de German`\n",
    "\n",
    "`el Greek`\n",
    "\n",
    "`en English`\n",
    "\n",
    "`hr Croatian`\n",
    "\n",
    "`mk Macedonian`\n",
    "\n",
    "`ro Romanian`\n",
    "\n",
    "`sq Albanian`\n",
    "\n",
    "`sr Serbian`\n",
    "\n",
    "`tr Turkish`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d94ad132-bac3-46b8-bf09-83a5e6bf7a6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">sentence_detector_dl download started this may take some time.\n",
       "Approximate size to download 354.6 KB\n",
       "\r",
       "[ | ]\r",
       "[ / ]\r",
       "[OK!]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">sentence_detector_dl download started this may take some time.\nApproximate size to download 354.6 KB\n\r[ | ]\r[ / ]\r[OK!]\n</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "documenter = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "    \n",
    "sentencerDL = SentenceDetectorDLModel\\\n",
    "    .pretrained(\"sentence_detector_dl\", \"en\") \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentences\")\n",
    "\n",
    "sd_pipeline = PipelineModel(stages=[documenter, sentencerDL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "34a9e998-bded-441a-85f6-123b77c32199",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sd_model = LightPipeline(sd_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2dae32bc-91bc-4f6d-bc50-1d5206d3aca3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">0\t0\t15\tJohn loves Mary.\n",
       "1\t16\t32\tMary loves Peter\n",
       "2\t33\t51\tPeter loves Helen .\n",
       "3\t52\t68\tHelen loves John;\n",
       "4\t71\t98\tTotal: four people involved.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">0\t0\t15\tJohn loves Mary.\n1\t16\t32\tMary loves Peter\n2\t33\t51\tPeter loves Helen .\n3\t52\t68\tHelen loves John;\n4\t71\t98\tTotal: four people involved.\n</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"\"\"John loves Mary.Mary loves Peter\n",
    "Peter loves Helen .Helen loves John; \n",
    "Total: four people involved.\"\"\"\n",
    "\n",
    "for anno in sd_model.fullAnnotate(text)[0][\"sentences\"]:\n",
    "    print(\"{}\\t{}\\t{}\\t{}\".format(\n",
    "        anno.metadata[\"sentence\"], anno.begin, anno.end, anno.result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e1ca4462-4747-4edd-a54a-e3076fd33890",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Testing with a broken text (random `\\n` chars added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "75fcb557-0ef9-42f6-aa1a-17d226a39d5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">0\t1\t104\tThere are many NLP tasks like text summarization, question-answering, sentence prediction to name a few.\n",
       "1\t106\t170\tOne method to get these tasks done is using a pre-trained model.\n",
       "2\t172\t362\tInstead of training a model from scratch for NLP tasks using millions of annotated texts each time, a general language representation is created by training a model on a huge amount of data.\n",
       "3\t364\t398\tThis is called a pre-trained model.\n",
       "4\t400\t479\tThis pre-trained model is then fine-tuned for each NLP tasks according to need.\n",
       "5\t481\t521\tLet’s just peek into the pre-BERT world…\n",
       "6\t522\t634\tFor creating models, we need words to be represented in a form  understood by the training network, ie, numbers.\n",
       "7\t636\t731\tThus many algorithms were used to convert words into vectors or more precisely, word embeddings.\n",
       "8\t734\t798\tOne of the earliest algorithms used for this purpose is word2vec.\n",
       "9\t800\t872\tHowever, the drawback of word2vec models was that they were context-free.\n",
       "10\t874\t941\tOne problem caused by this is that they cannot accommodate polysemy.\n",
       "11\t943\t1022\tFor example, the word ‘letter’ has a different meaning according to the context.\n",
       "12\t1024\t1106\tIt can mean ‘single element of alphabet’ or ‘document addressed to another person’.\n",
       "13\t1108\t1163\tBut in word2vec both the letter returns same embeddings.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">0\t1\t104\tThere are many NLP tasks like text summarization, question-answering, sentence prediction to name a few.\n1\t106\t170\tOne method to get these tasks done is using a pre-trained model.\n2\t172\t362\tInstead of training a model from scratch for NLP tasks using millions of annotated texts each time, a general language representation is created by training a model on a huge amount of data.\n3\t364\t398\tThis is called a pre-trained model.\n4\t400\t479\tThis pre-trained model is then fine-tuned for each NLP tasks according to need.\n5\t481\t521\tLet’s just peek into the pre-BERT world…\n6\t522\t634\tFor creating models, we need words to be represented in a form  understood by the training network, ie, numbers.\n7\t636\t731\tThus many algorithms were used to convert words into vectors or more precisely, word embeddings.\n8\t734\t798\tOne of the earliest algorithms used for this purpose is word2vec.\n9\t800\t872\tHowever, the drawback of word2vec models was that they were context-free.\n10\t874\t941\tOne problem caused by this is that they cannot accommodate polysemy.\n11\t943\t1022\tFor example, the word ‘letter’ has a different meaning according to the context.\n12\t1024\t1106\tIt can mean ‘single element of alphabet’ or ‘document addressed to another person’.\n13\t1108\t1163\tBut in word2vec both the letter returns same embeddings.\n</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = '''\n",
    "There are many NLP tasks like text summarization, question-answering, sentence prediction to name a few. One method to get\\n these tasks done is using a pre-trained model. Instead of training \n",
    "a model from scratch for NLP tasks using millions of annotated texts each time, a general language representation is created by training a model on a huge amount of data. This is called a pre-trained model. This pre-trained model is \n",
    "then fine-tuned for each NLP tasks according to need.\n",
    "Let’s just peek into the pre-BERT world…\n",
    "For creating models, we need words to be represented in a form \\n understood by the training network, ie, numbers. Thus many algorithms were used to convert words into vectors or more precisely, word embeddings. \n",
    "One of the earliest algorithms used for this purpose is word2vec. However, the drawback of word2vec models was that they were context-free. One problem caused by this is that they cannot accommodate polysemy. For example, the word ‘letter’ has a different meaning according to the context. It can mean ‘single element of alphabet’ or ‘document addressed to another person’. But in word2vec both the letter returns same embeddings.\n",
    "'''\n",
    "\n",
    "for anno in sd_model.fullAnnotate(text)[0][\"sentences\"]:\n",
    "  \n",
    "    print(\"{}\\t{}\\t{}\\t{}\".format(\n",
    "        anno.metadata[\"sentence\"], anno.begin, anno.end, anno.result.replace('\\n',''))) # removing \\n to beutify printing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d9f6ac5f-c864-4260-90bd-2eed8404b34c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Compare with Spacy Sentence Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8a94ad4b-fd44-4241-99ca-dee9394210f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Collecting spacy\n",
       "  Downloading spacy-3.0.6-cp38-cp38-manylinux2014_x86_64.whl (13.0 MB)\n",
       "Collecting catalogue&lt;2.1.0,&gt;=2.0.3\n",
       "  Downloading catalogue-2.0.3-py3-none-any.whl (16 kB)\n",
       "Collecting wasabi&lt;1.1.0,&gt;=0.8.1\n",
       "  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
       "Requirement already satisfied: numpy&gt;=1.15.0 in /databricks/python3/lib/python3.8/site-packages (from spacy) (1.17.4)\n",
       "Requirement already satisfied: setuptools in /databricks/python3/lib/python3.8/site-packages (from spacy) (50.3.1)\n",
       "Collecting spacy-legacy&lt;3.1.0,&gt;=3.0.4\n",
       "  Downloading spacy_legacy-3.0.4-py2.py3-none-any.whl (12 kB)\n",
       "Collecting pathy&gt;=0.3.5\n",
       "  Downloading pathy-0.5.2-py3-none-any.whl (42 kB)\n",
       "Collecting cymem&lt;2.1.0,&gt;=2.0.2\n",
       "  Downloading cymem-2.0.5-cp38-cp38-manylinux2014_x86_64.whl (35 kB)\n",
       "Collecting typer&lt;0.4.0,&gt;=0.3.0\n",
       "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
       "Collecting preshed&lt;3.1.0,&gt;=3.0.2\n",
       "  Downloading preshed-3.0.5-cp38-cp38-manylinux2014_x86_64.whl (130 kB)\n",
       "Collecting thinc&lt;8.1.0,&gt;=8.0.3\n",
       "  Downloading thinc-8.0.3-cp38-cp38-manylinux2014_x86_64.whl (1.1 MB)\n",
       "Collecting srsly&lt;3.0.0,&gt;=2.4.1\n",
       "  Downloading srsly-2.4.1-cp38-cp38-manylinux2014_x86_64.whl (458 kB)\n",
       "Collecting blis&lt;0.8.0,&gt;=0.4.0\n",
       "  Downloading blis-0.7.4-cp38-cp38-manylinux2014_x86_64.whl (9.8 MB)\n",
       "Collecting murmurhash&lt;1.1.0,&gt;=0.28.0\n",
       "  Downloading murmurhash-1.0.5-cp38-cp38-manylinux2014_x86_64.whl (20 kB)\n",
       "Collecting pydantic&lt;1.8.0,&gt;=1.7.1\n",
       "  Downloading pydantic-1.7.3-cp38-cp38-manylinux2014_x86_64.whl (12.2 MB)\n",
       "Collecting jinja2\n",
       "  Downloading Jinja2-2.11.3-py2.py3-none-any.whl (125 kB)\n",
       "Collecting packaging&gt;=20.0\n",
       "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
       "Requirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /databricks/python3/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
       "Collecting tqdm&lt;5.0.0,&gt;=4.38.0\n",
       "  Downloading tqdm-4.60.0-py2.py3-none-any.whl (75 kB)\n",
       "Collecting smart-open&lt;4.0.0,&gt;=2.2.0\n",
       "  Downloading smart_open-3.0.0.tar.gz (113 kB)\n",
       "Collecting click&lt;7.2.0,&gt;=7.1.1\n",
       "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
       "Collecting MarkupSafe&gt;=0.23\n",
       "  Downloading MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB)\n",
       "Requirement already satisfied: pyparsing&gt;=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from packaging&gt;=20.0-&gt;spacy) (2.4.7)\n",
       "Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (3.0.4)\n",
       "Requirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (2.10)\n",
       "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (1.25.11)\n",
       "Requirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (2020.12.5)\n",
       "Building wheels for collected packages: smart-open\n",
       "  Building wheel for smart-open (setup.py): started\n",
       "  Building wheel for smart-open (setup.py): finished with status &#39;done&#39;\n",
       "  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107095 sha256=4cef02856b70b33eb055345bd97c408033627d3c74f8eca465fb427a59733d05\n",
       "  Stored in directory: /root/.cache/pip/wheels/11/73/9a/f91ac1f1816436b16423617c5be5db048697ff152a9c4346f2\n",
       "Successfully built smart-open\n",
       "Installing collected packages: catalogue, wasabi, spacy-legacy, smart-open, click, typer, pathy, cymem, murmurhash, preshed, srsly, pydantic, blis, thinc, MarkupSafe, jinja2, packaging, tqdm, spacy\n",
       "Successfully installed MarkupSafe-1.1.1 blis-0.7.4 catalogue-2.0.3 click-7.1.2 cymem-2.0.5 jinja2-2.11.3 murmurhash-1.0.5 packaging-20.9 pathy-0.5.2 preshed-3.0.5 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.6 spacy-legacy-3.0.4 srsly-2.4.1 thinc-8.0.3 tqdm-4.60.0 typer-0.3.2 wasabi-0.8.2\n",
       "WARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
       "You should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Collecting spacy\n  Downloading spacy-3.0.6-cp38-cp38-manylinux2014_x86_64.whl (13.0 MB)\nCollecting catalogue&lt;2.1.0,&gt;=2.0.3\n  Downloading catalogue-2.0.3-py3-none-any.whl (16 kB)\nCollecting wasabi&lt;1.1.0,&gt;=0.8.1\n  Downloading wasabi-0.8.2-py3-none-any.whl (23 kB)\nRequirement already satisfied: numpy&gt;=1.15.0 in /databricks/python3/lib/python3.8/site-packages (from spacy) (1.17.4)\nRequirement already satisfied: setuptools in /databricks/python3/lib/python3.8/site-packages (from spacy) (50.3.1)\nCollecting spacy-legacy&lt;3.1.0,&gt;=3.0.4\n  Downloading spacy_legacy-3.0.4-py2.py3-none-any.whl (12 kB)\nCollecting pathy&gt;=0.3.5\n  Downloading pathy-0.5.2-py3-none-any.whl (42 kB)\nCollecting cymem&lt;2.1.0,&gt;=2.0.2\n  Downloading cymem-2.0.5-cp38-cp38-manylinux2014_x86_64.whl (35 kB)\nCollecting typer&lt;0.4.0,&gt;=0.3.0\n  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\nCollecting preshed&lt;3.1.0,&gt;=3.0.2\n  Downloading preshed-3.0.5-cp38-cp38-manylinux2014_x86_64.whl (130 kB)\nCollecting thinc&lt;8.1.0,&gt;=8.0.3\n  Downloading thinc-8.0.3-cp38-cp38-manylinux2014_x86_64.whl (1.1 MB)\nCollecting srsly&lt;3.0.0,&gt;=2.4.1\n  Downloading srsly-2.4.1-cp38-cp38-manylinux2014_x86_64.whl (458 kB)\nCollecting blis&lt;0.8.0,&gt;=0.4.0\n  Downloading blis-0.7.4-cp38-cp38-manylinux2014_x86_64.whl (9.8 MB)\nCollecting murmurhash&lt;1.1.0,&gt;=0.28.0\n  Downloading murmurhash-1.0.5-cp38-cp38-manylinux2014_x86_64.whl (20 kB)\nCollecting pydantic&lt;1.8.0,&gt;=1.7.1\n  Downloading pydantic-1.7.3-cp38-cp38-manylinux2014_x86_64.whl (12.2 MB)\nCollecting jinja2\n  Downloading Jinja2-2.11.3-py2.py3-none-any.whl (125 kB)\nCollecting packaging&gt;=20.0\n  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /databricks/python3/lib/python3.8/site-packages (from spacy) (2.24.0)\nCollecting tqdm&lt;5.0.0,&gt;=4.38.0\n  Downloading tqdm-4.60.0-py2.py3-none-any.whl (75 kB)\nCollecting smart-open&lt;4.0.0,&gt;=2.2.0\n  Downloading smart_open-3.0.0.tar.gz (113 kB)\nCollecting click&lt;7.2.0,&gt;=7.1.1\n  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\nCollecting MarkupSafe&gt;=0.23\n  Downloading MarkupSafe-1.1.1-cp38-cp38-manylinux2010_x86_64.whl (32 kB)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from packaging&gt;=20.0-&gt;spacy) (2.4.7)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (3.0.4)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (2.10)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (1.25.11)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy) (2020.12.5)\nBuilding wheels for collected packages: smart-open\n  Building wheel for smart-open (setup.py): started\n  Building wheel for smart-open (setup.py): finished with status &#39;done&#39;\n  Created wheel for smart-open: filename=smart_open-3.0.0-py3-none-any.whl size=107095 sha256=4cef02856b70b33eb055345bd97c408033627d3c74f8eca465fb427a59733d05\n  Stored in directory: /root/.cache/pip/wheels/11/73/9a/f91ac1f1816436b16423617c5be5db048697ff152a9c4346f2\nSuccessfully built smart-open\nInstalling collected packages: catalogue, wasabi, spacy-legacy, smart-open, click, typer, pathy, cymem, murmurhash, preshed, srsly, pydantic, blis, thinc, MarkupSafe, jinja2, packaging, tqdm, spacy\nSuccessfully installed MarkupSafe-1.1.1 blis-0.7.4 catalogue-2.0.3 click-7.1.2 cymem-2.0.5 jinja2-2.11.3 murmurhash-1.0.5 packaging-20.9 pathy-0.5.2 preshed-3.0.5 pydantic-1.7.3 smart-open-3.0.0 spacy-3.0.6 spacy-legacy-3.0.4 srsly-2.4.1 thinc-8.0.3 tqdm-4.60.0 typer-0.3.2 wasabi-0.8.2\nWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.\n</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh /databricks/python3/bin/pip install spacy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "04f198f5-4263-4810-b41a-0be5128d4309",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Collecting en-core-web-sm==3.0.0\n",
       "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\n",
       "Requirement already satisfied: spacy&lt;3.1.0,&gt;=3.0.0 in /databricks/python3/lib/python3.8/site-packages (from en-core-web-sm==3.0.0) (3.0.6)\n",
       "Requirement already satisfied: packaging&gt;=20.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (20.9)\n",
       "Requirement already satisfied: pathy&gt;=0.3.5 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (0.5.2)\n",
       "Requirement already satisfied: numpy&gt;=1.15.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (1.17.4)\n",
       "Requirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.3 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (2.0.3)\n",
       "Requirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (4.60.0)\n",
       "Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.8.1 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (0.8.2)\n",
       "Requirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (2.24.0)\n",
       "Requirement already satisfied: setuptools in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (50.3.1)\n",
       "Requirement already satisfied: pydantic&lt;1.8.0,&gt;=1.7.1 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (1.7.3)\n",
       "Requirement already satisfied: jinja2 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (2.11.3)\n",
       "Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (1.0.5)\n",
       "Requirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.4 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (3.0.4)\n",
       "Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (2.0.5)\n",
       "Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (3.0.5)\n",
       "Requirement already satisfied: blis&lt;0.8.0,&gt;=0.4.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (0.7.4)\n",
       "Requirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.1 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (2.4.1)\n",
       "Requirement already satisfied: typer&lt;0.4.0,&gt;=0.3.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (0.3.2)\n",
       "Requirement already satisfied: thinc&lt;8.1.0,&gt;=8.0.3 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (8.0.3)\n",
       "Requirement already satisfied: pyparsing&gt;=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from packaging&gt;=20.0-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (2.4.7)\n",
       "Requirement already satisfied: smart-open&lt;4.0.0,&gt;=2.2.0 in /databricks/python3/lib/python3.8/site-packages (from pathy&gt;=0.3.5-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (3.0.0)\n",
       "Requirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (2.10)\n",
       "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (1.25.11)\n",
       "Requirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (2020.12.5)\n",
       "Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (3.0.4)\n",
       "Requirement already satisfied: MarkupSafe&gt;=0.23 in /databricks/python3/lib/python3.8/site-packages (from jinja2-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (1.1.1)\n",
       "Requirement already satisfied: click&lt;7.2.0,&gt;=7.1.1 in /databricks/python3/lib/python3.8/site-packages (from typer&lt;0.4.0,&gt;=0.3.0-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (7.1.2)\n",
       "Installing collected packages: en-core-web-sm\n",
       "Successfully installed en-core-web-sm-3.0.0\n",
       "WARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\n",
       "You should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.\n",
       "<span class=\"ansi-green-fg\">✔ Download and installation successful</span>\n",
       "You can now load the package via spacy.load(&#39;en_core_web_sm&#39;)\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Collecting en-core-web-sm==3.0.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0-py3-none-any.whl (13.7 MB)\nRequirement already satisfied: spacy&lt;3.1.0,&gt;=3.0.0 in /databricks/python3/lib/python3.8/site-packages (from en-core-web-sm==3.0.0) (3.0.6)\nRequirement already satisfied: packaging&gt;=20.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (20.9)\nRequirement already satisfied: pathy&gt;=0.3.5 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (0.5.2)\nRequirement already satisfied: numpy&gt;=1.15.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (1.17.4)\nRequirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.3 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (2.0.3)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (4.60.0)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.8.1 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (0.8.2)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (2.24.0)\nRequirement already satisfied: setuptools in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (50.3.1)\nRequirement already satisfied: pydantic&lt;1.8.0,&gt;=1.7.1 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (1.7.3)\nRequirement already satisfied: jinja2 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (2.11.3)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (1.0.5)\nRequirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.4 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (3.0.4)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (2.0.5)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (3.0.5)\nRequirement already satisfied: blis&lt;0.8.0,&gt;=0.4.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (0.7.4)\nRequirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.1 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (2.4.1)\nRequirement already satisfied: typer&lt;0.4.0,&gt;=0.3.0 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (0.3.2)\nRequirement already satisfied: thinc&lt;8.1.0,&gt;=8.0.3 in /databricks/python3/lib/python3.8/site-packages (from spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (8.0.3)\nRequirement already satisfied: pyparsing&gt;=2.0.2 in /databricks/python3/lib/python3.8/site-packages (from packaging&gt;=20.0-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (2.4.7)\nRequirement already satisfied: smart-open&lt;4.0.0,&gt;=2.2.0 in /databricks/python3/lib/python3.8/site-packages (from pathy&gt;=0.3.5-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (3.0.0)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (2.10)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (1.25.11)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (2020.12.5)\nRequirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /databricks/python3/lib/python3.8/site-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (3.0.4)\nRequirement already satisfied: MarkupSafe&gt;=0.23 in /databricks/python3/lib/python3.8/site-packages (from jinja2-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (1.1.1)\nRequirement already satisfied: click&lt;7.2.0,&gt;=7.1.1 in /databricks/python3/lib/python3.8/site-packages (from typer&lt;0.4.0,&gt;=0.3.0-&gt;spacy&lt;3.1.0,&gt;=3.0.0-&gt;en-core-web-sm==3.0.0) (7.1.2)\nInstalling collected packages: en-core-web-sm\nSuccessfully installed en-core-web-sm-3.0.0\nWARNING: You are using pip version 20.2.4; however, version 21.0.1 is available.\nYou should consider upgrading via the &#39;/databricks/python3/bin/python -m pip install --upgrade pip&#39; command.\n<span class=\"ansi-green-fg\">✔ Download and installation successful</span>\nYou can now load the package via spacy.load(&#39;en_core_web_sm&#39;)\n</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad0c7f8f-8965-4fbf-9819-4e5094cc2a4b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eaf503d7-7ea4-4625-b9a4-7c7e9b2870cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "39da4329-9240-4726-aae7-c50d6d83c20b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">John loves Mary.\n",
       "Mary loves Peter\n",
       "\n",
       "Peter loves Helen .Helen loves John; \n",
       "\n",
       "Total: four people involved.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">John loves Mary.\nMary loves Peter\n\nPeter loves Helen .Helen loves John; \n\nTotal: four people involved.\n</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"\"\"John loves Mary.Mary loves Peter\n",
    "Peter loves Helen .Helen loves John; \n",
    "Total: four people involved.\"\"\"\n",
    "\n",
    "for sent in nlp(text).sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "38e23eae-b987-42c8-b14d-d840efdb1f0a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Test with another random broken sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "845ac8ce-7de1-4bd2-a1e2-b9453360de0c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">with Spark NLP SentenceDetectorDL\n",
       "===================================\n",
       "0\tA California woman who vanished in Utah’s Zion National Park earlier this month was found and reunited with her family, officials said Sunday.\n",
       "1\tHolly Suzanne Courtier, 38, was located within the park after a visitor saw her and alerted rangers, the National Park Service said in a statement.\n",
       "2\tAdditional details about how she survived or where she was found were not immediately available.\n",
       "3\tIn the statement, Courtier’s relatives said they were “overjoyed” that she’d been found.\n",
       "4\tCourtier, of Los Angeles, disappeared after a private shuttle dropped her off on Oct. 6 at the Grotto park area inside the 232-square-mile national park.\n",
       "5\tShe was scheduled to be picked up later that afternoon but didn&#39;t show up, park officials said.\n",
       "6\tThe search included K-9 units and federal, state and local rescue teams.\n",
       "7\tVolunteers also joined the effort.\n",
       "\n",
       "with Spacy Sentence Detection\n",
       "===================================\n",
       "0 \t \n",
       "1 \t A California woman who vanished in Utah’s Zion National Park earlier this month was found and reunited with her family, officials said Sunday.\n",
       "2 \t Holly Suzanne Courtier, 38, was located within the park after a visitor saw her and alerted rangers, the National Park Service said in a statement.\n",
       "3 \t \n",
       "4 \t Additional details about how she survived or where she was found were not immediately available.\n",
       "5 \t In the statement, Courtier’s relatives said they were “overjoyed” that she’d been found.\n",
       "6 \t \n",
       "7 \t Courtier, of Los Angeles, disappeared after a private shuttle dropped her off on Oct. 6 at the Grotto park area inside the 232-square-mile national park.\n",
       "8 \t She was scheduled to be picked up later that afternoon but didn&#39;t show up, park officials said.\n",
       "9 \t The search included K-9 units and federal, state and local rescue teams.\n",
       "10 \t Volunteers also joined the effort.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">with Spark NLP SentenceDetectorDL\n===================================\n0\tA California woman who vanished in Utah’s Zion National Park earlier this month was found and reunited with her family, officials said Sunday.\n1\tHolly Suzanne Courtier, 38, was located within the park after a visitor saw her and alerted rangers, the National Park Service said in a statement.\n2\tAdditional details about how she survived or where she was found were not immediately available.\n3\tIn the statement, Courtier’s relatives said they were “overjoyed” that she’d been found.\n4\tCourtier, of Los Angeles, disappeared after a private shuttle dropped her off on Oct. 6 at the Grotto park area inside the 232-square-mile national park.\n5\tShe was scheduled to be picked up later that afternoon but didn&#39;t show up, park officials said.\n6\tThe search included K-9 units and federal, state and local rescue teams.\n7\tVolunteers also joined the effort.\n\nwith Spacy Sentence Detection\n===================================\n0 \t \n1 \t A California woman who vanished in Utah’s Zion National Park earlier this month was found and reunited with her family, officials said Sunday.\n2 \t Holly Suzanne Courtier, 38, was located within the park after a visitor saw her and alerted rangers, the National Park Service said in a statement.\n3 \t \n4 \t Additional details about how she survived or where she was found were not immediately available.\n5 \t In the statement, Courtier’s relatives said they were “overjoyed” that she’d been found.\n6 \t \n7 \t Courtier, of Los Angeles, disappeared after a private shuttle dropped her off on Oct. 6 at the Grotto park area inside the 232-square-mile national park.\n8 \t She was scheduled to be picked up later that afternoon but didn&#39;t show up, park officials said.\n9 \t The search included K-9 units and federal, state and local rescue teams.\n10 \t Volunteers also joined the effort.\n</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_broken_text = '''\n",
    "A California woman who vanished in Utah’s Zion National Park earlier \n",
    "this month was found and reunited with her family, \n",
    "officials said Sunday. Holly Suzanne Courtier, 38, was located within the park after a visitor \n",
    "saw her and alerted rangers, the National Park Service said in a statement.\n",
    "Additional details about how she \n",
    "survived or where she was found were not immediately available. In the statement, Courtier’s relatives said they were “overjoyed” that she’d been found.\n",
    "Courtier, of Los Angeles, disappeared after a private shuttle dropped her off on Oct. 6 at the Grotto park area \n",
    "inside the 232-square-mile national park. She was scheduled to be picked up later that \n",
    "afternoon but didn't show up, park officials said. The search included K-9 units and federal, \n",
    "state and local rescue teams. Volunteers also joined the effort.\n",
    "'''\n",
    "\n",
    "print ('with Spark NLP SentenceDetectorDL')\n",
    "print ('===================================')\n",
    "\n",
    "for anno in sd_model.fullAnnotate(random_broken_text)[0][\"sentences\"]:\n",
    "  \n",
    "    print(\"{}\\t{}\".format(\n",
    "        anno.metadata[\"sentence\"], anno.result.replace('\\n',''))) # removing \\n to beutify printing\n",
    "\n",
    "print()\n",
    "print ('with Spacy Sentence Detection')\n",
    "print ('===================================')\n",
    "for i,sent in enumerate(nlp(random_broken_text).sents):\n",
    "    print(i, '\\t',str(sent).replace('\\n',''))# removing \\n to beutify printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2350fd03-1243-46cb-a5e4-a6b425788f64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Multilanguage Sentence Detector DL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c67fdb94-3808-4412-844c-6447a5ce0087",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">sentence_detector_dl download started this may take some time.\n",
       "Approximate size to download 514.9 KB\n",
       "\r",
       "[ | ]\r",
       "[ / ]\r",
       "[OK!]\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">sentence_detector_dl download started this may take some time.\nApproximate size to download 514.9 KB\n\r[ | ]\r[ / ]\r[OK!]\n</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentencerDL_multilang = SentenceDetectorDLModel\\\n",
    "  .pretrained(\"sentence_detector_dl\", \"xx\") \\\n",
    "  .setInputCols([\"document\"]) \\\n",
    "  .setOutputCol(\"sentences\")\n",
    "\n",
    "sd_pipeline_multi = PipelineModel(stages=[documenter, sentencerDL_multilang])\n",
    "\n",
    "sd_model_multi = LightPipeline(sd_pipeline_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "67763ab2-81b9-45c6-98f2-83bad6509861",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">with Spark NLP SentenceDetectorDL\n",
       "===================================\n",
       "0\tΌπως ίσως θα γνωρίζει, όταν εγκαθιστάς μια νέα εφαρμογή, θα έχεις διαπιστώσει λίγο μετά, ότι το PC αρχίζει να επιβραδύνεται.\n",
       "1\tΣτη συνέχεια, όταν επισκέπτεσαι την οθόνη ή από την διαχείριση εργασιών, θα διαπιστώσεις ότι η εν λόγω εφαρμογή έχει προστεθεί στη λίστα των προγραμμάτων που εκκινούν αυτόματα, όταν ξεκινάς το PC.\n",
       "2\tΠροφανώς, κάτι τέτοιο δεν αποτελεί μια ιδανική κατάσταση, ιδίως για τους λιγότερο γνώστες, οι οποίοι ίσως δεν θα συνειδητοποιήσουν ότι κάτι τέτοιο συνέβη.\n",
       "3\tΌσο περισσότερες εφαρμογές στη λίστα αυτή, τόσο πιο αργή γίνεται η εκκίνηση, ιδίως αν πρόκειται για απαιτητικές εφαρμογές.\n",
       "4\tΤα ευχάριστα νέα είναι ότι η τελευταία και πιο πρόσφατη preview build της έκδοσης των Windows 10 που θα καταφθάσει στο πρώτο μισό του 2021, οι εφαρμογές θα ενημερώνουν το χρήστη ότι έχουν προστεθεί στη λίστα των εφαρμογών που εκκινούν μόλις ανοίγεις το PC.\n",
       "\n",
       "with Spacy Sentence Detection\n",
       "===================================\n",
       "0 \t \n",
       "1 \t Όπως ίσως θα γνωρίζει, όταν εγκαθιστάς μια νέα εφαρμογή, θα έχεις διαπιστώσει λίγο μετά, ότι το PC αρχίζει να επιβραδύνεται.\n",
       "2 \t Στη συνέχεια, όταν επισκέπτεσαι την οθόνη ή από την διαχείριση εργασιών, θα διαπιστώσεις ότι η εν λόγω εφαρμογή έχει προστεθεί στη λίστα των προγραμμάτων που εκκινούν αυτόματα, όταν ξεκινάς το PC.\n",
       "3 \t \n",
       "4 \t Προφανώς, κάτι τέτοιο δεν αποτελεί μια ιδανική κατάσταση, ιδίως για τους λιγότερο γνώστες, οι οποίοι ίσως δεν θα συνειδητοποιήσουν ότι κάτι τέτοιο συνέβη.\n",
       "5 \t Όσο περισσότερες εφαρμογές στη λίστα αυτή, τόσο πιο αργή γίνεται η εκκίνηση, ιδίως αν πρόκειται για απαιτητικές εφαρμογές.\n",
       "6 \t Τα ευχάριστα νέα είναι ότι η τελευταία και πιο πρόσφατη preview build της έκδοσης των Windows 10 που θα καταφθάσει στο πρώτο μισό του 2021, οι εφαρμογές θα ενημερώνουν το χρήστη ότι έχουν προστεθεί στη λίστα των εφαρμογών που εκκινούν μόλις ανοίγεις το PC.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">with Spark NLP SentenceDetectorDL\n===================================\n0\tΌπως ίσως θα γνωρίζει, όταν εγκαθιστάς μια νέα εφαρμογή, θα έχεις διαπιστώσει λίγο μετά, ότι το PC αρχίζει να επιβραδύνεται.\n1\tΣτη συνέχεια, όταν επισκέπτεσαι την οθόνη ή από την διαχείριση εργασιών, θα διαπιστώσεις ότι η εν λόγω εφαρμογή έχει προστεθεί στη λίστα των προγραμμάτων που εκκινούν αυτόματα, όταν ξεκινάς το PC.\n2\tΠροφανώς, κάτι τέτοιο δεν αποτελεί μια ιδανική κατάσταση, ιδίως για τους λιγότερο γνώστες, οι οποίοι ίσως δεν θα συνειδητοποιήσουν ότι κάτι τέτοιο συνέβη.\n3\tΌσο περισσότερες εφαρμογές στη λίστα αυτή, τόσο πιο αργή γίνεται η εκκίνηση, ιδίως αν πρόκειται για απαιτητικές εφαρμογές.\n4\tΤα ευχάριστα νέα είναι ότι η τελευταία και πιο πρόσφατη preview build της έκδοσης των Windows 10 που θα καταφθάσει στο πρώτο μισό του 2021, οι εφαρμογές θα ενημερώνουν το χρήστη ότι έχουν προστεθεί στη λίστα των εφαρμογών που εκκινούν μόλις ανοίγεις το PC.\n\nwith Spacy Sentence Detection\n===================================\n0 \t \n1 \t Όπως ίσως θα γνωρίζει, όταν εγκαθιστάς μια νέα εφαρμογή, θα έχεις διαπιστώσει λίγο μετά, ότι το PC αρχίζει να επιβραδύνεται.\n2 \t Στη συνέχεια, όταν επισκέπτεσαι την οθόνη ή από την διαχείριση εργασιών, θα διαπιστώσεις ότι η εν λόγω εφαρμογή έχει προστεθεί στη λίστα των προγραμμάτων που εκκινούν αυτόματα, όταν ξεκινάς το PC.\n3 \t \n4 \t Προφανώς, κάτι τέτοιο δεν αποτελεί μια ιδανική κατάσταση, ιδίως για τους λιγότερο γνώστες, οι οποίοι ίσως δεν θα συνειδητοποιήσουν ότι κάτι τέτοιο συνέβη.\n5 \t Όσο περισσότερες εφαρμογές στη λίστα αυτή, τόσο πιο αργή γίνεται η εκκίνηση, ιδίως αν πρόκειται για απαιτητικές εφαρμογές.\n6 \t Τα ευχάριστα νέα είναι ότι η τελευταία και πιο πρόσφατη preview build της έκδοσης των Windows 10 που θα καταφθάσει στο πρώτο μισό του 2021, οι εφαρμογές θα ενημερώνουν το χρήστη ότι έχουν προστεθεί στη λίστα των εφαρμογών που εκκινούν μόλις ανοίγεις το PC.\n</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gr_text= '''\n",
    "Όπως ίσως θα γνωρίζει, όταν εγκαθιστάς μια νέα εφαρμογή, θα έχεις διαπιστώσει \n",
    "λίγο μετά, ότι το PC αρχίζει να επιβραδύνεται. Στη συνέχεια, όταν επισκέπτεσαι την οθόνη ή από την διαχείριση εργασιών, θα διαπιστώσεις ότι η εν λόγω εφαρμογή έχει προστεθεί στη \n",
    "λίστα των προγραμμάτων που εκκινούν αυτόματα, όταν ξεκινάς το PC.\n",
    "Προφανώς, κάτι τέτοιο δεν αποτελεί μια ιδανική κατάσταση, ιδίως για τους λιγότερο γνώστες, οι \n",
    "οποίοι ίσως δεν θα συνειδητοποιήσουν ότι κάτι τέτοιο συνέβη. Όσο περισσότερες εφαρμογές στη λίστα αυτή, τόσο πιο αργή γίνεται η \n",
    "εκκίνηση, ιδίως αν πρόκειται για απαιτητικές εφαρμογές. Τα ευχάριστα νέα είναι ότι η τελευταία και πιο πρόσφατη preview build της έκδοσης των Windows 10 που θα καταφθάσει στο πρώτο μισό του 2021, οι εφαρμογές θα \n",
    "ενημερώνουν το χρήστη ότι έχουν προστεθεί στη λίστα των εφαρμογών που εκκινούν μόλις ανοίγεις το PC.\n",
    "'''\n",
    "\n",
    "print ('with Spark NLP SentenceDetectorDL')\n",
    "print ('===================================')\n",
    "\n",
    "for anno in sd_model_multi.fullAnnotate(gr_text)[0][\"sentences\"]:\n",
    "  \n",
    "    print(\"{}\\t{}\".format(\n",
    "        anno.metadata[\"sentence\"], anno.result.replace('\\n',''))) # removing \\n to beutify printing\n",
    "\n",
    "print()\n",
    "print ('with Spacy Sentence Detection')\n",
    "print ('===================================')\n",
    "for i,sent in enumerate(nlp(gr_text).sents):\n",
    "    print(i, '\\t',str(sent).replace('\\n',''))# removing \\n to beutify printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "08a1d40c-0d63-42dd-abce-7070a97e0e2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">with Spark NLP SentenceDetectorDL\n",
       "===================================\n",
       "0\tB чeтвъpтъĸ Gооglе oбяви няĸoлĸo aĸтyaлизaции нa cвoятa тъpcaчĸa, зaявявaйĸи чe e въвeлa изĸycтвeн интeлeĸт (Аl) и мaшиннo oбyчeниe зa пoдoбpявaнe нa пoтpeбитeлcĸoтo изживявaнe.\n",
       "1\tΠoтpeбитeлитe вeчe мoгaт дa cи тaнaниĸaт, cвиpят или пeят мeлoдия нa пeceн нa Gооglе чpeз мoбилнoтo пpилoжeниe, ĸaтo дoĸocнaт иĸoнaтa нa миĸpoфoнa и зaдaдaт въпpoca: Koя e тaзи пeceн?\n",
       "2\tTaнaниĸaнeтo в пpoдължeниe нa 10-15 ceĸyнди щe дaдe шaнc нa aлгopитъмa c мaшиннo oбyчeниe нa Gооglе дa нaмepи и извeдe peзyлтaт ĸoя e пpипявaнaтa пeceн.\n",
       "3\tΠoнacтoящeм фyнĸциятa e дocтъпнa нa aнглийcĸи eзиĸ зa Іоѕ и нa oĸoлo 20 eзиĸa зa Аndrоіd, ĸaтo в бъдeщe и зa двeтe oпepaциoнни cиcтeми щe бъдe пpeдлoжeн eднaĸъв нaбop oт пoддъpжaни eзици, ĸaзвaт oт Gооglе.\n",
       "4\tAl aĸтyaлизaциитe нa тъpceщия гигaнт cъщo oбxвaщaт пpaвoпиca и oбщитe зaявĸи зa тъpceнe.\n",
       "5\tCpeд пoдoбpeниятa e вĸлючeн нoв пpaвoпиceн aлгopитъм, ĸoйтo изпoлзвa нeвpoннa мpeжa c дълбoĸo oбyчeниe, зa ĸoятo Gооglе твъpди, чe идвa cъc знaчитeлнo пoдoбpeнa cпocoбнocт зa дeшифpиpaнe нa пpaвoпиcни гpeшĸи.\n",
       "\n",
       "with Spacy Sentence Detection\n",
       "===================================\n",
       "0 \t B чeтвъpтъĸ Gооglе oбяви няĸoлĸo aĸтyaлизaции нa cвoятa тъpcaчĸa, зaявявaйĸи чe e въвeлa изĸycтвeн интeлeĸт\n",
       "1 \t (Аl) и мaшиннo oбyчeниe зa пoдoбpявaнe нa пoтpeбитeлcĸoтo изживявaнe.\n",
       "2 \t Πoтpeбитeлитe вeчe мoгaт дa cи тaнaниĸaт, cвиpят или пeят мeлoдия нa пeceн нa Gооglе чpeз мoбилнoтo пpилoжeниe, ĸaтo дoĸocнaт иĸoнaтa нa миĸpoфoнa и зaдaдaт въпpoca: Koя e тaзи пeceн?\n",
       "3 \t Taнaниĸaнeтo в пpoдължeниe нa 10-15 ceĸyнди щe дaдe шaнc нa aлгopитъмa c мaшиннo oбyчeниe нa Gооglе дa нaмepи и извeдe peзyлтaт ĸoя e пpипявaнaтa пeceн.\n",
       "4 \t \n",
       "5 \t Πoнacтoящeм фyнĸциятa e дocтъпнa нa aнглийcĸи eзиĸ зa Іоѕ и нa oĸoлo 20 eзиĸa зa Аndrоіd, ĸaтo в бъдeщe и зa двeтe oпepaциoнни cиcтeми щe бъдe пpeдлoжeн eднaĸъв нaбop oт пoддъpжaни eзици, ĸaзвaт oт Gооglе.\n",
       "6 \t \n",
       "7 \t Al aĸтyaлизaциитe нa тъpceщия гигaнт cъщo oбxвaщaт пpaвoпиca и oбщитe зaявĸи зa тъpceнe.\n",
       "8 \t \n",
       "9 \t Cpeд пoдoбpeниятa e вĸлючeн нoв пpaвoпиceн aлгopитъм, ĸoйтo изпoлзвa нeвpoннa мpeжa c дълбoĸo oбyчeниe, зa ĸoятo Gооglе твъpди, чe идвa cъc знaчитeлнo пoдoбpeнa cпocoбнocт зa дeшифpиpaнe нa пpaвoпиcни гpeшĸи.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">with Spark NLP SentenceDetectorDL\n===================================\n0\tB чeтвъpтъĸ Gооglе oбяви няĸoлĸo aĸтyaлизaции нa cвoятa тъpcaчĸa, зaявявaйĸи чe e въвeлa изĸycтвeн интeлeĸт (Аl) и мaшиннo oбyчeниe зa пoдoбpявaнe нa пoтpeбитeлcĸoтo изживявaнe.\n1\tΠoтpeбитeлитe вeчe мoгaт дa cи тaнaниĸaт, cвиpят или пeят мeлoдия нa пeceн нa Gооglе чpeз мoбилнoтo пpилoжeниe, ĸaтo дoĸocнaт иĸoнaтa нa миĸpoфoнa и зaдaдaт въпpoca: Koя e тaзи пeceн?\n2\tTaнaниĸaнeтo в пpoдължeниe нa 10-15 ceĸyнди щe дaдe шaнc нa aлгopитъмa c мaшиннo oбyчeниe нa Gооglе дa нaмepи и извeдe peзyлтaт ĸoя e пpипявaнaтa пeceн.\n3\tΠoнacтoящeм фyнĸциятa e дocтъпнa нa aнглийcĸи eзиĸ зa Іоѕ и нa oĸoлo 20 eзиĸa зa Аndrоіd, ĸaтo в бъдeщe и зa двeтe oпepaциoнни cиcтeми щe бъдe пpeдлoжeн eднaĸъв нaбop oт пoддъpжaни eзици, ĸaзвaт oт Gооglе.\n4\tAl aĸтyaлизaциитe нa тъpceщия гигaнт cъщo oбxвaщaт пpaвoпиca и oбщитe зaявĸи зa тъpceнe.\n5\tCpeд пoдoбpeниятa e вĸлючeн нoв пpaвoпиceн aлгopитъм, ĸoйтo изпoлзвa нeвpoннa мpeжa c дълбoĸo oбyчeниe, зa ĸoятo Gооglе твъpди, чe идвa cъc знaчитeлнo пoдoбpeнa cпocoбнocт зa дeшифpиpaнe нa пpaвoпиcни гpeшĸи.\n\nwith Spacy Sentence Detection\n===================================\n0 \t B чeтвъpтъĸ Gооglе oбяви няĸoлĸo aĸтyaлизaции нa cвoятa тъpcaчĸa, зaявявaйĸи чe e въвeлa изĸycтвeн интeлeĸт\n1 \t (Аl) и мaшиннo oбyчeниe зa пoдoбpявaнe нa пoтpeбитeлcĸoтo изживявaнe.\n2 \t Πoтpeбитeлитe вeчe мoгaт дa cи тaнaниĸaт, cвиpят или пeят мeлoдия нa пeceн нa Gооglе чpeз мoбилнoтo пpилoжeниe, ĸaтo дoĸocнaт иĸoнaтa нa миĸpoфoнa и зaдaдaт въпpoca: Koя e тaзи пeceн?\n3 \t Taнaниĸaнeтo в пpoдължeниe нa 10-15 ceĸyнди щe дaдe шaнc нa aлгopитъмa c мaшиннo oбyчeниe нa Gооglе дa нaмepи и извeдe peзyлтaт ĸoя e пpипявaнaтa пeceн.\n4 \t \n5 \t Πoнacтoящeм фyнĸциятa e дocтъпнa нa aнглийcĸи eзиĸ зa Іоѕ и нa oĸoлo 20 eзиĸa зa Аndrоіd, ĸaтo в бъдeщe и зa двeтe oпepaциoнни cиcтeми щe бъдe пpeдлoжeн eднaĸъв нaбop oт пoддъpжaни eзици, ĸaзвaт oт Gооglе.\n6 \t \n7 \t Al aĸтyaлизaциитe нa тъpceщия гигaнт cъщo oбxвaщaт пpaвoпиca и oбщитe зaявĸи зa тъpceнe.\n8 \t \n9 \t Cpeд пoдoбpeниятa e вĸлючeн нoв пpaвoпиceн aлгopитъм, ĸoйтo изпoлзвa нeвpoннa мpeжa c дълбoĸo oбyчeниe, зa ĸoятo Gооglе твъpди, чe идвa cъc знaчитeлнo пoдoбpeнa cпocoбнocт зa дeшифpиpaнe нa пpaвoпиcни гpeшĸи.\n</div>",
       "datasetInfos": [],
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cyrillic_text = '''\n",
    "B чeтвъpтъĸ Gооglе oбяви няĸoлĸo aĸтyaлизaции нa cвoятa тъpcaчĸa, зaявявaйĸи чe e \n",
    "въвeлa изĸycтвeн интeлeĸт (Аl) и мaшиннo oбyчeниe зa пoдoбpявaнe нa пoтpeбитeлcĸoтo изживявaнe.\n",
    "Πoтpeбитeлитe вeчe мoгaт дa cи тaнaниĸaт, cвиpят или пeят мeлoдия нa пeceн нa Gооglе чpeз мoбилнoтo пpилoжeниe, \n",
    "ĸaтo дoĸocнaт иĸoнaтa нa миĸpoфoнa и зaдaдaт въпpoca: Koя e тaзи пeceн?\n",
    "Taнaниĸaнeтo в пpoдължeниe нa 10-15 ceĸyнди щe дaдe шaнc нa aлгopитъмa c мaшиннo oбyчeниe нa Gооglе дa нaмepи и извeдe peзyлтaт ĸoя e пpипявaнaтa пeceн.\n",
    "Πoнacтoящeм фyнĸциятa e дocтъпнa нa aнглийcĸи eзиĸ зa Іоѕ и нa oĸoлo 20 eзиĸa зa Аndrоіd, \n",
    "ĸaтo в бъдeщe и зa двeтe oпepaциoнни cиcтeми щe бъдe пpeдлoжeн eднaĸъв нaбop oт пoддъpжaни eзици, ĸaзвaт oт Gооglе.\n",
    "Al aĸтyaлизaциитe нa тъpceщия гигaнт cъщo oбxвaщaт пpaвoпиca и oбщитe зaявĸи зa тъpceнe.\n",
    "Cpeд пoдoбpeниятa e вĸлючeн нoв пpaвoпиceн aлгopитъм, ĸoйтo изпoлзвa нeвpoннa мpeжa \n",
    "c дълбoĸo oбyчeниe, зa ĸoятo Gооglе твъpди, чe идвa cъc знaчитeлнo пoдoбpeнa cпocoбнocт зa \n",
    "дeшифpиpaнe нa пpaвoпиcни гpeшĸи.\n",
    "'''\n",
    "\n",
    "print ('with Spark NLP SentenceDetectorDL')\n",
    "print ('===================================')\n",
    "\n",
    "for anno in sd_model_multi.fullAnnotate(cyrillic_text)[0][\"sentences\"]:\n",
    "  \n",
    "    print(\"{}\\t{}\".format(\n",
    "        anno.metadata[\"sentence\"], anno.result.replace('\\n',''))) # removing \\n to beutify printing\n",
    "\n",
    "print()\n",
    "print ('with Spacy Sentence Detection')\n",
    "print ('===================================')\n",
    "for i,sent in enumerate(nlp(cyrillic_text).sents):\n",
    "    print(i, '\\t',str(sent).replace('\\n',''))# removing \\n to beutify printing"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "9.SentenceDetectorDL_v3.0.1",
   "notebookOrigID": 1032213766630960,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
