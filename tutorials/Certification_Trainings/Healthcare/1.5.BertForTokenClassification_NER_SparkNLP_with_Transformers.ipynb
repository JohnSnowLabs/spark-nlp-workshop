{"cells":[{"cell_type":"markdown","metadata":{"id":"gbt8Q6j3RJxW"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"]},{"cell_type":"markdown","metadata":{"id":"xZOMrR1fRZq3"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.5.BertForTokenClassification_NER_SparkNLP_with_Transformers.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"dhiz8WdthoJm"},"source":["# 1.5 BertForTokenClassification NER Model Training with Transformers\n","\n","In this notebook, you will find how to train BertForTokenClassification NER model with transformers and then import into Spark NLP. (There is no Approach() in this notebook, so you can use only transformers for training.)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hThUtoq-AGyI"},"outputs":[],"source":["import json\n","import os\n","\n","from google.colab import files\n","\n","if 'spark_jsl.json' not in os.listdir():\n","  license_keys = files.upload()\n","  os.rename(list(license_keys.keys())[0], 'spark_jsl.json')\n","\n","with open('spark_jsl.json') as f:\n","    license_keys = json.load(f)\n","\n","# Defining license key-value pairs as local variables\n","locals().update(license_keys)\n","os.environ.update(license_keys)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yna_jWKhWzXO"},"outputs":[],"source":["! pip install --upgrade -q seqeval\n","! pip install --upgrade -q transformers==4.25.1\n","! pip install --upgrade -q tensorflow==2.11.0"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1712960210973,"user":{"displayName":"Monster C","userId":"08787989274818793476"},"user_tz":-180},"id":"8564aPNsW2eK","outputId":"66e9e28e-0ab2-4771-c9aa-c572016d5f2f"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.11.0'"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["import tensorflow as tf\n","tf.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"srzr9nmhAJCn"},"outputs":[],"source":["# Installing pyspark and spark-nlp\n","! pip install --upgrade -q pyspark==3.4.1 spark-nlp==$PUBLIC_VERSION\n","\n","# Installing Spark NLP Healthcare\n","! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":4919,"status":"ok","timestamp":1712959848458,"user":{"displayName":"Monster C","userId":"08787989274818793476"},"user_tz":-180},"id":"SqSpuOmV6dQE","outputId":"f8d5090c-aa35-479f-db0f-dbe73f64e29c"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Tesla T4'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import transformers\n","\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from transformers import BertTokenizer, BertConfig\n","from transformers import BertForTokenClassification, TFBertForTokenClassification, AdamW\n","from transformers import get_linear_schedule_with_warmup\n","\n","#from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","from tqdm import tqdm, trange\n","from keras.utils import pad_sequences\n","import tensorflow as tf\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","\n","torch.cuda.get_device_name(0)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1712960891303,"user":{"displayName":"Monster C","userId":"08787989274818793476"},"user_tz":-180},"id":"VDTP74DUbNwE","outputId":"906c0e64-c5ef-4883-cbaf-0e6ad16c9abf"},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://393c4d4aad49:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.4.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Spark NLP Licensed</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7fece58e70d0>"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["import sparknlp\n","import sparknlp_jsl\n","from pyspark.sql import functions as F\n","\n","from sparknlp.training import CoNLL\n","\n","import numpy as np\n","import pandas as pd\n","\n","params = {\"spark.driver.memory\":\"16G\",\n","          \"spark.kryoserializer.buffer.max\":\"2000M\",\n","          \"spark.driver.maxResultSize\":\"2000M\"}\n","\n","spark = sparknlp_jsl.start(secret = license_keys[\"SECRET\"], params=params)\n","\n","print (\"Spark NLP Version :\", sparknlp.version())\n","print (\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n","\n","spark"]},{"cell_type":"markdown","metadata":{},"source":["## Healthcare NLP for Data Scientists Course\n","\n","If you are not familiar with the components in this notebook, you can check [Healthcare NLP for Data Scientists Udemy Course](https://www.udemy.com/course/healthcare-nlp-for-data-scientists/) and the [MOOC Notebooks](https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/Spark_NLP_Udemy_MOOC/Healthcare_NLP) for each components."]},{"cell_type":"markdown","metadata":{"id":"5kKaWwaX6Pos"},"source":["## Download NCBI Disease CoNLL Dataset"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":1136,"status":"ok","timestamp":1712959897337,"user":{"displayName":"Monster C","userId":"08787989274818793476"},"user_tz":-180},"id":"iBEuXQnw6VPG"},"outputs":[],"source":["!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/data/NER_NCBIconlltrain.txt\n","!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/data/NER_NCBIconlltest.txt"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1712959897338,"user":{"displayName":"Monster C","userId":"08787989274818793476"},"user_tz":-180},"id":"hO0VnBGa81Wb"},"outputs":[],"source":["PROJECT_NAME = 'ner_disease_main'\n","\n","train_set =  \"NER_NCBIconlltrain.txt\"\n","test_set = \"NER_NCBIconlltest.txt\"\n","\n","test_metrics = True\n","\n","# select any Bert model from >> https://huggingface.co/models?pipeline_tag=token-classification&sort=downloads&search=bert\n","\n","MODEL_TO_TRAIN = 'dmis-lab/biobert-base-cased-v1.2'\n","# emilyalsentzer/Bio_ClinicalBERT\n","\n","# Defining some key variables that will be used later on in the training\n","MAX_LEN = 128 # 512\n","TRAIN_BATCH_SIZE = 64 # 8\n","VALID_BATCH_SIZE = 64 # 8\n","EPOCHS = 5\n","LEARNING_RATE = 2e-05\n","\n","!mkdir {PROJECT_NAME}\n","!mkdir {PROJECT_NAME}/logs"]},{"cell_type":"markdown","metadata":{"id":"iebvp7r-t4tl"},"source":["## Run the follwing cells with no change"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["8969d63a158844eb95be3245d0bfb86f","bc31c379e5c84203a020a5646fbc6b23","153a8ae5703f4690b58e77c2c5d77856","ac16f661adcd444e99b7947474cfd47d","5d8961656e194d97b0134d9b0a7085eb","df4fc804fddd49f8898d8297772f3469","442bc79ffb39465a805d9cff91b24633","874c6c01893e499fbbf0e83418bc580f","0b43b767fb2a43b1a7e5cf93e3f08211","c874b18a6c254ebb9da2610f168575c7","89be1167bf7240d69c5fd215aaddc198","12942ae5f9334cb1bbe0f08305cc3921","de845a9f8b864ac3b58c8efbd0efa11f","40507fcdac65453faeab346ff4da3ed0","f4b5080af0ef40db8da783cb22a19ee5","93dc3bbda2ba4f88903a22cff3eb3913","09e1caa5dca346efb0c2ab47d1af2038","c60c085b868f483790bd5741b3644f8c","3dc63c65995c41988f557033e6a0452f","9071f6f53e9a42fbb6a76d0821f614e8","c9083b91e6544d9496369f514ae3454c","6126d4c90248442799365443a0d2e3b1","dd34ddd5d0a6410f893987d97d27ff5a","68ed90b139be43f49c2ac4cbf8fbece6","2e0cd9503b3b4a70a001ef738d44d83c","51987dc930a64fd3b97fcd8ca476daad","e8669976668a49f8bb6faab663a4e883","20b49f91edb24cb286877f3e029c615a","890f68bddc8a4bc787db30a4c0acffe7","1be2e27240d04cd8b6fa2188f6a1129f","7c0f97916f9f45de83535e97f71fa1dc","d51e1ebd90b04e27a66112801a994b02","5ba47e06a4a3488891f7c384d31d5a08"]},"executionInfo":{"elapsed":231523,"status":"ok","timestamp":1712960128858,"user":{"displayName":"Monster C","userId":"08787989274818793476"},"user_tz":-180},"id":"XvUBSCIo9ZWR","outputId":"e7bc3fb8-537b-4f76-fef8-dd246d54693c"},"outputs":[{"name":"stdout","output_type":"stream","text":["=== TRAINING SET DISTRIBUTION ===\n","tag\n","O            39427\n","I-Disease     3547\n","B-Disease     3093\n","Name: count, dtype: int64\n","=== TEST SET DISTRIBUTION ===\n","tag\n","O            9316\n","I-Disease     789\n","B-Disease     708\n","Name: count, dtype: int64\n","=== Getting sentences and labels ===\n","Example of train sentence:\n","['A', 'common', 'MSH2', 'mutation', 'in', 'English', 'and', 'North', 'American', 'HNPCC', 'families', ':', 'origin', ',', 'phenotypic', 'expression', ',', 'and', 'sex', 'specific', 'differences', 'in', 'colorectal', 'cancer', '.']\n","Example of test sentence:\n","['Two', 'of', 'seventeen', 'mutated', 'T', '-', 'PLL', 'samples', 'had', 'a', 'previously', 'reported', 'A', '-', 'T', 'allele', '.']\n","Example of train sentence:\n","['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Disease', 'I-Disease', 'O']\n","Example of test sentence:\n","['O', 'O', 'O', 'O', 'B-Disease', 'I-Disease', 'I-Disease', 'O', 'O', 'O', 'O', 'O', 'B-Disease', 'I-Disease', 'I-Disease', 'O', 'O']\n","['B-Disease', 'O', 'I-Disease', 'PAD']\n","{'B-Disease': 0, 'O': 1, 'I-Disease': 2, 'PAD': 3}\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8969d63a158844eb95be3245d0bfb86f","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"12942ae5f9334cb1bbe0f08305cc3921","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["['Two', 'of', 'seventeen', 'm', '##uta', '##ted', 'T', '-', 'P', '##LL', 'samples', 'had', 'a', 'previously', 'reported', 'A', '-', 'T', 'all', '##ele', '.']\n","['O', 'O', 'O', 'O', 'O', 'O', 'B-Disease', 'I-Disease', 'I-Disease', 'I-Disease', 'O', 'O', 'O', 'O', 'O', 'B-Disease', 'I-Disease', 'I-Disease', 'O', 'O', 'O']\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd34ddd5d0a6410f893987d97d27ff5a","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at dmis-lab/biobert-base-cased-v1.2 were not used when initializing BertForTokenClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Epoch:   0%|          | 0/5 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1555: UserWarning: `save_config` is deprecated and will be removed in v5 of Transformers. Use `is_main_process` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Epoch:  20%|██        | 1/5 [00:39<02:39, 39.78s/it]"]},{"name":"stdout","output_type":"stream","text":["Average train loss: 0.41772747315742353\n","\n","Validation loss: 0.12881709422383988\n","\n","              precision    recall  f1-score   support\n","\n","   B-Disease       0.69      0.41      0.51      1718\n","   I-Disease       0.55      0.75      0.64      1560\n","           O       0.95      0.93      0.94     11654\n","         PAD       0.00      0.00      0.00         0\n","\n","    accuracy                           0.85     14932\n","   macro avg       0.55      0.52      0.52     14932\n","weighted avg       0.88      0.85      0.86     14932\n","\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1555: UserWarning: `save_config` is deprecated and will be removed in v5 of Transformers. Use `is_main_process` instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","Epoch:  40%|████      | 2/5 [01:19<01:59, 39.77s/it]"]},{"name":"stdout","output_type":"stream","text":["Average train loss: 0.10027197707030508\n","\n","Validation loss: 0.05304280828152384\n","\n","              precision    recall  f1-score   support\n","\n","   B-Disease       0.79      0.76      0.77      1718\n","   I-Disease       0.78      0.80      0.79      1560\n","           O       0.97      0.97      0.97     11654\n","         PAD       0.00      0.00      0.00         0\n","\n","    accuracy                           0.93     14932\n","   macro avg       0.63      0.63      0.63     14932\n","weighted avg       0.93      0.93      0.93     14932\n","\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1555: UserWarning: `save_config` is deprecated and will be removed in v5 of Transformers. Use `is_main_process` instead.\n","  warnings.warn(\n","Epoch:  60%|██████    | 3/5 [02:02<01:22, 41.22s/it]"]},{"name":"stdout","output_type":"stream","text":["Average train loss: 0.04348131324406023\n","\n","Validation loss: 0.04318459757736751\n","\n","              precision    recall  f1-score   support\n","\n","   B-Disease       0.79      0.92      0.85      1718\n","   I-Disease       0.86      0.82      0.84      1560\n","           O       0.99      0.97      0.98     11654\n","\n","    accuracy                           0.95     14932\n","   macro avg       0.88      0.90      0.89     14932\n","weighted avg       0.95      0.95      0.95     14932\n","\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1555: UserWarning: `save_config` is deprecated and will be removed in v5 of Transformers. Use `is_main_process` instead.\n","  warnings.warn(\n","Epoch:  80%|████████  | 4/5 [02:47<00:42, 42.54s/it]"]},{"name":"stdout","output_type":"stream","text":["Average train loss: 0.029575310302553354\n","\n","Validation loss: 0.03860685069646154\n","\n","              precision    recall  f1-score   support\n","\n","   B-Disease       0.86      0.86      0.86      1718\n","   I-Disease       0.86      0.87      0.86      1560\n","           O       0.98      0.98      0.98     11654\n","\n","    accuracy                           0.95     14932\n","   macro avg       0.90      0.90      0.90     14932\n","weighted avg       0.95      0.95      0.95     14932\n","\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1555: UserWarning: `save_config` is deprecated and will be removed in v5 of Transformers. Use `is_main_process` instead.\n","  warnings.warn(\n","Epoch: 100%|██████████| 5/5 [03:30<00:00, 42.12s/it]"]},{"name":"stdout","output_type":"stream","text":["Average train loss: 0.023576022574195155\n","\n","Validation loss: 0.03863151744008064\n","\n","              precision    recall  f1-score   support\n","\n","   B-Disease       0.86      0.87      0.87      1718\n","   I-Disease       0.85      0.89      0.87      1560\n","           O       0.98      0.98      0.98     11654\n","\n","    accuracy                           0.95     14932\n","   macro avg       0.90      0.91      0.90     14932\n","weighted avg       0.95      0.95      0.95     14932\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["def get_conll_df(pth):\n","  data = CoNLL().readDataset(spark, pth)\n","  data = data.withColumn(\"sentence_idx\", F.monotonically_increasing_id())\n","  data = data.withColumn('unique', F.array_distinct(\"label.result\"))\\\n","              .withColumn('c', F.size('unique'))\\\n","              .filter(F.col('c')>1)\n","\n","  df = data.select('sentence_idx', F.explode(F.arrays_zip(data.token.result,\n","                                                          data.label.result,\n","                                                          data.pos.result)).alias(\"cols\")) \\\n","          .select('sentence_idx',\n","                  F.expr(\"cols['0']\").alias(\"word\"),\n","                  F.expr(\"cols['1']\").alias(\"tag\"),\n","                  F.expr(\"cols['2']\").alias(\"pos\")).toPandas()\n","\n","  return df\n","\n","\n","train_data_df = get_conll_df(train_set)\n","test_data_df = get_conll_df(test_set)\n","\n","print ('=== TRAINING SET DISTRIBUTION ===')\n","print (train_data_df['tag'].value_counts())\n","\n","print ('=== TEST SET DISTRIBUTION ===')\n","print (test_data_df['tag'].value_counts())\n","\n","if not test_metrics:\n","\n","  train_data_df = pd.concat([train_data_df, test_data_df])\n","\n","\n","## convert conll file to sentences\n","\n","class SentenceGetter(object):\n","\n","    def __init__(self, dataset):\n","        self.n_sent = 1\n","        self.dataset = dataset\n","        self.empty = False\n","        agg_func = lambda s: [(w,p, t) for w,p, t in zip(s[\"word\"].values.tolist(),\n","                                                       s['pos'].values.tolist(),\n","                                                        s[\"tag\"].values.tolist())]\n","        self.grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n","        self.sentences = [s for s in self.grouped]\n","\n","    def get_next(self):\n","        try:\n","            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n","            self.n_sent += 1\n","            return s\n","        except:\n","            return None\n","\n","train_getter = SentenceGetter(train_data_df)\n","\n","if test_metrics:\n","  test_getter = SentenceGetter(test_data_df)\n","\n","\n","print ('=== Getting sentences and labels ===')\n","\n","# Sentences\n","train_sentences = [[word[0] for word in sentence] for sentence in train_getter.sentences]\n","print(\"Example of train sentence:\")\n","print (train_sentences[5])\n","\n","if test_metrics:\n","  test_sentences = [[word[0] for word in sentence] for sentence in test_getter.sentences]\n","  print(\"Example of test sentence:\")\n","  print (test_sentences[5])\n","\n","# Labels\n","train_labels = [[s[2] for s in sentence] for sentence in train_getter.sentences]\n","print(\"Example of train sentence:\")\n","print(train_labels[5])\n","\n","if test_metrics:\n","  test_labels = [[s[2] for s in sentence] for sentence in test_getter.sentences]\n","  print(\"Example of test sentence:\")\n","  print(test_labels[5])\n","\n","\n","tag_values = list(set(train_data_df[\"tag\"].values))\n","tag_values.append(\"PAD\")\n","tag2idx = {t: i for i, t in enumerate(tag_values)}\n","\n","print(tag_values[:10])\n","print(tag2idx)\n","\n","tokenizer = BertTokenizer.from_pretrained(MODEL_TO_TRAIN, do_lower_case=False)\n","\n","\n","def tokenize_and_preserve_labels(sentence, text_labels):\n","    tokenized_sentence = []\n","    labels = []\n","\n","    for word, label in zip(sentence, text_labels):\n","\n","        # Tokenize the word and count # of subwords the word is broken into\n","        tokenized_word = tokenizer.tokenize(word)\n","        n_subwords = len(tokenized_word)\n","\n","        # Add the tokenized word to the final tokenized word list\n","        tokenized_sentence.extend(tokenized_word)\n","\n","        # Add the same label to the new list of labels `n_subwords` times\n","        labels.extend([label] * n_subwords)\n","\n","    return tokenized_sentence, labels\n","\n","\n","train_tokenized_texts_and_labels = [\n","    tokenize_and_preserve_labels(sent, labs)\n","    for sent, labs in zip(train_sentences, train_labels)\n","]\n","\n","if test_metrics:\n","\n","  test_tokenized_texts_and_labels = [\n","      tokenize_and_preserve_labels(sent, labs)\n","      for sent, labs in zip(test_sentences, test_labels)\n","  ]\n","\n","train_tokenized_texts_tokens = [token_label_pair[0] for token_label_pair in train_tokenized_texts_and_labels]\n","\n","if test_metrics:\n","  test_tokenized_texts_tokens = [token_label_pair[0] for token_label_pair in test_tokenized_texts_and_labels]\n","  print(test_tokenized_texts_tokens[5])\n","\n","train_tokenized_texts_labels = [token_label_pair[1] for token_label_pair in train_tokenized_texts_and_labels]\n","\n","if test_metrics:\n","  test_tokenized_texts_labels = [token_label_pair[1] for token_label_pair in test_tokenized_texts_and_labels]\n","  print(test_tokenized_texts_labels[5])\n","\n","\n","\n","train_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in train_tokenized_texts_tokens],\n","                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n","                          truncating=\"post\", padding=\"post\")\n","\n","if test_metrics:\n","\n","  test_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in test_tokenized_texts_tokens],\n","                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n","                          truncating=\"post\", padding=\"post\")\n","\n","train_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in train_tokenized_texts_labels],\n","                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n","                     dtype=\"long\", truncating=\"post\")\n","\n","train_attention_masks = [[float(i != 0.0) for i in ii] for ii in train_input_ids]\n","\n","if test_metrics:\n","\n","  test_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in test_tokenized_texts_labels],\n","                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n","                     dtype=\"long\", truncating=\"post\")\n","  test_attention_masks = [[float(i != 0.0) for i in ii] for ii in test_input_ids]\n","\n","\n","\n","\n","tr_inputs = torch.tensor(train_input_ids)\n","tr_tags = torch.tensor(train_tags)\n","tr_masks = torch.tensor(train_attention_masks)\n","\n","if test_metrics:\n","\n","  val_inputs = torch.tensor(test_input_ids)\n","  val_tags = torch.tensor(test_tags)\n","  val_masks = torch.tensor(test_attention_masks)\n","\n","\n","train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=TRAIN_BATCH_SIZE)\n","\n","if test_metrics:\n","\n","  valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n","  valid_sampler = SequentialSampler(valid_data)\n","  valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=TRAIN_BATCH_SIZE)\n","\n","\n","\n","model = BertForTokenClassification.from_pretrained(\n","    MODEL_TO_TRAIN,\n","    num_labels=len(tag2idx),\n","    output_attentions = False,\n","    output_hidden_states = False\n",")\n","model.to(device)\n","\n","FULL_FINETUNING = True\n","if FULL_FINETUNING:\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = ['bias', 'gamma', 'beta']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","         'weight_decay_rate': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","         'weight_decay_rate': 0.0}\n","    ]\n","else:\n","    param_optimizer = list(model.classifier.named_parameters())\n","    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n","\n","optimizer = AdamW(\n","    optimizer_grouped_parameters,\n","    lr=3e-5,\n","    eps=1e-8\n",")\n","\n","\n","epochs = EPOCHS\n","max_grad_norm = 1.0\n","\n","# Total number of training steps is number of batches * number of epochs.\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=total_steps\n",")\n","\n","\n","## Store the average loss after each epoch so we can plot them.\n","loss_values, validation_loss_values = [], []\n","\n","for EPOCH in trange(epochs, desc=\"Epoch\"):\n","    # Put the model into training mode.\n","    model.train()\n","    # Reset the total loss for this epoch.\n","    total_loss = 0\n","\n","    # Training loop\n","    for step, batch in enumerate(train_dataloader):\n","        # add batch to gpu\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        # Always clear any previously calculated gradients before performing a backward pass.\n","        model.zero_grad()\n","        # forward pass\n","        # This will return the loss (rather than the model output)\n","        # because we have provided the `labels`.\n","        outputs = model(b_input_ids, token_type_ids=None,\n","                        attention_mask=b_input_mask, labels=b_labels)\n","        # get the loss\n","        loss = outputs[0]\n","        # Perform a backward pass to calculate the gradients.\n","        loss.backward()\n","        # track train loss\n","        total_loss += loss.item()\n","        # Clip the norm of the gradient\n","        # This is to help prevent the \"exploding gradients\" problem.\n","        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n","        # update parameters\n","        optimizer.step()\n","        # Update the learning rate.\n","        scheduler.step()\n","\n","    # Calculate the average loss over the training data.\n","    avg_train_loss = total_loss / len(train_dataloader)\n","    tr_loss = f\"Average train loss: {str(avg_train_loss)}\\n\"\n","\n","    # Saving partial models (this creates the folder too)\n","    tokenizer.save_pretrained(f'{PROJECT_NAME}/{str(EPOCH)}/tokenizer/')\n","    model.save_pretrained(save_directory=f'{PROJECT_NAME}/{str(EPOCH)}/',\n","                          save_config=True, state_dict=model.state_dict())\n","\n","    # Saving checkpoint in case it crashes, to restore work\n","    torch.save({\n","        'epoch': EPOCH,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'loss': avg_train_loss,\n","        }, f'{PROJECT_NAME}/{str(EPOCH)}/checkpoint.pth')\n","\n","    # Store the loss value for plotting the learning curve.\n","    loss_values.append(avg_train_loss)\n","\n","    if test_metrics:\n","\n","      # Put the model into evaluation mode\n","      model.eval()\n","      # Reset the validation loss for this epoch.\n","      eval_loss, eval_accuracy = 0, 0\n","      nb_eval_steps, nb_eval_examples = 0, 0\n","      predictions , true_labels = [], []\n","      for batch in valid_dataloader:\n","          batch = tuple(t.to(device) for t in batch)\n","          b_input_ids, b_input_mask, b_labels = batch\n","\n","          # Telling the model not to compute or store gradients,\n","          # saving memory and speeding up validation\n","          with torch.no_grad():\n","              # Forward pass, calculate logit predictions.\n","              # This will return the logits rather than the loss because we have not provided labels.\n","              outputs = model(b_input_ids, token_type_ids=None,\n","                              attention_mask=b_input_mask, labels=b_labels)\n","          # Move logits and labels to CPU\n","          logits = outputs[1].detach().cpu().numpy()\n","          label_ids = b_labels.to('cpu').numpy()\n","\n","          # Calculate the accuracy for this batch of test sentences.\n","          eval_loss += outputs[0].mean().item()\n","          predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","          true_labels.extend(label_ids)\n","\n","      eval_loss = eval_loss / len(valid_dataloader)\n","      validation_loss_values.append(eval_loss)\n","\n","      val_loss = f\"Validation loss: {str(eval_loss)}\\n\"\n","\n","    # Saving losses log\n","    with open(f'{PROJECT_NAME}/logs/epoch_' + str(EPOCH) + '_loss.log', 'a') as f:\n","      f.write(tr_loss)\n","      f.write('')\n","      if test_metrics:\n","          f.write(val_loss)\n","\n","    # Calculating metrics\n","    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n","                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n","    valid_tags = [tag_values[l_i] for l in true_labels\n","                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n","\n","    report = classification_report(valid_tags, pred_tags)\n","\n","    # Saving metrics\n","    with open(f'{PROJECT_NAME}/logs/epoch_' + str(EPOCH) + '_metrics.log', 'a') as f:\n","      f.write(report)\n","\n","    # Printing also to stdout\n","    print(tr_loss)\n","\n","    if test_metrics:\n","      print(val_loss)\n","      print(report)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":1462,"status":"ok","timestamp":1712960130316,"user":{"displayName":"Monster C","userId":"08787989274818793476"},"user_tz":-180},"id":"uCXnULJwMwW-"},"outputs":[],"source":["!rm -rf /content/ner_disease_main/0\n","!rm -rf /content/ner_disease_main/1\n","!rm -rf /content/ner_disease_main/2\n","!rm -rf /content/ner_disease_main/3"]},{"cell_type":"markdown","metadata":{"id":"AJQEdfA0Npmz"},"source":["## Load the model as TF and save properly\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":558,"status":"ok","timestamp":1712960130871,"user":{"displayName":"Monster C","userId":"08787989274818793476"},"user_tz":-180},"id":"er8xdlkKNqZ4","outputId":"45f6553f-58cc-434c-de91-5f235a8365aa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Last successfull epoch: 4\n","model_epoch_4_pytorch\n","model_epoch_4_tf\n"]}],"source":["last_successfull_epoch = len(loss_values) - 1\n","if last_successfull_epoch < 0:\n","  last_successfull_epoch = None\n","\n","if last_successfull_epoch is None:\n","  print(\"No epochs finished successfully.\")\n","else:\n","  print(f\"Last successfull epoch: {str(last_successfull_epoch)}\")\n","\n","# first save the model as pytorch model (we'll cast later)\n","MODEL_NAME_PYTORCH = 'model_epoch_'+str(last_successfull_epoch)+'_pytorch'\n","MODEL_NAME_TF = 'model_epoch_'+str(last_successfull_epoch)+'_tf'\n","\n","print(MODEL_NAME_PYTORCH)\n","print(MODEL_NAME_TF)\n","\n","tokenizer.save_pretrained(f'./{PROJECT_NAME}/{MODEL_NAME_PYTORCH}_tokenizer/')\n","model.save_pretrained(f'./{PROJECT_NAME}/{MODEL_NAME_PYTORCH}', saved_model=True, save_format='tf')\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39982,"status":"ok","timestamp":1712960170851,"user":{"displayName":"Monster C","userId":"08787989274818793476"},"user_tz":-180},"id":"PCdFfLcvrAa8","outputId":"bca65de5-8aa5-481f-85f4-76f6d6d3e7ee"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForTokenClassification: ['bert.embeddings.position_ids']\n","- This IS expected if you are initializing TFBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n","WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, LayerNorm_layer_call_fn while saving (showing 5 of 416). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["['B-Disease', 'O', 'I-Disease', 'PAD']\n"]}],"source":["import tensorflow as tf\n","from transformers import TFBertForTokenClassification\n","\n","# now load the model as TF and save properly\n","\n","loaded_model = TFBertForTokenClassification.from_pretrained(f'./{PROJECT_NAME}/{MODEL_NAME_PYTORCH}', from_pt=True)\n","\n","# Define TF Signature\n","@tf.function(\n","  input_signature=[\n","      {\n","          \"input_ids\": tf.TensorSpec((None, None), tf.int32, name=\"input_ids\"),\n","          \"attention_mask\": tf.TensorSpec((None, None), tf.int32, name=\"attention_mask\"),\n","          \"token_type_ids\": tf.TensorSpec((None, None), tf.int32, name=\"token_type_ids\"),\n","      }\n","  ]\n",")\n","def serving_fn(input):\n","    return loaded_model(input)\n","loaded_model.save_pretrained(f'./{PROJECT_NAME}/{MODEL_NAME_TF}', saved_model=True, signatures={\"serving_default\": serving_fn})\n","labels = sorted(tag2idx, key=tag2idx.get)\n","\n","print (labels)\n","\n","with open(f'./{PROJECT_NAME}/{MODEL_NAME_TF}/saved_model/1/assets/labels.txt', 'w') as f:\n","    f.write('\\n'.join(labels))\n","\n","vocab_pth = f\"./{PROJECT_NAME}/{MODEL_NAME_PYTORCH}_tokenizer/vocab.txt\"\n","saved_model_pth = f'./{PROJECT_NAME}/{MODEL_NAME_TF}/saved_model/1/assets/'\n","\n","! cp $vocab_pth $saved_model_pth"]},{"cell_type":"markdown","metadata":{"id":"IcQNjWOeN83V"},"source":["## Load the saved model in Spark NLP and save it properly\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":14377,"status":"ok","timestamp":1712960185218,"user":{"displayName":"Monster C","userId":"08787989274818793476"},"user_tz":-180},"id":"A7lvrYkvOC3L"},"outputs":[],"source":["from sparknlp.annotator import BertForTokenClassification\n","#from sparknlp.base import *\n","\n","tokenClassifier = BertForTokenClassification.loadSavedModel(\n","     f'./{PROJECT_NAME}/{MODEL_NAME_TF}/saved_model/1',\n","     spark)\\\n","  .setInputCols([\"document\",'token'])\\\n","  .setOutputCol(\"ner\")\\\n","  .setCaseSensitive(True)\\\n","  .setMaxSentenceLength(128) # 512\n","\n","tokenClassifier.write().overwrite().save(f\"./{PROJECT_NAME}/{MODEL_NAME_TF}_spark_nlp\")"]},{"cell_type":"markdown","metadata":{"id":"tYDq9D-cOGbo"},"source":["## Test the imported model in Spark NLP¶\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17822,"status":"ok","timestamp":1712960203037,"user":{"displayName":"Monster C","userId":"08787989274818793476"},"user_tz":-180},"id":"fugqP283OG6u","outputId":"bd98b8a8-9483-4439-b4a8-9e1342644bff"},"outputs":[{"name":"stdout","output_type":"stream","text":["sentence_detector_dl download started this may take some time.\n","Approximate size to download 354.6 KB\n","[OK!]\n"]}],"source":["from sparknlp.base import *\n","from sparknlp.annotator import *\n","from pyspark.ml import Pipeline\n","from sparknlp_jsl.annotator import *\n","\n","documentAssembler = DocumentAssembler()\\\n","  .setInputCol(\"text\")\\\n","  .setOutputCol(\"document\")\n","\n","sentenceDetector = SentenceDetectorDLModel.pretrained()\\\n","  .setInputCols([\"document\"])\\\n","  .setOutputCol(\"sentence\")\n","\n","tokenizer = Tokenizer()\\\n","  .setInputCols(\"sentence\")\\\n","  .setOutputCol(\"token\")\n","\n","tokenClassifier = BertForTokenClassification.load(f\"./{PROJECT_NAME}/{MODEL_NAME_TF}_spark_nlp\")\\\n","  .setInputCols(\"token\", \"sentence\")\\\n","  .setOutputCol(\"label\")\\\n","  .setCaseSensitive(True)\n","\n","ner_converter = NerConverterInternal()\\\n","  .setInputCols([\"sentence\",\"token\",\"label\"])\\\n","  .setOutputCol(\"ner_chunk\")\n","\n","\n","pipeline =  Pipeline(\n","    stages=[\n","        documentAssembler,\n","        sentenceDetector,\n","        tokenizer,\n","        tokenClassifier,\n","        ner_converter\n","    ]\n",")\n","\n","p_model = pipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\"))"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":580,"status":"ok","timestamp":1712960203605,"user":{"displayName":"Monster C","userId":"08787989274818793476"},"user_tz":-180},"id":"k808iCsakEfn"},"outputs":[],"source":["text = 'A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced pancreatitis three years prior to presentation , associated with an acute hepatitis , and obesity with a body mass index ( BMI ) of 33.5 kg/m2 , presented with a one-week history of polyuria , polydipsia , poor appetite , and vomiting . Two weeks prior to presentation , she was treated with a five-day course of amoxicillin for a respiratory tract infection . She was on metformin , glipizide , and dapagliflozin for T2DM and atorvastatin and gemfibrozil for HTG . She had been on dapagliflozin for six months at the time of presentation . Physical examination on presentation was significant for dry oral mucosa ; significantly , her abdominal examination was benign with no tenderness , guarding , or rigidity . Pertinent laboratory findings on admission were : serum glucose 111 mg/dl , bicarbonate 18 mmol/l , anion gap 20 , creatinine 0.4 mg/dL , triglycerides 508 mg/dL , total cholesterol 122 mg/dL , glycated hemoglobin ( HbA1c ) 10% , and venous pH 7.27 . Serum lipase was normal at 43 U/L . Serum acetone levels could not be assessed as blood samples kept hemolyzing due to significant lipemia . The patient was initially admitted for starvation ketosis , as she reported poor oral intake for three days prior to admission . However , serum chemistry obtained six hours after presentation revealed her glucose was 186 mg/dL , the anion gap was still elevated at 21 , serum bicarbonate was 16 mmol/L , triglyceride level peaked at 2050 mg/dL , and lipase was 52 U/L . The β-hydroxybutyrate level was obtained and found to be elevated at 5.29 mmol/L - the original sample was centrifuged and the chylomicron layer removed prior to analysis due to interference from turbidity caused by lipemia again . The patient was treated with an insulin drip for euDKA and HTG with a reduction in the anion gap to 13 and triglycerides to 1400 mg/dL , within 24 hours . Her euDKA was thought to be precipitated by her respiratory tract infection in the setting of SGLT2 inhibitor use . The patient was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day . It was determined that all SGLT2 inhibitors should be discontinued indefinitely . She had close follow-up with endocrinology post discharge .'\n","\n","result = p_model.transform(spark.createDataFrame([[text]]).toDF('text'))"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5567,"status":"ok","timestamp":1712960209170,"user":{"displayName":"Monster C","userId":"08787989274818793476"},"user_tz":-180},"id":"mx1Qio5_LzKx","outputId":"4a00b5b5-a9f8-43c0-ccdb-81cb0f75a9fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------+---------+\n","|token       |label    |\n","+------------+---------+\n","|A           |O        |\n","|28-year-old |O        |\n","|female      |O        |\n","|with        |O        |\n","|a           |O        |\n","|history     |O        |\n","|of          |O        |\n","|gestational |B-Disease|\n","|diabetes    |I-Disease|\n","|mellitus    |I-Disease|\n","|diagnosed   |O        |\n","|eight       |O        |\n","|years       |O        |\n","|prior       |O        |\n","|to          |O        |\n","|presentation|O        |\n","|and         |O        |\n","|subsequent  |O        |\n","|type        |B-Disease|\n","|two         |B-Disease|\n","|diabetes    |I-Disease|\n","|mellitus    |I-Disease|\n","|(           |O        |\n","|T2DM        |B-Disease|\n","|),          |O        |\n","|one         |O        |\n","|prior       |O        |\n","|episode     |O        |\n","|of          |O        |\n","|HTG-induced |B-Disease|\n","|pancreatitis|I-Disease|\n","|three       |O        |\n","|years       |O        |\n","|prior       |O        |\n","|to          |O        |\n","|presentation|O        |\n","|,           |O        |\n","|associated  |O        |\n","|with        |O        |\n","|an          |O        |\n","|acute       |B-Disease|\n","|hepatitis   |B-Disease|\n","|,           |O        |\n","|and         |O        |\n","|obesity     |B-Disease|\n","|with        |O        |\n","|a           |O        |\n","|body        |O        |\n","|mass        |O        |\n","|index       |O        |\n","+------------+---------+\n","only showing top 50 rows\n","\n"]}],"source":["result.select(F.explode(F.arrays_zip(result.token.result,\n","                                     result.label.result)).alias(\"cols\")) \\\n","      .select(F.expr(\"cols['0']\").alias(\"token\"),\n","              F.expr(\"cols['1']\").alias(\"label\")).show(50, truncate=False)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1815,"status":"ok","timestamp":1712960210972,"user":{"displayName":"Monster C","userId":"08787989274818793476"},"user_tz":-180},"id":"pPtJkXRlkHD7","outputId":"720ea44d-cdb1-4478-97d1-2bc096a76d62"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------------------------+---------+\n","|chunk                        |ner_label|\n","+-----------------------------+---------+\n","|gestational diabetes mellitus|Disease  |\n","|type                         |Disease  |\n","|two diabetes mellitus        |Disease  |\n","|T2DM                         |Disease  |\n","|HTG-induced pancreatitis     |Disease  |\n","|acute                        |Disease  |\n","|hepatitis                    |Disease  |\n","|obesity                      |Disease  |\n","|polyuria                     |Disease  |\n","|polydipsia                   |Disease  |\n","|appetite                     |Disease  |\n","|vomiting                     |Disease  |\n","|respiratory                  |Disease  |\n","|T2DM                         |Disease  |\n","|HTG                          |Disease  |\n","|dry                          |Disease  |\n","|oral mucosa                  |Disease  |\n","|tenderness                   |Disease  |\n","|guarding                     |Disease  |\n","|lipemia                      |Disease  |\n","+-----------------------------+---------+\n","only showing top 20 rows\n","\n"]}],"source":["result.select(F.explode(F.arrays_zip(result.ner_chunk.result,\n","                                     result.ner_chunk.metadata)).alias(\"cols\")) \\\n","      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n","              F.expr(\"cols['1']['entity']\").alias(\"ner_label\")).show(truncate=False)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"09e1caa5dca346efb0c2ab47d1af2038":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b43b767fb2a43b1a7e5cf93e3f08211":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"12942ae5f9334cb1bbe0f08305cc3921":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_de845a9f8b864ac3b58c8efbd0efa11f","IPY_MODEL_40507fcdac65453faeab346ff4da3ed0","IPY_MODEL_f4b5080af0ef40db8da783cb22a19ee5"],"layout":"IPY_MODEL_93dc3bbda2ba4f88903a22cff3eb3913"}},"153a8ae5703f4690b58e77c2c5d77856":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_874c6c01893e499fbbf0e83418bc580f","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0b43b767fb2a43b1a7e5cf93e3f08211","value":213450}},"1be2e27240d04cd8b6fa2188f6a1129f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20b49f91edb24cb286877f3e029c615a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e0cd9503b3b4a70a001ef738d44d83c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1be2e27240d04cd8b6fa2188f6a1129f","max":435783451,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7c0f97916f9f45de83535e97f71fa1dc","value":435783451}},"3dc63c65995c41988f557033e6a0452f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40507fcdac65453faeab346ff4da3ed0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3dc63c65995c41988f557033e6a0452f","max":1110,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9071f6f53e9a42fbb6a76d0821f614e8","value":1110}},"442bc79ffb39465a805d9cff91b24633":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51987dc930a64fd3b97fcd8ca476daad":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d51e1ebd90b04e27a66112801a994b02","placeholder":"​","style":"IPY_MODEL_5ba47e06a4a3488891f7c384d31d5a08","value":" 436M/436M [00:01&lt;00:00, 379MB/s]"}},"5ba47e06a4a3488891f7c384d31d5a08":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d8961656e194d97b0134d9b0a7085eb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6126d4c90248442799365443a0d2e3b1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68ed90b139be43f49c2ac4cbf8fbece6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20b49f91edb24cb286877f3e029c615a","placeholder":"​","style":"IPY_MODEL_890f68bddc8a4bc787db30a4c0acffe7","value":"pytorch_model.bin: 100%"}},"7c0f97916f9f45de83535e97f71fa1dc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"874c6c01893e499fbbf0e83418bc580f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"890f68bddc8a4bc787db30a4c0acffe7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8969d63a158844eb95be3245d0bfb86f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bc31c379e5c84203a020a5646fbc6b23","IPY_MODEL_153a8ae5703f4690b58e77c2c5d77856","IPY_MODEL_ac16f661adcd444e99b7947474cfd47d"],"layout":"IPY_MODEL_5d8961656e194d97b0134d9b0a7085eb"}},"89be1167bf7240d69c5fd215aaddc198":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9071f6f53e9a42fbb6a76d0821f614e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"93dc3bbda2ba4f88903a22cff3eb3913":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac16f661adcd444e99b7947474cfd47d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c874b18a6c254ebb9da2610f168575c7","placeholder":"​","style":"IPY_MODEL_89be1167bf7240d69c5fd215aaddc198","value":" 213k/213k [00:00&lt;00:00, 1.31MB/s]"}},"bc31c379e5c84203a020a5646fbc6b23":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df4fc804fddd49f8898d8297772f3469","placeholder":"​","style":"IPY_MODEL_442bc79ffb39465a805d9cff91b24633","value":"vocab.txt: 100%"}},"c60c085b868f483790bd5741b3644f8c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c874b18a6c254ebb9da2610f168575c7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9083b91e6544d9496369f514ae3454c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d51e1ebd90b04e27a66112801a994b02":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd34ddd5d0a6410f893987d97d27ff5a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_68ed90b139be43f49c2ac4cbf8fbece6","IPY_MODEL_2e0cd9503b3b4a70a001ef738d44d83c","IPY_MODEL_51987dc930a64fd3b97fcd8ca476daad"],"layout":"IPY_MODEL_e8669976668a49f8bb6faab663a4e883"}},"de845a9f8b864ac3b58c8efbd0efa11f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09e1caa5dca346efb0c2ab47d1af2038","placeholder":"​","style":"IPY_MODEL_c60c085b868f483790bd5741b3644f8c","value":"config.json: 100%"}},"df4fc804fddd49f8898d8297772f3469":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e8669976668a49f8bb6faab663a4e883":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4b5080af0ef40db8da783cb22a19ee5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c9083b91e6544d9496369f514ae3454c","placeholder":"​","style":"IPY_MODEL_6126d4c90248442799365443a0d2e3b1","value":" 1.11k/1.11k [00:00&lt;00:00, 98.5kB/s]"}}}}},"nbformat":4,"nbformat_minor":0}
