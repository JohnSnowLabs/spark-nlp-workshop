{"cells":[{"cell_type":"markdown","metadata":{"id":"i8W51t04BN6B"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"]},{"cell_type":"markdown","metadata":{"id":"21lTnEqRBd0s"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.2.Contextual_Parser_Rule_Based_NER.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"3-vwpSj3BSbj"},"source":["# 1.2 ContextualParser (Rule Based NER)"]},{"cell_type":"markdown","metadata":{"id":"5SBh1CNgOg54"},"source":["## Colab Setup"]},{"cell_type":"code","source":["import json, os\n","from google.colab import files\n","\n","if 'spark_jsl.json' not in os.listdir():\n","  license_keys = files.upload()\n","  os.rename(list(license_keys.keys())[0], 'spark_jsl.json')\n","\n","with open('spark_jsl.json') as f:\n","    license_keys = json.load(f)\n","\n","# Defining license key-value pairs as local variables\n","locals().update(license_keys)\n","os.environ.update(license_keys)"],"metadata":{"id":"NsjRecTSvQA9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Installing pyspark and spark-nlp\n","! pip install --upgrade -q pyspark==3.3.0 spark-nlp==$PUBLIC_VERSION\n","\n","# Installing Spark NLP Healthcare\n","! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET\n","\n","# Installing Spark NLP Display Library for visualization\n","! pip install -q spark-nlp-display"],"metadata":{"id":"mPVM43jNvYA1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53Qom33h4nvu","colab":{"base_uri":"https://localhost:8080/","height":254},"executionInfo":{"status":"ok","timestamp":1693470122536,"user_tz":-180,"elapsed":34673,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"e861059b-d51f-444e-c850-e7082eb67b2b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Spark NLP Version : 5.0.2\n","Spark NLP_JSL Version : 5.0.2\n"]},{"output_type":"execute_result","data":{"text/plain":["<pyspark.sql.session.SparkSession at 0x7bf9782772e0>"],"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://47f2c211f6da:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.0</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Spark NLP Licensed</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "]},"metadata":{},"execution_count":2}],"source":["import sparknlp\n","import sparknlp_jsl\n","\n","from sparknlp.base import *\n","from sparknlp.annotator import *\n","from sparknlp_jsl.annotator import *\n","\n","from pyspark.sql import SparkSession\n","from pyspark.ml import Pipeline,PipelineModel\n","\n","params = {\"spark.driver.memory\":\"16G\",\n","          \"spark.kryoserializer.buffer.max\":\"2000M\",\n","          \"spark.driver.maxResultSize\":\"2000M\"}\n","\n","spark = sparknlp_jsl.start(license_keys['SECRET'],params=params)\n","\n","print (\"Spark NLP Version :\", sparknlp.version())\n","print (\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n","\n","spark"]},{"cell_type":"markdown","source":["# How the ContextualParser Works"],"metadata":{"id":"xZzN6yNBvb2b"}},{"cell_type":"markdown","source":["Spark NLP's `ContextualParser` is a licensed annotator that allows users to extract entities from a document based on pattern matching. It provides more functionality than its open-source counterpart `EntityRuler` by allowing users to customize specific characteristics for pattern matching. You're able to find entities using regex rules for full and partial matches, a dictionary with normalizing options and context parameters to take into account things such as token distances.\n","\n","There are 3 components necessary to understand when using the `ContextualParser` annotator:\n","\n","1. `ContextualParser` annotator's parameters\n","2. JSON configuration file\n","3. Dictionary"],"metadata":{"id":"nSO62FEY0iof"}},{"cell_type":"markdown","source":["## 1. ContextualParser Annotator Parameters"],"metadata":{"id":"T0_3bAzaPbip"}},{"cell_type":"markdown","source":["Here are all the parameters available to use with the `ContextualParserApproach`:"],"metadata":{"id":"PTSYc7RUQgKh"}},{"cell_type":"markdown","source":["```\n","contextualParser = ContextualParserApproach() \\\n","    .setInputCols([\"sentence\", \"token\"]) \\\n","    .setOutputCol(\"entity\") \\\n","    .setCaseSensitive(True) \\\n","    .setJsonPath(\"context_config.json\") \\\n","    .setPrefixAndSuffixMatch(True) \\\n","    .setCompleteContextMatch(True) \\\n","    .setDictionary(\"dictionary.tsv\", options={\"orientation\":\"vertical\"})\n","```\n"],"metadata":{"id":"vF4_Dm7qVTty"}},{"cell_type":"markdown","source":["We will dive deeper into the details of each parameter, but here's a quick overview:\n","\n","- `setCaseSensitive`: do you want the matching to be case sensitive (applies to all JSON properties apart from the regex property)\n","- `setJsonPath`: the path to your JSON configuration file\n","- `setPrefixAndSuffixMatch`: do you want to match using both the prefix AND suffix properties from the JSON configuration file\n","- `setCompleteContextMatch`: do you want an exact match of prefix and suffix.\n","- `setDictionary`: the path to your dictionary, used for normalizing entities\n","\n","Let's start by looking at the JSON configuration file."],"metadata":{"id":"uVmRKjoBZ7HQ"}},{"cell_type":"markdown","source":["## 2. JSON Configuration File"],"metadata":{"id":"GVO5m215TjXf"}},{"cell_type":"markdown","source":["Here is a fully utilized JSON configuration file."],"metadata":{"id":"HNJr5ISlaJsl"}},{"cell_type":"markdown","source":["```\n","{\n","  \"entity\": \"Gender\",\n","  \"ruleScope\": \"sentence\",\n","  \"regex\": \"girl|boy\",\n","  \"completeMatchRegex\": \"true\",\n","  \"matchScope\": \"token\",\n","  \"prefix\": [\"birth\", \"growing\", \"assessment\"],\n","  \"suffix\": [\"faster\", \"velocities\"],\n","  \"contextLength\": 100,\n","  \"contextException\": [\"slightly\"],\n","  \"exceptionDistance\": 40\n"," }\n"," ```"],"metadata":{"id":"4q1UuczZVhD_"}},{"cell_type":"markdown","source":["### 2.1. Basic Properties"],"metadata":{"id":"GChnk1cXaUIZ"}},{"cell_type":"markdown","source":["There are 5 basic properties you can set in your JSON configuration file:\n","\n","- `entity`\n","- `ruleScope`\n","- `regex`\n","- `completeMatchRegex`\n","- `matchScope`\n","\n","Let's first look at the 3 most essential properties to set:"],"metadata":{"id":"p-kJhmUe0f13"}},{"cell_type":"markdown","source":["```\n","{\n","  \"entity\": \"Digit\",\n","  \"ruleScope\": \"sentence\",\n","  \"regex\": \"\\\\d+\" # Note here: backslashes are escape characters in JSON, so for regex pattern \"\\d+\" we need to write it out as \"\\\\d+\"\n","}\n","```"],"metadata":{"id":"7RP8mwtgVkcj"}},{"cell_type":"markdown","source":["Here, we're looking for tokens in our text that match the regex: \"`\\d+`\" and assign the \"`Digit`\" entity to those tokens. When `ruleScope` is set to \"`sentence`\", we're looking for a match on each *token* of a **sentence**. You can change it to \"`document`\" to look for a match on each *sentence* of a **document**. The latter is particularly useful when working with multi-word matches, but we'll explore this at a later stage.\n","\n","The next properties to look at are `completeMatchRegex` and `matchScope`. To understand their use case, let's take a look at an example where we're trying to match all digits in our text.\n","\n","Let's say we come across the following string: ***XYZ987***\n","\n","Depending on how we set the `completeMatchRegex` and `matchScope` properties, we'll get the following results:"],"metadata":{"id":"pHSsBgoNcJiw"}},{"cell_type":"markdown","source":["```\n","{\n","  \"entity\": \"Digit\",\n","  \"ruleScope\": \"sentence\",\n","  \"regex\": \"\\\\d+\",\n","  \"completeMatchRegex\": \"false\",\n","  \"matchScope\": \"token\"\n","}\n","```"],"metadata":{"id":"tOxfFn8_VngD"}},{"cell_type":"markdown","source":["`OUTPUT: [XYZ987]`"],"metadata":{"id":"dZzkfdtDyavl"}},{"cell_type":"markdown","source":["```\n","{\n","  \"entity\": \"Digit\",\n","  \"ruleScope\": \"sentence\",\n","  \"regex\": \"\\\\d+\",  \n","  \"completeMatchRegex\": \"false\",\n","  \"matchScope\": \"sub-token\"\n","}\n","```"],"metadata":{"id":"K37Ucw75Vrog"}},{"cell_type":"markdown","source":["`OUTPUT: [987]`\n"],"metadata":{"id":"CkOfYVHGyb20"}},{"cell_type":"markdown","source":["```\n","{\n","  \"entity\": \"Digit\",\n","  \"ruleScope\": \"sentence\",\n","  \"regex\": \"\\\\d+\",\n","  \"completeMatchRegex\": \"true\"\n","  # matchScope is ignored here\n","}\n","```"],"metadata":{"id":"9Jpr2IkFVwKw"}},{"cell_type":"markdown","source":["`OUTPUT: []`"],"metadata":{"id":"ZiYD0oF7gJtw"}},{"cell_type":"markdown","source":["`\"completeMatchRegex\": \"true\"` will only return an output if our string was modified in the following way (to get a complete, exact match): **XYZ 987**"],"metadata":{"id":"bFE9Ri2N4xxT"}},{"cell_type":"markdown","source":["```\n","{\n","  \"entity\": \"Digit\",\n","  \"ruleScope\": \"sentence\",\n","  \"regex\": \"\\\\d+\",  \n","  \"completeMatchRegex\": \"true\",\n","  \"matchScope\": \"token\" # Note here: sub-token would return the same output\n","}\n","```"],"metadata":{"id":"L-_sXg5l5NBg"}},{"cell_type":"markdown","source":["`OUTPUT: [987]`"],"metadata":{"id":"zUeuct_05p3f"}},{"cell_type":"markdown","source":["### 2.2. Context Awareness Properties"],"metadata":{"id":"ZlIUAPKazpT9"}},{"cell_type":"markdown","source":["There are 5 properties related to context awareness:\n","\n","- `contextLength`\n","- `prefix`\n","- `suffix`\n","- `contextException`\n","- `exceptionDistance`\n","\n"],"metadata":{"id":"9D2UfFFBz7fX"}},{"cell_type":"markdown","source":["Let's look at a similar example. Say we have the following text: ***At birth, the typical boy is growing slightly faster than the typical girl, but growth rates become equal at about seven months.***\n","\n","If we want to match the gender that grows faster at birth, we can start by defining our regex: \"`girl|boy`\"\n","\n","Next, we add a prefix (\"`birth`\") and suffix (\"`faster`\") to ask the parser to match the regex only if the word \"`birth`\" comes before and only if the word \"`faster`\" comes after. Finally, we will need to set the `contextLength` - this is the maximum number of tokens after the prefix and before the suffix that will be searched to find a regex match.\n","\n","Here's what the JSON configuration file would look like:"],"metadata":{"id":"IcUlaqmgDL9R"}},{"cell_type":"markdown","source":["```\n","{\n","  \"entity\": \"Gender\",\n","  \"ruleScope\": \"sentence\",\n","  \"regex\": \"girl|boy\",\n","  \"contextLength\": 50,\n","  \"prefix\": [\"birth\"],\n","  \"suffix\": [\"faster\"]\n","}\n","```"],"metadata":{"id":"J5Tq5OJBVzCL"}},{"cell_type":"markdown","source":["`OUTPUT: [boy]`"],"metadata":{"id":"G5Y5r9pO92B0"}},{"cell_type":"markdown","source":["If you remember, the annotator has a `setPrefixAndSuffixMatch()` parameter. If you set it to `True`, the previous output would remain as is. However, if you had set it to `False` and used the following JSON configuration:"],"metadata":{"id":"IXchjZZC_Gm0"}},{"cell_type":"markdown","source":["```\n","{\n","  \"entity\": \"Gender\",\n","  \"ruleScope\": \"sentence\",\n","  \"regex\": \"girl|boy\",\n","  \"contextLength\": 50,\n","  \"prefix\": [\"birth\"],\n","  \"suffix\": [\"faster\", \"rates\"]\n","}\n","```"],"metadata":{"id":"xO64udOnV1GJ"}},{"cell_type":"markdown","source":["`OUTPUT: [boy, girl]`"],"metadata":{"id":"Xm-y_c_RAJpF"}},{"cell_type":"markdown","source":["The parser now takes into account either the prefix OR suffix, only one of the condition has to be fulfilled for a match to count."],"metadata":{"id":"Yk0nox1sAdw_"}},{"cell_type":"markdown","source":["If you remember, the annotator has a `setCompleteContextMatch()` parameter. If you set it to `True`, and used the following JSON configuration :"],"metadata":{"id":"MyTdPDlgLmxL"}},{"cell_type":"markdown","source":["```\n","{\n","  \"entity\": \"Gender\",\n","  \"ruleScope\": \"sentence\",\n","  \"regex\": \"girl|boy\",\n","  \"contextLength\": 50,\n","  \"prefix\": [\"birth\"],\n","  \"suffix\": [\"fast\"]\n","}\n","```"],"metadata":{"id":"1ycrvCMpMJ_J"}},{"cell_type":"markdown","source":["`OUTPUT: []`"],"metadata":{"id":"r5A67yBZMj2N"}},{"cell_type":"markdown","source":["However if we set `setCompleteContextMatch()` as `False`, and use the same JSON configuration as above, we get the following output :"],"metadata":{"id":"N1duAqATM-ir"}},{"cell_type":"markdown","source":["`OUTPUT: [boy]`"],"metadata":{"id":"s0vd_jfsNi7y"}},{"cell_type":"markdown","source":["Here's the sentence again: ***At birth, the typical boy is growing slightly faster than the typical girl, but growth rates become equal at about seven months.***\n","\n","The last 2 properties related to context awareness are `contextException` and `exceptionDistance`. This rules out matches based on a given exception:"],"metadata":{"id":"vdEFJZM1DGQ1"}},{"cell_type":"markdown","source":["```\n","{\n","  \"entity\": \"Gender\",\n","  \"ruleScope\": \"sentence\",\n","  \"regex\": \"girl|boy\",\n","  \"contextLength\": 50,\n","  \"prefix\": [\"birth\"],\n","  \"suffix\": [\"faster\", \"rates\"],\n","  \"contextException\": [\"At\"],\n","  \"exceptionDistance\": 5\n","}\n","```"],"metadata":{"id":"0JluGupMV5SR"}},{"cell_type":"markdown","source":["`OUTPUT: [girl]`"],"metadata":{"id":"EnFzqpHlC3Qz"}},{"cell_type":"markdown","source":["Here we've asked the parser to ignore a match if the token \"`At`\" is within 5 tokens of the matched regex. This caused the token \"`boy`\" to be ignored."],"metadata":{"id":"JT09xrE-DCiO"}},{"cell_type":"markdown","source":["If the annotator's `setOptionalContextRules` parameter is set `True`, it allows us to output regex matches regardless of context match (prefix, suffix configuration). For usage of the `setOptionalContextRules` parameter go to the [Example2 output](#scrollTo=1tdgMbaWDhNC&line=1&uniqifier=1)."],"metadata":{"id":"nQ9-Fkr94qAH"}},{"cell_type":"markdown","source":["When `shortestContextMatch` parameter is set to `True`, it will stop finding for matches when one of prefix and suffix data is found in the text.\",\n","                                "],"metadata":{"id":"NBZ1D92g5aEB"}},{"cell_type":"markdown","source":["Confidence Value Scenarios:\n","* When there is regex match only, the confidence value will be 0.5.\n","* When there are regex and prefix matches together, the confidence value will be > 0.5 depending on the distance between target token and the prefix.\n","* When there are regex and suffix matches together, the confidence value will be > 0.5 depending on the distance between target token and the suffix.\n","* When there are regex, prefix, and suffix matches all together, the confidence value will be > than the other scenarios."],"metadata":{"id":"41wdqkIX5WAy"}},{"cell_type":"markdown","source":["## 3. Dictionary"],"metadata":{"id":"VzFSjw7aVO2b"}},{"cell_type":"markdown","source":["Another key feature of the `ContextualParser` annotator is the use of dictionaries. You can specify a path to a dictionary in `tsv` or `csv` format using the `setDictionary()` parameter. Using a dictionary is a useful when you have a list of exact words that you want the parser to pick up when processing some text."],"metadata":{"id":"5NPiJZx-Va8b"}},{"cell_type":"markdown","source":["### 3.1. Orientation\n","\n","The first feature to be aware of when it comes to feeding dictionaries is the format of the dictionaries. The `ContextualParser` annotator will accept dictionaries in the horizontal format and in a vertical format. This is how they would look in practice:"],"metadata":{"id":"hmB9hadyXYeM"}},{"cell_type":"markdown","source":["Horizontal:\n","\n","| normalize | word1 | word2 | word3     |\n","|-----------|-------|-------|-----------|\n","| female    | woman | girl  | lady      |\n","| male      | man   | boy   | gentleman |\n"],"metadata":{"id":"eyDmIqLmWGRy"}},{"cell_type":"markdown","source":["\n","Vertical:\n","\n","| female    | normalize |\n","|-----------|-----------|\n","| woman     | word1     |\n","| girl      | word2     |\n","| lady      | word3     |"],"metadata":{"id":"ToFbLxsDYInk"}},{"cell_type":"markdown","source":["As you can see, your dictionary needs to have a `normalize` field that lets the annotator know which entity labels to use, and another field that lets the annotator know a list of words it should be looking to match. Here's how to set the format that your dictionary uses:"],"metadata":{"id":"dCrIw4CRYeBC"}},{"cell_type":"markdown","source":["```\n","contextualParser = ContextualParserApproach() \\\n","    .setDictionary(\"dictionary.tsv\", options={\"orientation\":\"vertical\"}) # default is horizontal\n","```"],"metadata":{"id":"Z_epZRQiV85Y"}},{"cell_type":"markdown","source":["### 3.2. Dictionary-related JSON Properties\n","\n","When working with dictionaries, there are 2 properties in the JSON configuration file to be aware of:\n","\n","- `ruleScope`\n","- `matchScope`\n","\n","This is especially true when you have multi-word entities in your dictionary.\n","\n","Let's take an example of a dictionary that contains a list of cities, sometimes made up of multiple words:\n","\n","| normalize | word1 | word2 | word3     |\n","|-----------|-------|-------|-----------|\n","| City      | New York | Salt Lake City  | Washington      |\n","\n","\n"],"metadata":{"id":"_CvM-FgLZgKk"}},{"cell_type":"markdown","source":["Let's say we're working with the following text: ***I love New York. Salt Lake City is nice too.***"],"metadata":{"id":"v5weyyiIMzNr"}},{"cell_type":"markdown","source":["With the following JSON properties, here's what you would get:"],"metadata":{"id":"xg8eJwuTJcB6"}},{"cell_type":"markdown","source":["```\n","{\n","  \"entity\": \"City\",\n","  \"ruleScope\": \"sentence\",\n","  \"matchScope\": \"sub-token\",\n","}\n","```"],"metadata":{"id":"R1TNWtywoGWb"}},{"cell_type":"markdown","source":["`OUTPUT: []`"],"metadata":{"id":"Wweg5_C9JuWm"}},{"cell_type":"markdown","source":["When `ruleScope` is set to `\"sentence\"`, the annotator attempts to find matches at the token level, parsing through each token in the sentence one by one, looking for a match with the dictionary items. Since `\"New York\"` and `\"Salt Lake City\"` are made up of multiple tokens, the annotator would never find a match from the dictionary. Let's change `ruleScope` to `\"document\"`:"],"metadata":{"id":"pNYWPr2eLJ5d"}},{"cell_type":"markdown","source":["```\n","{\n","  \"entity\": \"City\",\n","  \"ruleScope\": \"document\",\n","  \"matchScope\": \"sub-token\",\n","}\n","```"],"metadata":{"id":"Xib9zHN7oBvV"}},{"cell_type":"markdown","source":["`OUTPUT: [New York, Salt Lake City]`"],"metadata":{"id":"MYmkGdtALXzK"}},{"cell_type":"markdown","source":["When `ruleScope` is set to `\"document\"`, the annotator attempts to find matches by parsing through each sentence in the document one by one, looking for a match with the dictionary items. Beware of how you set `matchScope`. Taking the previous example, if we were to set `matchScope` to `\"token\"` instead of `\"sub-token\"`, here's what would happen:"],"metadata":{"id":"Nmc7rvfsdFK9"}},{"cell_type":"markdown","source":["```\n","{\n","  \"entity\": \"City\",\n","  \"ruleScope\": \"document\",\n","  \"matchScope\": \"token\"\n","}\n","```"],"metadata":{"id":"vkzDTcgen9aS"}},{"cell_type":"markdown","source":["`OUTPUT: [I love New York., Salt Lake City is nice too.]`"],"metadata":{"id":"np7rBSQmetA9"}},{"cell_type":"markdown","source":["As you can see, when `ruleScope` is at the document level, if you set your `matchScope` to the token level, the annotator will output each sentence containing the matched entities as individual chunks."],"metadata":{"id":"w5nNtvLaeOv9"}},{"cell_type":"markdown","source":["### 3.3. Working with Multi-Word Matches\n","\n","Although not directly related to dictionaries, if we build on top of what we've just seen, there is a use-case that is particularly in demand when working with the `ContextualParser` annotator: finding regex matches for chunks of words that span across multiple tokens.\n","\n","Let's re-iterate how the `ruleScope` property works: when `ruleScope` is set to `\"sentence\"`, we're looking for a match on each token of a sentence. When `ruleScope` is set to `\"document\"`, we're looking for a match on each sentence of a document.\n","\n","So now let's imagine you're parsing through medical documents trying to tag the *Family History* headers in those documents."],"metadata":{"id":"7zTtAFwqQdNy"}},{"cell_type":"markdown","source":["```\n","{\n","  \"entity\": \"Family History Header\",\n","  \"regex\": \"[f|F]amily\\s+[h|H]istory\",  \n","  \"ruleScope\": \"document\",\n","  \"matchScope\": \"sub-token\"\n","}\n","```\n"],"metadata":{"id":"eJWXRrg0Qu0q"}},{"cell_type":"markdown","source":["`OUTPUT: [Family History, family history, Family history]`"],"metadata":{"id":"OP4H1t3CQyg5"}},{"cell_type":"markdown","source":["If you had set `ruleScope` to  `\"sentence\"`, here's what would have happened:"],"metadata":{"id":"X8PgD_N9RFTP"}},{"cell_type":"markdown","source":["```\n","{\n","  \"entity\": \"Family History Header\",\n","  \"regex\": \"[f|F]amily\\s+[h|H]istory\",  \n","  \"ruleScope\": \"sentence\",\n","  \"matchScope\": \"sub-token\"\n","}\n","```"],"metadata":{"id":"ljqiVjWaRJPe"}},{"cell_type":"markdown","source":["`OUTPUT: []`"],"metadata":{"id":"GfcGKW81RMKN"}},{"cell_type":"markdown","source":["Since Family History is divided into two different tokens, the annotator will never find a match since it's now looking for a match on each token of a sentence."],"metadata":{"id":"yMMgQdK5RPYb"}},{"cell_type":"markdown","source":["# Running a Pipeline"],"metadata":{"id":"ehWiHjziPfGV"}},{"cell_type":"markdown","source":["## Example 1: Detecting Cities"],"metadata":{"id":"VGz-nXwCDLgb"}},{"cell_type":"markdown","source":["Let's try running through some examples to build on top of what you've learned so far."],"metadata":{"id":"thaF2bObwDXd"}},{"cell_type":"code","source":["# Here's some sample text\n","sample_text = \"\"\"Peter Parker is a nice guy and lives in New York . Bruce Wayne is also a nice guy and lives in San Antonio and Gotham City . \"\"\""],"metadata":{"id":"mcWKcPZO-upM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a dictionary to detect cities\n","cities = \"\"\"City\\nNew York\\nGotham City\\nSan Antonio\\nSalt Lake City\"\"\"\n","\n","with open('cities.tsv', 'w') as f:\n","    f.write(cities)\n","\n","# Check what dictionary looks like\n","!cat cities.tsv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y4Ipms-4PwoN","outputId":"7f885272-06c3-4c6d-f04d-33cec7a82172","executionInfo":{"status":"ok","timestamp":1693470122536,"user_tz":-180,"elapsed":6,"user":{"displayName":"Monster C","userId":"08787989274818793476"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["City\n","New York\n","Gotham City\n","San Antonio\n","Salt Lake City"]}]},{"cell_type":"code","source":["# Create JSON file\n","cities = {\n","  \"entity\": \"City\",\n","  \"ruleScope\": \"document\",\n","  \"matchScope\":\"sub-token\",\n","  \"completeMatchRegex\": \"false\"\n","}\n","\n","import json\n","with open('cities.json', 'w') as f:\n","    json.dump(cities, f)"],"metadata":{"id":"Jvi1mbqY_LeA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build pipeline\n","document_assembler = DocumentAssembler() \\\n","    .setInputCol(\"text\") \\\n","    .setOutputCol(\"document\")\n","\n","sentence_detector = SentenceDetector() \\\n","    .setInputCols([\"document\"]) \\\n","    .setOutputCol(\"sentence\")\n","\n","tokenizer = Tokenizer() \\\n","    .setInputCols([\"sentence\"]) \\\n","    .setOutputCol(\"token\")\n","\n","contextual_parser = ContextualParserApproach() \\\n","    .setInputCols([\"sentence\", \"token\"])\\\n","    .setOutputCol(\"entity\")\\\n","    .setJsonPath(\"cities.json\")\\\n","    .setCaseSensitive(True)\\\n","    .setDictionary('cities.tsv', options={\"orientation\":\"vertical\"})\n","\n","chunk_converter = ChunkConverter() \\\n","    .setInputCols([\"entity\"]) \\\n","    .setOutputCol(\"ner_chunk\")\n","\n","parserPipeline = Pipeline(stages=[\n","    document_assembler,\n","    sentence_detector,\n","    tokenizer,\n","    contextual_parser,\n","    chunk_converter,\n","    ])"],"metadata":{"id":"yjQmEjWNQHvx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a lightpipeline model\n","empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n","\n","parserModel = parserPipeline.fit(empty_data)\n","\n","light_model = LightPipeline(parserModel)"],"metadata":{"id":"HxPVaa4fYCb5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Annotate the sample text\n","annotations = light_model.fullAnnotate(sample_text)[0]"],"metadata":{"id":"3T1xnn0vYOMA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check outputs\n","annotations.get('ner_chunk')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NY5zFoeSb8fg","outputId":"b3e9b78b-a423-49a4-9341-c06ed9ecaf21","executionInfo":{"status":"ok","timestamp":1693470131526,"user_tz":-180,"elapsed":7,"user":{"displayName":"Monster C","userId":"08787989274818793476"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Annotation(chunk, 40, 47, New York, {'field': 'City', 'tokenIndex': '9', 'ner_source': 'ner_chunk', 'normalized': 'City', 'confidenceValue': '0.50', 'entity': 'City', 'sentence': '0'}, []),\n"," Annotation(chunk, 95, 105, San Antonio, {'field': 'City', 'tokenIndex': '10', 'ner_source': 'ner_chunk', 'normalized': 'City', 'confidenceValue': '0.50', 'entity': 'City', 'sentence': '1'}, []),\n"," Annotation(chunk, 111, 121, Gotham City, {'field': 'City', 'tokenIndex': '13', 'ner_source': 'ner_chunk', 'normalized': 'City', 'confidenceValue': '0.50', 'entity': 'City', 'sentence': '1'}, [])]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# Visualize outputs\n","from sparknlp_display import NerVisualizer\n","\n","visualiser = NerVisualizer()\n","\n","visualiser.display(annotations, label_col='ner_chunk', document_col='document', save_path=\"display_result.html\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"id":"BBRie-vWXzK6","outputId":"ee05cf14-a44e-4f7b-c151-ec923e373de7","executionInfo":{"status":"ok","timestamp":1693470132276,"user_tz":-180,"elapsed":754,"user":{"displayName":"Monster C","userId":"08787989274818793476"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n","    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n","    \n","    .spark-nlp-display-scroll-entities {\n","        border: 1px solid #E7EDF0;\n","        border-radius: 3px;\n","        text-align: justify;\n","        \n","    }\n","    .spark-nlp-display-scroll-entities span {  \n","        font-size: 14px;\n","        line-height: 24px;\n","        color: #536B76;\n","        font-family: 'Montserrat', sans-serif !important;\n","    }\n","    \n","    .spark-nlp-display-entity-wrapper{\n","    \n","        display: inline-grid;\n","        text-align: center;\n","        border-radius: 4px;\n","        margin: 0 2px 5px 2px;\n","        padding: 1px\n","    }\n","    .spark-nlp-display-entity-name{\n","        font-size: 14px;\n","        line-height: 24px;\n","        font-family: 'Montserrat', sans-serif !important;\n","        \n","        background: #f1f2f3;\n","        border-width: medium;\n","        text-align: center;\n","        \n","        font-weight: 400;\n","        \n","        border-radius: 5px;\n","        padding: 2px 5px;\n","        display: block;\n","        margin: 3px 2px;\n","    \n","    }\n","    .spark-nlp-display-entity-type{\n","        font-size: 14px;\n","        line-height: 24px;\n","        color: #ffffff;\n","        font-family: 'Montserrat', sans-serif !important;\n","        \n","        text-transform: uppercase;\n","        \n","        font-weight: 500;\n","\n","        display: block;\n","        padding: 3px 5px;\n","    }\n","    \n","    .spark-nlp-display-entity-resolution{\n","        font-size: 14px;\n","        line-height: 24px;\n","        color: #ffffff;\n","        font-family: 'Vistol Regular', sans-serif !important;\n","        \n","        text-transform: uppercase;\n","        \n","        font-weight: 500;\n","\n","        display: block;\n","        padding: 3px 5px;\n","    }\n","    \n","    .spark-nlp-display-others{\n","        font-size: 14px;\n","        line-height: 24px;\n","        font-family: 'Montserrat', sans-serif !important;\n","        \n","        font-weight: 400;\n","    }\n","\n","</style>\n"," <span class=\"spark-nlp-display-others\" style=\"background-color: white\">Peter Parker is a nice guy and lives in </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #128B7C\"><span class=\"spark-nlp-display-entity-name\">New York </span><span class=\"spark-nlp-display-entity-type\">City</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> . Bruce Wayne is also a nice guy and lives in </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #128B7C\"><span class=\"spark-nlp-display-entity-name\">San Antonio </span><span class=\"spark-nlp-display-entity-type\">City</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> and </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #128B7C\"><span class=\"spark-nlp-display-entity-name\">Gotham City </span><span class=\"spark-nlp-display-entity-type\">City</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> . </span></div>"]},"metadata":{}}]},{"cell_type":"markdown","source":["Feel free to experiment with the annotator parameters and JSON properties to see how the output might change."],"metadata":{"id":"gtD1HJ7SABU7"}},{"cell_type":"markdown","source":["## Example 2: Detect Gender and Age"],"metadata":{"id":"lR6FnTsyBAjn"}},{"cell_type":"code","source":["# Here's some sample text\n","sample_text = \"\"\"A 28 year old female with a history of gestational diabetes mellitus diagnosed 8 years ago.\n","                 3 years ago, he reported an episode of HTG-induced pancreatitis .\n","                 5 months old boy with repeated concussions.\"\"\""],"metadata":{"id":"tXK5CLYfDhNB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a dictionary to detect gender\n","gender = '''male,man,male,boy,gentleman,he,him\n","female,woman,female,girl,lady,old-lady,she,her\n","neutral,they,neutral,it'''\n","\n","with open('gender.csv', 'w') as f:\n","    f.write(gender)\n","\n","# Check what dictionary looks like\n","!cat gender.csv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6ba6f169-4a10-4451-91cc-6807a14db2cb","id":"0OZqYhDFDhNB","executionInfo":{"status":"ok","timestamp":1693470132277,"user_tz":-180,"elapsed":6,"user":{"displayName":"Monster C","userId":"08787989274818793476"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["male,man,male,boy,gentleman,he,him\n","female,woman,female,girl,lady,old-lady,she,her\n","neutral,they,neutral,it"]}]},{"cell_type":"code","source":["# Create JSON file for gender\n","gender = {\n","  \"entity\": \"Gender\",\n","  \"ruleScope\": \"sentence\",\n","  \"completeMatchRegex\": \"true\",\n","  \"matchScope\":\"token\"\n","}\n","\n","import json\n","with open('gender.json', 'w') as f:\n","    json.dump(gender, f)"],"metadata":{"id":"U5-tYW1iDhNB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create JSON file for age\n","age = {\n","  \"entity\": \"Age\",\n","  \"ruleScope\": \"sentence\",\n","  \"matchScope\":\"token\",\n","  \"regex\":\"\\\\d{1,3}\",\n","  \"prefix\":[\"age of\", \"age\"],\n","  \"suffix\": [\"-years-old\", \"years-old\", \"-year-old\",\n","             \"-months-old\", \"-month-old\", \"-months-old\",\n","             \"-day-old\", \"-days-old\", \"month old\",\n","             \"days old\", \"year old\", \"years old\",\n","             \"years\", \"year\", \"months\", \"old\"],\n","  \"contextLength\": 25,\n","  \"contextException\": [\"ago\"],\n","  \"exceptionDistance\": 12\n","}\n","\n","with open('age.json', 'w') as f:\n","    json.dump(age, f)"],"metadata":{"id":"bynTPmlQHEPv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build pipeline\n","document_assembler = DocumentAssembler() \\\n","    .setInputCol(\"text\") \\\n","    .setOutputCol(\"document\")\n","\n","sentence_detector = SentenceDetector() \\\n","    .setInputCols([\"document\"]) \\\n","    .setOutputCol(\"sentence\")\n","\n","tokenizer = Tokenizer() \\\n","    .setInputCols([\"sentence\"]) \\\n","    .setOutputCol(\"token\")\n","\n","gender_contextual_parser = ContextualParserApproach() \\\n","    .setInputCols([\"sentence\", \"token\"]) \\\n","    .setOutputCol(\"chunk_gender\") \\\n","    .setJsonPath(\"gender.json\") \\\n","    .setCaseSensitive(False) \\\n","    .setDictionary('gender.csv', options={\"delimiter\":\",\"}) \\\n","    .setPrefixAndSuffixMatch(False)\n","\n","age_contextual_parser = ContextualParserApproach() \\\n","    .setInputCols([\"sentence\", \"token\"]) \\\n","    .setOutputCol(\"chunk_age\") \\\n","    .setJsonPath(\"age.json\") \\\n","    .setCaseSensitive(False) \\\n","    .setPrefixAndSuffixMatch(False)\\\n","    .setShortestContextMatch(True)\\\n","    .setOptionalContextRules(False)\n","\n","chunk_merger = ChunkMergeApproach() \\\n","    .setInputCols([\"chunk_gender\", \"chunk_age\"]) \\\n","    .setOutputCol(\"ner_chunk\")\n","\n","parserPipeline = Pipeline(stages=[\n","    document_assembler,\n","    sentence_detector,\n","    tokenizer,\n","    gender_contextual_parser,\n","    age_contextual_parser,\n","    chunk_merger\n","    ])"],"metadata":{"id":"pBQVujx1DhNC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create a lightpipeline model\n","empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n","\n","parserModel = parserPipeline.fit(empty_data)\n","\n","light_model = LightPipeline(parserModel)"],"metadata":{"id":"36ZxeslFDhNC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Annotate the sample text\n","annotations = light_model.fullAnnotate(sample_text)[0]"],"metadata":{"id":"OIhdQ4IjDhNC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check outputs\n","annotations.get('ner_chunk')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"78f75b4c-af44-44cc-9ee3-4a89ad248071","id":"1tdgMbaWDhNC","executionInfo":{"status":"ok","timestamp":1693470134720,"user_tz":-180,"elapsed":13,"user":{"displayName":"Monster C","userId":"08787989274818793476"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Annotation(chunk, 2, 3, 28, {'tokenIndex': '1', 'entity': 'Age', 'field': 'Age', 'chunk': '0', 'normalized': '', 'sentence': '0', 'confidenceValue': '0.74'}, []),\n"," Annotation(chunk, 14, 19, female, {'tokenIndex': '4', 'entity': 'Gender', 'field': 'Gender', 'chunk': '1', 'normalized': 'female', 'sentence': '0', 'confidenceValue': '0.50'}, []),\n"," Annotation(chunk, 122, 123, he, {'tokenIndex': '4', 'entity': 'Gender', 'field': 'Gender', 'chunk': '2', 'normalized': 'male', 'sentence': '1', 'confidenceValue': '0.50'}, []),\n"," Annotation(chunk, 192, 192, 5, {'tokenIndex': '0', 'entity': 'Age', 'field': 'Age', 'chunk': '3', 'normalized': '', 'sentence': '2', 'confidenceValue': '0.74'}, []),\n"," Annotation(chunk, 205, 207, boy, {'tokenIndex': '3', 'entity': 'Gender', 'field': 'Gender', 'chunk': '4', 'normalized': 'male', 'sentence': '2', 'confidenceValue': '0.50'}, [])]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# Visualize outputs\n","from sparknlp_display import NerVisualizer\n","\n","visualiser = NerVisualizer()\n","\n","visualiser.display(annotations, label_col='ner_chunk', document_col='document', save_path=\"display_result_2.html\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":230},"outputId":"918e14aa-bbf2-475c-9b30-8556b02cfd30","id":"xw0mQKMxDhND","executionInfo":{"status":"ok","timestamp":1693470134721,"user_tz":-180,"elapsed":10,"user":{"displayName":"Monster C","userId":"08787989274818793476"}}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n","    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n","    \n","    .spark-nlp-display-scroll-entities {\n","        border: 1px solid #E7EDF0;\n","        border-radius: 3px;\n","        text-align: justify;\n","        \n","    }\n","    .spark-nlp-display-scroll-entities span {  \n","        font-size: 14px;\n","        line-height: 24px;\n","        color: #536B76;\n","        font-family: 'Montserrat', sans-serif !important;\n","    }\n","    \n","    .spark-nlp-display-entity-wrapper{\n","    \n","        display: inline-grid;\n","        text-align: center;\n","        border-radius: 4px;\n","        margin: 0 2px 5px 2px;\n","        padding: 1px\n","    }\n","    .spark-nlp-display-entity-name{\n","        font-size: 14px;\n","        line-height: 24px;\n","        font-family: 'Montserrat', sans-serif !important;\n","        \n","        background: #f1f2f3;\n","        border-width: medium;\n","        text-align: center;\n","        \n","        font-weight: 400;\n","        \n","        border-radius: 5px;\n","        padding: 2px 5px;\n","        display: block;\n","        margin: 3px 2px;\n","    \n","    }\n","    .spark-nlp-display-entity-type{\n","        font-size: 14px;\n","        line-height: 24px;\n","        color: #ffffff;\n","        font-family: 'Montserrat', sans-serif !important;\n","        \n","        text-transform: uppercase;\n","        \n","        font-weight: 500;\n","\n","        display: block;\n","        padding: 3px 5px;\n","    }\n","    \n","    .spark-nlp-display-entity-resolution{\n","        font-size: 14px;\n","        line-height: 24px;\n","        color: #ffffff;\n","        font-family: 'Vistol Regular', sans-serif !important;\n","        \n","        text-transform: uppercase;\n","        \n","        font-weight: 500;\n","\n","        display: block;\n","        padding: 3px 5px;\n","    }\n","    \n","    .spark-nlp-display-others{\n","        font-size: 14px;\n","        line-height: 24px;\n","        font-family: 'Montserrat', sans-serif !important;\n","        \n","        font-weight: 400;\n","    }\n","\n","</style>\n"," <span class=\"spark-nlp-display-others\" style=\"background-color: white\">A </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #ffe0ac\"><span class=\"spark-nlp-display-entity-name\">28 </span><span class=\"spark-nlp-display-entity-type\">Age</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> year old </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #ffacb7\"><span class=\"spark-nlp-display-entity-name\">female </span><span class=\"spark-nlp-display-entity-type\">Gender</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> with a history of gestational diabetes mellitus diagnosed 8 years ago.<br>                 3 years ago, </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #ffacb7\"><span class=\"spark-nlp-display-entity-name\">he </span><span class=\"spark-nlp-display-entity-type\">Gender</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> reported an episode of HTG-induced pancreatitis .<br>                 </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #ffe0ac\"><span class=\"spark-nlp-display-entity-name\">5 </span><span class=\"spark-nlp-display-entity-type\">Age</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> months old </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #ffacb7\"><span class=\"spark-nlp-display-entity-name\">boy </span><span class=\"spark-nlp-display-entity-type\">Gender</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> with repeated concussions.</span></div>"]},"metadata":{}}]},{"cell_type":"markdown","source":["Feel free to experiment with the annotator parameters and JSON properties to see how the output might change. If you're looking to work on running the pipeline on a full dataset, just make sure to use the `fit()` and `transform()` methods directly on your dataset instead of using the lightpipeline."],"metadata":{"id":"iOtVACnl_t3n"}},{"cell_type":"code","source":["# Create example dataframe with sample text\n","data = spark.createDataFrame([[sample_text]]).toDF(\"text\")\n","\n","# Fit and show\n","results = parserPipeline.fit(data).transform(data)\n","results.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L6nRUUrRRfoB","outputId":"8614dafc-cd86-4019-ed4a-0930ba326ed1","executionInfo":{"status":"ok","timestamp":1693470141502,"user_tz":-180,"elapsed":6788,"user":{"displayName":"Monster C","userId":"08787989274818793476"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|                text|            document|            sentence|               token|        chunk_gender|           chunk_age|           ner_chunk|\n","+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","|A 28 year old fem...|[{document, 0, 23...|[{document, 0, 90...|[{token, 0, 0, A,...|[{chunk, 14, 19, ...|[{chunk, 2, 3, 28...|[{chunk, 2, 3, 28...|\n","+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n","\n"]}]},{"cell_type":"code","source":["results.select(\"chunk_age.result\").show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"orB8zsF9G6H4","outputId":"dfec5041-eec8-4e50-cba0-0a6d6c1a5eab","executionInfo":{"status":"ok","timestamp":1693470142361,"user_tz":-180,"elapsed":862,"user":{"displayName":"Monster C","userId":"08787989274818793476"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------+\n","| result|\n","+-------+\n","|[28, 5]|\n","+-------+\n","\n"]}]},{"cell_type":"code","source":["results.select(\"chunk_gender.result\").show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qzn7Qv3_HERi","outputId":"bd677835-52a5-40d6-d05f-7fba87f56476","executionInfo":{"status":"ok","timestamp":1693470142852,"user_tz":-180,"elapsed":494,"user":{"displayName":"Monster C","userId":"08787989274818793476"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------+\n","|           result|\n","+-----------------+\n","|[female, he, boy]|\n","+-----------------+\n","\n"]}]},{"cell_type":"markdown","source":["# Pretrained Contextual Parser Models"],"metadata":{"id":"N7zEB_0hp5xo"}},{"cell_type":"markdown","source":["<center><b>Contextual Parser Model List</b>\n","\n","|index|model| description|\n","|-----:|:-----|:-----|\n","| 1| [date_of_birth_parser](https://nlp.johnsnowlabs.com/2023/08/22/date_of_birth_parser_en.html)  | This model can extract date-of-birth (DOB) entities in clinical texts. |\n"],"metadata":{"id":"lEvxRyunrKeT"}},{"cell_type":"markdown","source":["## date-of-birth"],"metadata":{"id":"I4RLv0i_P1zi"}},{"cell_type":"code","source":["document_assembler = DocumentAssembler() \\\n","    .setInputCol(\"text\") \\\n","    .setOutputCol(\"document\")\n","\n","sentence_detector = SentenceDetectorDLModel.pretrained(\"sentence_detector_dl_healthcare\",\"en\",\"clinical/models\")\\\n","    .setInputCols([\"document\"])\\\n","    .setOutputCol(\"sentence\")\n","\n","tokenizer = Tokenizer() \\\n","    .setInputCols([\"sentence\"]) \\\n","    .setOutputCol(\"token\")\n","\n","dob_contextual_parser = ContextualParserModel.pretrained(\"date_of_birth_parser\", \"en\", \"clinical/models\") \\\n","    .setInputCols([\"sentence\", \"token\"]) \\\n","    .setOutputCol(\"chunk_dob\")\n","\n","chunk_converter = ChunkConverter() \\\n","    .setInputCols([\"chunk_dob\"]) \\\n","    .setOutputCol(\"ner_chunk\")\n","\n","parserPipeline = Pipeline(stages=[\n","    document_assembler,\n","    sentence_detector,\n","    tokenizer,\n","    dob_contextual_parser,\n","    chunk_converter\n","    ])\n","\n","model = parserPipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aiWf1HAvp_Zt","executionInfo":{"status":"ok","timestamp":1693470162211,"user_tz":-180,"elapsed":19362,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"f170e253-b4c6-4938-bc72-ba7ae3d0cc4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sentence_detector_dl_healthcare download started this may take some time.\n","Approximate size to download 367.3 KB\n","[OK!]\n","date_of_birth_parser download started this may take some time.\n","[OK!]\n"]}]},{"cell_type":"code","source":["text = \"\"\"\n","Record date : 2081-01-04\n","DB : 11.04.1962\n","DT : 12-03-1978\n","DOD : 10.25.23\n","\n","SOCIAL HISTORY:\n","She was born on Nov 04, 1962 in London and got married on 04/05/1979. When she got pregnant on 15 May 1079, the doctor wanted to verify her DOB was November 4, 1962. Her date of birth was confirmed to be 11-04-1962, the patient is 45 years old on 25 Sep 2007.\n","\n","PROCEDURES:\n","Patient was evaluated on 1988-03-15 for allergies. She was seen by the endocrinology service and she was discharged on 9/23/1988.\n","\n","MEDICATIONS\n","1. Coumadin 1 mg daily. Last INR was on August 14, 2007, and her INR was 2.3.\"\"\"\n","\n","result = model.transform(spark.createDataFrame([[text]]).toDF(\"text\"))"],"metadata":{"id":"uYvSmz9PqhBl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","\n","result.select(F.explode(F.arrays_zip(result.chunk_dob.result,\n","                                     result.chunk_dob.begin,\n","                                     result.chunk_dob.end,\n","                                     result.chunk_dob.metadata)).alias(\"cols\")) \\\n","      .select(F.expr(\"cols['3']['sentence']\").alias(\"sentence_id\"),\n","              F.expr(\"cols['0']\").alias(\"chunk\"),\n","              F.expr(\"cols['2']\").alias(\"end\"),\n","              F.expr(\"cols['3']\").alias(\"ner_label\"))\\\n","      .show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7M5I3p7QqF2D","executionInfo":{"status":"ok","timestamp":1693470164039,"user_tz":-180,"elapsed":1832,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"b99226f4-82f8-491a-9f25-6288a9af557c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------+----------------+---+----------------------------------------------------------------------------------------+\n","|sentence_id|chunk           |end|ner_label                                                                               |\n","+-----------+----------------+---+----------------------------------------------------------------------------------------+\n","|1          |11.04.1962      |40 |{field -> DOB, tokenIndex -> 2, normalized -> , confidenceValue -> 0.72, sentence -> 1} |\n","|3          |10.25.23        |71 |{field -> DOB, tokenIndex -> 2, normalized -> , confidenceValue -> 0.53, sentence -> 3} |\n","|3          |Nov 04, 1962    |117|{field -> DOB, tokenIndex -> 10, normalized -> , confidenceValue -> 0.70, sentence -> 3}|\n","|4          |November 4, 1962|253|{field -> DOB, tokenIndex -> 17, normalized -> , confidenceValue -> 0.70, sentence -> 4}|\n","|5          |11-04-1962      |303|{field -> DOB, tokenIndex -> 8, normalized -> , confidenceValue -> 0.54, sentence -> 5} |\n","+-----------+----------------+---+----------------------------------------------------------------------------------------+\n","\n"]}]},{"cell_type":"code","source":["# Visualize outputs\n","from sparknlp_display import NerVisualizer\n","\n","light_model = LightPipeline(model)\n","\n","# Annotate the sample text\n","annotations = light_model.fullAnnotate(text)[0]\n","\n","visualiser = NerVisualizer()\n","\n","visualiser.display(annotations, label_col='ner_chunk', document_col='document', save_path=\"display_result_2.html\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":565},"id":"rSbo2tUlqJzJ","executionInfo":{"status":"ok","timestamp":1693470164362,"user_tz":-180,"elapsed":326,"user":{"displayName":"Monster C","userId":"08787989274818793476"}},"outputId":"4f86d626-e7d4-4118-fb3a-1d03d40f7009"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","<style>\n","    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');\n","    @import url('https://fonts.googleapis.com/css2?family=Vistol Regular:wght@300;400;500;600;700&display=swap');\n","    \n","    .spark-nlp-display-scroll-entities {\n","        border: 1px solid #E7EDF0;\n","        border-radius: 3px;\n","        text-align: justify;\n","        \n","    }\n","    .spark-nlp-display-scroll-entities span {  \n","        font-size: 14px;\n","        line-height: 24px;\n","        color: #536B76;\n","        font-family: 'Montserrat', sans-serif !important;\n","    }\n","    \n","    .spark-nlp-display-entity-wrapper{\n","    \n","        display: inline-grid;\n","        text-align: center;\n","        border-radius: 4px;\n","        margin: 0 2px 5px 2px;\n","        padding: 1px\n","    }\n","    .spark-nlp-display-entity-name{\n","        font-size: 14px;\n","        line-height: 24px;\n","        font-family: 'Montserrat', sans-serif !important;\n","        \n","        background: #f1f2f3;\n","        border-width: medium;\n","        text-align: center;\n","        \n","        font-weight: 400;\n","        \n","        border-radius: 5px;\n","        padding: 2px 5px;\n","        display: block;\n","        margin: 3px 2px;\n","    \n","    }\n","    .spark-nlp-display-entity-type{\n","        font-size: 14px;\n","        line-height: 24px;\n","        color: #ffffff;\n","        font-family: 'Montserrat', sans-serif !important;\n","        \n","        text-transform: uppercase;\n","        \n","        font-weight: 500;\n","\n","        display: block;\n","        padding: 3px 5px;\n","    }\n","    \n","    .spark-nlp-display-entity-resolution{\n","        font-size: 14px;\n","        line-height: 24px;\n","        color: #ffffff;\n","        font-family: 'Vistol Regular', sans-serif !important;\n","        \n","        text-transform: uppercase;\n","        \n","        font-weight: 500;\n","\n","        display: block;\n","        padding: 3px 5px;\n","    }\n","    \n","    .spark-nlp-display-others{\n","        font-size: 14px;\n","        line-height: 24px;\n","        font-family: 'Montserrat', sans-serif !important;\n","        \n","        font-weight: 400;\n","    }\n","\n","</style>\n"," <span class=\"spark-nlp-display-others\" style=\"background-color: white\"><br>Record date : 2081-01-04<br>DB : </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #AE9531\"><span class=\"spark-nlp-display-entity-name\">11.04.1962 </span><span class=\"spark-nlp-display-entity-type\">DOB</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"><br>DT : 12-03-1978<br>DOD : </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #AE9531\"><span class=\"spark-nlp-display-entity-name\">10.25.23 </span><span class=\"spark-nlp-display-entity-type\">DOB</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"><br><br>SOCIAL HISTORY:<br>She was born on </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #AE9531\"><span class=\"spark-nlp-display-entity-name\">Nov 04, 1962 </span><span class=\"spark-nlp-display-entity-type\">DOB</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\"> in London and got married on 04/05/1979. When she got pregnant on 15 May 1079, the doctor wanted to verify her DOB was </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #AE9531\"><span class=\"spark-nlp-display-entity-name\">November 4, 1962 </span><span class=\"spark-nlp-display-entity-type\">DOB</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">. Her date of birth was confirmed to be </span><span class=\"spark-nlp-display-entity-wrapper\" style=\"background-color: #AE9531\"><span class=\"spark-nlp-display-entity-name\">11-04-1962 </span><span class=\"spark-nlp-display-entity-type\">DOB</span></span><span class=\"spark-nlp-display-others\" style=\"background-color: white\">, the patient is 45 years old on 25 Sep 2007.<br><br>PROCEDURES:<br>Patient was evaluated on 1988-03-15 for allergies. She was seen by the endocrinology service and she was discharged on 9/23/1988.<br><br>MEDICATIONS<br>1. Coumadin 1 mg daily. Last INR was on August 14, 2007, and her INR was 2.3.</span></div>"]},"metadata":{}}]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}