{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.2.Contextual_Parser_Rule_Based_NER_v276.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8W51t04BN6B"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21lTnEqRBd0s"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/Spark%20v2.7.6%20Notebooks/1.2.Contextual_Parser_Rule_Based_NER.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-vwpSj3BSbj"
      },
      "source": [
        "# 1.2 ContextualParser (Rule Based NER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SBh1CNgOg54"
      },
      "source": [
        "## Colab Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iW6JfkPdNnG4"
      },
      "source": [
        "import json\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "license_keys = files.upload()\n",
        "\n",
        "with open(list(license_keys.keys())[0]) as f:\n",
        "    license_keys = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6shpUJDy83Io"
      },
      "source": [
        "%%capture\n",
        "for k,v in license_keys.items(): \n",
        "    %set_env $k=$v\n",
        "\n",
        "!wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/jsl_colab_setup.sh\n",
        "!bash jsl_colab_setup.sh -p 2.4.4\n",
        "\n",
        "! pip install spark-nlp-display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53Qom33h4nvu",
        "outputId": "56198042-85a7-49c6-b24c-4871e7770083"
      },
      "source": [
        "import json\n",
        "import os\n",
        "from pyspark.ml import Pipeline,PipelineModel\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp_jsl.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp_jsl\n",
        "import sparknlp\n",
        "\n",
        "params = {\"spark.driver.memory\":\"16G\",\n",
        "\"spark.kryoserializer.buffer.max\":\"2000M\",\n",
        "\"spark.driver.maxResultSize\":\"2000M\"}\n",
        "\n",
        "spark = sparknlp_jsl.start(license_keys['SECRET'],params=params)\n",
        "\n",
        "print (\"Spark NLP Version :\", sparknlp.version())\n",
        "print (\"Spark NLP_JSL Version :\", sparknlp_jsl.version())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark NLP Version : 2.7.4\n",
            "Spark NLP_JSL Version : 2.7.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "TYAJM4qXQ0HP",
        "outputId": "818b744c-e473-4f47-fa22-a50c00ae938e"
      },
      "source": [
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://4797ffb0b537:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.4</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP Licensed</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f03bc5756d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwBzjFCdSQe7"
      },
      "source": [
        "## How it works\n",
        "\n",
        "This annotator is a kind of RegexMatcher based on a JSON file, that is defined through the parameter `setJsonPath()`\n",
        "\n",
        "In this JSON file, you define the regex that you want to match along with the information that will output on metadata field.\n",
        "\n",
        "For example here, you define the name of an entity that will categorize the matches, the regex value and the  `matchScope` that will tell the regex whether to make a full match or a partial match\n",
        "\n",
        "```\n",
        "{\n",
        "  \"entity\": \"Stage\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"[cpyrau]?[T][0-9X?][a-z^cpyrau]*\",\n",
        "  \"matchScope\": \"token\"\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "Ignore the `ruleScope` for the moment, it's always at a `sentence` level. Which means find match on each sentence. So, for example for this text:\n",
        "```\n",
        "A patient has liver metastases pT1bN0M0 and the T5 primary site may be colon or lung. If the primary site is not clearly identified , this case is cT4bcN2M1, Stage Grouping 88. N4 A child T?N3M1  has soft tissue aM3 sarcoma and the staging has been left unstaged. Both clinical and pathologic staging would be coded pT1bN0M0 as unstageable cT3cN2.Medications started.\n",
        "```\n",
        "\n",
        "The expected result will be:\n",
        "```\n",
        "val expectedResult = Array(\"pT1bN0M0\", \"T5\", \"cT4bcN2M1\", \"T?N3M1\", \"pT1bN0M0\", \"cT3cN2.Medications\")\n",
        "val expectedMetadata =\n",
        "Array(Map(\"field\" -> \"Stage\", \"normalized\" -> \"\", \"confidenceValue\" -> \"0.13\", \"hits\" -> \"regex\", \"sentence\" -> \"0\"),\n",
        "\t  Map(\"field\" -> \"Stage\", \"normalized\" -> \"\", \"confidenceValue\" -> \"0.13\", \"hits\" -> \"regex\", \"sentence\" -> \"0\"),\n",
        "\t  Map(\"field\" -> \"Stage\", \"normalized\" -> \"\", \"confidenceValue\" -> \"0.13\", \"hits\" -> \"regex\", \"sentence\" -> \"1\"),\n",
        "\t  Map(\"field\" -> \"Stage\", \"normalized\" -> \"\", \"confidenceValue\" -> \"0.13\", \"hits\" -> \"regex\", \"sentence\" -> \"2\"),\n",
        "\t  Map(\"field\" -> \"Stage\", \"normalized\" -> \"\", \"confidenceValue\" -> \"0.13\", \"hits\" -> \"regex\", \"sentence\" -> \"3\"),\n",
        "\t  Map(\"field\" -> \"Stage\", \"normalized\" -> \"\", \"confidenceValue\" -> \"0.13\", \"hits\" -> \"regex\", \"sentence\" -> \"3\")\n",
        "\t )\n",
        "```\n",
        "\n",
        "Whereas, using a `matchScope` at sub-token level it will output:\n",
        "\n",
        "```\n",
        "val expectedResult = Array(\"pT1b\", \"T5\", \"cT4bc\", \"T?\", \"pT1b\", \"cT3c\")\n",
        "val expectedMetadata =\n",
        "Array(Map(\"field\" -> \"Stage\", \"normalized\" -> \"\", \"confidenceValue\" -> \"0.13\", \"hits\" -> \"regex\", \"sentence\" -> \"0\"),\n",
        "Map(\"field\" -> \"Stage\", \"normalized\" -> \"\", \"confidenceValue\" -> \"0.13\", \"hits\" -> \"regex\", \"sentence\" -> \"0\"),\n",
        "Map(\"field\" -> \"Stage\", \"normalized\" -> \"\", \"confidenceValue\" -> \"0.13\", \"hits\" -> \"regex\", \"sentence\" -> \"1\"),\n",
        "Map(\"field\" -> \"Stage\", \"normalized\" -> \"\", \"confidenceValue\" -> \"0.13\", \"hits\" -> \"regex\", \"sentence\" -> \"2\"),\n",
        "Map(\"field\" -> \"Stage\", \"normalized\" -> \"\", \"confidenceValue\" -> \"0.13\", \"hits\" -> \"regex\", \"sentence\" -> \"3\"),\n",
        "Map(\"field\" -> \"Stage\", \"normalized\" -> \"\", \"confidenceValue\" -> \"0.13\", \"hits\" -> \"regex\", \"sentence\" -> \"3\")\n",
        ")\n",
        "```\n",
        "\n",
        "The `confidence` value is another feature, which is computed  basically using a heuristic approach based on how many matches it has.\n",
        "\n",
        "To clarify how many matches, this is an example of the JSON file with additional fields that will define the match we want to get\n",
        "\n",
        "```\n",
        "{\n",
        "  \"entity\": \"Gender\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"matchScope\": \"token\",\n",
        "  \"prefix\": [\"birth\", \"growing\", \"assessment\"],\n",
        "  \"suffix\": [\"faster\", \"velocities\"],\n",
        "  \"contextLength\": 50,\n",
        "  \"context\": [\"typical\", \"grows\"]\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "for example, `prefix` and `suffix` refer to the words that are required to be near the word we want to match.\n",
        "\n",
        "This two work also with `contextLength` that will tell the maximum distance that prefix or suffix words can be away from the word to match, whereas `context` are words that must be immediately after or before the word to match\n",
        "\n",
        "Now, there is another feature that can be used. The `dictionary` parameter. In this parameter, you define the set of words that you want to match and the word that will replace this match.\n",
        "\n",
        "For example, with this definition, you are telling `ContextualParser` that when words `woman`, `female`, and `girl` are matched those will be replaced by `female`, whereas `man`, `male`, `boy` and `gentleman` are matched those will be replaced by `male`. \n",
        "\n",
        "```\n",
        "female  woman   female  girl\n",
        "male    man male    boy gentleman\n",
        "```\n",
        "\n",
        "So, for example for this text:\n",
        "\n",
        "```\n",
        "At birth, the typical boy is growing slightly faster than the typical girl, but the velocities become equal at about seven months, and then the girl grows faster until four years. From then until adolescence no differences in velocity can be detected.\n",
        "```\n",
        "\n",
        "The expected output of the annotator will be:\n",
        "\n",
        "```\n",
        "val expectedResult = Array(\"boy\", \"girl\", \"girl\")\n",
        "val expectedMetadata =\n",
        "Array(Map(\"field\" -> \"Gender\", \"normalized\" -> \"male\", \"confidenceValue\" -> \"0.13\", \"hits\" -> \"regex\", \"sentence\" -> \"0\"),\n",
        "Map(\"field\" -> \"Gender\", \"normalized\" -> \"female\", \"confidenceValue\" -> \"0.13\", \"hits\" -> \"regex\", \"sentence\" -> \"0\"),\n",
        "Map(\"field\" -> \"Gender\", \"normalized\" -> \"female\", \"confidenceValue\" -> \"0.13\", \"hits\" -> \"regex\", \"sentence\" -> \"0\"))\n",
        "```\n",
        "\n",
        "For the `dictionary`, you just need to define a csv or tsv file, where the first element of the row is the normalized word, the other elements will be the values to match. You can define several words and elements to match just by adding another row and you set the path to the file on the parameter `setDictionary`.\n",
        "\n",
        "The `dictionary` parameter is of the type` ExternalResource` by default the delimiter is `\"\\t\"` you cand set another delimiter if you want according to your dictionary file format.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOAs_iQnPGzP",
        "outputId": "8b9f55f5-557c-48dc-ea72-c85ef99a4d72"
      },
      "source": [
        "sample_text = \"\"\"A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to \n",
        "presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced pancreatitis \n",
        "three years prior to presentation , associated with an acute hepatitis , and obesity with a body mass index \n",
        "( BMI ) of 33.5 kg/m2 , presented with a one-week history of polyuria , polydipsia , poor appetite , and vomiting.\n",
        "Two weeks prior to presentation , she was treated with a five-day course of amoxicillin for a respiratory tract infection . \n",
        "She was on metformin , glipizide , and dapagliflozin for T2DM and atorvastatin and gemfibrozil for HTG . \n",
        "She had been on dapagliflozin for six months at the time of presentation . Physical examination on presentation was \n",
        "significant for dry oral mucosa ; significantly , her abdominal examination was benign with no tenderness , guarding , \n",
        "or rigidity . Pertinent laboratory findings on admission were : serum glucose 111 mg/dl , bicarbonate 18 mmol/l , \n",
        "anion gap 20 , creatinine 0.4 mg/dL , triglycerides 508 mg/dL , total cholesterol 122 mg/dL , glycated hemoglobin \n",
        "( HbA1c ) 10% , and venous pH 7.27 . Serum lipase was normal at 43 U/L . Serum acetone levels could not be assessed \n",
        "as blood samples kept hemolyzing due to significant lipemia .\n",
        "The patient was initially admitted for starvation ketosis , as she reported poor oral intake for three days prior \n",
        "to admission . However , serum chemistry obtained six hours after presentation revealed her glucose was 186 mg/dL , \n",
        "the anion gap was still elevated at 21 , serum bicarbonate was 16 mmol/L , triglyceride level peaked at 2050 mg/dL , \n",
        "and lipase was 52 U/L .\n",
        " β-hydroxybutyrate level was obtained and found to be elevated at 5.29 mmol/L - the original sample was centrifuged \n",
        " and the chylomicron layer removed prior to analysis due to interference from turbidity caused by lipemia again . \n",
        " The patient was treated with an insulin drip for euDKA and HTG with a reduction in the anion gap to 13 and triglycerides \n",
        " to 1400 mg/dL , within 24 hours .\n",
        " Twenty days ago.\n",
        " Her euDKA was thought to be precipitated by her respiratory tract infection in the setting of SGLT2 inhibitor use . \n",
        " At birth the typical boy is growing slightly faster than the typical girl, but the velocities become equal at about \n",
        " seven months, and then the girl grows faster until four years. \n",
        " From then until adolescence no differences in velocity \n",
        " can be detected. 21-02-2020 \n",
        "21/04/2020\n",
        "\"\"\"\n",
        "data = spark.createDataFrame([[sample_text]]).toDF(\"text\").cache()\n",
        "\n",
        "data.show(truncate = 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------------------------------------------------------------------------------------------------+\n",
            "|                                                                                                text|\n",
            "+----------------------------------------------------------------------------------------------------+\n",
            "|A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior ...|\n",
            "+----------------------------------------------------------------------------------------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx9O46cNRcJy"
      },
      "source": [
        "## Rules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJOOQl5hgLau"
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VK9g0WPRbIG"
      },
      "source": [
        "gender = '''male,man,male,boy,gentleman,he,him\n",
        "female,woman,female,girl,lady,old-lady,she,her\n",
        "neutral,neutral'''\n",
        "\n",
        "with open('data/gender.csv', 'w') as f:\n",
        "    f.write(gender)\n",
        "\n",
        "\n",
        "gender = {\n",
        "  \"entity\": \"Gender\",\n",
        "  \"ruleScope\": \"sentence\", \n",
        "  \"completeMatchRegex\": \"true\"\n",
        "}\n",
        "\n",
        "import json\n",
        "\n",
        "with open('data/gender.json', 'w') as f:\n",
        "    json.dump(gender, f)\n",
        "\n",
        "\n",
        "date = {\n",
        "  \"entity\": \"Date \",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"regex\": \"\\\\d{1,2}[\\\\/\\\\-\\\\:]{1}(\\\\d{1,2}[\\\\/\\\\-\\\\:]{1}){0,1}\\\\d{2,4}\",\n",
        "  \"valuesDefinition\":[],\n",
        "  \"prefix\": [],\n",
        "  \"suffix\": [],\n",
        "  \"contextLength\": 150,\n",
        "  \"context\": []\n",
        "}\n",
        "\n",
        "with open('data/date.json', 'w') as f:\n",
        "    json.dump(date, f)\n",
        "\n",
        "\n",
        "age = {\n",
        "  \"entity\": \"Age\",\n",
        "  \"ruleScope\": \"sentence\",\n",
        "  \"matchScope\":\"token\",\n",
        "  \"regex\":\"\\\\s*(0?[1-9]|[1-9][0-9]|[1][1-9][1-9]|200){1,2}[\\\\s-,]+|(?i)\\\\b(?:zero|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|twenty)\\\\b(?=\\\\s*year)|\\\\b(?:(?:one|two|three|four|five|six|seven|eight|nine)? hundred(?:\\\\sand)?\\\\s)?(?:(?:twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety)[\\\\s-]?)?\\\\b(?:one|two|three|four|five|six|seven|eight|nine)?(?=\\\\syear)\",\n",
        "  \"prefix\":[\"age of\"],\n",
        "  \"suffix\": [\"-years-old\",\n",
        "             \"years-old\",\n",
        "             \"-year-old\",\n",
        "             \"-months-old\",\n",
        "             \"-month-old\",\n",
        "             \"-months-old\",\n",
        "             \"-day-old\",\n",
        "             \"-days-old\",\n",
        "             \"month old\",\n",
        "             \"days old\",\n",
        "             \"year old\",\n",
        "             \"years old\", \n",
        "             \"years\",\n",
        "             \"year\", \n",
        "             \"months\", \n",
        "             \"old\"\n",
        "              ],\n",
        "  \"contextLength\": 25,\n",
        "  \"context\": [],\n",
        "  \"contextException\": [\"ago\"],\n",
        "  \"exceptionDistance\": 10\n",
        "}\n",
        "\n",
        "with open('data/age.json', 'w') as f:\n",
        "    json.dump(age, f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDBxU88hRE3p"
      },
      "source": [
        "## Pipeline definition\n",
        "\n",
        "All rule files from the rule folder are added to the pipeline. They will generate different annotation labels that need to be consolidated. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ELXhIv6RGEq"
      },
      "source": [
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentence_detector = SentenceDetector() \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"sentence\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"token\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYToDI-ZRHg8",
        "outputId": "f2c2e127-2104-46bd-f01c-6be061c788b7"
      },
      "source": [
        "!cd data && ls -lt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 16\n",
            "-rw-r--r-- 1 root root 780 Apr  5 12:17 age.json\n",
            "-rw-r--r-- 1 root root 205 Apr  5 12:17 date.json\n",
            "-rw-r--r-- 1 root root  97 Apr  5 12:17 gender.csv\n",
            "-rw-r--r-- 1 root root  75 Apr  5 12:17 gender.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIarJj0-Tn1n"
      },
      "source": [
        "gender_contextual_parser = ContextualParserApproach() \\\n",
        "        .setInputCols([\"sentence\", \"token\"]) \\\n",
        "        .setOutputCol(\"entity_gender\") \\\n",
        "        .setJsonPath(\"data/gender.json\") \\\n",
        "        .setCaseSensitive(False) \\\n",
        "        .setContextMatch(True)\\\n",
        "        .setDictionary('data/gender.csv', read_as=ReadAs.TEXT, options={\"delimiter\":\",\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAI5XZ76UfJn"
      },
      "source": [
        "age_contextual_parser = ContextualParserApproach() \\\n",
        "        .setInputCols([\"sentence\", \"token\"]) \\\n",
        "        .setOutputCol(\"entity_age\") \\\n",
        "        .setJsonPath(\"data/age.json\") \\\n",
        "        .setCaseSensitive(False) \\\n",
        "        .setContextMatch(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqkPMs0xUs3r"
      },
      "source": [
        "date_contextual_parser = ContextualParserApproach() \\\n",
        "        .setInputCols([\"sentence\", \"token\"]) \\\n",
        "        .setOutputCol(\"entity_date\") \\\n",
        "        .setJsonPath(\"data/date.json\") \\\n",
        "        .setCaseSensitive(False) \\\n",
        "        .setContextMatch(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjkdEV_uU2vQ"
      },
      "source": [
        "parserPipeline = Pipeline(stages=[\n",
        "        document_assembler, \n",
        "        sentence_detector,\n",
        "        tokenizer,\n",
        "        gender_contextual_parser,\n",
        "        age_contextual_parser,\n",
        "        date_contextual_parser\n",
        "        ])\n",
        "\n",
        "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "\n",
        "parserModel = parserPipeline.fit(empty_data)\n",
        "\n",
        "light_model = LightPipeline(parserModel)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoFFRWxQTUOr",
        "outputId": "f277d4df-e8ce-4677-ee9e-63024a8ec0d9"
      },
      "source": [
        "annotations = light_model.fullAnnotate(sample_text)[0]\n",
        "annotations.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['document', 'entity_gender', 'token', 'entity_date', 'entity_age', 'sentence'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knG87gHJVWtS",
        "outputId": "907fe627-c64b-4985-a543-84cc09e4b715"
      },
      "source": [
        "print (annotations['entity_gender'])\n",
        "print (annotations['entity_age'])\n",
        "print (annotations['entity_date'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n",
            "[]\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmxsbggxVYUd"
      },
      "source": [
        "import random\n",
        "\n",
        "def get_color():\n",
        "    r = lambda: random.randint(100,255)\n",
        "    return '#%02X%02X%02X' % (r(),r(),r())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atAnfqYvX7eX"
      },
      "source": [
        "ner_chunks = []\n",
        "label_color = {}\n",
        "unified_entities = {'entity':[]}\n",
        "for ent_name in annotations.keys():\n",
        "    if \"entity\" in ent_name and len(annotations[ent_name])>0:\n",
        "        ner_chunks.append(ent_name)\n",
        "        label = annotations[ent_name][0].metadata['field']\n",
        "        label_color[label] = get_color()\n",
        "        unified_entities['entity'].extend(annotations[ent_name])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDqyL9w7X-nE"
      },
      "source": [
        "unified_entities['entity'].sort(key=lambda x: x.begin, reverse=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VOsWqREYPH3",
        "outputId": "1cb50484-abdb-4f32-a314-4b09f0cc6912"
      },
      "source": [
        "unified_entities['entity']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8M6YOniYU0O"
      },
      "source": [
        "## Highlighting the entites with html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN33NTAMYQL6"
      },
      "source": [
        "html_output = ''\n",
        "pos = 0\n",
        "\n",
        "for n in unified_entities['entity']:\n",
        "    if pos < n.begin and pos < len(sample_text):\n",
        "        white_text = sample_text[pos:n.begin]\n",
        "        html_output += '<span class=\"others\" style=\"background-color: white\">{}</span>'.format(white_text)\n",
        "    pos = n.end+1\n",
        "    html_output += '<span class=\"entity-wrapper\" style=\"background-color: {}\"><span class=\"entity-name\">{} </span><span class=\"entity-type\">[{}]</span></span>'.format(\n",
        "        label_color[n.metadata['field']],\n",
        "        n.result,\n",
        "        n.metadata['field'])\n",
        "\n",
        "if pos < len(sample_text):\n",
        "    html_output += '<span class=\"others\" style=\"background-color: white\">{}</span>'.format(sample_text[pos:])\n",
        "\n",
        "html_output += \"\"\"</div>\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "rjzcQ3ffYYZW",
        "outputId": "2bdc5b75-9ca4-459d-be43-961d8d7cb066"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML(html_output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<span class=\"others\" style=\"background-color: white\">A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to \n",
              "presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced pancreatitis \n",
              "three years prior to presentation , associated with an acute hepatitis , and obesity with a body mass index \n",
              "( BMI ) of 33.5 kg/m2 , presented with a one-week history of polyuria , polydipsia , poor appetite , and vomiting.\n",
              "Two weeks prior to presentation , she was treated with a five-day course of amoxicillin for a respiratory tract infection . \n",
              "She was on metformin , glipizide , and dapagliflozin for T2DM and atorvastatin and gemfibrozil for HTG . \n",
              "She had been on dapagliflozin for six months at the time of presentation . Physical examination on presentation was \n",
              "significant for dry oral mucosa ; significantly , her abdominal examination was benign with no tenderness , guarding , \n",
              "or rigidity . Pertinent laboratory findings on admission were : serum glucose 111 mg/dl , bicarbonate 18 mmol/l , \n",
              "anion gap 20 , creatinine 0.4 mg/dL , triglycerides 508 mg/dL , total cholesterol 122 mg/dL , glycated hemoglobin \n",
              "( HbA1c ) 10% , and venous pH 7.27 . Serum lipase was normal at 43 U/L . Serum acetone levels could not be assessed \n",
              "as blood samples kept hemolyzing due to significant lipemia .\n",
              "The patient was initially admitted for starvation ketosis , as she reported poor oral intake for three days prior \n",
              "to admission . However , serum chemistry obtained six hours after presentation revealed her glucose was 186 mg/dL , \n",
              "the anion gap was still elevated at 21 , serum bicarbonate was 16 mmol/L , triglyceride level peaked at 2050 mg/dL , \n",
              "and lipase was 52 U/L .\n",
              " β-hydroxybutyrate level was obtained and found to be elevated at 5.29 mmol/L - the original sample was centrifuged \n",
              " and the chylomicron layer removed prior to analysis due to interference from turbidity caused by lipemia again . \n",
              " The patient was treated with an insulin drip for euDKA and HTG with a reduction in the anion gap to 13 and triglycerides \n",
              " to 1400 mg/dL , within 24 hours .\n",
              " Twenty days ago.\n",
              " Her euDKA was thought to be precipitated by her respiratory tract infection in the setting of SGLT2 inhibitor use . \n",
              " At birth the typical boy is growing slightly faster than the typical girl, but the velocities become equal at about \n",
              " seven months, and then the girl grows faster until four years. \n",
              " From then until adolescence no differences in velocity \n",
              " can be detected. 21-02-2020 \n",
              "21/04/2020\n",
              "</span></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}