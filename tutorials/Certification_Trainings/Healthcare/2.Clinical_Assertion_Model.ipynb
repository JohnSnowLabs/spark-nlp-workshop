{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I08sFJYCxR0Z"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXZNJ2EirHSN"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/2.Clinical_Assertion_Model.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Niy3mZAjoayg"
      },
      "source": [
        "# Clinical Assertion Status Model \n",
        "\n",
        "\n",
        "The deep neural network architecture for assertion status detection in Spark NLP is based on a Bi-LSTM framework, and is a modified version of the architecture proposed by Federico Fancellu, Adam Lopez and Bonnie Webber ([Neural Networks For Negation Scope Detection](https://aclanthology.org/P16-1047.pdf)). Its goal is to classify the assertions made on given medical concepts as being present, absent, or possible in the patient, conditionally present in the patient under certain circumstances,\n",
        "hypothetically present in the patient at some future point, and\n",
        "mentioned in the patient report but associated with someoneelse.\n",
        "In the proposed implementation, input units depend on the\n",
        "target tokens (a named entity) and the neighboring words that\n",
        "are explicitly encoded as a sequence using word embeddings.\n",
        "Similar to paper mentioned above,  it is observed that that 95% of the scope tokens (neighboring words) fall in a window of 9 tokens to the left and 15\n",
        "to the right of the target tokens in the same dataset. Therefore, the same window size was implemented and it following parameters were used: learning\n",
        "rate 0.0012, dropout 0.05, batch size 64 and a maximum sentence length 250. The model has been implemented within\n",
        "Spark NLP as an annotator called AssertionDLModel. After\n",
        "training 20 epoch and measuring accuracy on the official test\n",
        "set, this implementation exceeds the latest state-of-the-art\n",
        "accuracy benchmarks as summarized as following table:\n",
        "\n",
        "|Assertion Label|Spark NLP|Latest Best|\n",
        "|-|-|-|\n",
        "|Absent       |0.944 |0.937|\n",
        "|Someone-else |0.904|0.869|\n",
        "|Conditional  |0.441|0.422|\n",
        "|Hypothetical |0.862|0.890|\n",
        "|Possible     |0.680|0.630|\n",
        "|Present      |0.953|0.957|\n",
        "|micro F1     |0.939|0.934|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okhT7AcXxben"
      },
      "source": [
        "**Colab Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPYaQizloMiq"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "if 'spark_jsl.json' not in os.listdir():\n",
        "  license_keys = files.upload()\n",
        "  os.rename(list(license_keys.keys())[0], 'spark_jsl.json')\n",
        "\n",
        "with open('spark_jsl.json') as f:\n",
        "    license_keys = json.load(f)\n",
        "\n",
        "# Defining license key-value pairs as local variables\n",
        "locals().update(license_keys)\n",
        "os.environ.update(license_keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jphatV_n_u-T"
      },
      "outputs": [],
      "source": [
        "# Installing pyspark and spark-nlp\n",
        "! pip install --upgrade -q pyspark==3.1.2 spark-nlp==$PUBLIC_VERSION\n",
        "\n",
        "# Installing NLU\n",
        "! pip install --upgrade --q nlu==4.0.1rc4 --no-dependencies\n",
        "\n",
        "# Installing Spark NLP Healthcare\n",
        "! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET\n",
        "\n",
        "# Installing Spark NLP Display Library for visualization\n",
        "! pip install -q spark-nlp-display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "id": "kMtWWqFBAMVW",
        "outputId": "fa021a8c-bb4b-4f22-c1b6-181bea4053dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark NLP Version : 4.3.0\n",
            "Spark NLP_JSL Version : 4.3.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fcecfa48e50>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b6c0b2294efe:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.1.2</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP Licensed</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "from pyspark.ml import Pipeline,PipelineModel\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "import nlu\n",
        "import sparknlp\n",
        "import sparknlp_jsl\n",
        "\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp_jsl.annotator import *\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "params = {\"spark.driver.memory\":\"16G\", \n",
        "          \"spark.kryoserializer.buffer.max\":\"2000M\", \n",
        "          \"spark.driver.maxResultSize\":\"2000M\"} \n",
        "\n",
        "spark = sparknlp_jsl.start(license_keys['SECRET'],params=params)\n",
        "\n",
        "print(\"Spark NLP Version :\", sparknlp.version())\n",
        "print(\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77mvgqSpS1Kz"
      },
      "outputs": [],
      "source": [
        "# if you want to start the session with custom params as in start function above\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def start(SECRET):\n",
        "    builder = SparkSession.builder \\\n",
        "        .appName(\"Spark NLP Licensed\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .config(\"spark.driver.memory\", \"16G\") \\\n",
        "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "        .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
        "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:\"+PUBLIC_VERSION) \\\n",
        "        .config(\"spark.jars\", \"https://pypi.johnsnowlabs.com/\"+SECRET+\"/spark-nlp-jsl-\"+JSL_VERSION+\".jar\")\n",
        "      \n",
        "    return builder.getOrCreate()\n",
        "\n",
        "#spark = start(SECRET)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMU4sAJQ0Rhs"
      },
      "source": [
        "# Clinical Assertion Models (with pretrained models)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kql2KmQ35H7H"
      },
      "source": [
        "|    | model_name              |Predicted Entities|\n",
        "|---:|:------------------------|-|\n",
        "|  1 | [assertion_dl](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_en.html)            |Present, Absent, Possible, conditional, hypothetical, associated_with_someone_else|\n",
        "|  2 | [assertion_dl_biobert](https://nlp.johnsnowlabs.com/2021/01/26/assertion_dl_biobert_en.html)    |Present, Absent, Possible, conditional, hypothetical, associated_with_someone_else|\n",
        "|  3 | [assertion_dl_healthcare](https://nlp.johnsnowlabs.com/2020/09/23/assertion_dl_healthcare_en.html) |Present, Absent, Possible, conditional, hypothetical, associated_with_someone_else|\n",
        "|  4 | [assertion_dl_large](https://nlp.johnsnowlabs.com/2020/05/21/assertion_dl_large_en.html)      |Present, Absent, Possible, conditional, hypothetical, associated_with_someone_else|\n",
        "|  5 | [assertion_dl_radiology](https://nlp.johnsnowlabs.com/2021/03/18/assertion_dl_radiology_en.html)   |Confirmed, Suspected, Negative|\n",
        "|  6 | [assertion_jsl](https://nlp.johnsnowlabs.com/2021/07/24/assertion_jsl_en.html)           |Present, Absent, Possible, Planned, Someoneelse, Past, Family, Hypotetical|\n",
        "|  7 | [assertion_jsl_large](https://nlp.johnsnowlabs.com/2021/07/24/assertion_jsl_large_en.html)     |present, absent, possible, planned, someoneelse, past, hypothetical|\n",
        "|  8 |  [assertion_ml](https://nlp.johnsnowlabs.com/2020/01/30/assertion_ml_en.html) |Hypothetical, Present, Absent, Possible, Conditional, Associated_with_someone_else|\n",
        "|  9 | [assertion_dl_scope_L10R10](https://nlp.johnsnowlabs.com/2022/03/17/assertion_dl_scope_L10R10_en_3_0.html)| hypothetical, associated_with_someone_else, conditional, possible, absent, present|\n",
        "| 10 | [assertion_dl_biobert_scope_L10R10](https://nlp.johnsnowlabs.com/2022/03/24/assertion_dl_biobert_scope_L10R10_en_2_4.html)| hypothetical, associated_with_someone_else, conditional, possible, absent, present|\n",
        "| 11 | [assertion_jsl_augmented](https://nlp.johnsnowlabs.com/2022/09/15/assertion_jsl_augmented_en.html)| Present, Absent, Possible, Planned, Past, Family, Hypotetical, SomeoneElse|\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDbKhFUkw8bt"
      },
      "source": [
        "\n",
        "\n",
        "### Oncology Assertion Models\n",
        "|    | model_name              |Predicted Entities|\n",
        "|---:|:------------------------|-|\n",
        "| 1 | **assertion_oncology_wip** | Medical_History, Family_History, Possible, Hypothetical_Or_Absent|\n",
        "| 2 | **assertion_oncology_problem_wip** |Present, Possible, Hypothetical, Absent, Family|\n",
        "| 3 | **assertion_oncology_treatment_wip** |Present, Planned, Past, Hypothetical, Absent|\n",
        "| 4 | **assertion_oncology_response_to_treatment_wip** |Present_Or_Past, Hypothetical_Or_Absent|\n",
        "| 5 | **assertion_oncology_test_binary_wip** |Present_Or_Past, Hypothetical_Or_Absent|\n",
        "| 6 | **assertion_oncology_smoking_status_wip** |Absent, Past, Present|\n",
        "| 7 | **assertion_oncology_family_history_wip** |Family_History, Other|\n",
        "| 8 | **assertion_oncology_demographic_binary_wip** |Patient, Someone_Else|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voZj8-NSbe4j"
      },
      "source": [
        "### Pretrained `assertion_jsl_augmented` model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEa5SITBxmY0"
      },
      "outputs": [],
      "source": [
        "# Annotator that transforms a text column from dataframe into an Annotation ready for NLP\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "# Sentence Detector annotator, processes various sentences per line\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"sentence\")\n",
        "\n",
        "# Tokenizer splits words in a relevant format for NLP\n",
        "tokenizer = Tokenizer()\\\n",
        "    .setInputCols([\"sentence\"])\\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "# Clinical word embeddings trained on PubMED dataset\n",
        "word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
        "    .setInputCols([\"sentence\", \"token\"])\\\n",
        "    .setOutputCol(\"embeddings\")\n",
        "\n",
        "# NER model trained on i2b2 (sampled from MIMIC) dataset\n",
        "clinical_ner = MedicalNerModel.pretrained(\"ner_jsl\", \"en\", \"clinical/models\") \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"ner\")\\\n",
        "    #.setIncludeAllConfidenceScores(False)\n",
        "\n",
        "ner_converter = NerConverterInternal() \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
        "    .setOutputCol(\"ner_chunk\")\\\n",
        "    .setWhiteList([\"SYMPTOM\",\"VS_FINDING\",\"DISEASE_SYNDROME_DISORDER\",\"ADMISSION_DISCHARGE\",\"PROCEDURE\"])\n",
        "\n",
        "# Assertion model trained on i2b2 (sampled from MIMIC) dataset\n",
        "clinical_assertion = AssertionDLModel.pretrained(\"assertion_jsl_augmented\", \"en\", \"clinical/models\") \\\n",
        "    .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"assertion\")\n",
        "    \n",
        "nlpPipeline = Pipeline(stages=[\n",
        "    documentAssembler, \n",
        "    sentenceDetector,\n",
        "    tokenizer,\n",
        "    word_embeddings,\n",
        "    clinical_ner,\n",
        "    ner_converter,\n",
        "    clinical_assertion\n",
        "    ])\n",
        "\n",
        "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "\n",
        "model = nlpPipeline.fit(empty_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z232ubQeyTu"
      },
      "outputs": [],
      "source": [
        "AssertionDLApproach().extractParamMap()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMzVzklLw7B3"
      },
      "outputs": [],
      "source": [
        "# we also have a LogReg based Assertion Model.\n",
        "'''\n",
        "clinical_assertion_ml = AssertionLogRegModel.pretrained(\"assertion_ml\", \"en\", \"clinical/models\") \\\n",
        "    .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"assertion\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Adsv1fRNYq4-"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "GENERAL: He is an elderly gentleman in no acute distress. He is sitting up in bed eating his breakfast. He is alert and oriented and answering questions appropriately.\n",
        "HEENT: Sclerae show mild arcus senilis in the right. Left is clear. Pupils are equally round and reactive to light. Extraocular movements are intact. Oropharynx is clear.\n",
        "NECK: Supple. Trachea is midline. No jugular venous pressure distention is noted. No adenopathy in the cervical, supraclavicular, or axillary areas.\n",
        "ABDOMEN: Soft and nontender. There may be some fullness in the left upper quadrant, although I do not appreciate a true spleen with inspiration.\n",
        "EXTREMITIES: There is some edema, but no cyanosis and clubbing .\n",
        "IMPRESSION: At this time is refractory anemia, which is transfusion dependent. He is on B12, iron, folic acid, and Procrit. There are no sign or symptom of blood loss and a recent esophagogastroduodenoscopy, which was negative. His creatinine was 1. \n",
        "  My impression at this time is that he probably has an underlying myelodysplastic syndrome or bone marrow failure. His creatinine on this hospitalization was up slightly to 1.6 and this may contribute to his anemia.\n",
        "  At this time, my recommendation for the patient is that he undergoes further serologic evaluation with reticulocyte count, serum protein, and electrophoresis, LDH, B12, folate, erythropoietin level, and he should undergo a bone marrow aspiration and biopsy. \n",
        "  I have discussed the procedure in detail which the patient. I have discussed the risks, benefits, and successes of that treatment and usefulness of the bone marrow and predicting his cause of refractory anemia and further therapeutic interventions, which might be beneficial to him. \n",
        "  He is willing to proceed with the studies I have described to him. We will order an ultrasound of his abdomen because of the possible fullness of the spleen, and I will probably see him in follow up after this hospitalization.\n",
        "  As always, we greatly appreciate being able to participate in the care of your patient. We appreciate the consultation of the patient. \n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkxKWB_gZlqz"
      },
      "outputs": [],
      "source": [
        "light_model = LightPipeline(model)\n",
        "\n",
        "light_result = light_model.fullAnnotate(text)[0]\n",
        "\n",
        "chunks=[]\n",
        "entities=[]\n",
        "status=[]\n",
        "confidence=[]\n",
        "\n",
        "for n,m in zip(light_result['ner_chunk'],light_result['assertion']):\n",
        "    \n",
        "    chunks.append(n.result)\n",
        "    entities.append(n.metadata['entity']) \n",
        "    status.append(m.result)\n",
        "    confidence.append(m.metadata['confidence'])\n",
        "        \n",
        "df = pd.DataFrame({'chunks':chunks, 'entities':entities, 'assertion':status, 'confidence':confidence})\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_S4nrCllzSm_"
      },
      "outputs": [],
      "source": [
        "light_model = LightPipeline(model)\n",
        "\n",
        "light_result = light_model.fullAnnotate(text)[0]\n",
        "\n",
        "from sparknlp_display import AssertionVisualizer\n",
        "\n",
        "vis = AssertionVisualizer()\n",
        "\n",
        "vis.set_label_colors({'TEST':'#008080', 'PROBLEM':'#800080'})\n",
        "\n",
        "vis.display(light_result, 'ner_chunk', 'assertion')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofNgYFb6a5Mp"
      },
      "outputs": [],
      "source": [
        "nlu.to_pretty_df(model,text,output_level='chunk').columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzHYs1ipa752"
      },
      "outputs": [],
      "source": [
        "cols = [\n",
        "     'entities_ner_chunk',\n",
        "     'entities_ner_chunk_class', \n",
        "     'assertion',\n",
        "     'assertion_confidence']\n",
        "     \n",
        "df = nlu.to_pretty_df(model,text,output_level='chunk')[cols].reset_index(drop=True)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OP0I7ELJ0his"
      },
      "outputs": [],
      "source": [
        "# Downloading sample datasets.\n",
        "! wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/data/mt_samples_10.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJcaCtUvbC3H"
      },
      "outputs": [],
      "source": [
        "mt_samples_df = spark.createDataFrame(pd.read_csv(\"/content/mt_samples_10.csv\", sep=',', index_col=[\"index\"]).reset_index())\n",
        "                \n",
        "mt_samples_df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFGHwBSxbDIp"
      },
      "outputs": [],
      "source": [
        "mt_samples_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZqaQYV8bIRl"
      },
      "outputs": [],
      "source": [
        "result = model.transform(mt_samples_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qF9NT4LXbIsY"
      },
      "outputs": [],
      "source": [
        "result.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFpsLxGCbMNc"
      },
      "outputs": [],
      "source": [
        "result.select('sentence.result').take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sXyYJsRbMR1"
      },
      "outputs": [],
      "source": [
        "result.select(F.explode(F.arrays_zip(result.ner_chunk.result,  \n",
        "                                     result.ner_chunk.begin, \n",
        "                                     result.ner_chunk.end, \n",
        "                                     result.ner_chunk.metadata, \n",
        "                                     result.assertion.result,\n",
        "                                     result.assertion.metadata)).alias(\"cols\")) \\\n",
        "      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
        "              F.expr(\"cols['1']\").alias(\"begin\"),\n",
        "              F.expr(\"cols['2']\").alias(\"end\"),\n",
        "              F.expr(\"cols['3']['entity']\").alias(\"ner_label\"),\n",
        "              F.expr(\"cols['3']['sentence']\").alias(\"sent_id\"),\n",
        "              F.expr(\"cols['4']\").alias(\"assertion\"),\n",
        "              F.expr(\"cols['5']['confidence']\").alias(\"confidence\") ).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYN97Es2be4p"
      },
      "source": [
        "### Pretrained `assertion_dl_radiology` model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMl2QAeNbe4p"
      },
      "outputs": [],
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "# Sentence Detector annotator, processes various sentences per line\n",
        "sentenceDetector = SentenceDetectorDLModel\\\n",
        "    .pretrained(\"sentence_detector_dl_healthcare\",\"en\",\"clinical/models\") \\\n",
        "    .setInputCols([\"document\"]) \\\n",
        "    .setOutputCol(\"sentence\")\n",
        "\n",
        "# Tokenizer splits words in a relevant format for NLP\n",
        "tokenizer = Tokenizer()\\\n",
        "    .setInputCols([\"sentence\"])\\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "# Clinical word embeddings trained on PubMED dataset\n",
        "word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
        "    .setInputCols([\"sentence\", \"token\"])\\\n",
        "    .setOutputCol(\"embeddings\")\n",
        "\n",
        "# NER model for radiology\n",
        "radiology_ner = MedicalNerModel.pretrained(\"ner_radiology\", \"en\", \"clinical/models\") \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"ner\")\\\n",
        "    #.setIncludeAllConfidenceScores(False)\n",
        "\n",
        "ner_converter = NerConverterInternal() \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
        "    .setOutputCol(\"ner_chunk\")\\\n",
        "    .setWhiteList([\"ImagingFindings\"])\n",
        "\n",
        "# Assertion model trained on radiology dataset\n",
        "radiology_assertion = AssertionDLModel.pretrained(\"assertion_dl_radiology\", \"en\", \"clinical/models\") \\\n",
        "    .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"assertion\")\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        "    documentAssembler, \n",
        "    sentenceDetector,\n",
        "    tokenizer,\n",
        "    word_embeddings,\n",
        "    radiology_ner,\n",
        "    ner_converter,\n",
        "    radiology_assertion\n",
        "    ])\n",
        "\n",
        "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "radiologyAssertion_model = nlpPipeline.fit(empty_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ehm7WwJrbe4r"
      },
      "outputs": [],
      "source": [
        "# A sample text from a radiology report\n",
        "\n",
        "text = \"\"\"No right-sided pleural effusion or pneumothorax is definitively seen and there are mildly displaced fractures of the left lateral 8th and likely 9th ribs.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1iCxtKjbe4s"
      },
      "outputs": [],
      "source": [
        "data = spark.createDataFrame([[text]]).toDF(\"text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSpEvzDObe4s"
      },
      "outputs": [],
      "source": [
        "result = radiologyAssertion_model.transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL7-olcBTXQ0"
      },
      "outputs": [],
      "source": [
        "result.select(F.explode(F.arrays_zip(result.ner_chunk.result, \n",
        "                                     result.ner_chunk.metadata, \n",
        "                                     result.assertion.result)).alias(\"cols\")) \\\n",
        "      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
        "              F.expr(\"cols['1']['entity']\").alias(\"ner_label\"),\n",
        "              F.expr(\"cols['1']['sentence']\").alias(\"sent_id\"),\n",
        "              F.expr(\"cols['2']\").alias(\"assertion\")).show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vWIDpk50QfG"
      },
      "source": [
        "## Writing a generic Assertion + NER function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltGPQYr60F6G"
      },
      "outputs": [],
      "source": [
        "def get_base_pipeline (embeddings = 'embeddings_clinical'):\n",
        "\n",
        "    documentAssembler = DocumentAssembler()\\\n",
        "        .setInputCol(\"text\")\\\n",
        "        .setOutputCol(\"document\")\n",
        "\n",
        "  # Sentence Detector annotator, processes various sentences per line\n",
        "    sentenceDetector = SentenceDetector()\\\n",
        "        .setInputCols([\"document\"])\\\n",
        "        .setOutputCol(\"sentence\")\n",
        "\n",
        "  # Tokenizer splits words in a relevant format for NLP\n",
        "    tokenizer = Tokenizer()\\\n",
        "        .setInputCols([\"sentence\"])\\\n",
        "        .setOutputCol(\"token\")\n",
        "\n",
        "  # Clinical word embeddings trained on PubMED dataset\n",
        "    word_embeddings = WordEmbeddingsModel.pretrained(embeddings, \"en\", \"clinical/models\")\\\n",
        "        .setInputCols([\"sentence\", \"token\"])\\\n",
        "        .setOutputCol(\"embeddings\")\n",
        "\n",
        "    base_pipeline = Pipeline(stages=[\n",
        "                        documentAssembler,\n",
        "                        sentenceDetector,\n",
        "                        tokenizer,\n",
        "                        word_embeddings])\n",
        "\n",
        "    return base_pipeline\n",
        "\n",
        "\n",
        "\n",
        "def get_clinical_assertion (embeddings, spark_df, nrows = 100, ner_model_name = 'ner_clinical', assertion_model_name=\"assertion_dl\"):\n",
        "\n",
        "  # NER model trained on i2b2 (sampled from MIMIC) dataset\n",
        "    loaded_ner_model = MedicalNerModel.pretrained(ner_model_name, \"en\", \"clinical/models\") \\\n",
        "        .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
        "        .setOutputCol(\"ner\")\n",
        "\n",
        "    ner_converter = NerConverterInternal() \\\n",
        "        .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
        "        .setOutputCol(\"ner_chunk\")\n",
        "\n",
        "  # Assertion model trained on i2b2 (sampled from MIMIC) dataset\n",
        "  # coming from sparknlp_jsl.annotator !!\n",
        "    clinical_assertion = AssertionDLModel.pretrained(assertion_model_name, \"en\", \"clinical/models\") \\\n",
        "        .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\n",
        "        .setOutputCol(\"assertion\")\n",
        "      \n",
        "\n",
        "    base_model = get_base_pipeline (embeddings)\n",
        "\n",
        "    nlpPipeline = Pipeline(stages=[\n",
        "        base_model,\n",
        "        loaded_ner_model,\n",
        "        ner_converter,\n",
        "        clinical_assertion])\n",
        "\n",
        "    empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "\n",
        "    model = nlpPipeline.fit(empty_data)\n",
        "\n",
        "    result = model.transform(spark_df.limit(nrows))\n",
        "\n",
        "    result = result.withColumn(\"id\", F.monotonically_increasing_id())\n",
        "\n",
        "    result_df = result.select(F.explode(F.arrays_zip(result.ner_chunk.result, \n",
        "                                                     result.ner_chunk.metadata, \n",
        "                                                     result.assertion.result,\n",
        "                                                     result.assertion.metadata)).alias(\"cols\")) \\\n",
        "                      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
        "                              F.expr(\"cols['1']['entity']\").alias(\"ner_label\"),\n",
        "                              F.expr(\"cols['2']\").alias(\"assertion\"),\n",
        "                              F.expr(\"cols['3']['confidence']\").alias(\"confidence\"))\\\n",
        "                      .filter(\"ner_label!='O'\")\n",
        "\n",
        "    return result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V81zaAe13qLU"
      },
      "outputs": [],
      "source": [
        "embeddings = 'embeddings_clinical'\n",
        "\n",
        "ner_model_name = 'ner_clinical_large'\n",
        "\n",
        "nrows = 100\n",
        "\n",
        "ner_df = get_clinical_assertion (embeddings, mt_samples_df, nrows, ner_model_name)\n",
        "\n",
        "ner_df.show(30,truncate=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmLPlfCw5hh-"
      },
      "outputs": [],
      "source": [
        "embeddings = 'embeddings_clinical'\n",
        "\n",
        "ner_model_name = 'ner_posology'\n",
        "\n",
        "nrows = 100\n",
        "\n",
        "ner_df = get_clinical_assertion (embeddings, mt_samples_df, nrows, ner_model_name)\n",
        "\n",
        "ner_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQ9xeS3kwClE"
      },
      "outputs": [],
      "source": [
        "embeddings = 'embeddings_clinical'\n",
        "\n",
        "ner_model_name = 'ner_posology_greedy'\n",
        "\n",
        "entry_data = spark.createDataFrame([[\"The patient did not take a capsule of Advil.\"]]).toDF(\"text\")\n",
        "\n",
        "ner_df = get_clinical_assertion (embeddings, entry_data, nrows, ner_model_name)\n",
        "\n",
        "ner_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djV_FKNtYcoP"
      },
      "outputs": [],
      "source": [
        "embeddings = 'embeddings_clinical'\n",
        "\n",
        "ner_model_name = 'ner_clinical'\n",
        "\n",
        "entry_data = spark.createDataFrame([[\"The patient has no fever\"]]).toDF(\"text\")\n",
        "\n",
        "ner_df = get_clinical_assertion (embeddings, entry_data, nrows, ner_model_name)\n",
        "\n",
        "ner_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P87ScBcJqcNs"
      },
      "outputs": [],
      "source": [
        "def get_clinical_assertion_light (light_model, text):\n",
        "\n",
        "  light_result = light_model.fullAnnotate(text)[0]\n",
        "\n",
        "  chunks=[]\n",
        "  entities=[]\n",
        "  status=[]\n",
        "  confidence=[]\n",
        "\n",
        "  for n,m in zip(light_result['ner_chunk'],light_result['assertion']):\n",
        "      \n",
        "      chunks.append(n.result)\n",
        "      entities.append(n.metadata['entity']) \n",
        "      status.append(m.result)\n",
        "      confidence.append(m.metadata['confidence'])\n",
        "          \n",
        "  df = pd.DataFrame({'chunks':chunks, 'entities':entities, 'assertion':status,'confidence':confidence})\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqKH9Mw98z4x"
      },
      "outputs": [],
      "source": [
        "clinical_text = \"\"\"\n",
        "Patient with severe fever and sore throat. \n",
        "He shows no stomach pain and he maintained on an epidural and PCA for pain control.\n",
        "He also became short of breath with climbing a flight of stairs.\n",
        "After CT, lung tumor located at the right lower lobe. Father with Alzheimer.\n",
        "\"\"\"\n",
        "\n",
        "light_model = LightPipeline(model)\n",
        "\n",
        "# get_clinical_assertion_light (light_model, clinical_text)\n",
        "\n",
        "cols = [\n",
        "     'entities_ner_chunk',\n",
        "     'entities_ner_chunk_class', \n",
        "     'assertion',\n",
        "     'assertion_confidence']\n",
        "     \n",
        "df = nlu.to_pretty_df(light_model,clinical_text, output_level='chunk')[cols]\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVeR4zew_urR"
      },
      "source": [
        "# Oncological Assertion Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNKJ3lfkc_N7"
      },
      "source": [
        "Oncology Assertion Models\n",
        "\n",
        "|    | model_name              |Predicted Entities|\n",
        "|---:|:------------------------|-|\n",
        "| 1 | [assertion_oncology_wip](https://nlp.johnsnowlabs.com/2022/10/11/assertion_oncology_wip_en.html) | Medical_History, Family_History, Possible, Hypothetical_Or_Absent|\n",
        "| 2 | [assertion_oncology_problem_wip](https://nlp.johnsnowlabs.com/2022/10/11/assertion_oncology_problem_wip_en.html) |Present, Possible, Hypothetical, Absent, Family|\n",
        "| 3 | [assertion_oncology_treatment_wip](https://nlp.johnsnowlabs.com/2022/10/11/assertion_oncology_treatment_binary_wip_en.html) |Present, Planned, Past, Hypothetical, Absent|\n",
        "| 3 | [assertion_oncology_treatment_wip]() |Present, Planned, Past, Hypothetical, Absent|\n",
        "| 4 | [assertion_oncology_response_to_treatment_wip](https://nlp.johnsnowlabs.com/2022/10/11/assertion_oncology_response_to_treatment_wip_en.html) |Present_Or_Past, Hypothetical_Or_Absent|\n",
        "| 5 | [assertion_oncology_test_binary_wip](https://nlp.johnsnowlabs.com/2022/10/01/assertion_oncology_test_binary_wip_en.html) |Present_Or_Past, Hypothetical_Or_Absent|\n",
        "| 6 | [assertion_oncology_smoking_status_wip](https://nlp.johnsnowlabs.com/2022/10/11/assertion_oncology_smoking_status_wip_en.html) |Absent, Past, Present|\n",
        "| 7 | [assertion_oncology_family_history_wip](https://nlp.johnsnowlabs.com/2022/10/11/assertion_oncology_family_history_wip_en.html) |Family_History, Other|\n",
        "| 8 | [assertion_oncology_demographic_binary_wip](https://nlp.johnsnowlabs.com/2022/10/11/assertion_oncology_demographic_binary_wip_en.html) |Patient, Someone_Else|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1Ffv9qT_AYV"
      },
      "outputs": [],
      "source": [
        "embeddings = 'embeddings_clinical'\n",
        "\n",
        "ner_model_name = 'ner_oncology_wip'\n",
        "\n",
        "assertion_model_name='assertion_oncology_wip'\n",
        "\n",
        "nrows = 100\n",
        "\n",
        "ner_df = get_clinical_assertion (embeddings, mt_samples_df, nrows, ner_model_name,assertion_model_name )\n",
        "\n",
        "ner_df.show(truncate = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jECAdcYGcJJT"
      },
      "source": [
        "# Assertion Filterer\n",
        "AssertionFilterer will allow you to filter out the named entities by the list of acceptable assertion statuses. This annotator would be quite handy if you want to set a white list for the acceptable assertion statuses like present or conditional; and do not want absent conditions get out of your pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8xRF4yScwP3"
      },
      "outputs": [],
      "source": [
        "# Annotator that transforms a text column from dataframe into an Annotation ready for NLP\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "# Sentence Detector annotator, processes various sentences per line\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"sentence\")\n",
        "\n",
        "# Tokenizer splits words in a relevant format for NLP\n",
        "tokenizer = Tokenizer()\\\n",
        "    .setInputCols([\"sentence\"])\\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "# Clinical word embeddings trained on PubMED dataset\n",
        "word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
        "    .setInputCols([\"sentence\", \"token\"])\\\n",
        "    .setOutputCol(\"embeddings\")\n",
        "\n",
        "clinical_ner = MedicalNerModel.pretrained(\"ner_clinical\", \"en\", \"clinical/models\") \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"ner\")\\\n",
        "    #.setIncludeAllConfidenceScores(False)\n",
        "\n",
        "ner_converter = NerConverterInternal() \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
        "    .setOutputCol(\"ner_chunk\")\n",
        "\n",
        "clinical_assertion = AssertionDLModel.pretrained(\"assertion_jsl_augmented\", \"en\", \"clinical/models\") \\\n",
        "    .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"assertion\")\n",
        "\n",
        "assertion_filterer = AssertionFilterer()\\\n",
        "    .setInputCols(\"sentence\",\"ner_chunk\",\"assertion\")\\\n",
        "    .setOutputCol(\"assertion_filtered\")\\\n",
        "    .setCaseSensitive(False)\\\n",
        "    .setWhiteList([\"PREsent\"])\n",
        "\n",
        "nlpPipeline = Pipeline(stages=[\n",
        "      documentAssembler, \n",
        "      sentenceDetector,\n",
        "      tokenizer,\n",
        "      word_embeddings,\n",
        "      clinical_ner,\n",
        "      ner_converter,\n",
        "      clinical_assertion,\n",
        "      assertion_filterer\n",
        "    ])\n",
        "\n",
        "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "assertionFilter_model = nlpPipeline.fit(empty_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KE6Qt20Sfqc-"
      },
      "outputs": [],
      "source": [
        "text = 'Patient has a headache for the last 2 weeks, needs to get a head CT, and appears anxious when she walks fast. Alopecia noted. She denies pain.'\n",
        "\n",
        "light_model = LightPipeline(assertionFilter_model)\n",
        "light_result = light_model.annotate(text)\n",
        "\n",
        "light_result.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfTSTBn8rYrj"
      },
      "outputs": [],
      "source": [
        "list(zip(light_result['ner_chunk'], light_result['assertion']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjuVg_4N15y2"
      },
      "outputs": [],
      "source": [
        "assertion_filterer.getWhiteList()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoqvwZmp82CH"
      },
      "outputs": [],
      "source": [
        "chunks=[]\n",
        "entities=[]\n",
        "status=[]\n",
        "confidence=[]\n",
        "\n",
        "light_result = light_model.fullAnnotate(text)[0]\n",
        "\n",
        "for m in light_result['assertion_filtered']:\n",
        "    \n",
        "    chunks.append(m.result)\n",
        "    entities.append(m.metadata['entity']) \n",
        "    status.append(m.metadata['assertion'])\n",
        "    confidence.append(m.metadata['confidence'])\n",
        "        \n",
        "df = pd.DataFrame({'chunks':chunks, 'entities':entities, 'assertion':status, 'confidence':confidence})\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WREO4af-9faq"
      },
      "source": [
        "As you see, there is no \"pain\" chunk since it has \"absent\" assertion label. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUPXaUJorB_R"
      },
      "source": [
        "# AssertionChunkConverter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07KlFddHOQJ3"
      },
      "source": [
        "In some cases, there may be issues while creating the chunk column by using token indices and losing some data while training and testing the assertion status model if there are issues in these token indices. So we developed a new `AssertionChunkConverter` annotator that takes **begin and end indices of the chunks** as input and creates an extended chunk column with metadata that can be used for assertion status detection model training.\n",
        "\n",
        "*NOTE*: Chunk begin and end indices in the assertion status model training dataframe can be populated using the new version of ALAB module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3T-vyPqrBhE"
      },
      "outputs": [],
      "source": [
        "data = spark.createDataFrame([[\"An angiography showed bleeding in two vessels off of the Minnie supplying the sigmoid that were succesfully embolized.\", \"Minnie\", 57, 63],\n",
        "     [\"After discussing this with his PCP, Leon was clear that the patient had had recurrent DVTs and ultimately a PE and his PCP felt strongly that he required long-term anticoagulation \", \"PCP\", 31, 34]])\\\n",
        "     .toDF(\"text\", \"target\", \"char_begin\", \"char_end\")\n",
        "\n",
        "data.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c1xD4qIrNZj"
      },
      "outputs": [],
      "source": [
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"text\") \\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"sentence\")\n",
        "\n",
        "tokenizer = Tokenizer() \\\n",
        "    .setInputCols([\"sentence\"]) \\\n",
        "    .setOutputCol(\"tokens\")\n",
        "\n",
        "converter = AssertionChunkConverter() \\\n",
        "    .setInputCols(\"tokens\")\\\n",
        "    .setChunkTextCol(\"target\")\\\n",
        "    .setChunkBeginCol(\"char_begin\")\\\n",
        "    .setChunkEndCol(\"char_end\")\\\n",
        "    .setOutputTokenBeginCol(\"token_begin\")\\\n",
        "    .setOutputTokenEndCol(\"token_end\")\\\n",
        "    .setOutputCol(\"chunk\")\n",
        "\n",
        "pipeline = Pipeline().setStages([document_assembler,sentenceDetector, tokenizer, converter])\n",
        "\n",
        "results = pipeline.fit(data).transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSzD7n-RrOvr"
      },
      "outputs": [],
      "source": [
        "results\\\n",
        "    .selectExpr(\n",
        "        \"target\",\n",
        "        \"char_begin\",\n",
        "        \"char_end\",\n",
        "        \"token_begin\",\n",
        "        \"token_end\",\n",
        "        \"tokens[token_begin].result\",\n",
        "        \"tokens[token_end].result\",\n",
        "        \"target\",\n",
        "        \"chunk\")\\\n",
        "    .show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjfUo0aTKFEL"
      },
      "source": [
        "# Train a custom Assertion Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPFTJxtlIKQT"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/data/i2b2_assertion_sample_short.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "llRofRtgLF7S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRFW3nyALHx0"
      },
      "outputs": [],
      "source": [
        "assertion_df = spark.read.option(\"header\", True).option(\"inferSchema\", \"True\").csv(\"i2b2_assertion_sample_short.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQn7bJZtL9uX"
      },
      "outputs": [],
      "source": [
        "assertion_df.show(3, truncate=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6IuDZ9Qw7Cu"
      },
      "outputs": [],
      "source": [
        "(training_data, test_data) = assertion_df.randomSplit([0.8, 0.2], seed = 100)\n",
        "print(\"Training Dataset Count: \" + str(training_data.count()))\n",
        "print(\"Test Dataset Count: \" + str(test_data.count()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fktQ3EbBZwo-"
      },
      "outputs": [],
      "source": [
        "training_data.groupBy('label').count().orderBy('count', ascending=False).show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTNhwoSDLLVG"
      },
      "outputs": [],
      "source": [
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "chunk = Doc2Chunk()\\\n",
        "    .setInputCols(\"document\")\\\n",
        "    .setOutputCol(\"chunk\")\\\n",
        "    .setChunkCol(\"target\")\\\n",
        "    .setStartCol(\"start\")\\\n",
        "    .setStartColByTokenIndex(True)\\\n",
        "    .setFailOnMissing(False)\\\n",
        "    .setLowerCase(True)\n",
        "\n",
        "token = Tokenizer()\\\n",
        "    .setInputCols(['document'])\\\n",
        "    .setOutputCol('token')\n",
        "\n",
        "embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
        "    .setInputCols([\"document\", \"token\"])\\\n",
        "    .setOutputCol(\"embeddings\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkp7KmK6pv0-"
      },
      "source": [
        "We will transform our test data with a pipeline consisting of same steps with the pipeline which contains AssertionDLApproach.\n",
        "By doing this, we enable that test data will have same columns with training data in AssertionDLApproach. <br/>\n",
        "The goal of this implementation is enabling the usage of `setTestDataset()` parameter in AssertionDLApproach. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7DScL2-uIOm"
      },
      "outputs": [],
      "source": [
        "clinical_assertion_pipeline = Pipeline(\n",
        "    stages = [\n",
        "    document,\n",
        "    chunk,\n",
        "    token,\n",
        "    embeddings])\n",
        "\n",
        "assertion_test_data = clinical_assertion_pipeline.fit(test_data).transform(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwuyumqGuHwY"
      },
      "outputs": [],
      "source": [
        "assertion_test_data.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE-99quUsJ5V"
      },
      "source": [
        "We save the test data in parquet format to use in `AssertionDLApproach()`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3Hx0w05uQ7E"
      },
      "outputs": [],
      "source": [
        "assertion_test_data.write.parquet('i2b2_assertion_sample_test_data.parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTishXbut1MS"
      },
      "source": [
        "## Graph setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hTawAHemzGn"
      },
      "outputs": [],
      "source": [
        "!pip install -q tensorflow==2.7.0\n",
        "!pip install -q tensorflow-addons"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ShZT8BBo4FY"
      },
      "source": [
        "We will use TFGraphBuilder annotator which can be used to create graphs in the model training pipeline. \n",
        "\n",
        "TFGraphBuilder inspects the data and creates the proper graph if a suitable version of TensorFlow (<= 2.7 ) is available. The graph is stored in the defined folder and loaded by the approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojPhRpo6o0xM"
      },
      "outputs": [],
      "source": [
        "from sparknlp_jsl.annotator import TFGraphBuilder\n",
        "\n",
        "graph_folder= \"./tf_graphs\"\n",
        "\n",
        "assertion_graph_builder = TFGraphBuilder()\\\n",
        "    .setModelName(\"assertion_dl\")\\\n",
        "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
        "    .setLabelColumn(\"label\")\\\n",
        "    .setGraphFolder(graph_folder)\\\n",
        "    .setGraphFile(\"assertion_graph.pb\")\\\n",
        "    .setMaxSequenceLength(250)\\\n",
        "    .setHiddenUnitsNumber(25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zl1xBA65IZ2j"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# ready to use tf_graph\n",
        "\n",
        "!mkdir training_logs\n",
        "!mkdir assertion_tf_graph\n",
        "\n",
        "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/tf_graphs/blstm_34_32_30_200_2.pb -P /content/assertion_tf_graph\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNzTsoSeXezx"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "# create custom graph\n",
        "\n",
        "from sparknlp_jsl.training import tf_graph\n",
        "tf_graph.print_model_params(\"assertion_dl\")\n",
        "\n",
        "feat_size = 200\n",
        "n_classes = 6\n",
        "\n",
        "tf_graph.build(\"assertion_dl\",\n",
        "              build_params={\"n_classes\": n_classes},\n",
        "              model_location= \"./tf_graphs\", \n",
        "              model_filename=\"blstm_34_32_30_{}_{}.pb\".format(feat_size, n_classes))\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D0Ng7nMUjJa"
      },
      "source": [
        "**Setting the Scope Window (Target Area) Dynamically in Assertion Status Detection Models**\n",
        "\n",
        "\n",
        "This parameter allows you to train the Assertion Status Models to focus on specific context windows when resolving the status of a NER chunk. The window is in format `[X,Y]` being `X` the number of tokens to consider on the left of the chunk, and `Y` the max number of tokens to consider on the right. Let’s take a look at what different windows mean:\n",
        "\n",
        "\n",
        "*   By default, the window is `[-1,-1]` which means that the Assertion Status will look at all of the tokens in the sentence/document (up to a maximum of tokens set in `setMaxSentLen()` ).\n",
        "*   `[0,0]` means “don’t pay attention to any token except the ner_chunk”, what basically is not considering any context for the Assertion resolution.\n",
        "*   `[9,15]` is what empirically seems to be the best baseline, meaning that we look up to 9 tokens on the left and 15 on the right of the ner chunk to understand the context and resolve the status.\n",
        "\n",
        "\n",
        "Check this [Scope Window Tuning Assertion Status Detection notebook](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/2.1.Scope_window_tuning_assertion_status_detection.ipynb)  that illustrates the effect of the different windows and how to properly fine-tune your AssertionDLModels to get the best of them.\n",
        "\n",
        "In our case, the best Scope Window is around [10,10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qG2o6Yq2xk4N"
      },
      "outputs": [],
      "source": [
        "scope_window = [10,10]\n",
        "\n",
        "assertionStatus = AssertionDLApproach()\\\n",
        "    .setLabelCol(\"label\")\\\n",
        "    .setInputCols(\"document\", \"chunk\", \"embeddings\")\\\n",
        "    .setOutputCol(\"assertion\")\\\n",
        "    .setBatchSize(128)\\\n",
        "    .setDropout(0.1)\\\n",
        "    .setLearningRate(0.001)\\\n",
        "    .setEpochs(15)\\\n",
        "    .setValidationSplit(0.2)\\\n",
        "    .setStartCol(\"start\")\\\n",
        "    .setEndCol(\"end\")\\\n",
        "    .setMaxSentLen(250)\\\n",
        "    .setIncludeConfidence(True)\\\n",
        "    .setEnableOutputLogs(True)\\\n",
        "    .setOutputLogsPath('training_logs/')\\\n",
        "    .setGraphFolder(graph_folder)\\\n",
        "    .setGraphFile(f\"{graph_folder}/assertion_graph.pb\")\\\n",
        "    .setTestDataset(path=\"/content/i2b2_assertion_sample_test_data.parquet\")\\\n",
        "    .setScopeWindow(scope_window)\n",
        "\n",
        "'''\n",
        "If .setTestDataset parameter is employed, raw test data cannot be fitted. .setTestDataset only works for dataframes which are correctly transformed\n",
        "by a pipeline consisting of document, chunk, embeddings stages.\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97ATs6a41jdE"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "assertionStatus = AssertionLogRegApproach()\\\n",
        "    .setLabelCol(\"label\")\\\n",
        "    .setInputCols(\"document\", \"chunk\", \"embeddings\")\\\n",
        "    .setOutputCol(\"assertion\")\\\n",
        "    .setMaxIter(100) # default: 26\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKRL7Oy4MC1C"
      },
      "outputs": [],
      "source": [
        "clinical_assertion_pipeline = Pipeline(\n",
        "    stages = [\n",
        "    document,\n",
        "    chunk,\n",
        "    token,\n",
        "    embeddings,\n",
        "    assertion_graph_builder,\n",
        "    assertionStatus])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ei54XasnMU0U"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "assertion_model = clinical_assertion_pipeline.fit(training_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1T77siXt3Dy"
      },
      "source": [
        "## Checking the results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30SmcTiSpnWa"
      },
      "source": [
        "Checking the results saved in the log file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOiu1vuspKut"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "log_files = os.listdir(\"./training_logs\")\n",
        "log_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcQV0-fIrJHz"
      },
      "outputs": [],
      "source": [
        "with open(\"./training_logs/\"+log_files[0]) as log_file:\n",
        "    print(log_file.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k2WrFkRyQyP"
      },
      "outputs": [],
      "source": [
        "preds = assertion_model.transform(test_data).select('label','assertion.result')\n",
        "\n",
        "preds.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yI73lwG2xk5"
      },
      "outputs": [],
      "source": [
        "preds_df = preds.toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRXZFGlQ3Z2U"
      },
      "outputs": [],
      "source": [
        "preds_df['result'] = preds_df['result'].apply(lambda x : x[0])\n",
        "preds_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hb1kyGAE0Gn"
      },
      "outputs": [],
      "source": [
        "# We are going to use sklearn to evalute the results on test dataset\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print (classification_report( preds_df['label'], preds_df['result']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atNaIlnP3gKr"
      },
      "outputs": [],
      "source": [
        "# save model\n",
        "assertion_model.stages[-1].write().overwrite().save('assertion_custom_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObVtthsied4l"
      },
      "source": [
        "## Load saved model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJ2IyAZ1UzE_"
      },
      "outputs": [],
      "source": [
        "documentAssembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "# Sentence Detector annotator, processes various sentences per line\n",
        "sentenceDetector = SentenceDetector()\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"sentence\")\n",
        "\n",
        "# Tokenizer splits words in a relevant format for NLP\n",
        "tokenizer = Tokenizer()\\\n",
        "    .setInputCols([\"sentence\"])\\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "# Clinical word embeddings trained on PubMED dataset\n",
        "word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
        "    .setInputCols([\"sentence\", \"token\"])\\\n",
        "    .setOutputCol(\"embeddings\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19w8VSGHefOq"
      },
      "outputs": [],
      "source": [
        "clinical_ner = MedicalNerModel.pretrained(\"ner_clinical\", \"en\", \"clinical/models\") \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"ner\")\n",
        "\n",
        "ner_converter = NerConverterInternal() \\\n",
        "    .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
        "    .setOutputCol(\"ner_chunk\")\n",
        "\n",
        "clinical_assertion = AssertionDLModel.load(\"assertion_custom_model\") \\\n",
        "    .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\n",
        "    .setOutputCol(\"assertion\")\n",
        "    \n",
        "nlpPipeline = Pipeline(stages=[\n",
        "    documentAssembler, \n",
        "    sentenceDetector,\n",
        "    tokenizer,\n",
        "    word_embeddings,\n",
        "    clinical_ner,\n",
        "    ner_converter,\n",
        "    clinical_assertion\n",
        "    ])\n",
        "\n",
        "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "\n",
        "model = nlpPipeline.fit(empty_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiYrWPuLemSh"
      },
      "outputs": [],
      "source": [
        "text = 'Patient has a headache for the last 2 weeks, needs to get a head CT, and appears anxious when she walks fast. No alopecia and pain noted'\n",
        "\n",
        "\n",
        "light_model = LightPipeline(model)\n",
        "\n",
        "light_result = light_model.fullAnnotate(text)[0]\n",
        "\n",
        "print(text)\n",
        "\n",
        "chunks=[]\n",
        "entities=[]\n",
        "status=[]\n",
        "confidence=[]\n",
        "\n",
        "for n,m in zip(light_result['ner_chunk'],light_result['assertion']):\n",
        "    \n",
        "    chunks.append(n.result)\n",
        "    entities.append(n.metadata['entity']) \n",
        "    status.append(m.result)\n",
        "    confidence.append(m.metadata['confidence'])\n",
        "        \n",
        "df = pd.DataFrame({'chunks':chunks, 'entities':entities, 'assertion':status, 'confidence':confidence})\n",
        "\n",
        "df"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('tf-gpu')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "3f47d918ae832c68584484921185f5c85a1760864bf927a683dc6fb56366cc77"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}