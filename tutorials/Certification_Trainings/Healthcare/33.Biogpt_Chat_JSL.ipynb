{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)\n"
   ],
   "metadata": {
    "id": "X64RTULpsvUT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/33.Biogpt_Chat_JSL.ipynb)\n",
    "\n"
   ],
   "metadata": {
    "id": "TGSKYVuqsuE5"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BC5b1eU_QDg"
   },
   "source": [
    "# **BioGPT - Chat JSL - Closed Book Question Answering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sEjqwB2PHuuS"
   },
   "source": [
    "The objective of this notebook is to explore the Biomedical Generative Pre-trained Transformer (BioGPT) models - `biogpt_chat_jsl` and `biogpt_chat_jsl_conversational_en`, for closed book question answering. These models are pre-trained on large biomedical text data and can generate coherent and relevant responses to biomedical questions.\n",
    "\n",
    "üìñ Learning Objectives:\n",
    "\n",
    "- Learn how to use the BioGPT models in Spark NLP for closed book question answering tasks, including loading pre-trained models and configuring the pipeline.\n",
    "\n",
    "- Understand the parameters and options available for the BioGPT models to customize the text generation process based on specific use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxThSOgQIA8P"
   },
   "source": [
    "# ‚öíÔ∏è Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xX2IV_h13Pch"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "license_keys = files.upload()\n",
    "\n",
    "with open(list(license_keys.keys())[0]) as f:\n",
    "    license_keys = json.load(f)\n",
    "\n",
    "# Defining license key-value pairs as local variables\n",
    "locals().update(license_keys)\n",
    "\n",
    "# Adding license key-value pairs to environment variables\n",
    "os.environ.update(license_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEhXeWX63T2t"
   },
   "outputs": [],
   "source": [
    "# Installing pyspark and spark-nlp\n",
    "! pip install --upgrade -q pyspark==3.1.2  spark-nlp==$PUBLIC_VERSION\n",
    "\n",
    "# Installing Spark NLP Healthcare\n",
    "! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET\n",
    "\n",
    "# Installing Spark NLP Display Library for visualization\n",
    "! pip install -q spark-nlp-display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "oxDDgz683YG1",
    "outputId": "29550e02-42d6-40ed-c9b0-91d1351df1ec"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import sparknlp\n",
    "import sparknlp_jsl\n",
    "\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "import textwrap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "params = {\"spark.driver.memory\":\"16G\", \n",
    "          \"spark.kryoserializer.buffer.max\":\"2000M\", \n",
    "          \"spark.driver.maxResultSize\":\"2000M\"} \n",
    "\n",
    "print(\"Spark NLP Version :\", sparknlp.version())\n",
    "print(\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
    "\n",
    "spark = sparknlp_jsl.start(license_keys['SECRET'],params=params)\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-62Qs6RAIC1V"
   },
   "source": [
    "# \tüìéüè• `biogpt_chat_jsl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twxRx_PGIGm7"
   },
   "source": [
    "This model is based on BioGPT finetuned with medical conversations happening in a clinical settings and can answer clinical questions related to symptoms, drugs, tests, and diseases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DCFN2tYF3X-Z",
    "outputId": "06fb6273-5da5-4c3e-8371-002c38eb40d3"
   },
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"documents\")\n",
    "    \n",
    "gpt_qa = MedicalTextGenerator().pretrained(\"biogpt_chat_jsl\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols(\"documents\")\\\n",
    "    .setOutputCol(\"answer\")\\\n",
    "    .setMaxNewTokens(299)\\\n",
    "    .setStopAtEos(True)\\\n",
    "    .setDoSample(False)\\\n",
    "    .setTopK(3)\\\n",
    "    .setRandomSeed(42)\n",
    "    \n",
    "pipeline = Pipeline().setStages([document_assembler, gpt_qa])\n",
    "\n",
    "question = \"What medications are commonly used to treat emphysema?\"\n",
    "TEXT = [f\"question: {question} answer:\"]\n",
    "data = spark.createDataFrame([TEXT]).toDF(\"text\")\n",
    "\n",
    "result = pipeline.fit(data).transform(data)\n",
    "result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s9vKgjtjIjLA",
    "outputId": "122f8a2d-b1dd-49d7-f259-0b276541a52a"
   },
   "outputs": [],
   "source": [
    "result.select(\"answer.result\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dv3a3Mm8aTh6"
   },
   "source": [
    "## **üìç LightPipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FIMoP1-fvfF1",
    "outputId": "027fabd5-43f4-400b-b544-d67f213ba7f6"
   },
   "outputs": [],
   "source": [
    "gpt_qa = MedicalTextGenerator().pretrained(\"biogpt_chat_jsl\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols(\"documents\")\\\n",
    "    .setOutputCol(\"answer\")\\\n",
    "    .setMaxNewTokens(299)\\\n",
    "    .setStopAtEos(True)\\\n",
    "    .setDoSample(False)\\\n",
    "    .setTopK(3)\\\n",
    "    .setRandomSeed(42)\n",
    "    \n",
    "pipeline = Pipeline().setStages([document_assembler, gpt_qa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSfsStyhCUKa"
   },
   "outputs": [],
   "source": [
    "question = \"What are the risk factors for developing heart disease?\"\n",
    "TEXT = [f\"question: {question} answer:\"]\n",
    "\n",
    "model = pipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\"))\n",
    "light_model = LightPipeline(model)\n",
    "light_result = light_model.annotate(TEXT)\n",
    "answer_text = light_result[0][\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RfIJU_0ACYfU",
    "outputId": "b9849896-9ed7-4cf1-d8cd-cfc10262ccfa"
   },
   "outputs": [],
   "source": [
    "# Extract the text after 'answer:'\n",
    "final_answer = answer_text[0][len(TEXT[0]) + 1:].strip()\n",
    "\n",
    "# Format the text into paragraphs\n",
    "wrapped_text = textwrap.fill(final_answer, width=120)\n",
    "\n",
    "print(\"‚û§ Answer: \\n{}\".format(wrapped_text))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ta3HNysnaYy4"
   },
   "source": [
    "## üö© `setMaxNewTokens`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fpp0aiEzbH37"
   },
   "source": [
    "- This parameter sets the maximum number of new tokens that the GPT model will generate for the output, constraining the length of the generated response and managing the computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrdPu1wK8-Pw"
   },
   "source": [
    "Pipeline with `setMaxNewTokens(128)` and `setMaxNewTokens(299)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3jbbh6hy_w11",
    "outputId": "c8f76068-d11d-4b26-d09d-2e11dfaf6056"
   },
   "outputs": [],
   "source": [
    "# Default parameters\n",
    "gpt_qa = MedicalTextGenerator().pretrained(\"biogpt_chat_jsl\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols(\"documents\") \\\n",
    "    .setOutputCol(\"answer\") \\\n",
    "    .setStopAtEos(True)\\\n",
    "    .setDoSample(False)\\\n",
    "    .setTopK(3) \\\n",
    "    .setRandomSeed(42)\n",
    "\n",
    "MaxNewTokens = [128, 299]\n",
    "\n",
    "\n",
    "# Sample question\n",
    "question = \"How can asthma be treated?\"\n",
    "TEXT = [f\"question: {question} answer:\"]\n",
    "\n",
    "for j in MaxNewTokens:\n",
    "    print(\"Question:\", question)\n",
    "    print(\"Parameters:\") \n",
    "    print(f\"\\nsetMaxNewTokens({j}):\")\n",
    "    gpt_qa.setMaxNewTokens(j)\n",
    "    pipeline = Pipeline().setStages([document_assembler, gpt_qa])\n",
    "\n",
    "    light_model = LightPipeline(pipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\")))\n",
    "    answer_default = light_model.annotate(TEXT)\n",
    "    \n",
    "    answer_text = answer_default[0][\"answer\"][0][len(TEXT[0]) + 1:].strip()\n",
    "    wrapped_answer_text = textwrap.fill(answer_text, width=150)\n",
    "    token_count = len(answer_text.split())\n",
    "    print(\"‚û§ Answer:\")\n",
    "    print(wrapped_answer_text)\n",
    "    print(f\"Number of tokens used: {token_count}\")\n",
    "    print(\"-\" * 40)  # Separator line\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b><h1><font color='darkred'>!!! ATTENTION !!! </font><h1><b>\n",
    "\n",
    "<b>before running the following cells, <font color='darkred'>RESTART the COLAB RUNTIME </font> than start your session and go ahead.<b>"
   ],
   "metadata": {
    "id": "M6sjuM3NW-ZS"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYOrd_2OLSyD"
   },
   "source": [
    "# \tüìéüè• `biogpt_chat_jsl_conversational`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4vCaKWzyBCX"
   },
   "source": [
    "This model is based on BioGPT finetuned with medical conversations happening in a clinical settings and can answer clinical questions related to symptoms, drugs, tests, and diseases. The difference between this model and `biogpt_chat_jsl` is that this model produces more concise/smaller response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uXW20vHBLd3u",
    "outputId": "9c5a5682-0f84-4abb-d406-b30ad5a4f946"
   },
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"documents\")\n",
    "    \n",
    "gpt_qa = MedicalTextGenerator().pretrained(\"biogpt_chat_jsl_conversational\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols(\"documents\")\\\n",
    "    .setOutputCol(\"answer\")\\\n",
    "    .setMaxNewTokens(399)\\\n",
    "    .setStopAtEos(True)\\\n",
    "    .setDoSample(False)\\\n",
    "    .setTopK(1)\\\n",
    "    .setRandomSeed(42)\n",
    "    \n",
    "pipeline = Pipeline().setStages([document_assembler, gpt_qa])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zy_Jt_MkCfvn"
   },
   "outputs": [],
   "source": [
    "question = \"What is the difference between melanoma and sarcoma?\"\n",
    "TEXT = [f\"question: {question} answer:\"]\n",
    "\n",
    "model = pipeline.fit(spark.createDataFrame([[\"\"]]).toDF(\"text\"))\n",
    "light_model = LightPipeline(model)\n",
    "light_result = light_model.annotate(TEXT)\n",
    "answer_text = light_result[0][\"answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pqxGMy4Chmz",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7c643ff8-6977-4ab4-c05d-c34028c33282"
   },
   "outputs": [],
   "source": [
    "# Extract the text after 'answer:'\n",
    "final_answer = answer_text[0][len(TEXT[0]) + 1:].strip()\n",
    "\n",
    "# Format the text into paragraphs\n",
    "wrapped_text = textwrap.fill(final_answer, width=120)\n",
    "\n",
    "print(\"‚û§ Answer: \\n{}\".format(wrapped_text))\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}