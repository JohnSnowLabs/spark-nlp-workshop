{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91Ih6rLKTq7h"
   },
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQsEwwyrTwAh"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/40.1.Text_Matcher_Internal.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using the `johnsnowlabs` library, please use this ¬†[01.7.Text_Matcher_Internal](https://github.com/JohnSnowLabs/spark-nlp-workshop/blob/master/healthcare-nlp/01.7.Text_Matcher_Internal.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-0OhC7jTznW"
   },
   "source": [
    "# üìúTextMatcherInternal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97fnzEt3T5CS"
   },
   "source": [
    "üîç What is TextMatcherInternal?\n",
    "TextMatcherInternal is an annotator in Spark NLP used to detect exact phrase matches within a document, based on a predefined list of entity phrases.\n",
    "\n",
    "This component performs token-level exact matching between the input text and a set of phrases provided by the user, typically through an external text file. When a match is found, it creates a CHUNK annotation containing the matched text span.\n",
    "\n",
    "‚úÖ When to Use It\n",
    "TextMatcherInternal is particularly useful in scenarios such as:\n",
    "\n",
    "Detecting predefined terms in clinical, legal, or domain-specific documents\n",
    "\n",
    "Keyword-based filtering or rule-based entity recognition\n",
    "\n",
    "Pre-labeling data for supervised training or rule-based NER systems\n",
    "\n",
    "\n",
    "**üîó Helpful Links:**\n",
    "\n",
    "For extended examples of usage, see the [Spark NLP Workshop Repository](https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/healthcare-nlp)\n",
    "\n",
    "Python Documentation: [TextMatcher](https://sparknlp.org/api/python/reference/autosummary/sparknlp/annotator/matcher/text_matcher/index.html#sparknlp.annotator.matcher.text_matcher.TextMatcher)\n",
    "\n",
    "Scala Documentation: [TextMatcher](https://sparknlp.org/api/com/johnsnowlabs/nlp/annotators/TextMatcher)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qg3slXBwVAEF"
   },
   "source": [
    "\n",
    "## **üñ®Ô∏è Input/Output Annotation Types**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DM370suhVGn4"
   },
   "source": [
    "- Input: `SENTECE`, `TOKEN`\n",
    "\n",
    "- Output: `CHUNK`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jiePCiPtVNVg"
   },
   "source": [
    "## **üîé Parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfk75w8gVQIE"
   },
   "source": [
    "## TextMatcherInternal Parameters\n",
    "\n",
    "| **Parameter** | **Description** |\n",
    "| --- | --- |\n",
    "| **entities** | ExternalResource for entities. A text file of predefined phrases must be provided. |\n",
    "| **caseSensitive** | Whether to match regardless of case. Defaults to `True`. |\n",
    "| **mergeOverlapping** | Whether to merge overlapping matched chunks. Defaults to `False`. |\n",
    "| **entityValue** | Value for the entity metadata field. |\n",
    "| **buildFromTokens** | Whether the matcher should take the CHUNK from TOKEN. |\n",
    "| **dictionary** | External dictionary for lemmatizer. |\n",
    "| **enableLemmatizer** | Whether to enable lemmatizer. Defaults to `False`. |\n",
    "| **enableStemmer** | Whether to enable stemmer. Defaults to `False`. |\n",
    "| **stopWords** | List of stop words to be removed. Defaults to `None`. |\n",
    "| **cleanStopWords** | Whether to clean stop words. Defaults to `False`. |\n",
    "| **shuffleEntitySubTokens** | Whether to generate and use variations (permutations) of the entity phrases. Defaults to `False`. |\n",
    "| **safeKeywords** | Keywords to preserve during stopword removal when `cleanStopWords` is enabled. Defaults to empty. |\n",
    "| **excludePunctuation** | If `True`, punctuation will be removed from the text. Defaults to `True`. |\n",
    "| **cleanKeywords** | Additional keywords to be removed alongside default stopwords. Defaults to empty. |\n",
    "| **excludeRegexPatterns** | Regex patterns used to drop matched chunks. Defaults to empty. |\n",
    "| **returnChunks** | Controls whether to return the original text chunks from input or the matched (e.g., stemmed/lemmatized) phrases. Can be `'original'` or `'matched'`. Defaults to `'original'`. |\n",
    "| **skipMatcherAugmentation** | Whether to skip matcher augmentation. Defaults to `False`. |\n",
    "| **skipSourceTextAugmentation** | Whether to skip source text augmentation. Defaults to `False`. |\n",
    "\n",
    "---\n",
    "\n",
    "## TextMatcherInternalModel Parameters\n",
    "\n",
    "| **Parameter** | **Description** |\n",
    "| --- | --- |\n",
    "| **mergeOverlapping** | Whether to merge overlapping matched chunks. Defaults to `False`. |\n",
    "| **entityValue** | Value for the entity metadata field. |\n",
    "| **buildFromTokens** | Whether the matcher should take the CHUNK from TOKEN. |\n",
    "| **enableLemmatizer** | Whether to enable lemmatizer. Defaults to `False`. |\n",
    "| **enableStemmer** | Whether to enable stemmer. Defaults to `False`. |\n",
    "| **stopWords** | List of stop words to be removed. Defaults to `None`. |\n",
    "| **cleanStopWords** | Whether to clean stop words. Defaults to `False`. |\n",
    "| **returnChunks** | Whether to return original chunks or matched chunks. Defaults to original chunks. |\n",
    "| **safeKeywords** | Keywords to preserve during stopword removal when `cleanStopWords` is enabled. Defaults to empty. |\n",
    "| **excludePunctuation** | If `True`, punctuation will be removed from the text. Defaults to `True`. |\n",
    "| **cleanKeywords** | Additional keywords to be removed alongside default stopwords. Defaults to empty. |\n",
    "| **excludeRegexPatterns** | Regex patterns used to drop matched chunks. Defaults to empty. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpBEf4WUU1VX"
   },
   "source": [
    "## **üé¨ Colab Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHWr26lLWpoY"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from google.colab import files\n",
    "\n",
    "if 'spark_jsl.json' not in os.listdir():\n",
    "  license_keys = files.upload()\n",
    "  os.rename(list(license_keys.keys())[0], 'spark_jsl.json')\n",
    "\n",
    "with open('spark_jsl.json') as f:\n",
    "    license_keys = json.load(f)\n",
    "\n",
    "# Defining license key-value pairs as local variables\n",
    "locals().update(license_keys)\n",
    "os.environ.update(license_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kODrwKTpXDKB"
   },
   "outputs": [],
   "source": [
    "# Installing pyspark and spark-nlp\n",
    "! pip install --upgrade -q pyspark==3.5.1  spark-nlp==$PUBLIC_VERSION\n",
    "\n",
    "# Installing Spark NLP Healthcare\n",
    "! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET\n",
    "\n",
    "# Installing Spark NLP Display Library for visualization\n",
    "! pip install --upgrade -q spark-nlp-display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "executionInfo": {
     "elapsed": 66245,
     "status": "ok",
     "timestamp": 1758950795970,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "4a3gs7_xXFfQ",
    "outputId": "efef315e-839c-454b-a1c8-254e1847f97e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP Version : 6.1.3\n",
      "Spark NLP_JSL Version : 6.1.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c61cd8baa17a:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP Licensed</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7b26977e3d40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import sparknlp\n",
    "import sparknlp_jsl\n",
    "\n",
    "from sparknlp.base import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.training import *\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "from sparknlp_jsl.base import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "\n",
    "params = {\n",
    "    \"spark.driver.memory\":\"50G\",\n",
    "    \"spark.driver.maxResultSize\":\"5G\",\n",
    "}\n",
    "\n",
    "spark = sparknlp_jsl.start(license_keys['SECRET'],params=params)\n",
    "print(\"Spark NLP Version :\", sparknlp.version())\n",
    "print(\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Px2CNvOWXf9i"
   },
   "source": [
    "# Basic Usage of `TextMatcherInternal`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84Av5fSaY5oO"
   },
   "source": [
    "## How to Use `TextMatcherInternal`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkxDEfHBXjm9"
   },
   "source": [
    "First of all, we should create a source file that includes all the chunks or tokens we need to capture. In the example below, we use `#` as a delimiter to separate the label and entity. So we need to set parameter like this `setDelimiter('#')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1758950804117,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "VssVIW6yXpCr"
   },
   "outputs": [],
   "source": [
    "matcher_drug = \"\"\"\n",
    "Aspirin 100mg#Drug\n",
    "aspirin#Drug\n",
    "paracetamol#Drug\n",
    "amoxicillin#Drug\n",
    "ibuprofen#Drug\n",
    "lansoprazole#Drug\n",
    "\"\"\"\n",
    "\n",
    "with open ('matcher_drug.csv', 'w') as f:\n",
    "  f.write(matcher_drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 10536,
     "status": "ok",
     "timestamp": 1758950822994,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "dVKqPa7OXprV"
   },
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "entityExtractor = TextMatcherInternal()\\\n",
    "    .setInputCols([\"document\", \"token\"])\\\n",
    "    .setEntities(\"matcher_drug.csv\")\\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setCaseSensitive(False)\\\n",
    "    .setDelimiter(\"#\")\\\n",
    "    .setMergeOverlapping(False)\n",
    "\n",
    "mathcer_pipeline = Pipeline(\n",
    "    stages=[\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        entityExtractor\n",
    "])\n",
    "\n",
    "text = \"\"\"\n",
    "John's doctor prescribed aspirin 100mg for his heart condition, along with paracetamol for his fever,\n",
    "amoxicillin for his tonsilitis, ibuprofen for his inflammation, and lansoprazole for his GORD.\n",
    "\"\"\"\n",
    "\n",
    "data = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "\n",
    "result = mathcer_pipeline.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8113,
     "status": "ok",
     "timestamp": 1758950839660,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "JqUj8W9AXpod",
    "outputId": "846f867e-be2b-490d-ebc8-7ffc6cfbf242"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+---+-----+\n",
      "|        chunk|begin|end|label|\n",
      "+-------------+-----+---+-----+\n",
      "|aspirin 100mg|   26| 38| Drug|\n",
      "|  amoxicillin|  103|113| Drug|\n",
      "| lansoprazole|  171|182| Drug|\n",
      "|  paracetamol|   76| 86| Drug|\n",
      "|      aspirin|   26| 32| Drug|\n",
      "|    ibuprofen|  135|143| Drug|\n",
      "+-------------+-----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select(F.explode(F.arrays_zip(result.matched_text.result,\n",
    "                                     result.matched_text.begin,\n",
    "                                     result.matched_text.end,\n",
    "                                     result.matched_text.metadata)).alias(\"cols\"))\\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
    "              F.expr(\"cols['1']\").alias(\"begin\"),\n",
    "              F.expr(\"cols['2']\").alias(\"end\"),\n",
    "              F.expr(\"cols['3']['entity']\").alias('label')).show(truncate=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfqXkcy2XxWm"
   },
   "source": [
    "*As* you see above mather_drug file includes 2 similar entities aspirin and aspirin 100mg and our text includes both of them So if you want to see both of them you need to set `MergeOverlapping` parameter as `False`. You can look at the below example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2324,
     "status": "ok",
     "timestamp": 1758950844936,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "oqUtw9WNX0r2"
   },
   "outputs": [],
   "source": [
    "entityExtractor = TextMatcherInternal()\\\n",
    "    .setInputCols([\"document\", \"token\"])\\\n",
    "    .setEntities(\"matcher_drug.csv\") \\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setDelimiter(\"#\")\\\n",
    "    .setCaseSensitive(False)\\\n",
    "    .setMergeOverlapping(False)\n",
    "\n",
    "mathcer_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        entityExtractor\n",
    "])\n",
    "\n",
    "result = mathcer_pipeline.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2813,
     "status": "ok",
     "timestamp": 1758950850443,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "D-Co2PuCX1SM",
    "outputId": "b07d7d33-f530-469b-964e-4abba9cc0477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+---+-----+\n",
      "|        chunk|begin|end|label|\n",
      "+-------------+-----+---+-----+\n",
      "|aspirin 100mg|   26| 38| Drug|\n",
      "|  amoxicillin|  103|113| Drug|\n",
      "| lansoprazole|  171|182| Drug|\n",
      "|  paracetamol|   76| 86| Drug|\n",
      "|      aspirin|   26| 32| Drug|\n",
      "|    ibuprofen|  135|143| Drug|\n",
      "+-------------+-----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select(F.explode(F.arrays_zip(result.matched_text.result,\n",
    "                                     result.matched_text.begin,\n",
    "                                     result.matched_text.end,\n",
    "                                     result.matched_text.metadata)).alias(\"cols\"))\\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
    "              F.expr(\"cols['1']\").alias(\"begin\"),\n",
    "              F.expr(\"cols['2']\").alias(\"end\"),\n",
    "              F.expr(\"cols['3']['entity']\").alias('label')).show(truncate=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jx_EpYq2X5yg"
   },
   "source": [
    "When we set the `CaseSensitive` parameter to `True`, it means we're considering the case sensitivity of chunks in the source file. Consequently, some chunks may not be visible due to differences in their case compared to the source file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1870,
     "status": "ok",
     "timestamp": 1758950855180,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "Tz8QWAYRX1Pj"
   },
   "outputs": [],
   "source": [
    "entityExtractor = TextMatcherInternal()\\\n",
    "    .setInputCols([\"document\", \"token\"])\\\n",
    "    .setEntities(\"matcher_drug.csv\") \\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setDelimiter(\"#\")\\\n",
    "    .setCaseSensitive(True)\\\n",
    "    .setMergeOverlapping(False)\n",
    "\n",
    "mathcer_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        entityExtractor\n",
    "])\n",
    "\n",
    "matcher_model = mathcer_pipeline.fit(data)\n",
    "result = matcher_model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2532,
     "status": "ok",
     "timestamp": 1758950861159,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "Lfc93ehTX1Mq",
    "outputId": "4941443e-ab48-4a34-f347-47c5979cab8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+---+-----+\n",
      "|       chunk|begin|end|label|\n",
      "+------------+-----+---+-----+\n",
      "| amoxicillin|  103|113| Drug|\n",
      "|lansoprazole|  171|182| Drug|\n",
      "| paracetamol|   76| 86| Drug|\n",
      "|     aspirin|   26| 32| Drug|\n",
      "|   ibuprofen|  135|143| Drug|\n",
      "+------------+-----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select(F.explode(F.arrays_zip(result.matched_text.result,\n",
    "                                     result.matched_text.begin,\n",
    "                                     result.matched_text.end,\n",
    "                                     result.matched_text.metadata)).alias(\"cols\"))\\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
    "              F.expr(\"cols['1']\").alias(\"begin\"),\n",
    "              F.expr(\"cols['2']\").alias(\"end\"),\n",
    "              F.expr(\"cols['3']['entity']\").alias('label')).show(truncate=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BEZrYQaX_26"
   },
   "source": [
    "## Multiple Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1758950864193,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "0WPxeWzpYE-E"
   },
   "outputs": [],
   "source": [
    "multiple_entites= \"\"\"\n",
    "Aspirin 100mg#Drug\n",
    "paracetamol#Drug\n",
    "amoxicillin#Drug\n",
    "ibuprofen#Drug\n",
    "lansoprazole#Drug\n",
    "fever#Symptom\n",
    "headache#Symptom\n",
    "tonsilitis#Disease\n",
    "GORD#Disease\n",
    "heart condition#Disease\n",
    "\"\"\"\n",
    "\n",
    "with open ('multiple_entities.csv', 'w') as f:\n",
    "  f.write(multiple_entites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 1794,
     "status": "ok",
     "timestamp": 1758950869180,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "tlodnCiGYF7a"
   },
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "entityExtractor = TextMatcherInternal() \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setEntities(\"multiple_entities.csv\") \\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setDelimiter(\"#\")\\\n",
    "    .setCaseSensitive(False)\\\n",
    "    .setDelimiter(\"#\")\n",
    "\n",
    "matcher_pipeline = Pipeline().setStages([\n",
    "                  documentAssembler,\n",
    "                  tokenizer,\n",
    "                  entityExtractor])\n",
    "\n",
    "text = \"\"\"\n",
    "John's doctor prescribed aspirin 100mg for his heart condition, along with paracetamol for his fever and headache,\n",
    "amoxicillin for his tonsilitis, ibuprofen for his inflammation, and lansoprazole for his GORD.\n",
    "\"\"\"\n",
    "data = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "\n",
    "matcher_model = matcher_pipeline.fit(data)\n",
    "result = matcher_model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2742,
     "status": "ok",
     "timestamp": 1758950876639,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "OA305yW0YF4n",
    "outputId": "da5f81ad-fe36-478a-8d64-17590ab7de85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+---+-------+\n",
      "|          chunk|begin|end|  label|\n",
      "+---------------+-----+---+-------+\n",
      "|  aspirin 100mg|   26| 38|   Drug|\n",
      "|    amoxicillin|  116|126|   Drug|\n",
      "|   lansoprazole|  184|195|   Drug|\n",
      "|    paracetamol|   76| 86|   Drug|\n",
      "|      ibuprofen|  148|156|   Drug|\n",
      "|          fever|   96|100|Symptom|\n",
      "|       headache|  106|113|Symptom|\n",
      "|heart condition|   48| 62|Disease|\n",
      "|     tonsilitis|  136|145|Disease|\n",
      "|           GORD|  205|208|Disease|\n",
      "+---------------+-----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.select(F.explode(F.arrays_zip(result.matched_text.result,\n",
    "                                     result.matched_text.begin,\n",
    "                                     result.matched_text.end,\n",
    "                                     result.matched_text.metadata,)).alias(\"cols\"))\\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
    "              F.expr(\"cols['1']\").alias(\"begin\"),\n",
    "              F.expr(\"cols['2']\").alias(\"end\"),\n",
    "              F.expr(\"cols['3']['entity']\").alias('label')).show(truncate=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Id1ejvlYU2G"
   },
   "source": [
    "## `TextMatcherInternalModel`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 1625,
     "status": "ok",
     "timestamp": 1758950881829,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "OF-9mJuNYF19"
   },
   "outputs": [],
   "source": [
    "entityExtractor = TextMatcherInternal() \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setEntities(\"matcher_drug.csv\") \\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setCaseSensitive(False)\\\n",
    "    .setDelimiter(\"#\")\n",
    "\n",
    "matcher_pipeline = Pipeline().setStages([\n",
    "                  documentAssembler,\n",
    "                  tokenizer,\n",
    "                  entityExtractor])\n",
    "\n",
    "text = \"\"\"John's doctor prescribed aspirin 100mg for his heart condition, along with paracetamol for his fever and headache,\n",
    "amoxicillin for his tonsilitis, ibuprofen for his inflammation, and lansoprazole for his GORD.\"\"\"\n",
    "\n",
    "data = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "\n",
    "result = matcher_pipeline.fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6_erUQQYZo4"
   },
   "source": [
    "Saving the approach to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 2957,
     "status": "ok",
     "timestamp": 1758950889028,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "VawxxEzdYFzK"
   },
   "outputs": [],
   "source": [
    "matcher_model.stages[-1].write().overwrite().save(\"matcher_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGHDYXtcX_0E"
   },
   "source": [
    "Loading the saved model and using it with the `TextMatcherModel()` via `load`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 4959,
     "status": "ok",
     "timestamp": 1758950895022,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "m3lIoEt4X1Ju"
   },
   "outputs": [],
   "source": [
    "entity_ruler = TextMatcherInternalModel.load('./matcher_model') \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages=[\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        entity_ruler\n",
    "])\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "pipeline_model = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7N7EBoqMYj2p"
   },
   "source": [
    "Checking the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1737,
     "status": "ok",
     "timestamp": 1758950898803,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "xoR8TSzPYjF-",
    "outputId": "a0b74b26-bbbf-4167-dedf-682890a09812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+---+-------+\n",
      "|          chunk|begin|end|  label|\n",
      "+---------------+-----+---+-------+\n",
      "|heart condition|   47| 61|Disease|\n",
      "|     tonsilitis|  135|144|Disease|\n",
      "|           GORD|  204|207|Disease|\n",
      "|  aspirin 100mg|   25| 37|   Drug|\n",
      "|    amoxicillin|  115|125|   Drug|\n",
      "|   lansoprazole|  183|194|   Drug|\n",
      "|    paracetamol|   75| 85|   Drug|\n",
      "|      ibuprofen|  147|155|   Drug|\n",
      "|          fever|   95| 99|Symptom|\n",
      "|       headache|  105|112|Symptom|\n",
      "+---------------+-----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = pipeline_model.transform(data)\n",
    "\n",
    "result.select(F.explode(F.arrays_zip(result.matched_text.result,\n",
    "                                     result.matched_text.begin,\n",
    "                                     result.matched_text.end,\n",
    "                                     result.matched_text.metadata)).alias(\"cols\"))\\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
    "              F.expr(\"cols['1']\").alias(\"begin\"),\n",
    "              F.expr(\"cols['2']\").alias(\"end\"),\n",
    "              F.expr(\"cols['3']['entity']\").alias('label')).show(truncate=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyvZCqGLZCkS"
   },
   "source": [
    "#Advanced Usage of TextMatcherInternal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMmKStkIaoFB"
   },
   "source": [
    "In this section, we demonstrate advanced features of the `TextMatcherInternal` annotator that enhance its flexibility and robustness in text matching tasks. These features include lemmatization and stemming for matching different word forms, customizable stop word removal with safe keywords preservation, punctuation exclusion, pattern-based chunk exclusion via regex, and the ability to control whether to return original or transformed matched text. We also show how to generate phrase permutations for more comprehensive matching, and options to skip automatic augmentation steps.\n",
    "\n",
    "Consider the following example sentence for illustration:\n",
    "\n",
    "> \"Patient was able to talk briefly about recent life stressors during evaluation of psychiatric state.\n",
    "She reports difficulty sleeping and ongoing anxiety. Denies suicidal ideation.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rOaRIjLedwZE"
   },
   "source": [
    "##Basic Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11166,
     "status": "ok",
     "timestamp": 1758950914344,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "e4liI4ZZd0NW",
    "outputId": "d326e49f-9d35-44f5-c47d-d7b8c2b787d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_detector_dl_healthcare download started this may take some time.\n",
      "Approximate size to download 367.3 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = SentenceDetectorDLModel\\\n",
    "    .pretrained(\"sentence_detector_dl_healthcare\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols([\"sentence\"])\\\n",
    "    .setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6y3N2CbZ0zA"
   },
   "source": [
    "##Example Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1758950918132,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "LI07Z7xPXMi_"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Patient was able to talk briefly about recent life stressors during evaluation of psychiatric state.\n",
    "She reports difficulty sleeping and ongoing anxiety. Denies suicidal ideation.\n",
    "\"\"\"\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "data = spark.createDataFrame([[text]]).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cjlw15CWdVuL"
   },
   "source": [
    "### üå± Demonstrating Lemmatizer and Stemmer Features\n",
    "\n",
    "To showcase the effects of lemmatization and stemming on text matching, we use a set of example phrases that include different word forms and variations.  \n",
    "\n",
    "For instance, words like **\"sleeping\"**  will be matched to their base forms  **\"sleep\"** when lemmatization is enabled.\n",
    "\n",
    "Here is a sample list of phrases saved to a file (`test-phrases.txt`) that we will use for matching against input text to demonstrate these features:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1758950922194,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "d-pN06Opc6ek"
   },
   "outputs": [],
   "source": [
    "test_phrases = \"\"\"\n",
    "stressor\n",
    "difficulty sleep\n",
    "\"\"\"\n",
    "\n",
    "with open(\"test-phrases.txt\", \"w\") as file:\n",
    "    file.write(test_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 1698,
     "status": "ok",
     "timestamp": 1758950927908,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "OG54pwaDXP-9"
   },
   "outputs": [],
   "source": [
    "text_matcher = TextMatcherInternal()\\\n",
    "    .setInputCols([\"sentence\",\"token\"])\\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setEntities(\"./test-phrases.txt\")\\\n",
    "    .setEnableLemmatizer(True) \\\n",
    "    .setEnableStemmer(True) \\\n",
    "    .setCleanStopWords(True) \\\n",
    "    .setBuildFromTokens(True)\\\n",
    "    .setReturnChunks(\"original\")\\\n",
    "    .setExcludePunctuation(True)\n",
    "\n",
    "text_matcher_pipeline= Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        text_matcher\n",
    "])\n",
    "\n",
    "text_matcher_model = text_matcher_pipeline.fit(empty_data)\n",
    "text_matcher_result_df = text_matcher_model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3503,
     "status": "ok",
     "timestamp": 1758950934713,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "kOLzdL0vXWzN",
    "outputId": "263da4be-1ed5-42a3-858c-1c27f99f2a38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+-------------------+----------------+\n",
      "|entity|begin|end|result             |matched         |\n",
      "+------+-----+---+-------------------+----------------+\n",
      "|entity|52   |60 |stressors          |stressor        |\n",
      "|entity|114  |132|difficulty sleeping|difficulty sleep|\n",
      "+------+-----+---+-------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"matched_text\") \\\n",
    "    .setExplodeSelectedFields({\n",
    "        \"matched_text\": [\n",
    "            \"metadata.entity as entity\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            \"metadata.original_or_matched as matched \"\n",
    "            ]\n",
    "         })\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhedYPzzqVD0"
   },
   "source": [
    "### üîÅ shuffleEntitySubTokens\n",
    "\n",
    "\n",
    "The **`shuffleEntitySubTokens`** parameter controls whether token-level permutations of entity phrases should be generated and used during matching.\n",
    "\n",
    "When set to `True`, the matcher will automatically generate all possible orderings (permutations) of the tokens within each entity phrase and try to match them against the input text.\n",
    "\n",
    "For example, if `\"sleep difficulty\"` is in your entity list:\n",
    "- With `shuffleEntitySubTokens=True`, phrases like `\"difficulty sleep\"` will also match.\n",
    "- With `shuffleEntitySubTokens=False`, only the exact phrase `\"sleep difficulty\"` will match.\n",
    "\n",
    "> ‚ö†Ô∏è **Note:** This parameter is only supported in the `TextMatcherInternal` class (the training/approach component). It is **not available** in the `TextMatcherInternalModel` class (the trained model component)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1758950938114,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "jLRO4cB5qin1"
   },
   "outputs": [],
   "source": [
    "test_phrases = \"\"\"\n",
    "suicidal deny\n",
    "sleep difficulty\n",
    "\"\"\"\n",
    "\n",
    "with open(\"test-phrases.txt\", \"w\") as file:\n",
    "    file.write(test_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 1597,
     "status": "ok",
     "timestamp": 1758950942617,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "yGnFosbhqU19"
   },
   "outputs": [],
   "source": [
    "text_matcher = TextMatcherInternal()\\\n",
    "    .setInputCols([\"sentence\",\"token\"])\\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setEntities(\"./test-phrases.txt\")\\\n",
    "    .setEnableLemmatizer(True) \\\n",
    "    .setEnableStemmer(True) \\\n",
    "    .setCleanStopWords(True) \\\n",
    "    .setShuffleEntitySubTokens(True)\\\n",
    "    .setBuildFromTokens(True)\\\n",
    "    .setReturnChunks(\"original\")\\\n",
    "\n",
    "text_matcher_pipeline= Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        text_matcher\n",
    "])\n",
    "\n",
    "text_matcher_model = text_matcher_pipeline.fit(empty_data)\n",
    "text_matcher_result_df = text_matcher_model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3240,
     "status": "ok",
     "timestamp": 1758950949441,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "zFoMMWRlqUzA",
    "outputId": "fff1bcf1-0b3b-4333-80d9-09a8ac0684e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+-------------------+----------------+\n",
      "|entity|begin|end|result             |matched         |\n",
      "+------+-----+---+-------------------+----------------+\n",
      "|entity|114  |132|difficulty sleeping|difficulty sleep|\n",
      "|entity|155  |169|Denies suicidal    |deni suicidal   |\n",
      "+------+-----+---+-------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"matched_text\") \\\n",
    "    .setExplodeSelectedFields({\n",
    "        \"matched_text\": [\n",
    "            \"metadata.entity as entity\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            \"metadata.original_or_matched as matched\"\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEgpVjspt5E5"
   },
   "source": [
    "### üõë Demonstrating Stop Word Removal\n",
    "\n",
    "In this section, we demonstrate how removing stop words from both the **input text** and the **entity phrases** can significantly improve the accuracy and flexibility of phrase matching.\n",
    "\n",
    "Stop words are common words such as **\"and\"**, **\"the\"**, or **\"about\"** that typically carry limited semantic meaning. These words can interfere with matching by creating unnecessary mismatches, especially when your entity phrases are clean and concise.\n",
    "\n",
    "#### ‚öôÔ∏è How it works:\n",
    "- By enabling `.setCleanStopWords(True)`, the matcher will activate stop word filtering.\n",
    "- If you do not specify a custom list using `.setStopWords([...])`, it defaults to **Spark ML‚Äôs built-in English stop word list**.\n",
    "- These stop words will be removed from **both the source text** and the **entity phrases** before matching.\n",
    "- To preserve specific terms that appear in the stop word list, you can use `.setSafeKeywords([...])`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1758950955418,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "BeOOV3xUmW8h"
   },
   "outputs": [],
   "source": [
    "test_phrases = \"\"\"\n",
    "evaluation psychiatric state\n",
    "\"\"\"\n",
    "\n",
    "with open(\"test-phrases.txt\", \"w\") as file:\n",
    "    file.write(test_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 2778,
     "status": "ok",
     "timestamp": 1758950960918,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "15MgDXlMex4n"
   },
   "outputs": [],
   "source": [
    "text_matcher = TextMatcherInternal()\\\n",
    "    .setInputCols([\"sentence\",\"token\"])\\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setEntities(\"./test-phrases.txt\")\\\n",
    "    .setEnableLemmatizer(True) \\\n",
    "    .setEnableStemmer(True) \\\n",
    "    .setCleanStopWords(True) \\\n",
    "\n",
    "text_matcher_pipeline= Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        text_matcher\n",
    "])\n",
    "\n",
    "text_matcher_model = text_matcher_pipeline.fit(empty_data)\n",
    "text_matcher_result_df = text_matcher_model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2370,
     "status": "ok",
     "timestamp": 1758950965644,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "6VUf9WI-e0D5",
    "outputId": "ce347560-5e6c-425c-9bb1-c6ed7759d7d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+-------------------------------+----------------------------+\n",
      "|entity|begin|end|result                         |matched                     |\n",
      "+------+-----+---+-------------------------------+----------------------------+\n",
      "|entity|69   |99 |evaluation of psychiatric state|evaluation psychiatric state|\n",
      "+------+-----+---+-------------------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"matched_text\") \\\n",
    "    .setExplodeSelectedFields({\n",
    "        \"matched_text\": [\n",
    "            \"metadata.entity as entity\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            \"metadata.original_or_matched as matched\"\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1n_LUhB8nMcy"
   },
   "source": [
    "###  üßπ Customizing Text Cleaning with Stop Words, Safe Keywords, and Punctuation Settings\n",
    "\n",
    "\n",
    "The `TextMatcherInternal` annotator provides flexible options to control how the input text is cleaned before matching. This allows users to fine-tune what content should be removed or preserved for optimal matching accuracy. The following parameters are especially important:\n",
    "\n",
    "- **`setStopWords`**: Defines a custom list of stop words to be removed from both the input text and the entity phrases. These words are excluded before any matching is performed.\n",
    "\n",
    "- **`cleanKeywords`**: Specifies additional words (beyond the stop words) that should also be removed. Useful for removing domain-specific noise words.\n",
    "\n",
    "- **`safeKeywords`**: Lists important keywords that should **not** be removed even if they appear in the stop word list. This ensures that key terms are preserved during cleaning.\n",
    "\n",
    "- **`excludePunctuation`**: When set to `true`, all punctuation characters (such as `.`, `,`, `!`, etc.) are removed from the text before matching.\n",
    "\n",
    "These options provide granular control over the preprocessing phase, allowing you to reduce noise while preserving meaningful content. This is particularly helpful when dealing with informal, noisy, or user-generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1758950972013,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "iNRO2VcRnx0S"
   },
   "outputs": [],
   "source": [
    "test_phrases = \"\"\"\n",
    "evaluation psychiatric state\n",
    "\"\"\"\n",
    "\n",
    "with open(\"test-phrases.txt\", \"w\") as file:\n",
    "    file.write(test_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 1993,
     "status": "ok",
     "timestamp": 1758950975169,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "shR2sNpJu280"
   },
   "outputs": [],
   "source": [
    "text_matcher = TextMatcherInternal()\\\n",
    "    .setInputCols([\"sentence\",\"token\"])\\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setEntities(\"./test-phrases.txt\")\\\n",
    "    .setEnableLemmatizer(True) \\\n",
    "    .setEnableStemmer(True) \\\n",
    "    .setStopWords([\"and\", \"in\"]) \\\n",
    "    .setCleanStopWords(True) \\\n",
    "    .setCleanKeywords([\"of\"]) \\\n",
    "    .setExcludePunctuation(True)\\\n",
    "\n",
    "\n",
    "text_matcher_pipeline= Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        text_matcher\n",
    "    ])\n",
    "\n",
    "text_matcher_model = text_matcher_pipeline.fit(empty_data)\n",
    "text_matcher_result_df = text_matcher_model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2227,
     "status": "ok",
     "timestamp": 1758950980047,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "QE8MzODMu4WD",
    "outputId": "ba7f9984-ac2f-4429-90cd-b2070b6c439c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+-------------------------------+----------------------------+\n",
      "|entity|begin|end|result                         |matched                     |\n",
      "+------+-----+---+-------------------------------+----------------------------+\n",
      "|entity|69   |99 |evaluation of psychiatric state|evaluation psychiatric state|\n",
      "+------+-----+---+-------------------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"matched_text\") \\\n",
    "    .setExplodeSelectedFields({\n",
    "        \"matched_text\": [\n",
    "            \"metadata.entity as entity\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            \"metadata.original_or_matched as matched\"\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZkSAITQpoul"
   },
   "source": [
    "### üîß Matcher & Source Augmentation Settings\n",
    "\n",
    "\n",
    "The parameters **`skipMatcherAugmentation`** and **`skipSourceTextAugmentation`** control whether automatic variations of the phrases (such as token permutations or transformations) are considered during matching. These augmentations are useful to improve match coverage but can be turned off for stricter or faster matching.\n",
    "\n",
    "- **`skipMatcherAugmentation`**  \n",
    "  When set to `True`, disables augmentation of the matcher entities.  \n",
    "  For example, if the phrase is `\"study biology\"`, by default, variations like `\"biology study\"` might also be generated and matched ‚Äî unless this is skipped.\n",
    "\n",
    "- **`skipSourceTextAugmentation`**  \n",
    "  When set to `True`, prevents augmentation (such as reordering tokens or normalizing text) of the **input** source text before matching.  \n",
    "  Useful when you want to match the input exactly as it is written, without any alterations.\n",
    "\n",
    "These parameters give you more control over how flexible or strict the matching process should be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1758950984288,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "MD7kxfKQ7YwF"
   },
   "outputs": [],
   "source": [
    "test_phrases = \"\"\"\n",
    "stressor\n",
    "evaluation psychiatric state\n",
    "\"\"\"\n",
    "\n",
    "with open(\"test-phrases.txt\", \"w\") as file:\n",
    "    file.write(test_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 1703,
     "status": "ok",
     "timestamp": 1758950988697,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "vTJ3f7pDprbj"
   },
   "outputs": [],
   "source": [
    "text_matcher = TextMatcherInternal()\\\n",
    "    .setInputCols([\"sentence\",\"token\"])\\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setEntities(\"./test-phrases.txt\")\\\n",
    "    .setEnableLemmatizer(True) \\\n",
    "    .setEnableStemmer(True) \\\n",
    "    .setCleanStopWords(True) \\\n",
    "    .setBuildFromTokens(True)\\\n",
    "    .setReturnChunks(\"original\")\\\n",
    "    .setSkipMatcherAugmentation(True)\\\n",
    "    .setSkipSourceTextAugmentation(False)\n",
    "\n",
    "\n",
    "text_matcher_pipeline= Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        text_matcher\n",
    "    ])\n",
    "\n",
    "text_matcher_model = text_matcher_pipeline.fit(empty_data)\n",
    "text_matcher_result_df = text_matcher_model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2293,
     "status": "ok",
     "timestamp": 1758950992971,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "ByAbTZ1cprY4",
    "outputId": "07888c21-c243-4d61-aaa8-dc8f59321ef2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+----------------------------+----------------------------+\n",
      "|entity|begin|end|result                      |matched                     |\n",
      "+------+-----+---+----------------------------+----------------------------+\n",
      "|entity|69   |99 |evaluation psychiatric state|evaluation psychiatric state|\n",
      "|entity|52   |60 |stressors                   |stressor                    |\n",
      "+------+-----+---+----------------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"matched_text\") \\\n",
    "    .setExplodeSelectedFields({\n",
    "        \"matched_text\": [\n",
    "            \"metadata.entity as entity\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            \"metadata.original_or_matched as matched\"\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq4LhrTDq-BO"
   },
   "source": [
    "### üîÑ `returnChunks`\n",
    "\n",
    "The **`returnChunks`** parameter controls the format of the matched phrases returned by the annotator.\n",
    "\n",
    "You can set it to either:\n",
    "\n",
    "- `\"original\"` ‚Äì Returns the chunk exactly as it appears in the input text.\n",
    "- `\"matched\"` ‚Äì Returns the normalized version of the matched phrase (e.g., after stemming or lemmatization).\n",
    "\n",
    "This setting is especially useful when using normalization techniques like stemming or lemmatization and you want to analyze which version of the entity was responsible for the match.\n",
    "\n",
    "#### üìå Important Note:\n",
    "Even when `returnChunks` is set to `\"matched\"`, the **`begin`** and **`end`** indices in the resulting `CHUNK` annotation still refer to the **original text**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1758950998309,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "GjXzOdIKrRO_"
   },
   "outputs": [],
   "source": [
    "test_phrases = \"\"\"\n",
    "stressor\n",
    "evaluation psychiatric state\n",
    "difficulty sleep\n",
    "\"\"\"\n",
    "\n",
    "with open(\"test-phrases.txt\", \"w\") as file:\n",
    "    file.write(test_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 1779,
     "status": "ok",
     "timestamp": 1758951002644,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "QDKvutMYprWR"
   },
   "outputs": [],
   "source": [
    "text_matcher = TextMatcherInternal()\\\n",
    "    .setInputCols([\"sentence\",\"token\"])\\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setEntities(\"./test-phrases.txt\")\\\n",
    "    .setEnableLemmatizer(True) \\\n",
    "    .setEnableStemmer(True) \\\n",
    "    .setCleanStopWords(True) \\\n",
    "    .setBuildFromTokens(True)\\\n",
    "    .setReturnChunks(\"matched\")\\\n",
    "\n",
    "text_matcher_pipeline= Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        text_matcher\n",
    "])\n",
    "\n",
    "text_matcher_model = text_matcher_pipeline.fit(empty_data)\n",
    "text_matcher_result_df = text_matcher_model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2161,
     "status": "ok",
     "timestamp": 1758951007785,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "9t8s31--qyJf",
    "outputId": "f77152b5-e0d3-4282-a167-ae015bb502a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+----------------------------+----------------------------+\n",
      "|entity|begin|end|result                      |original                    |\n",
      "+------+-----+---+----------------------------+----------------------------+\n",
      "|entity|69   |99 |evaluation psychiatric state|evaluation psychiatric state|\n",
      "|entity|52   |60 |stressor                    |stressors                   |\n",
      "|entity|114  |132|difficulty sleep            |difficulty sleeping         |\n",
      "+------+-----+---+----------------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"matched_text\") \\\n",
    "    .setExplodeSelectedFields(\n",
    "        {\"matched_text\": [\n",
    "            \"metadata.entity as entity\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            \"metadata.original_or_matched as original\"\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyVe2Fq4DHEl"
   },
   "source": [
    "### üö´ `excludeRegexPatterns`\n",
    "\n",
    "The **`excludeRegexPatterns`** parameter allows you to filter out matched chunks based on **regular expression rules**.\n",
    "\n",
    "You can provide a list of regex patterns:\n",
    "\n",
    "- If a matched chunk **matches any of these patterns**, it will be **excluded** from the output.\n",
    "- If the list is empty (default), **no matches will be filtered**.\n",
    "\n",
    "This is especially useful when you want to remove:\n",
    "\n",
    "- noisy or overly generic matches  \n",
    "- specific codes or token formats  \n",
    "- matches that follow certain undesirable patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 115,
     "status": "ok",
     "timestamp": 1758951012604,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "91GB6sBnECTr"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"APNEA:\n",
    "Presumed apnea of prematurity since < 34 wks gestation at birth.\n",
    "HYPERBILIRUBINEMIA: At risk for hyperbilirubinemia d/t prematurity.\n",
    "1/25-1/30: Received Amp/Gent while undergoing sepsis evaluation.\"\"\"\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "data = spark.createDataFrame([[text]]).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7876,
     "status": "ok",
     "timestamp": 1758951021536,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "wT-yj8lLEB-4",
    "outputId": "e9e5eef4-ce28-43ca-d2cd-b509235affa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hpo_matcher download started this may take some time.\n",
      "Approximate size to download 2 MB\n",
      "\r[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/tmp/ipython-input-1545077470.py:4: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  .setExcludeRegexPatterns([\"^[A-Z][A-Z\\s\\-0-9]{2,}$\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "text_matcher = TextMatcherInternalModel.pretrained(\"hpo_matcher\",\"en\",\"clinical/models\")\\\n",
    "    .setInputCols([\"sentence\",\"token\"])\\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setExcludeRegexPatterns([\"^[A-Z][A-Z\\s\\-0-9]{2,}$\"])\n",
    "\n",
    "text_matcher_pipeline= Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        text_matcher\n",
    "])\n",
    "\n",
    "text_matcher_model = text_matcher_pipeline.fit(empty_data)\n",
    "text_matcher_result_df = text_matcher_model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2555,
     "status": "ok",
     "timestamp": 1758951026517,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "ei7sxyjiEB8D",
    "outputId": "7244c4ff-41a2-49fd-b13a-ae59b77feb2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+--------------------+\n",
      "|entity|begin|end|result              |\n",
      "+------+-----+---+--------------------+\n",
      "|HPO   |16   |20 |apnea               |\n",
      "|HPO   |16   |35 |apnea of prematurity|\n",
      "|HPO   |104  |121|hyperbilirubinemia  |\n",
      "|HPO   |186  |191|sepsis              |\n",
      "+------+-----+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"matched_text\") \\\n",
    "    .setExplodeSelectedFields({\n",
    "        \"matched_text\": [\n",
    "            \"metadata.entity as entity\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbPJ04wor7gW"
   },
   "source": [
    "#Usage of Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 101,
     "status": "ok",
     "timestamp": 1758951029939,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "6seZz117VaKp"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"APNEA: Presumed apnea of prematurity since < 34 wks gestation at birth.\n",
    "HYPERBILIRUBINEMIA: At risk for hyperbilirubinemia d/t prematurity.\n",
    "1/25-1/30: Received Amp/Gent while undergoing sepsis evaluation.\"\"\"\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "data = spark.createDataFrame([[text]]).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2870,
     "status": "ok",
     "timestamp": 1758951034793,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "fLVWpZrOe_o7",
    "outputId": "98cd567e-6a20-4654-8b4b-5e49f03e5353"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hpo_matcher download started this may take some time.\n",
      "Approximate size to download 2 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "text_matcher = TextMatcherInternalModel.pretrained(\"hpo_matcher\",\"en\",\"clinical/models\")\\\n",
    "    .setInputCols([\"sentence\",\"token\"])\\\n",
    "    .setOutputCol(\"matched_text\")\n",
    "\n",
    "\n",
    "text_matcher_pipeline= Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        text_matcher\n",
    "])\n",
    "\n",
    "text_matcher_model = text_matcher_pipeline.fit(empty_data)\n",
    "text_matcher_result_df = text_matcher_model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1866,
     "status": "ok",
     "timestamp": 1758951039675,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "M-ECGw6OXTvf",
    "outputId": "9b35191d-fdc5-4896-cf5a-de01e53db621"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+--------------------+\n",
      "|entity|begin|end|result              |\n",
      "+------+-----+---+--------------------+\n",
      "|HPO   |16   |20 |apnea               |\n",
      "|HPO   |16   |35 |apnea of prematurity|\n",
      "|HPO   |104  |121|hyperbilirubinemia  |\n",
      "|HPO   |186  |191|sepsis              |\n",
      "+------+-----+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"matched_text\") \\\n",
    "    .setExplodeSelectedFields({\n",
    "        \"matched_text\": [\n",
    "            \"metadata.entity as entity\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fC2oxHfqNlKV"
   },
   "source": [
    "# Usage TextMatcherInternal with MetadataAnnotationConverter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E20F_L08rRcJ"
   },
   "source": [
    "In our pipeline, TextMatcher uses a stopword-reduced version of phrases to improve matching flexibility (e.g., matching denies pain to she denies having any pain).\n",
    "\n",
    "However, once a match is found, the original (non-reduced) context is passed to the assertion model.\n",
    "\n",
    "üëâ Therefore, assertion labeling is not impacted by stopword removal during matching.\n",
    "Assertion cues like no, not, denies, and temporal markers are still present in the input the model sees.\n",
    "\n",
    "‚úÖ Matching is flexible.  \n",
    "‚úÖ Assertion is accurate.  \n",
    "üõ°Ô∏è Each step uses what it needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "executionInfo": {
     "elapsed": 139,
     "status": "ok",
     "timestamp": 1758951044164,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "EyKh7UJne0e3"
   },
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "The patient reports no pain in the chest and denies any history of hypertension.\n",
    "No signs of infection or fever were noted. No signs of nausea or vomiting were observed.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "data = spark.createDataFrame([[text]]).toDF(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1758951046074,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "kas0j9kPeWma"
   },
   "outputs": [],
   "source": [
    "test_phrases = \"\"\"\n",
    "pain\n",
    "hypertension\n",
    "medication\n",
    "fever\n",
    "vomit\n",
    "\"\"\"\n",
    "\n",
    "with open(\"test-phrases.txt\", \"w\") as file:\n",
    "    file.write(test_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94728,
     "status": "ok",
     "timestamp": 1758951142985,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "Un-LKalMNque",
    "outputId": "27642f28-931e-4feb-f9be-e2d0ec8a04fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_detector_dl_healthcare download started this may take some time.\n",
      "Approximate size to download 367.3 KB\n",
      "[OK!]\n",
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n",
      "assertion_dl download started this may take some time.\n",
      "Approximate size to download 1.3 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = SentenceDetectorDLModel\\\n",
    "    .pretrained(\"sentence_detector_dl_healthcare\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols([\"sentence\"])\\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "text_matcher = TextMatcherInternal()\\\n",
    "    .setInputCols([\"sentence\",\"token\"])\\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setEntities(\"./test-phrases.txt\")\\\n",
    "    .setEnableLemmatizer(True) \\\n",
    "    .setEnableStemmer(True) \\\n",
    "    .setCleanStopWords(True) \\\n",
    "    .setBuildFromTokens(False)\\\n",
    "    .setReturnChunks(\"original\")\\\n",
    "\n",
    "metadata_annotation_converter = MetadataAnnotationConverter()\\\n",
    "    .setInputCols(\"matched_text\")\\\n",
    "    .setInputType(\"chunk\") \\\n",
    "    .setBeginField(\"begin\") \\\n",
    "    .setEndField(\"end\") \\\n",
    "    .setResultField(\"original_or_matched\") \\\n",
    "    .setOutputCol(\"new_chunk\")\n",
    "\n",
    "word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols([\"sentence\", \"token\"])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "clinical_assertion_dl = AssertionDLModel.pretrained(\"assertion_dl\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentence\", \"new_chunk\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"assertion_dl\")\\\n",
    "\n",
    "\n",
    "text_matcher_pipeline= Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        word_embeddings,\n",
    "        text_matcher,\n",
    "        metadata_annotation_converter,\n",
    "        clinical_assertion_dl\n",
    "])\n",
    "\n",
    "text_matcher_model = text_matcher_pipeline.fit(empty_data)\n",
    "text_matcher_result_df = text_matcher_model.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2028,
     "status": "ok",
     "timestamp": 1758951149941,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "fTLsDDBxe6Ct",
    "outputId": "61239e2b-1674-440b-acba-8c079faa7de3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+------------+------------+\n",
      "|entity|begin|end|result      |matched     |\n",
      "+------+-----+---+------------+------------+\n",
      "|entity|24   |27 |pain        |pain        |\n",
      "|entity|68   |79 |hypertension|hypertension|\n",
      "|entity|107  |111|fever       |fever       |\n",
      "|entity|147  |154|vomiting    |vomit       |\n",
      "+------+-----+---+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"matched_text\") \\\n",
    "    .setExplodeSelectedFields({\n",
    "        \"matched_text\": [\n",
    "            \"metadata.entity as entity\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            \"metadata.original_or_matched as matched\"\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2281,
     "status": "ok",
     "timestamp": 1758951155229,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "pORSHGsMO1tm",
    "outputId": "f49dd38c-71e3-42ee-80b7-84f26b28eb84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---+------------+\n",
      "|entity|begin|end|result      |\n",
      "+------+-----+---+------------+\n",
      "|entity|24   |27 |pain        |\n",
      "|entity|68   |79 |hypertension|\n",
      "|entity|107  |111|fever       |\n",
      "|entity|147  |154|vomit       |\n",
      "+------+-----+---+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"new_chunk\") \\\n",
    "    .setExplodeSelectedFields({\n",
    "        \"new_chunk\": [\n",
    "            \"metadata.entity as entity\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3985,
     "status": "ok",
     "timestamp": 1758951163164,
     "user": {
      "displayName": "Mehmet Daƒü",
      "userId": "14052875917891496135"
     },
     "user_tz": -180
    },
    "id": "n6V3Kh5pCJCx",
    "outputId": "ac54f89c-0084-499b-8581-3b63d0d06c18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+---+------+----------+\n",
      "|chunk       |begin|end|result|confidence|\n",
      "+------------+-----+---+------+----------+\n",
      "|pain        |24   |27 |absent|0.9921    |\n",
      "|hypertension|68   |79 |absent|0.9998    |\n",
      "|fever       |107  |111|absent|0.9999    |\n",
      "|vomit       |147  |154|absent|1.0       |\n",
      "+------------+-----+---+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"assertion_dl\") \\\n",
    "    .setExplodeSelectedFields({\n",
    "        \"assertion_dl\": [\n",
    "            \"metadata.ner_chunk as chunk\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            \"metadata.confidence as confidence\"\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
