{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ],
   "metadata": {
    "id": "91Ih6rLKTq7h"
   }
  },
  {
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/40.1.Text_Matcher_Internal.ipynb)",
   "metadata": {
    "id": "EQsEwwyrTwAh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# üìúTextMatcherInternal"
   ],
   "metadata": {
    "id": "T-0OhC7jTznW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "üîç What is TextMatcherInternal?\n",
    "TextMatcherInternal is an annotator in Spark NLP used to detect exact phrase matches within a document, based on a predefined list of entity phrases.\n",
    "\n",
    "This component performs token-level exact matching between the input text and a set of phrases provided by the user, typically through an external text file. When a match is found, it creates a CHUNK annotation containing the matched text span.\n",
    "\n",
    "‚úÖ When to Use It\n",
    "TextMatcherInternal is particularly useful in scenarios such as:\n",
    "\n",
    "Detecting predefined terms in clinical, legal, or domain-specific documents\n",
    "\n",
    "Keyword-based filtering or rule-based entity recognition\n",
    "\n",
    "Pre-labeling data for supervised training or rule-based NER systems\n",
    "\n",
    "\n",
    "**üîó Helpful Links:**\n",
    "\n",
    "For extended examples of usage, see the [Spark NLP Workshop Repository](https://github.com/JohnSnowLabs/spark-nlp-workshop/tree/master/healthcare-nlp)\n",
    "\n",
    "Python Documentation: [TextMatcher](https://sparknlp.org/api/python/reference/autosummary/sparknlp/annotator/matcher/text_matcher/index.html#sparknlp.annotator.matcher.text_matcher.TextMatcher)\n",
    "\n",
    "Scala Documentation: [TextMatcher](https://sparknlp.org/api/com/johnsnowlabs/nlp/annotators/TextMatcher)\n"
   ],
   "metadata": {
    "id": "97fnzEt3T5CS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## **üñ®Ô∏è Input/Output Annotation Types**"
   ],
   "metadata": {
    "id": "qg3slXBwVAEF"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Input: `SENTECE`, `TOKEN`\n",
    "\n",
    "- Output: `CHUNK`"
   ],
   "metadata": {
    "id": "DM370suhVGn4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **üîé Parameters**"
   ],
   "metadata": {
    "id": "jiePCiPtVNVg"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TextMatcherInternal Parameters\n",
    "\n",
    "| **Parameter** | **Description** |\n",
    "| --- | --- |\n",
    "| **entities** | ExternalResource for entities. A text file of predefined phrases must be provided. |\n",
    "| **caseSensitive** | Whether to match regardless of case. Defaults to `True`. |\n",
    "| **mergeOverlapping** | Whether to merge overlapping matched chunks. Defaults to `False`. |\n",
    "| **entityValue** | Value for the entity metadata field. |\n",
    "| **buildFromTokens** | Whether the matcher should take the CHUNK from TOKEN. |\n",
    "| **dictionary** | External dictionary for lemmatizer. |\n",
    "| **enableLemmatizer** | Whether to enable lemmatizer. Defaults to `False`. |\n",
    "| **enableStemmer** | Whether to enable stemmer. Defaults to `False`. |\n",
    "| **stopWords** | List of stop words to be removed. Defaults to `None`. |\n",
    "| **cleanStopWords** | Whether to clean stop words. Defaults to `False`. |\n",
    "| **shuffleEntitySubTokens** | Whether to generate and use variations (permutations) of the entity phrases. Defaults to `False`. |\n",
    "| **safeKeywords** | Keywords to preserve during stopword removal when `cleanStopWords` is enabled. Defaults to empty. |\n",
    "| **excludePunctuation** | If `True`, punctuation will be removed from the text. Defaults to `True`. |\n",
    "| **cleanKeywords** | Additional keywords to be removed alongside default stopwords. Defaults to empty. |\n",
    "| **excludeRegexPatterns** | Regex patterns used to drop matched chunks. Defaults to empty. |\n",
    "| **returnChunks** | Controls whether to return the original text chunks from input or the matched (e.g., stemmed/lemmatized) phrases. Can be `'original'` or `'matched'`. Defaults to `'original'`. |\n",
    "| **skipMatcherAugmentation** | Whether to skip matcher augmentation. Defaults to `False`. |\n",
    "| **skipSourceTextAugmentation** | Whether to skip source text augmentation. Defaults to `False`. |\n",
    "\n",
    "---\n",
    "\n",
    "## TextMatcherInternalModel Parameters\n",
    "\n",
    "| **Parameter** | **Description** |\n",
    "| --- | --- |\n",
    "| **mergeOverlapping** | Whether to merge overlapping matched chunks. Defaults to `False`. |\n",
    "| **entityValue** | Value for the entity metadata field. |\n",
    "| **buildFromTokens** | Whether the matcher should take the CHUNK from TOKEN. |\n",
    "| **enableLemmatizer** | Whether to enable lemmatizer. Defaults to `False`. |\n",
    "| **enableStemmer** | Whether to enable stemmer. Defaults to `False`. |\n",
    "| **stopWords** | List of stop words to be removed. Defaults to `None`. |\n",
    "| **cleanStopWords** | Whether to clean stop words. Defaults to `False`. |\n",
    "| **returnChunks** | Whether to return original chunks or matched chunks. Defaults to original chunks. |\n",
    "| **safeKeywords** | Keywords to preserve during stopword removal when `cleanStopWords` is enabled. Defaults to empty. |\n",
    "| **excludePunctuation** | If `True`, punctuation will be removed from the text. Defaults to `True`. |\n",
    "| **cleanKeywords** | Additional keywords to be removed alongside default stopwords. Defaults to empty. |\n",
    "| **excludeRegexPatterns** | Regex patterns used to drop matched chunks. Defaults to empty. |\n"
   ],
   "metadata": {
    "id": "bfk75w8gVQIE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **üé¨ Colab Setup**"
   ],
   "metadata": {
    "id": "hpBEf4WUU1VX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "hHWr26lLWpoY",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "outputId": "d063b2a2-8c50-40b8-8dac-7443b6d7a11f"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-33503615-9200-45c7-a275-615361784465\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-33503615-9200-45c7-a275-615361784465\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Saving text_matcher.json to text_matcher.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "from google.colab import files\n",
    "\n",
    "if 'spark_jsl.json' not in os.listdir():\n",
    "  license_keys = files.upload()\n",
    "  os.rename(list(license_keys.keys())[0], 'spark_jsl.json')\n",
    "\n",
    "with open('spark_jsl.json') as f:\n",
    "    license_keys = json.load(f)\n",
    "\n",
    "# Defining license key-value pairs as local variables\n",
    "locals().update(license_keys)\n",
    "os.environ.update(license_keys)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Installing pyspark and spark-nlp\n",
    "! pip install --upgrade -q pyspark==3.5.1  spark-nlp==$PUBLIC_VERSION\n",
    "\n",
    "# Installing Spark NLP Healthcare\n",
    "! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET\n",
    "\n",
    "# Installing Spark NLP Display Library for visualization\n",
    "! pip install --upgrade -q spark-nlp-display"
   ],
   "metadata": {
    "id": "kODrwKTpXDKB",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7e5b07af-ca03-45bb-8bdb-bd984f22f964"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[2K   \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m709.9/709.9 kB\u001B[0m \u001B[31m10.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K     \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m549.5/549.5 kB\u001B[0m \u001B[31m6.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m95.6/95.6 kB\u001B[0m \u001B[31m4.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m66.9/66.9 kB\u001B[0m \u001B[31m4.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[2K   \u001B[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001B[0m \u001B[32m1.6/1.6 MB\u001B[0m \u001B[31m27.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n",
      "\u001B[?25h"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import sparknlp\n",
    "import sparknlp_jsl\n",
    "\n",
    "from sparknlp.base import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.training import *\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "from sparknlp_jsl.base import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "\n",
    "params = {\n",
    "    \"spark.driver.memory\":\"50G\",\n",
    "    \"spark.driver.maxResultSize\":\"5G\",\n",
    "}\n",
    "\n",
    "spark = sparknlp_jsl.start(license_keys['SECRET'],params=params)\n",
    "print(\"Spark NLP Version :\", sparknlp.version())\n",
    "print(\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
    "\n",
    "spark"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "4a3gs7_xXFfQ",
    "outputId": "08ebe12a-3472-4d00-e5be-20dc68e1ec81"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Spark NLP Version : 6.0.2\n",
      "Spark NLP_JSL Version : 6.0.2\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5c46e66390>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://6887b29fce18:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP Licensed</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Basic Usage of `TextMatcherInternal`"
   ],
   "metadata": {
    "id": "Px2CNvOWXf9i"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How to Use `TextMatcherInternal`"
   ],
   "metadata": {
    "id": "84Av5fSaY5oO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First of all, we should create a source file that includes all the chunks or tokens we need to capture. In the example below, we use `#` as a delimiter to separate the label and entity. So we need to set parameter like this `setDelimiter('#')`."
   ],
   "metadata": {
    "id": "QkxDEfHBXjm9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "matcher_drug = \"\"\"\n",
    "Aspirin 100mg#Drug\n",
    "aspirin#Drug\n",
    "paracetamol#Drug\n",
    "amoxicillin#Drug\n",
    "ibuprofen#Drug\n",
    "lansoprazole#Drug\n",
    "\"\"\"\n",
    "\n",
    "with open ('matcher_drug.csv', 'w') as f:\n",
    "  f.write(matcher_drug)"
   ],
   "metadata": {
    "id": "VssVIW6yXpCr"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "entityExtractor = TextMatcherInternal()\\\n",
    "    .setInputCols([\"document\", \"token\"])\\\n",
    "    .setEntities(\"matcher_drug.csv\")\\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setCaseSensitive(False)\\\n",
    "    .setDelimiter(\"#\")\\\n",
    "    .setMergeOverlapping(False)\n",
    "\n",
    "mathcer_pipeline = Pipeline(\n",
    "    stages=[\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        entityExtractor\n",
    "])\n",
    "\n",
    "text = \"\"\"\n",
    "John's doctor prescribed aspirin 100mg for his heart condition, along with paracetamol for his fever,\n",
    "amoxicillin for his tonsilitis, ibuprofen for his inflammation, and lansoprazole for his GORD.\n",
    "\"\"\"\n",
    "\n",
    "data = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "\n",
    "result = mathcer_pipeline.fit(data).transform(data)"
   ],
   "metadata": {
    "id": "dVKqPa7OXprV"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "result.select(F.explode(F.arrays_zip(result.matched_text.result,\n",
    "                                     result.matched_text.begin,\n",
    "                                     result.matched_text.end,\n",
    "                                     result.matched_text.metadata)).alias(\"cols\"))\\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
    "              F.expr(\"cols['1']\").alias(\"begin\"),\n",
    "              F.expr(\"cols['2']\").alias(\"end\"),\n",
    "              F.expr(\"cols['3']['entity']\").alias('label')).show(truncate=70)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JqUj8W9AXpod",
    "outputId": "a144a003-015c-42dd-db20-94bf32860343"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+-----+---+-----+\n",
      "|        chunk|begin|end|label|\n",
      "+-------------+-----+---+-----+\n",
      "|aspirin 100mg|   26| 38| Drug|\n",
      "|  amoxicillin|  103|113| Drug|\n",
      "| lansoprazole|  171|182| Drug|\n",
      "|  paracetamol|   76| 86| Drug|\n",
      "|      aspirin|   26| 32| Drug|\n",
      "|    ibuprofen|  135|143| Drug|\n",
      "+-------------+-----+---+-----+\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "*As* you see above mather_drug file includes 2 similar entities aspirin and aspirin 100mg and our text includes both of them So if you want to see both of them you need to set `MergeOverlapping` parameter as `False`. You can look at the below example."
   ],
   "metadata": {
    "id": "KfqXkcy2XxWm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "entityExtractor = TextMatcherInternal()\\\n",
    "    .setInputCols([\"document\", \"token\"])\\\n",
    "    .setEntities(\"matcher_drug.csv\") \\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setDelimiter(\"#\")\\\n",
    "    .setCaseSensitive(False)\\\n",
    "    .setMergeOverlapping(False)\n",
    "\n",
    "mathcer_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        entityExtractor\n",
    "])\n",
    "\n",
    "result = mathcer_pipeline.fit(data).transform(data)"
   ],
   "metadata": {
    "id": "oqUtw9WNX0r2"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "result.select(F.explode(F.arrays_zip(result.matched_text.result,\n",
    "                                     result.matched_text.begin,\n",
    "                                     result.matched_text.end,\n",
    "                                     result.matched_text.metadata)).alias(\"cols\"))\\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
    "              F.expr(\"cols['1']\").alias(\"begin\"),\n",
    "              F.expr(\"cols['2']\").alias(\"end\"),\n",
    "              F.expr(\"cols['3']['entity']\").alias('label')).show(truncate=70)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-Co2PuCX1SM",
    "outputId": "2fbfaa61-53b6-4fff-b489-01835f1e79d7"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+-----+---+-----+\n",
      "|        chunk|begin|end|label|\n",
      "+-------------+-----+---+-----+\n",
      "|aspirin 100mg|   26| 38| Drug|\n",
      "|  amoxicillin|  103|113| Drug|\n",
      "| lansoprazole|  171|182| Drug|\n",
      "|  paracetamol|   76| 86| Drug|\n",
      "|      aspirin|   26| 32| Drug|\n",
      "|    ibuprofen|  135|143| Drug|\n",
      "+-------------+-----+---+-----+\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "When we set the `CaseSensitive` parameter to `True`, it means we're considering the case sensitivity of chunks in the source file. Consequently, some chunks may not be visible due to differences in their case compared to the source file."
   ],
   "metadata": {
    "id": "Jx_EpYq2X5yg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "entityExtractor = TextMatcherInternal()\\\n",
    "    .setInputCols([\"document\", \"token\"])\\\n",
    "    .setEntities(\"matcher_drug.csv\") \\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setDelimiter(\"#\")\\\n",
    "    .setCaseSensitive(True)\\\n",
    "    .setMergeOverlapping(False)\n",
    "\n",
    "mathcer_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        entityExtractor\n",
    "])\n",
    "\n",
    "matcher_model = mathcer_pipeline.fit(data)\n",
    "result = matcher_model.transform(data)"
   ],
   "metadata": {
    "id": "Tz8QWAYRX1Pj"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "result.select(F.explode(F.arrays_zip(result.matched_text.result,\n",
    "                                     result.matched_text.begin,\n",
    "                                     result.matched_text.end,\n",
    "                                     result.matched_text.metadata)).alias(\"cols\"))\\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
    "              F.expr(\"cols['1']\").alias(\"begin\"),\n",
    "              F.expr(\"cols['2']\").alias(\"end\"),\n",
    "              F.expr(\"cols['3']['entity']\").alias('label')).show(truncate=70)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lfc93ehTX1Mq",
    "outputId": "8b131a9d-0ae9-4537-9b6f-1dbd34209ff0"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------+-----+---+-----+\n",
      "|       chunk|begin|end|label|\n",
      "+------------+-----+---+-----+\n",
      "| amoxicillin|  103|113| Drug|\n",
      "|lansoprazole|  171|182| Drug|\n",
      "| paracetamol|   76| 86| Drug|\n",
      "|     aspirin|   26| 32| Drug|\n",
      "|   ibuprofen|  135|143| Drug|\n",
      "+------------+-----+---+-----+\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multiple Entities"
   ],
   "metadata": {
    "id": "3BEZrYQaX_26"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "multiple_entites= \"\"\"\n",
    "Aspirin 100mg#Drug\n",
    "paracetamol#Drug\n",
    "amoxicillin#Drug\n",
    "ibuprofen#Drug\n",
    "lansoprazole#Drug\n",
    "fever#Symptom\n",
    "headache#Symptom\n",
    "tonsilitis#Disease\n",
    "GORD#Disease\n",
    "heart condition#Disease\n",
    "\"\"\"\n",
    "\n",
    "with open ('multiple_entities.csv', 'w') as f:\n",
    "  f.write(multiple_entites)"
   ],
   "metadata": {
    "id": "0WPxeWzpYE-E"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"text\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "entityExtractor = TextMatcherInternal() \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setEntities(\"multiple_entities.csv\") \\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setDelimiter(\"#\")\\\n",
    "    .setCaseSensitive(False)\\\n",
    "    .setDelimiter(\"#\")\n",
    "\n",
    "matcher_pipeline = Pipeline().setStages([\n",
    "                  documentAssembler,\n",
    "                  tokenizer,\n",
    "                  entityExtractor])\n",
    "\n",
    "text = \"\"\"\n",
    "John's doctor prescribed aspirin 100mg for his heart condition, along with paracetamol for his fever and headache,\n",
    "amoxicillin for his tonsilitis, ibuprofen for his inflammation, and lansoprazole for his GORD.\n",
    "\"\"\"\n",
    "data = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "\n",
    "matcher_model = matcher_pipeline.fit(data)\n",
    "result = matcher_model.transform(data)"
   ],
   "metadata": {
    "id": "tlodnCiGYF7a"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "result.select(F.explode(F.arrays_zip(result.matched_text.result,\n",
    "                                     result.matched_text.begin,\n",
    "                                     result.matched_text.end,\n",
    "                                     result.matched_text.metadata,)).alias(\"cols\"))\\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
    "              F.expr(\"cols['1']\").alias(\"begin\"),\n",
    "              F.expr(\"cols['2']\").alias(\"end\"),\n",
    "              F.expr(\"cols['3']['entity']\").alias('label')).show(truncate=70)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OA305yW0YF4n",
    "outputId": "4f2882cf-2e6f-42cf-890d-8125ec01dd6b"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------+-----+---+-------+\n",
      "|          chunk|begin|end|  label|\n",
      "+---------------+-----+---+-------+\n",
      "|heart condition|   48| 62|Disease|\n",
      "|     tonsilitis|  136|145|Disease|\n",
      "|           GORD|  205|208|Disease|\n",
      "|          fever|   96|100|Symptom|\n",
      "|       headache|  106|113|Symptom|\n",
      "|  aspirin 100mg|   26| 38|   Drug|\n",
      "|    amoxicillin|  116|126|   Drug|\n",
      "|   lansoprazole|  184|195|   Drug|\n",
      "|    paracetamol|   76| 86|   Drug|\n",
      "|      ibuprofen|  148|156|   Drug|\n",
      "+---------------+-----+---+-------+\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `TextMatcherInternalModel`\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "3Id1ejvlYU2G"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "entityExtractor = TextMatcherInternal() \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setEntities(\"matcher_drug.csv\") \\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setCaseSensitive(False)\\\n",
    "    .setDelimiter(\"#\")\n",
    "\n",
    "matcher_pipeline = Pipeline().setStages([\n",
    "                  documentAssembler,\n",
    "                  tokenizer,\n",
    "                  entityExtractor])\n",
    "\n",
    "text = \"\"\"John's doctor prescribed aspirin 100mg for his heart condition, along with paracetamol for his fever and headache,\n",
    "amoxicillin for his tonsilitis, ibuprofen for his inflammation, and lansoprazole for his GORD.\"\"\"\n",
    "\n",
    "data = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "\n",
    "result = matcher_pipeline.fit(data).transform(data)"
   ],
   "metadata": {
    "id": "OF-9mJuNYF19"
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saving the approach to disk"
   ],
   "metadata": {
    "id": "a6_erUQQYZo4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "matcher_model.stages[-1].write().overwrite().save(\"matcher_model\")"
   ],
   "metadata": {
    "id": "VawxxEzdYFzK"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading the saved model and using it with the `TextMatcherModel()` via `load`."
   ],
   "metadata": {
    "id": "EGHDYXtcX_0E"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "entity_ruler = TextMatcherInternalModel.load('./matcher_model') \\\n",
    "    .setInputCols([\"document\", \"token\"]) \\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages=[\n",
    "        documentAssembler,\n",
    "        tokenizer,\n",
    "        entity_ruler\n",
    "])\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "pipeline_model = pipeline.fit(data)"
   ],
   "metadata": {
    "id": "m3lIoEt4X1Ju"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Checking the result"
   ],
   "metadata": {
    "id": "7N7EBoqMYj2p"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "result = pipeline_model.transform(data)\n",
    "\n",
    "result.select(F.explode(F.arrays_zip(result.matched_text.result,\n",
    "                                     result.matched_text.begin,\n",
    "                                     result.matched_text.end,\n",
    "                                     result.matched_text.metadata)).alias(\"cols\"))\\\n",
    "      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
    "              F.expr(\"cols['1']\").alias(\"begin\"),\n",
    "              F.expr(\"cols['2']\").alias(\"end\"),\n",
    "              F.expr(\"cols['3']['entity']\").alias('label')).show(truncate=70)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xoR8TSzPYjF-",
    "outputId": "4ab7e463-8f93-44cc-ccfd-4ef2f4f276e1"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+---------------+-----+---+-------+\n",
      "|          chunk|begin|end|  label|\n",
      "+---------------+-----+---+-------+\n",
      "|heart condition|   47| 61|Disease|\n",
      "|     tonsilitis|  135|144|Disease|\n",
      "|           GORD|  204|207|Disease|\n",
      "|          fever|   95| 99|Symptom|\n",
      "|       headache|  105|112|Symptom|\n",
      "|  aspirin 100mg|   25| 37|   Drug|\n",
      "|    amoxicillin|  115|125|   Drug|\n",
      "|   lansoprazole|  183|194|   Drug|\n",
      "|    paracetamol|   75| 85|   Drug|\n",
      "|      ibuprofen|  147|155|   Drug|\n",
      "+---------------+-----+---+-------+\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Advanced Usage of TextMatcherInternal"
   ],
   "metadata": {
    "id": "vyvZCqGLZCkS"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this section, we demonstrate advanced features of the `TextMatcherInternal` annotator that enhance its flexibility and robustness in text matching tasks. These features include lemmatization and stemming for matching different word forms, customizable stop word removal with safe keywords preservation, punctuation exclusion, pattern-based chunk exclusion via regex, and the ability to control whether to return original or transformed matched text. We also show how to generate phrase permutations for more comprehensive matching, and options to skip automatic augmentation steps.\n",
    "\n",
    "Consider the following example sentence for illustration:\n",
    "\n",
    "> \"I am living in Canada. They were running happily and talking about their lives. He studies biology and loves animals.\"\n"
   ],
   "metadata": {
    "id": "JMmKStkIaoFB"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Basic Pipeline Components"
   ],
   "metadata": {
    "id": "rOaRIjLedwZE"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "document_assembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = SentenceDetectorDLModel\\\n",
    "    .pretrained(\"sentence_detector_dl_healthcare\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols([\"sentence\"])\\\n",
    "    .setOutputCol(\"token\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4liI4ZZd0NW",
    "outputId": "5167653d-3106-40bd-a4f9-286adcb3ae50"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sentence_detector_dl_healthcare download started this may take some time.\n",
      "Approximate size to download 367.3 KB\n",
      "[OK!]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Example Text"
   ],
   "metadata": {
    "id": "J6y3N2CbZ0zA"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "text = \"\"\"\n",
    "Patient was able to talk briefly about recent life stressors during evaluation of psychiatric state.\n",
    "She reports difficulty sleeping and ongoing anxiety. Denies suicidal ideation.\n",
    "\"\"\"\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "data = spark.createDataFrame([[text]]).toDF(\"text\")"
   ],
   "metadata": {
    "id": "LI07Z7xPXMi_"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üå± Demonstrating Lemmatizer and Stemmer Features\n",
    "\n",
    "To showcase the effects of lemmatization and stemming on text matching, we use a set of example phrases that include different word forms and variations.  \n",
    "\n",
    "For instance, words like **\"talking\"** and **\"lives\"** will be matched to their base forms **\"talk\"** and **\"life\"** respectively when lemmatization is enabled. Similarly, stemming will reduce words to their root stems such as matching **\"running\"** with **\"run\"** or **\"studies\"** with **\"study\"**.  \n",
    "\n",
    "Here is a sample list of phrases saved to a file (`test-phrases.txt`) that we will use for matching against input text to demonstrate these features:\n",
    "\n"
   ],
   "metadata": {
    "id": "Cjlw15CWdVuL"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_phrases = \"\"\"\n",
    "stressor\n",
    "difficulty sleep\n",
    "\"\"\"\n",
    "\n",
    "with open(\"test-phrases.txt\", \"w\") as file:\n",
    "    file.write(test_phrases)"
   ],
   "metadata": {
    "id": "d-pN06Opc6ek"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text_matcher = TextMatcherInternal()\\\n",
    "    .setInputCols([\"sentence\",\"token\"])\\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setEntities(\"./test-phrases.txt\")\\\n",
    "    .setEnableLemmatizer(True) \\\n",
    "    .setEnableStemmer(True) \\\n",
    "    .setCleanStopWords(True) \\\n",
    "    .setBuildFromTokens(True)\\\n",
    "    .setReturnChunks(\"original\")\\\n",
    "    .setExcludePunctuation(True)\n",
    "\n",
    "text_matcher_pipeline= Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        text_matcher\n",
    "])\n",
    "\n",
    "text_matcher_model = text_matcher_pipeline.fit(empty_data)\n",
    "text_matcher_result_df = text_matcher_model.transform(data)"
   ],
   "metadata": {
    "id": "OG54pwaDXP-9"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"matched_text\") \\\n",
    "    .setExplodeSelectedFields({\n",
    "        \"matched_text\": [\n",
    "            \"metadata.entity as entity\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            \"metadata.original_or_matched as matched \"\n",
    "            ]\n",
    "         })\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOLzdL0vXWzN",
    "outputId": "a4e7c11b-e6f2-4a65-d824-80353a2c2d32"
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-----+---+-------------------+----------------+\n",
      "|entity|begin|end|result             |matched         |\n",
      "+------+-----+---+-------------------+----------------+\n",
      "|entity|52   |60 |stressors          |stressor        |\n",
      "|entity|114  |132|difficulty sleeping|difficulty sleep|\n",
      "+------+-----+---+-------------------+----------------+\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üîÅ shuffleEntitySubTokens\n",
    "\n",
    "\n",
    "The **`shuffleEntitySubTokens`** parameter controls whether token-level permutations of entity phrases should be generated and used during matching.\n",
    "\n",
    "When set to `True`, the matcher will automatically generate all possible orderings (permutations) of the tokens within each entity phrase and try to match them against the input text.\n",
    "\n",
    "For example, if `\"sleep difficulty\"` is in your entity list:\n",
    "- With `shuffleEntitySubTokens=True`, phrases like `\"difficulty sleep\"` will also match.\n",
    "- With `shuffleEntitySubTokens=False`, only the exact phrase `\"sleep difficulty\"` will match.\n",
    "\n",
    "> ‚ö†Ô∏è **Note:** This parameter is only supported in the `TextMatcherInternal` class (the training/approach component). It is **not available** in the `TextMatcherInternalModel` class (the trained model component)."
   ],
   "metadata": {
    "id": "EhedYPzzqVD0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_phrases = \"\"\"\n",
    "suicidal deny\n",
    "sleep difficulty\n",
    "\"\"\"\n",
    "\n",
    "with open(\"test-phrases.txt\", \"w\") as file:\n",
    "    file.write(test_phrases)"
   ],
   "metadata": {
    "id": "jLRO4cB5qin1"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text_matcher = TextMatcherInternal()\\\n",
    "    .setInputCols([\"sentence\",\"token\"])\\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setEntities(\"./test-phrases.txt\")\\\n",
    "    .setEnableLemmatizer(True) \\\n",
    "    .setEnableStemmer(True) \\\n",
    "    .setCleanStopWords(True) \\\n",
    "    .setShuffleEntitySubTokens(True)\\\n",
    "    .setBuildFromTokens(True)\\\n",
    "    .setReturnChunks(\"original\")\\\n",
    "\n",
    "text_matcher_pipeline= Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        text_matcher\n",
    "])\n",
    "\n",
    "text_matcher_model = text_matcher_pipeline.fit(empty_data)\n",
    "text_matcher_result_df = text_matcher_model.transform(data)"
   ],
   "metadata": {
    "id": "yGnFosbhqU19"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"matched_text\") \\\n",
    "    .setExplodeSelectedFields({\n",
    "        \"matched_text\": [\n",
    "            \"metadata.entity as entity\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            \"metadata.original_or_matched as matched\"\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zFoMMWRlqUzA",
    "outputId": "a1444319-f09c-4b0c-cde8-07d5f2b7d47b"
   },
   "execution_count": 25,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-----+---+-------------------+----------------+\n",
      "|entity|begin|end|result             |matched         |\n",
      "+------+-----+---+-------------------+----------------+\n",
      "|entity|114  |132|difficulty sleeping|difficulty sleep|\n",
      "|entity|155  |169|Denies suicidal    |deni suicidal   |\n",
      "+------+-----+---+-------------------+----------------+\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üõë Demonstrating Stop Word Removal\n",
    "\n",
    "In this section, we demonstrate how removing stop words from both the **input text** and the **entity phrases** can significantly improve the accuracy and flexibility of phrase matching.\n",
    "\n",
    "Stop words are common words such as **\"and\"**, **\"the\"**, or **\"about\"** that typically carry limited semantic meaning. These words can interfere with matching by creating unnecessary mismatches, especially when your entity phrases are clean and concise.\n",
    "\n",
    "#### ‚öôÔ∏è How it works:\n",
    "- By enabling `.setCleanStopWords(True)`, the matcher will activate stop word filtering.\n",
    "- If you do not specify a custom list using `.setStopWords([...])`, it defaults to **Spark ML‚Äôs built-in English stop word list**.\n",
    "- These stop words will be removed from **both the source text** and the **entity phrases** before matching.\n",
    "- To preserve specific terms that appear in the stop word list, you can use `.setSafeKeywords([...])`.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "tEgpVjspt5E5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_phrases = \"\"\"\n",
    "evaluation psychiatric state\n",
    "\"\"\"\n",
    "\n",
    "with open(\"test-phrases.txt\", \"w\") as file:\n",
    "    file.write(test_phrases)"
   ],
   "metadata": {
    "id": "BeOOV3xUmW8h"
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text_matcher = TextMatcherInternal()\\\n",
    "    .setInputCols([\"sentence\",\"token\"])\\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setEntities(\"./test-phrases.txt\")\\\n",
    "    .setEnableLemmatizer(True) \\\n",
    "    .setEnableStemmer(True) \\\n",
    "    .setCleanStopWords(True) \\\n",
    "\n",
    "text_matcher_pipeline= Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        text_matcher\n",
    "])\n",
    "\n",
    "text_matcher_model = text_matcher_pipeline.fit(empty_data)\n",
    "text_matcher_result_df = text_matcher_model.transform(data)"
   ],
   "metadata": {
    "id": "15MgDXlMex4n"
   },
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"matched_text\") \\\n",
    "    .setExplodeSelectedFields({\n",
    "        \"matched_text\": [\n",
    "            \"metadata.entity as entity\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            \"metadata.original_or_matched as matched\"\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ],
   "metadata": {
    "id": "6VUf9WI-e0D5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "27ce8590-bcac-41e9-fcee-ebe67eaa4dd3"
   },
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-----+---+-------------------------------+----------------------------+\n",
      "|entity|begin|end|result                         |matched                     |\n",
      "+------+-----+---+-------------------------------+----------------------------+\n",
      "|entity|69   |99 |evaluation of psychiatric state|evaluation psychiatric state|\n",
      "+------+-----+---+-------------------------------+----------------------------+\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "###  üßπ Customizing Text Cleaning with Stop Words, Safe Keywords, and Punctuation Settings\n",
    "\n",
    "\n",
    "The `TextMatcherInternal` annotator provides flexible options to control how the input text is cleaned before matching. This allows users to fine-tune what content should be removed or preserved for optimal matching accuracy. The following parameters are especially important:\n",
    "\n",
    "- **`setStopWords`**: Defines a custom list of stop words to be removed from both the input text and the entity phrases. These words are excluded before any matching is performed.\n",
    "\n",
    "- **`cleanKeywords`**: Specifies additional words (beyond the stop words) that should also be removed. Useful for removing domain-specific noise words.\n",
    "\n",
    "- **`safeKeywords`**: Lists important keywords that should **not** be removed even if they appear in the stop word list. This ensures that key terms are preserved during cleaning.\n",
    "\n",
    "- **`excludePunctuation`**: When set to `true`, all punctuation characters (such as `.`, `,`, `!`, etc.) are removed from the text before matching.\n",
    "\n",
    "These options provide granular control over the preprocessing phase, allowing you to reduce noise while preserving meaningful content. This is particularly helpful when dealing with informal, noisy, or user-generated text."
   ],
   "metadata": {
    "id": "1n_LUhB8nMcy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_phrases = \"\"\"\n",
    "evaluation psychiatric state\n",
    "\"\"\"\n",
    "\n",
    "with open(\"test-phrases.txt\", \"w\") as file:\n",
    "    file.write(test_phrases)"
   ],
   "metadata": {
    "id": "iNRO2VcRnx0S"
   },
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text_matcher = TextMatcherInternal()\\\n",
    "    .setInputCols([\"sentence\",\"token\"])\\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setEntities(\"./test-phrases.txt\")\\\n",
    "    .setEnableLemmatizer(True) \\\n",
    "    .setEnableStemmer(True) \\\n",
    "    .setStopWords([\"and\", \"in\"]) \\\n",
    "    .setCleanStopWords(True) \\\n",
    "    .setCleanKeywords([\"of\"]) \\\n",
    "    .setExcludePunctuation(True)\\\n",
    "\n",
    "\n",
    "text_matcher_pipeline= Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        text_matcher\n",
    "    ])\n",
    "\n",
    "text_matcher_model = text_matcher_pipeline.fit(empty_data)\n",
    "text_matcher_result_df = text_matcher_model.transform(data)"
   ],
   "metadata": {
    "id": "shR2sNpJu280"
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"matched_text\") \\\n",
    "    .setExplodeSelectedFields({\n",
    "        \"matched_text\": [\n",
    "            \"metadata.entity as entity\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            \"metadata.original_or_matched as matched\"\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ],
   "metadata": {
    "id": "QE8MzODMu4WD",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "653d8efb-a8ab-426a-b70a-cf13d97d470d"
   },
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-----+---+-------------------------------+----------------------------+\n",
      "|entity|begin|end|result                         |matched                     |\n",
      "+------+-----+---+-------------------------------+----------------------------+\n",
      "|entity|69   |99 |evaluation of psychiatric state|evaluation psychiatric state|\n",
      "+------+-----+---+-------------------------------+----------------------------+\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üîß Matcher & Source Augmentation Settings\n",
    "\n",
    "\n",
    "The parameters **`skipMatcherAugmentation`** and **`skipSourceTextAugmentation`** control whether automatic variations of the phrases (such as token permutations or transformations) are considered during matching. These augmentations are useful to improve match coverage but can be turned off for stricter or faster matching.\n",
    "\n",
    "- **`skipMatcherAugmentation`**  \n",
    "  When set to `True`, disables augmentation of the matcher entities.  \n",
    "  For example, if the phrase is `\"study biology\"`, by default, variations like `\"biology study\"` might also be generated and matched ‚Äî unless this is skipped.\n",
    "\n",
    "- **`skipSourceTextAugmentation`**  \n",
    "  When set to `True`, prevents augmentation (such as reordering tokens or normalizing text) of the **input** source text before matching.  \n",
    "  Useful when you want to match the input exactly as it is written, without any alterations.\n",
    "\n",
    "These parameters give you more control over how flexible or strict the matching process should be.\n"
   ],
   "metadata": {
    "id": "tZkSAITQpoul"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_phrases = \"\"\"\n",
    "stressor\n",
    "evaluation psychiatric state\n",
    "\"\"\"\n",
    "\n",
    "with open(\"test-phrases.txt\", \"w\") as file:\n",
    "    file.write(test_phrases)"
   ],
   "metadata": {
    "id": "MD7kxfKQ7YwF"
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text_matcher = TextMatcherInternal()\\\n",
    "    .setInputCols([\"sentence\",\"token\"])\\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setEntities(\"./test-phrases.txt\")\\\n",
    "    .setEnableLemmatizer(True) \\\n",
    "    .setEnableStemmer(True) \\\n",
    "    .setCleanStopWords(True) \\\n",
    "    .setBuildFromTokens(True)\\\n",
    "    .setReturnChunks(\"original\")\\\n",
    "    .setSkipMatcherAugmentation(True)\\\n",
    "    .setSkipSourceTextAugmentation(False)\n",
    "\n",
    "\n",
    "text_matcher_pipeline= Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        text_matcher\n",
    "    ])\n",
    "\n",
    "text_matcher_model = text_matcher_pipeline.fit(empty_data)\n",
    "text_matcher_result_df = text_matcher_model.transform(data)"
   ],
   "metadata": {
    "id": "vTJ3f7pDprbj"
   },
   "execution_count": 33,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"matched_text\") \\\n",
    "    .setExplodeSelectedFields({\n",
    "        \"matched_text\": [\n",
    "            \"metadata.entity as entity\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            \"metadata.original_or_matched as matched\"\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ByAbTZ1cprY4",
    "outputId": "77211547-eab4-4d11-efff-a8819ef7414b"
   },
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-----+---+----------------------------+----------------------------+\n",
      "|entity|begin|end|result                      |matched                     |\n",
      "+------+-----+---+----------------------------+----------------------------+\n",
      "|entity|69   |99 |evaluation psychiatric state|evaluation psychiatric state|\n",
      "|entity|52   |60 |stressors                   |stressor                    |\n",
      "+------+-----+---+----------------------------+----------------------------+\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### üîÑ `returnChunks`\n",
    "\n",
    "The **`returnChunks`** parameter controls the format of the matched phrases returned by the annotator.\n",
    "\n",
    "You can set it to either:\n",
    "\n",
    "- `\"original\"` ‚Äì Returns the chunk exactly as it appears in the input text.\n",
    "- `\"matched\"` ‚Äì Returns the normalized version of the matched phrase (e.g., after stemming or lemmatization).\n",
    "\n",
    "This setting is especially useful when using normalization techniques like stemming or lemmatization and you want to analyze which version of the entity was responsible for the match.\n",
    "\n",
    "#### üìå Important Note:\n",
    "Even when `returnChunks` is set to `\"matched\"`, the **`begin`** and **`end`** indices in the resulting `CHUNK` annotation still refer to the **original text**."
   ],
   "metadata": {
    "id": "bq4LhrTDq-BO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_phrases = \"\"\"\n",
    "stressor\n",
    "evaluation psychiatric state\n",
    "difficulty sleep\n",
    "\"\"\"\n",
    "\n",
    "with open(\"test-phrases.txt\", \"w\") as file:\n",
    "    file.write(test_phrases)"
   ],
   "metadata": {
    "id": "GjXzOdIKrRO_"
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text_matcher = TextMatcherInternal()\\\n",
    "    .setInputCols([\"sentence\",\"token\"])\\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setEntities(\"./test-phrases.txt\")\\\n",
    "    .setEnableLemmatizer(True) \\\n",
    "    .setEnableStemmer(True) \\\n",
    "    .setCleanStopWords(True) \\\n",
    "    .setBuildFromTokens(True)\\\n",
    "    .setReturnChunks(\"matched\")\\\n",
    "\n",
    "text_matcher_pipeline= Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        text_matcher\n",
    "])\n",
    "\n",
    "text_matcher_model = text_matcher_pipeline.fit(empty_data)\n",
    "text_matcher_result_df = text_matcher_model.transform(data)"
   ],
   "metadata": {
    "id": "QDKvutMYprWR"
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"matched_text\") \\\n",
    "    .setExplodeSelectedFields(\n",
    "        {\"matched_text\": [\n",
    "            \"metadata.entity as entity\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            \"metadata.original_or_matched as original\"\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9t8s31--qyJf",
    "outputId": "8c227278-212a-4818-9010-5a2886157a57"
   },
   "execution_count": 37,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-----+---+----------------------------+----------------------------+\n",
      "|entity|begin|end|result                      |original                    |\n",
      "+------+-----+---+----------------------------+----------------------------+\n",
      "|entity|69   |99 |evaluation psychiatric state|evaluation psychiatric state|\n",
      "|entity|52   |60 |stressor                    |stressors                   |\n",
      "|entity|114  |132|difficulty sleep            |difficulty sleeping         |\n",
      "+------+-----+---+----------------------------+----------------------------+\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Usage of Pretrained Model"
   ],
   "metadata": {
    "id": "xbPJ04wor7gW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "text = \"\"\"APNEA: Presumed apnea of prematurity since < 34 wks gestation at birth.\n",
    "HYPERBILIRUBINEMIA: At risk for hyperbilirubinemia d/t prematurity.\n",
    "1/25-1/30: Received Amp/Gent while undergoing sepsis evaluation.\"\"\"\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "data = spark.createDataFrame([[text]]).toDF(\"text\")"
   ],
   "metadata": {
    "id": "6seZz117VaKp"
   },
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text_matcher = TextMatcherInternalModel.pretrained(\"hpo_matcher\",\"en\",\"clinical/models\")\\\n",
    "    .setInputCols([\"sentence\",\"token\"])\\\n",
    "    .setOutputCol(\"matched_text\")\n",
    "\n",
    "\n",
    "text_matcher_pipeline= Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        text_matcher\n",
    "])\n",
    "\n",
    "text_matcher_model = text_matcher_pipeline.fit(empty_data)\n",
    "text_matcher_result_df = text_matcher_model.transform(data)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fLVWpZrOe_o7",
    "outputId": "3394797d-b8dc-425c-d681-813aff8b388c"
   },
   "execution_count": 39,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "hpo_matcher download started this may take some time.\n",
      "[OK!]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"matched_text\") \\\n",
    "    .setExplodeSelectedFields({\n",
    "        \"matched_text\": [\n",
    "            \"metadata.entity as entity\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M-ECGw6OXTvf",
    "outputId": "43d9cbc3-f25b-4f0c-b524-98b951712329"
   },
   "execution_count": 40,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-----+---+--------------------+\n",
      "|entity|begin|end|result              |\n",
      "+------+-----+---+--------------------+\n",
      "|HPO   |0    |4  |APNEA               |\n",
      "|HPO   |16   |20 |apnea               |\n",
      "|HPO   |16   |35 |apnea of prematurity|\n",
      "|HPO   |72   |89 |HYPERBILIRUBINEMIA  |\n",
      "|HPO   |104  |121|hyperbilirubinemia  |\n",
      "|HPO   |186  |191|sepsis              |\n",
      "+------+-----+---+--------------------+\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Usage TextMatcherInternal with MetadataAnnotationConverter"
   ],
   "metadata": {
    "id": "fC2oxHfqNlKV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "text = \"\"\"\n",
    "Patient was able to talk briefly about recent life stressors during evaluation of psychiatric state.\n",
    "She reports difficulty sleeping and ongoing anxiety. Denies suicidal ideation.\n",
    "\"\"\"\n",
    "\n",
    "empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "data = spark.createDataFrame([[text]]).toDF(\"text\")"
   ],
   "metadata": {
    "id": "EyKh7UJne0e3"
   },
   "execution_count": 46,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_phrases = \"\"\"\n",
    "stressor\n",
    "evaluation psychiatric state\n",
    "difficulty sleep\n",
    "\"\"\"\n",
    "\n",
    "with open(\"test-phrases.txt\", \"w\") as file:\n",
    "    file.write(test_phrases)"
   ],
   "metadata": {
    "id": "kas0j9kPeWma"
   },
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "document_assembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "sentence_detector = SentenceDetectorDLModel\\\n",
    "    .pretrained(\"sentence_detector_dl_healthcare\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"sentence\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols([\"sentence\"])\\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "text_matcher = TextMatcherInternal()\\\n",
    "    .setInputCols([\"sentence\",\"token\"])\\\n",
    "    .setOutputCol(\"matched_text\")\\\n",
    "    .setEntities(\"./test-phrases.txt\")\\\n",
    "    .setEnableLemmatizer(True) \\\n",
    "    .setEnableStemmer(True) \\\n",
    "    .setCleanStopWords(True) \\\n",
    "    .setBuildFromTokens(False)\\\n",
    "    .setReturnChunks(\"original\")\\\n",
    "\n",
    "metadata_annotation_converter = MetadataAnnotationConverter()\\\n",
    "    .setInputCols(\"matched_text\")\\\n",
    "    .setInputType(\"chunk\") \\\n",
    "    .setBeginField(\"begin\") \\\n",
    "    .setEndField(\"end\") \\\n",
    "    .setResultField(\"original_or_matched\") \\\n",
    "    .setOutputCol(\"new_chunk\")\n",
    "\n",
    "word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "    .setInputCols([\"sentence\", \"token\"])\\\n",
    "    .setOutputCol(\"embeddings\")\n",
    "\n",
    "clinical_assertion_dl = AssertionDLModel.pretrained(\"assertion_dl\", \"en\", \"clinical/models\") \\\n",
    "    .setInputCols([\"sentence\", \"new_chunk\", \"embeddings\"]) \\\n",
    "    .setOutputCol(\"assertion_dl\")\\\n",
    "\n",
    "\n",
    "text_matcher_pipeline= Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "        sentence_detector,\n",
    "        tokenizer,\n",
    "        word_embeddings,\n",
    "        text_matcher,\n",
    "        metadata_annotation_converter,\n",
    "        clinical_assertion_dl\n",
    "])\n",
    "\n",
    "text_matcher_model = text_matcher_pipeline.fit(empty_data)\n",
    "text_matcher_result_df = text_matcher_model.transform(data)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Un-LKalMNque",
    "outputId": "69c6fb70-c491-46d7-f660-8f2071abb90e"
   },
   "execution_count": 50,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "sentence_detector_dl_healthcare download started this may take some time.\n",
      "Approximate size to download 367.3 KB\n",
      "[OK!]\n",
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n",
      "assertion_dl download started this may take some time.\n",
      "[OK!]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"matched_text\") \\\n",
    "    .setExplodeSelectedFields({\n",
    "        \"matched_text\": [\n",
    "            \"metadata.entity as entity\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            \"metadata.original_or_matched as matched\"\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTLsDDBxe6Ct",
    "outputId": "e90a41c4-1106-49c6-f60c-d17bb9a267b2"
   },
   "execution_count": 51,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-----+---+-------------------------------+----------------------------+\n",
      "|entity|begin|end|result                         |matched                     |\n",
      "+------+-----+---+-------------------------------+----------------------------+\n",
      "|entity|69   |99 |evaluation of psychiatric state|evaluation psychiatric state|\n",
      "|entity|52   |60 |stressors                      |stressor                    |\n",
      "|entity|114  |132|difficulty sleeping            |difficulty sleep            |\n",
      "+------+-----+---+-------------------------------+----------------------------+\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"new_chunk\") \\\n",
    "    .setExplodeSelectedFields({\n",
    "        \"new_chunk\": [\n",
    "            \"metadata.entity as entity\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pORSHGsMO1tm",
    "outputId": "d06aaf25-8bd7-41bd-e4b5-3f05cfcc57b8"
   },
   "execution_count": 52,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------+-----+---+----------------------------+\n",
      "|entity|begin|end|result                      |\n",
      "+------+-----+---+----------------------------+\n",
      "|entity|69   |99 |evaluation psychiatric state|\n",
      "|entity|52   |60 |stressor                    |\n",
      "|entity|114  |132|difficulty sleep            |\n",
      "+------+-----+---+----------------------------+\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "flattener_text_matcher = Flattener()\\\n",
    "    .setInputCols(\"assertion_dl\") \\\n",
    "    .setExplodeSelectedFields({\n",
    "        \"assertion_dl\": [\n",
    "            \"metadata.ner_chunk as chunk\",\n",
    "            \"begin as begin\",\n",
    "            \"end as end\",\n",
    "            \"result as result\",\n",
    "            \"metadata.confidence as confidence\"\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "\n",
    "flattener_text_matcher.transform(text_matcher_result_df).show(n=30,truncate=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n6V3Kh5pCJCx",
    "outputId": "46731c0f-f328-416e-c6c9-e38c42d45620"
   },
   "execution_count": 58,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------------------+-----+---+-------+----------+\n",
      "|chunk                       |begin|end|result |confidence|\n",
      "+----------------------------+-----+---+-------+----------+\n",
      "|evaluation psychiatric state|69   |99 |present|0.9998    |\n",
      "|stressor                    |52   |60 |present|0.998     |\n",
      "|difficulty sleep            |114  |132|present|0.9993    |\n",
      "+----------------------------+-----+---+-------+----------+\n",
      "\n"
     ]
    }
   ]
  }
 ]
}
