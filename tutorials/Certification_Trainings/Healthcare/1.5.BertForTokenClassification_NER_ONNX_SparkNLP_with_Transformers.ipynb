{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQPGw6UQOjgo"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulQyWepeOlg3"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/healthcare-nlp/1.5.BertForTokenClassification_NER_ONNX_SparkNLP_with_Transformers.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnLd2l8QDSfl"
      },
      "source": [
        "# Medical NER: BertForTokenClassification → ONNX → Spark NLP JSL\n",
        "\n",
        "This notebook trains a Named Entity Recognition model on NCBI Disease corpus using BertForTokenClassification, exports it to ONNX format, and prepares it for **licensed** `johnsnowlabs` deployment.\n",
        "\n",
        "## Pipeline Steps:\n",
        "1. Download NCBI CoNLL dataset\n",
        "2. Train BertForTokenClassification model\n",
        "3. Export to ONNX format\n",
        "4. Test ONNX inference\n",
        "5. Package for Spark NLP JSL\n",
        "\n",
        "**Dataset:** NCBI Disease CoNLL format from John Snow Labs workshop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqI862qGDSfw"
      },
      "source": [
        "## 1. Installation and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtfLycQxDSfy"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q torch transformers datasets\n",
        "!pip install -q onnx onnxruntime\n",
        "!pip install -q seqeval scikit-learn\n",
        "\n",
        "print(\"✅ All packages installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeNm_vbXDSf0"
      },
      "source": [
        "## 2. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCPq0R-6DSf1",
        "outputId": "9bcc5869-0539-4773-a2f1-b42487f0cfad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import (\n",
        "    BertTokenizer,\n",
        "    BertForTokenClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForTokenClassification\n",
        ")\n",
        "import numpy as np\n",
        "import onnx\n",
        "import onnxruntime\n",
        "from typing import Dict, List, Tuple\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from seqeval.metrics import classification_report, f1_score\n",
        "from sklearn.metrics import classification_report as sklearn_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from collections import Counter\n",
        "\n",
        "print(\"✅ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL9sPb_6DSf2"
      },
      "source": [
        "## 3. Parse CoNLL Format Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF_sbMBvDSf4",
        "outputId": "8f488904-c25e-4595-a259-838fc60d3c00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Updated parse_conll_file defined\n"
          ]
        }
      ],
      "source": [
        "def parse_conll_file(file_path):\n",
        "    \"\"\"\n",
        "    Parse CoNLL format file and filter sentences with multiple unique tags\n",
        "\n",
        "    Format:\n",
        "    token1 tag1\n",
        "    token2 tag2\n",
        "    (empty line = sentence boundary)\n",
        "\n",
        "    Filters out sentences with only one unique tag (e.g., all \"O\" tags)\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    tags = []\n",
        "\n",
        "    current_tokens = []\n",
        "    current_tags = []\n",
        "\n",
        "    special_char_tokens = []\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "\n",
        "            if line == \"\" or line.startswith(\"-DOCSTART-\"):\n",
        "                if current_tokens:\n",
        "                    unique_tags = set(current_tags)\n",
        "                    if len(unique_tags) > 1:\n",
        "                        sentences.append(current_tokens)\n",
        "                        tags.append(current_tags)\n",
        "\n",
        "                    current_tokens = []\n",
        "                    current_tags = []\n",
        "            else:\n",
        "                parts = line.split()\n",
        "                if len(parts) >= 2:\n",
        "                    token = parts[0]\n",
        "                    tag = parts[-1]\n",
        "                    current_tokens.append(token)\n",
        "                    current_tags.append(tag)\n",
        "\n",
        "    if current_tokens:\n",
        "        unique_tags = set(current_tags)\n",
        "        if len(unique_tags) > 1:\n",
        "            sentences.append(current_tokens)\n",
        "            tags.append(current_tags)\n",
        "\n",
        "\n",
        "    return sentences, tags\n",
        "\n",
        "\n",
        "print(\"✅ Updated parse_conll_file defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyP-unet8AWg"
      },
      "source": [
        "## Load CoNLL Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDWfY81v8Fbe",
        "outputId": "59a09e51-be37-4e1e-d5df-bd0bc804cc00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Files downloaded successfully!\n",
            "  - train.conll\n",
            "  - test.conll\n"
          ]
        }
      ],
      "source": [
        "# Download NCBI CoNLL files\n",
        "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/refs/heads/master/tutorials/Certification_Trainings/Healthcare/data/NER_NCBIconlltrain.txt -O train.conll\n",
        "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/refs/heads/master/tutorials/Certification_Trainings/Healthcare/data/NER_NCBIconlltest.txt -O test.conll\n",
        "\n",
        "print(\"✅ Files downloaded successfully!\")\n",
        "print(\"  - train.conll\")\n",
        "print(\"  - test.conll\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIrvUzVS78Mg",
        "outputId": "89d773ba-8fd1-4c48-a639-660ab9946a4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📚 Loading CoNLL dataset...\n",
            "✓ Dataset loaded\n",
            "  - Train sentences: 1700\n",
            "  - Test sentences: 392\n",
            "  - Unique labels: ['B-Disease', 'I-Disease', 'O']\n",
            "  - Number of labels: 3\n",
            "\n",
            "📊 Label distribution in training set:\n",
            "  O: 39427\n",
            "  I-Disease: 3547\n",
            "  B-Disease: 3093\n",
            "\n",
            "📊 Label distribution in test set:\n",
            "  O: 9316\n",
            "  I-Disease: 789\n",
            "  B-Disease: 708\n"
          ]
        }
      ],
      "source": [
        "def load_conll_data():\n",
        "    \"\"\"Load CoNLL dataset\"\"\"\n",
        "\n",
        "    print(\"\\n📚 Loading CoNLL dataset...\")\n",
        "\n",
        "    # Parse train and test files\n",
        "    train_sentences, train_tags = parse_conll_file(\"train.conll\")\n",
        "    test_sentences, test_tags = parse_conll_file(\"test.conll\")\n",
        "\n",
        "    # Get unique labels\n",
        "    all_tags = set()\n",
        "    for tags in train_tags + test_tags:\n",
        "        all_tags.update(tags)\n",
        "\n",
        "    label_list = sorted(list(all_tags))\n",
        "    label2id = {label: i for i, label in enumerate(label_list)}\n",
        "    id2label = {i: label for i, label in enumerate(label_list)}\n",
        "\n",
        "    print(f\"✓ Dataset loaded\")\n",
        "    print(f\"  - Train sentences: {len(train_sentences)}\")\n",
        "    print(f\"  - Test sentences: {len(test_sentences)}\")\n",
        "    print(f\"  - Unique labels: {label_list}\")\n",
        "    print(f\"  - Number of labels: {len(label_list)}\")\n",
        "\n",
        "    # Print label distribution\n",
        "    train_tag_counts = Counter([tag for tags in train_tags for tag in tags])\n",
        "    print(f\"\\n📊 Label distribution in training set:\")\n",
        "    for label, count in train_tag_counts.most_common():\n",
        "        print(f\"  {label}: {count}\")\n",
        "\n",
        "    test_tag_counts = Counter([tag for tags in test_tags for tag in tags])\n",
        "    print(f\"\\n📊 Label distribution in test set:\")\n",
        "    for label, count in test_tag_counts.most_common():\n",
        "        print(f\"  {label}: {count}\")\n",
        "\n",
        "    return {\n",
        "        'train': {'sentences': train_sentences, 'tags': train_tags},\n",
        "        'test': {'sentences': test_sentences, 'tags': test_tags},\n",
        "        'label_list': label_list,\n",
        "        'label2id': label2id,\n",
        "        'id2label': id2label\n",
        "    }\n",
        "\n",
        "# Load the data\n",
        "data = load_conll_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMJvEQdXDSf5"
      },
      "source": [
        "## 4. Create PyTorch Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuO2_M5lDSf6",
        "outputId": "bec3f038-9d39-4c7e-9eec-575b2239e9e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ NERDataset class defined\n"
          ]
        }
      ],
      "source": [
        "class NERDataset(Dataset):\n",
        "    \"\"\"Custom NER Dataset for CoNLL format\"\"\"\n",
        "\n",
        "    def __init__(self, sentences, tags, tokenizer, label2id, max_length=128):\n",
        "        self.sentences = sentences\n",
        "        self.tags = tags\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label2id = label2id\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        tokens = self.sentences[idx]\n",
        "        labels = self.tags[idx]\n",
        "\n",
        "        # Tokenize with is_split_into_words=True\n",
        "        encoding = self.tokenizer(\n",
        "            tokens,\n",
        "            is_split_into_words=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "\n",
        "        # Align labels with subword tokens\n",
        "        word_ids = encoding.word_ids(batch_index=0)\n",
        "        label_ids = []\n",
        "        previous_word_idx = None\n",
        "\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                # Special tokens ([CLS], [SEP], [PAD]) get -100\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                # First subword of a word gets the original label\n",
        "                label_ids.append(self.label2id[labels[word_idx]])\n",
        "            else:\n",
        "                # Continuation subwords get I- version of the label\n",
        "                original_label = labels[word_idx]\n",
        "\n",
        "                if original_label == 'O':\n",
        "                    # O labels stay O\n",
        "                    label_ids.append(self.label2id['O'])\n",
        "                elif original_label.startswith('B-'):\n",
        "                    # B- becomes I- for continuation subwords\n",
        "                    entity_type = original_label[2:]  # Remove 'B-'\n",
        "                    continuation_label = f'I-{entity_type}'\n",
        "                    label_ids.append(self.label2id[continuation_label])\n",
        "                elif original_label.startswith('I-'):\n",
        "                    # I- stays I- for continuation subwords\n",
        "                    label_ids.append(self.label2id[original_label])\n",
        "                else:\n",
        "                    # Fallback: use -100\n",
        "                    label_ids.append(-100)\n",
        "\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label_ids)\n",
        "        }\n",
        "\n",
        "print(\"✅ NERDataset class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXAuKLHiDSf7"
      },
      "source": [
        "## 5. Define Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uqT4aAHADSf8"
      },
      "outputs": [],
      "source": [
        "label_list = data['label_list']\n",
        "\n",
        "# Initialize tracking variables\n",
        "epoch_counter = {\n",
        "    \"current\": 1,\n",
        "    \"best_epoch\": 0,\n",
        "    \"best_f1\": 0.0,\n",
        "    \"best_metrics\": {}\n",
        "}\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    \"\"\"Compute NER metrics using sklearn (token-level) for each epoch\"\"\"\n",
        "\n",
        "    predictions, labels = pred\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "\n",
        "    # Flatten predictions and labels, removing ignored index (-100)\n",
        "    flat_predictions = []\n",
        "    flat_labels = []\n",
        "\n",
        "    for prediction, label in zip(predictions, labels):\n",
        "        for p, l in zip(prediction, label):\n",
        "            if l != -100:  # Skip special tokens\n",
        "                flat_predictions.append(p)\n",
        "                flat_labels.append(l)\n",
        "\n",
        "    # Convert to label names\n",
        "    pred_labels = [label_list[p] for p in flat_predictions]\n",
        "    true_labels = [label_list[l] for l in flat_labels]\n",
        "\n",
        "    # Compute overall metrics for tracking\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        true_labels,\n",
        "        pred_labels,\n",
        "        average='weighted',\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    # Track best epoch silently (only during training)\n",
        "    if not epoch_counter.get('is_final', False) and f1 > epoch_counter['best_f1']:\n",
        "        epoch_counter['best_epoch'] = epoch_counter['current']\n",
        "        epoch_counter['best_f1'] = f1\n",
        "        epoch_counter['best_metrics'] = {\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1\n",
        "        }\n",
        "\n",
        "    # Print header based on mode\n",
        "    if epoch_counter.get('is_final', False):\n",
        "        header = \"📊 FINAL TOKEN-LEVEL METRICS\"\n",
        "    else:\n",
        "        header = f\"📊 METRICS - Epoch {epoch_counter['current']}\"\n",
        "\n",
        "    # Print detailed sklearn classification report (token-level)\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(header)\n",
        "    print(\"=\"*70)\n",
        "    report = sklearn_report(\n",
        "        true_labels,\n",
        "        pred_labels,\n",
        "        digits=4,\n",
        "        zero_division=0\n",
        "    )\n",
        "    print(report)\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Increment counter only during training\n",
        "    if not epoch_counter.get('is_final', False):\n",
        "        epoch_counter['current'] += 1\n",
        "\n",
        "    results = {\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PyPw8o_DSf8"
      },
      "source": [
        "## 6. Train BertForTokenClassification Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rg_dRy2NDSf8",
        "outputId": "40f80f57-9041-41b1-d478-4ba917818de9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🔧 Initializing BertTokenizer and BertForTokenClassification...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-base-cased-v1.2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Model initialized with 3 labels\n",
            "\n",
            "📝 Creating PyTorch datasets...\n",
            "✓ Train dataset: 1700 samples\n",
            "✓ Test dataset: 392 samples\n",
            "🚀 We are starting training...\n",
            "======================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='639' max='639' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [639/639 01:11, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.086200</td>\n",
              "      <td>0.084476</td>\n",
              "      <td>0.972302</td>\n",
              "      <td>0.971467</td>\n",
              "      <td>0.971728</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.037300</td>\n",
              "      <td>0.082123</td>\n",
              "      <td>0.974228</td>\n",
              "      <td>0.973677</td>\n",
              "      <td>0.973876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.010300</td>\n",
              "      <td>0.102188</td>\n",
              "      <td>0.973502</td>\n",
              "      <td>0.973074</td>\n",
              "      <td>0.973229</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "📊 METRICS - Epoch 1\n",
            "======================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   B-Disease     0.8898    0.9124    0.9010       708\n",
            "   I-Disease     0.9140    0.9638    0.9383      2570\n",
            "           O     0.9902    0.9767    0.9834     11652\n",
            "\n",
            "    accuracy                         0.9715     14930\n",
            "   macro avg     0.9313    0.9510    0.9409     14930\n",
            "weighted avg     0.9723    0.9715    0.9717     14930\n",
            "\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "📊 METRICS - Epoch 2\n",
            "======================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   B-Disease     0.8814    0.9237    0.9021       708\n",
            "   I-Disease     0.9305    0.9584    0.9442      2570\n",
            "           O     0.9895    0.9801    0.9848     11652\n",
            "\n",
            "    accuracy                         0.9737     14930\n",
            "   macro avg     0.9338    0.9541    0.9437     14930\n",
            "weighted avg     0.9742    0.9737    0.9739     14930\n",
            "\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "📊 METRICS - Epoch 3\n",
            "======================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   B-Disease     0.9000    0.9153    0.9076       708\n",
            "   I-Disease     0.9258    0.9564    0.9409      2570\n",
            "           O     0.9885    0.9803    0.9844     11652\n",
            "\n",
            "    accuracy                         0.9731     14930\n",
            "   macro avg     0.9381    0.9506    0.9443     14930\n",
            "weighted avg     0.9735    0.9731    0.9732     14930\n",
            "\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "🎯 TRAINING COMPLETE - BEST MODEL SUMMARY\n",
            "======================================================================\n",
            "🏆 Best Epoch: 2\n",
            "📊 Best F1 Score: 0.9739\n",
            "📊 Best Precision: 0.9742\n",
            "📊 Best Recall: 0.9737\n",
            "======================================================================\n",
            "\n",
            "✅ Best epoch info saved to ./ncbi_ner_model/best_epoch_info.json\n"
          ]
        }
      ],
      "source": [
        "# Initialize tracking variables\n",
        "epoch_counter = {\n",
        "    \"current\": 1,\n",
        "    \"best_epoch\": 0,\n",
        "    \"best_f1\": 0.0,\n",
        "    \"best_metrics\": {}\n",
        "}\n",
        "\n",
        "# Configuration\n",
        "MODEL_NAME = 'dmis-lab/biobert-base-cased-v1.2'\n",
        "OUTPUT_DIR = \"./ncbi_ner_model\"\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 3e-05\n",
        "\n",
        "print(\"\\n🔧 Initializing BertTokenizer and BertForTokenClassification...\")\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification\n",
        "\n",
        "# CRITICAL: Configure tokenizer properly for medical text\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    do_lower_case=False,          # BioBERT is cased\n",
        "    strip_accents=None,            # Let model decide\n",
        "    clean_text=True,               # Remove control chars\n",
        "    tokenize_chinese_chars=True,   # Standard BERT behavior\n",
        "    do_basic_tokenize=True,         # Essential for proper tokenization\n",
        "    never_split=['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
        ")\n",
        "\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(label_list),\n",
        "    id2label=data['id2label'],\n",
        "    label2id=data['label2id']\n",
        ")\n",
        "\n",
        "print(f\"✓ Model initialized with {len(label_list)} labels\")\n",
        "\n",
        "# Create datasets\n",
        "print(\"\\n📝 Creating PyTorch datasets...\")\n",
        "train_dataset = NERDataset(\n",
        "    data['train']['sentences'],\n",
        "    data['train']['tags'],\n",
        "    tokenizer,\n",
        "    data['label2id']\n",
        ")\n",
        "\n",
        "test_dataset = NERDataset(\n",
        "    data['test']['sentences'],\n",
        "    data['test']['tags'],\n",
        "    tokenizer,\n",
        "    data['label2id']\n",
        ")\n",
        "\n",
        "print(f\"✓ Train dataset: {len(train_dataset)} samples\")\n",
        "print(f\"✓ Test dataset: {len(test_dataset)} samples\")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    push_to_hub=False,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    processing_class=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(\"🚀 We are starting training...\")\n",
        "print(\"=\" * 70)\n",
        "trainer.train()\n",
        "\n",
        "# Display best model information\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🎯 TRAINING COMPLETE - BEST MODEL SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"🏆 Best Epoch: {epoch_counter['best_epoch']}\")\n",
        "print(f\"📊 Best F1 Score: {epoch_counter['best_f1']:.4f}\")\n",
        "print(f\"📊 Best Precision: {epoch_counter['best_metrics']['precision']:.4f}\")\n",
        "print(f\"📊 Best Recall: {epoch_counter['best_metrics']['recall']:.4f}\")\n",
        "print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "# Save best epoch info with the model\n",
        "best_epoch_info = {\n",
        "    \"best_epoch\": epoch_counter['best_epoch'],\n",
        "    \"best_f1\": float(epoch_counter['best_f1']),\n",
        "    \"best_precision\": float(epoch_counter['best_metrics']['precision']),\n",
        "    \"best_recall\": float(epoch_counter['best_metrics']['recall'])\n",
        "}\n",
        "\n",
        "with open(f\"{OUTPUT_DIR}/best_epoch_info.json\", \"w\") as f:\n",
        "    json.dump(best_epoch_info, f, indent=2)\n",
        "\n",
        "print(f\"✅ Best epoch info saved to {OUTPUT_DIR}/best_epoch_info.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGddp0TnDSf-"
      },
      "source": [
        "## 7. Evaluate and Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGfiTngODSf-",
        "outputId": "0d7c01b2-d889-4c55-bf01-ae4942366958"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Seqeval evaluation function defined\n"
          ]
        }
      ],
      "source": [
        "def evaluate_with_seqeval(model, dataset, tokenizer, label_list, batch_size=16):\n",
        "    \"\"\"Evaluate model using seqeval (chunk-level metrics)\"\"\"\n",
        "    from seqeval.metrics import classification_report, precision_score, recall_score, f1_score\n",
        "    from torch.utils.data import DataLoader\n",
        "\n",
        "    model.eval()\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    all_predictions, all_labels = [], []\n",
        "\n",
        "    # Get predictions\n",
        "    for batch in dataloader:\n",
        "        with torch.no_grad():\n",
        "            input_ids = batch['input_ids'].to(model.device)\n",
        "            attention_mask = batch['attention_mask'].to(model.device)\n",
        "            labels = batch['labels']\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()\n",
        "            labels = labels.numpy()\n",
        "\n",
        "            # Convert to label sequences\n",
        "            for pred, label in zip(predictions, labels):\n",
        "                pred_seq = [label_list[p] for p, l in zip(pred, label) if l != -100]\n",
        "                label_seq = [label_list[l] for l in label if l != -100]\n",
        "                if pred_seq:\n",
        "                    all_predictions.append(pred_seq)\n",
        "                    all_labels.append(label_seq)\n",
        "\n",
        "    # Compute and print metrics\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"🎯FINAL ENTITY-LEVEL EVALUATION\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Precision: {precision_score(all_labels, all_predictions):.4f}\")\n",
        "    print(f\"Recall:    {recall_score(all_labels, all_predictions):.4f}\")\n",
        "    print(f\"F1-Score:  {f1_score(all_labels, all_predictions):.4f}\\n\")\n",
        "    print(classification_report(all_labels, all_predictions, digits=4))\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return {\n",
        "        \"precision\": precision_score(all_labels, all_predictions),\n",
        "        \"recall\": recall_score(all_labels, all_predictions),\n",
        "        \"f1\": f1_score(all_labels, all_predictions)\n",
        "    }\n",
        "\n",
        "print(\"✅ Seqeval evaluation function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "id": "VCeCXoklzLSS",
        "outputId": "d360d0ab-8c47-4bee-af2c-4ea6596b6144"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='49' max='49' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [49/49 00:01]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "📊 FINAL TOKEN-LEVEL METRICS\n",
            "======================================================================\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   B-Disease     0.8814    0.9237    0.9021       708\n",
            "   I-Disease     0.9305    0.9584    0.9442      2570\n",
            "           O     0.9895    0.9801    0.9848     11652\n",
            "\n",
            "    accuracy                         0.9737     14930\n",
            "   macro avg     0.9338    0.9541    0.9437     14930\n",
            "weighted avg     0.9742    0.9737    0.9739     14930\n",
            "\n",
            "======================================================================\n",
            "\n",
            "F1:        0.9739\n",
            "Precision: 0.9742\n",
            "Recall:    0.9737\n",
            "Loss:      0.0821\n",
            "\n",
            "======================================================================\n",
            "🎯FINAL ENTITY-LEVEL EVALUATION\n",
            "======================================================================\n",
            "Precision: 0.8206\n",
            "Recall:    0.8983\n",
            "F1-Score:  0.8577\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Disease     0.8206    0.8983    0.8577       708\n",
            "\n",
            "   micro avg     0.8206    0.8983    0.8577       708\n",
            "   macro avg     0.8206    0.8983    0.8577       708\n",
            "weighted avg     0.8206    0.8983    0.8577       708\n",
            "\n",
            "======================================================================\n",
            "\n",
            "======================================================================\n",
            "🏆 BEST MODEL SUMMARY\n",
            "======================================================================\n",
            "Best Epoch: 2\n",
            "Best F1:    0.9739\n",
            "Precision:  0.9742\n",
            "Recall:     0.9737\n",
            "======================================================================\n",
            "\n",
            "💾 Saving model and results...\n",
            "\n",
            "✅ All results saved to ./ncbi_ner_model\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Set flag for final evaluation (don't reset counter!)\n",
        "epoch_counter['is_final'] = True\n",
        "\n",
        "# Token-level evaluation\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"F1:        {eval_results['eval_f1']:.4f}\")\n",
        "print(f\"Precision: {eval_results['eval_precision']:.4f}\")\n",
        "print(f\"Recall:    {eval_results['eval_recall']:.4f}\")\n",
        "print(f\"Loss:      {eval_results['eval_loss']:.4f}\")\n",
        "\n",
        "# Entity-level evaluation\n",
        "seqeval_results = evaluate_with_seqeval(model, test_dataset, tokenizer, label_list, batch_size=16)\n",
        "\n",
        "# Show best epoch summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🏆 BEST MODEL SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Best Epoch: {epoch_counter['best_epoch']}\")\n",
        "print(f\"Best F1:    {epoch_counter['best_f1']:.4f}\")\n",
        "print(f\"Precision:  {epoch_counter['best_metrics']['precision']:.4f}\")\n",
        "print(f\"Recall:     {epoch_counter['best_metrics']['recall']:.4f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Save everything\n",
        "print(\"\\n💾 Saving model and results...\")\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "# Save labels and mappings\n",
        "with open(f\"{OUTPUT_DIR}/tags.txt\", \"w\") as f:\n",
        "    f.write('\\n'.join(label_list))\n",
        "\n",
        "label_info = {\n",
        "    \"label2id\": data['label2id'],\n",
        "    \"id2label\": data['id2label'],\n",
        "    \"labels\": label_list\n",
        "}\n",
        "with open(f\"{OUTPUT_DIR}/label_mappings.json\", \"w\") as f:\n",
        "    json.dump(label_info, f, indent=2)\n",
        "\n",
        "# Save evaluation results\n",
        "eval_summary = {\n",
        "    \"best_epoch\": {\n",
        "        \"epoch\": epoch_counter['best_epoch'],\n",
        "        \"f1\": float(epoch_counter['best_f1']),\n",
        "        \"precision\": float(epoch_counter['best_metrics']['precision']),\n",
        "        \"recall\": float(epoch_counter['best_metrics']['recall'])\n",
        "    },\n",
        "    \"final_token_level\": {\n",
        "        \"precision\": float(eval_results['eval_precision']),\n",
        "        \"recall\": float(eval_results['eval_recall']),\n",
        "        \"f1\": float(eval_results['eval_f1']),\n",
        "        \"loss\": float(eval_results['eval_loss'])\n",
        "    },\n",
        "    \"final_entity_level\": {\n",
        "        \"precision\": float(seqeval_results['precision']),\n",
        "        \"recall\": float(seqeval_results['recall']),\n",
        "        \"f1\": float(seqeval_results['f1'])\n",
        "    }\n",
        "}\n",
        "with open(f\"{OUTPUT_DIR}/evaluation_results.json\", \"w\") as f:\n",
        "    json.dump(eval_summary, f, indent=2)\n",
        "\n",
        "print(f\"\\n✅ All results saved to {OUTPUT_DIR}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Reset flag\n",
        "epoch_counter['is_final'] = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfbkNfS7DSf-"
      },
      "source": [
        "## 8. Test Model Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfoDawPMtPdR",
        "outputId": "cf1f2924-5934-4355-8ebd-b531c8450402"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧪 Testing model predictions...\n",
            "\n",
            "📝 Text: Breast cancer is a disease in which cells in the breast grow out of control.\n",
            "\n",
            "🎯 Detected Entities:\n",
            "  - Breast cancer (Disease)\n",
            "\n",
            "📝 Text: Patients with diabetes mellitus require insulin therapy.\n",
            "\n",
            "🎯 Detected Entities:\n",
            "  - diabetes mellitus (Disease)\n",
            "\n",
            "📝 Text: Alzheimer disease is characterized by progressive cognitive deterioration.\n",
            "\n",
            "🎯 Detected Entities:\n",
            "  - Alzheimer disease (Disease)\n",
            "  - cognitive deterioration (Disease)\n",
            "\n",
            "✅ Testing completed!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n🧪 Testing model predictions...\")\n",
        "\n",
        "# Load model for inference\n",
        "model = BertForTokenClassification.from_pretrained(OUTPUT_DIR)\n",
        "tokenizer = BertTokenizer.from_pretrained(OUTPUT_DIR)\n",
        "model.eval()\n",
        "\n",
        "with open(f\"{OUTPUT_DIR}/label_mappings.json\", \"r\") as f:\n",
        "    label_info = json.load(f)\n",
        "id2label = {int(k): v for k, v in label_info['id2label'].items()}\n",
        "\n",
        "# Test examples\n",
        "test_texts = [\n",
        "    \"Breast cancer is a disease in which cells in the breast grow out of control.\",\n",
        "    \"Patients with diabetes mellitus require insulin therapy.\",\n",
        "    \"Alzheimer disease is characterized by progressive cognitive deterioration.\"\n",
        "]\n",
        "\n",
        "for text in test_texts:\n",
        "    print(f\"\\n📝 Text: {text}\")\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    # Predict\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    predictions = torch.argmax(outputs.logits, dim=2)\n",
        "\n",
        "    # Get tokens and labels\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    pred_labels = [id2label[p.item()] for p in predictions[0]]\n",
        "\n",
        "    # Extract entities with proper subword reconstruction\n",
        "    entities = []\n",
        "    current_entity = \"\"  # ← STRING, not list!\n",
        "    current_label = None\n",
        "\n",
        "    for token, label in zip(tokens, pred_labels):\n",
        "        if token in ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]']:\n",
        "            continue\n",
        "\n",
        "        if label.startswith('B-'):\n",
        "            # Save previous entity\n",
        "            if current_entity:\n",
        "                entities.append((current_label, current_entity.strip()))\n",
        "\n",
        "            # Start new entity\n",
        "            current_label = label[2:]\n",
        "            if token.startswith('##'):\n",
        "                current_entity = token[2:]  # Remove ## without space\n",
        "            else:\n",
        "                current_entity = token\n",
        "\n",
        "        elif label.startswith('I-') and current_label:\n",
        "            # Continue entity\n",
        "            if token.startswith('##'):\n",
        "                current_entity += token[2:]  # ← Concatenate WITHOUT space\n",
        "            else:\n",
        "                current_entity += \" \" + token  # ← Add space for new word\n",
        "\n",
        "        else:\n",
        "            # Non-entity token\n",
        "            if current_entity:\n",
        "                entities.append((current_label, current_entity.strip()))\n",
        "            current_entity = \"\"\n",
        "            current_label = None\n",
        "\n",
        "    # Don't forget last entity\n",
        "    if current_entity:\n",
        "        entities.append((current_label, current_entity.strip()))\n",
        "\n",
        "    # Print detected entities\n",
        "    if entities:\n",
        "        print(\"\\n🎯 Detected Entities:\")\n",
        "        for label, entity in entities:\n",
        "            print(f\"  - {entity} ({label})\")\n",
        "    else:\n",
        "        print(\"\\n  No entities detected\")\n",
        "\n",
        "print(\"\\n✅ Testing completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM1PsPzDDSf_"
      },
      "source": [
        "## 9. Export to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr1ivKXwDSf_",
        "outputId": "93f7314d-ddd2-4b4e-e86a-40397bcfbf96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📦 Exporting to ONNX...\n",
            "🔄 Converting to ONNX (opset 14)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2736940308.py:27: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
            "  torch.onnx.export(\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/modeling_attn_mask_utils.py:196: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "  inverted_mask = torch.tensor(1.0, dtype=dtype) - expanded_mask\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Patching ONNX model for Spark NLP...\n",
            "✓ Renaming: input_ids -> medical_input_ids\n",
            "✅ ONNX model saved to ./ncbi_ner_model.onnx\n",
            "   Inputs: ['medical_input_ids', 'attention_mask', 'token_type_ids']\n",
            "   Outputs: ['logits']\n"
          ]
        }
      ],
      "source": [
        "# Export to ONNX for Spark NLP Healthcare\n",
        "import onnx\n",
        "from onnx import TensorProto\n",
        "\n",
        "ONNX_PATH = \"./ncbi_ner_model.onnx\"\n",
        "MAX_LENGTH = 512\n",
        "OPSET_VERSION = 14\n",
        "\n",
        "print(\"\\n📦 Exporting to ONNX...\")\n",
        "\n",
        "# Load model and create dummy input\n",
        "model = BertForTokenClassification.from_pretrained(OUTPUT_DIR)\n",
        "tokenizer = BertTokenizer.from_pretrained(OUTPUT_DIR)\n",
        "model.eval()\n",
        "\n",
        "inputs = tokenizer(\n",
        "    \"Sample text for ONNX export\",\n",
        "    return_tensors=\"pt\",\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    is_split_into_words=True,\n",
        ")\n",
        "\n",
        "# Export to ONNX\n",
        "print(f\"🔄 Converting to ONNX (opset {OPSET_VERSION})...\")\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    (inputs[\"input_ids\"], inputs[\"attention_mask\"], inputs.get(\"token_type_ids\")),\n",
        "    ONNX_PATH,\n",
        "    input_names=[\"input_ids\", \"attention_mask\", \"token_type_ids\"],\n",
        "    output_names=[\"logits\"],\n",
        "    dynamic_axes={\n",
        "        \"input_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
        "        \"attention_mask\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
        "        \"token_type_ids\": {0: \"batch_size\", 1: \"sequence_length\"},\n",
        "        \"logits\": {0: \"batch_size\", 1: \"sequence_length\"}\n",
        "    },\n",
        "    opset_version=OPSET_VERSION,\n",
        "    do_constant_folding=True\n",
        ")\n",
        "\n",
        "# Patch for Spark NLP Healthcare: INT64 inputs + rename for medical_input_ids\n",
        "print(\"✓ Patching ONNX model for Spark NLP...\")\n",
        "onnx_model = onnx.load(ONNX_PATH)\n",
        "onnx.checker.check_model(onnx_model)\n",
        "\n",
        "# Rename input_ids -> medical_input_ids\n",
        "name_map = {\"input_ids\": \"medical_input_ids\"}\n",
        "\n",
        "# Force INT64 on all inputs\n",
        "for ip in onnx_model.graph.input:\n",
        "    if ip.type.tensor_type.elem_type != TensorProto.INT64:\n",
        "        ip.type.tensor_type.elem_type = TensorProto.INT64\n",
        "    if ip.name in name_map:\n",
        "        new_name = name_map[ip.name]\n",
        "        print(f\"✓ Renaming: {ip.name} -> {new_name}\")\n",
        "        ip.name = new_name\n",
        "\n",
        "# Update node references\n",
        "for node in onnx_model.graph.node:\n",
        "    node.input[:] = [name_map.get(n, n) for n in node.input]\n",
        "\n",
        "# Update initializers\n",
        "for init in onnx_model.graph.initializer:\n",
        "    if init.name in name_map:\n",
        "        init.name = name_map[init.name]\n",
        "\n",
        "onnx.save(onnx_model, ONNX_PATH)\n",
        "print(f\"✅ ONNX model saved to {ONNX_PATH}\")\n",
        "print(f\"   Inputs: {[i.name for i in onnx_model.graph.input]}\")\n",
        "print(f\"   Outputs: {[o.name for o in onnx_model.graph.output]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqmAA3-BDSgA"
      },
      "source": [
        "## 10. Test ONNX Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHCbUtX2DSgA",
        "outputId": "29293fb2-939f-4072-effe-b0d31dea2992"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧪 Testing ONNX model...\n",
            "Model expects inputs: ['medical_input_ids', 'attention_mask', 'token_type_ids']\n",
            "✅ ONNX forward pass OK. Logits shape: (1, 512, 3)\n"
          ]
        }
      ],
      "source": [
        "# Test ONNX model with dynamic input-name handling\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "\n",
        "print(\"\\n🧪 Testing ONNX model...\")\n",
        "\n",
        "# Load inputs\n",
        "test_text = \"Breast cancer and diabetes are common diseases.\"\n",
        "inputs = tokenizer(\n",
        "    test_text,\n",
        "    return_tensors=\"np\",\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_token_type_ids=True  # Add this explicitly\n",
        ")\n",
        "\n",
        "# Start session and inspect expected names\n",
        "session = ort.InferenceSession(ONNX_PATH)\n",
        "expected = [i.name for i in session.get_inputs()]\n",
        "print(\"Model expects inputs:\", expected)\n",
        "\n",
        "def as_i64(x):\n",
        "    import numpy as _np\n",
        "    return _np.asarray(x, dtype=_np.int64)\n",
        "\n",
        "# Build feed dict based on expected naming\n",
        "if set(expected) == set([\"medical_input_ids\", \"attention_mask\", \"token_type_ids\"]):\n",
        "    ort_inputs = {\n",
        "        \"medical_input_ids\": as_i64(inputs[\"input_ids\"]),\n",
        "        \"attention_mask\": as_i64(inputs[\"attention_mask\"]),\n",
        "        \"token_type_ids\": as_i64(inputs.get(\"token_type_ids\", np.zeros_like(inputs[\"input_ids\"])))\n",
        "    }\n",
        "else:\n",
        "    raise ValueError(f\"Unexpected input names in model: {expected}\")\n",
        "\n",
        "# Run inference\n",
        "outputs = session.run(None, ort_inputs)\n",
        "logits = outputs[0]\n",
        "print(\"✅ ONNX forward pass OK. Logits shape:\", logits.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7izBIT6eDSgA"
      },
      "source": [
        "## 11. Import the Model to Spark NLP Healthcare Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qu-IK6ZA3Ag2"
      },
      "source": [
        "### Upload License File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "GNQDpQDaceh5"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "if 'spark_jsl.json' not in os.listdir():\n",
        "  license_keys = files.upload()\n",
        "  os.rename(list(license_keys.keys())[0], 'spark_jsl.json')\n",
        "\n",
        "with open('spark_jsl.json') as f:\n",
        "    license_keys = json.load(f)\n",
        "\n",
        "# Defining license key-value pairs as local variables\n",
        "locals().update(license_keys)\n",
        "os.environ.update(license_keys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ezfFpqv3Vxl"
      },
      "source": [
        "### Install Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX0k7eHXFvVF"
      },
      "outputs": [],
      "source": [
        "# Installing pyspark and spark-nlp\n",
        "! pip install --upgrade -q pyspark==3.4.1 spark-nlp==$PUBLIC_VERSION\n",
        "\n",
        "# Installing Spark NLP Healthcare\n",
        "! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET\n",
        "\n",
        "# Installing Spark NLP Display Library for visualization\n",
        "! pip install -q spark-nlp-display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jXpAurS3epn"
      },
      "source": [
        "### Import Libraries and Start Spark Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "1R6BEvKSTIxz",
        "outputId": "91029627-2e62-4837-e526-c3160ec562a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark NLP Version : 6.1.3\n",
            "Spark NLP_JSL Version : 6.1.1\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://b3a0bddbb5a0:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP Licensed</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7b0a1042b7d0>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "import sparknlp\n",
        "import sparknlp_jsl\n",
        "\n",
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp_jsl.annotator import *\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.ml import Pipeline,PipelineModel\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Spark NLP Version :\", sparknlp.version())\n",
        "print(\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
        "\n",
        "spark = sparknlp_jsl.start(license_keys['SECRET'])\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDxXFvqF3wgM"
      },
      "source": [
        "## 12. Prepare Spark NLP JSL Model Package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRnj__sdDSgA",
        "outputId": "2e89e1cf-30ff-4598-a458-a887c9f052b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📋 Preparing Spark NLP JSL model...\n",
            "✓ Copied model.onnx to root\n",
            "  ONNX inputs: ['medical_input_ids', 'attention_mask', 'token_type_ids']\n",
            "\n",
            "📝 Extracting labels from model...\n",
            "✓ Saved labels.txt (3 labels)\n",
            "✓ Copied vocab.txt to assets/\n",
            "\n",
            "📁 Copying tokenizer config files to root...\n",
            "  ✓ Copied tokenizer_config.json to root\n",
            "  ✓ Copied special_tokens_map.json to root\n",
            "  ✓ Copied tokenizer.json to root\n",
            "✓ Created config.json in root\n",
            "\n",
            "📁 Final directory structure:\n",
            "spark_nlp_jsl_ncbi_ner/\n",
            "  config.json (160 bytes)\n",
            "  model.onnx (431144990 bytes)\n",
            "  special_tokens_map.json (125 bytes)\n",
            "  tokenizer.json (669188 bytes)\n",
            "  tokenizer_config.json (1389 bytes)\n",
            "  assets/\n",
            "    labels.txt (21 bytes)\n",
            "    vocab.txt (213450 bytes)\n",
            "\n",
            "✅ Spark NLP JSL model preparation complete!\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import onnx\n",
        "import json\n",
        "\n",
        "SPARK_NLP_PATH = \"./spark_nlp_jsl_ncbi_ner\"\n",
        "MODEL_NAME = \"ncbi_disease_ner_bert\"\n",
        "\n",
        "print(\"\\n📋 Preparing Spark NLP JSL model...\")\n",
        "\n",
        "# 1. Create directory structure\n",
        "os.makedirs(SPARK_NLP_PATH, exist_ok=True)\n",
        "assets_path = os.path.join(SPARK_NLP_PATH, \"assets\")\n",
        "os.makedirs(assets_path, exist_ok=True)\n",
        "\n",
        "# 2. Copy ONNX model to ROOT as model.onnx\n",
        "onnx_dest = os.path.join(SPARK_NLP_PATH, \"model.onnx\")\n",
        "shutil.copy(ONNX_PATH, onnx_dest)\n",
        "print(f\"✓ Copied model.onnx to root\")\n",
        "\n",
        "# 3. Verify ONNX inputs\n",
        "onnx_model = onnx.load(onnx_dest)\n",
        "input_names = [i.name for i in onnx_model.graph.input]\n",
        "print(f\"  ONNX inputs: {input_names}\")\n",
        "\n",
        "# 4. Load model and extract labels\n",
        "print(\"\\n📝 Extracting labels from model...\")\n",
        "if 'model' not in dir():\n",
        "    from transformers import BertForTokenClassification\n",
        "    model = BertForTokenClassification.from_pretrained(OUTPUT_DIR)\n",
        "\n",
        "labels_dict = model.config.label2id\n",
        "labels_sorted = sorted(labels_dict, key=labels_dict.get)\n",
        "\n",
        "# 5. Save labels to assets folder\n",
        "with open(os.path.join(assets_path, 'labels.txt'), 'w') as f:\n",
        "    f.write('\\n'.join(labels_sorted))\n",
        "print(f\"✓ Saved labels.txt ({len(labels_sorted)} labels)\")\n",
        "\n",
        "# 6. Copy vocab.txt to assets\n",
        "vocab_src = os.path.join(OUTPUT_DIR, \"vocab.txt\")\n",
        "if os.path.exists(vocab_src):\n",
        "    shutil.copy(vocab_src, os.path.join(assets_path, \"vocab.txt\"))\n",
        "    print(\"✓ Copied vocab.txt to assets/\")\n",
        "\n",
        "# 7. Copy tokenizer files to ROOT directory (not assets!)\n",
        "print(\"\\n📁 Copying tokenizer config files to root...\")\n",
        "tokenizer_files = [\"tokenizer_config.json\", \"special_tokens_map.json\", \"tokenizer.json\"]\n",
        "for file in tokenizer_files:\n",
        "    src = os.path.join(OUTPUT_DIR, file)\n",
        "    if os.path.exists(src):\n",
        "        shutil.copy(src, os.path.join(SPARK_NLP_PATH, file))  # Copy to ROOT\n",
        "        print(f\"  ✓ Copied {file} to root\")\n",
        "\n",
        "# 8. Create config.json in ROOT\n",
        "config = {\n",
        "    \"architectures\": [\"BertForTokenClassification\"],\n",
        "    \"model_type\": \"bert\",\n",
        "    \"max_position_embeddings\": 512,\n",
        "    \"hidden_size\": 768,\n",
        "    \"num_labels\": len(labels_sorted)\n",
        "}\n",
        "with open(os.path.join(SPARK_NLP_PATH, \"config.json\"), \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "print(\"✓ Created config.json in root\")\n",
        "\n",
        "# 9. Verify final structure\n",
        "print(\"\\n📁 Final directory structure:\")\n",
        "for root, dirs, files in os.walk(SPARK_NLP_PATH):\n",
        "    level = root.replace(SPARK_NLP_PATH, '').count(os.sep)\n",
        "    indent = ' ' * 2 * level\n",
        "    print(f'{indent}{os.path.basename(root)}/')\n",
        "    subindent = ' ' * 2 * (level + 1)\n",
        "    for file in sorted(files):\n",
        "        file_path = os.path.join(root, file)\n",
        "        file_size = os.path.getsize(file_path)\n",
        "        print(f'{subindent}{file} ({file_size} bytes)')\n",
        "\n",
        "print(\"\\n✅ Spark NLP JSL model preparation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6sb3n7f4OC5"
      },
      "source": [
        "## Load the Saved Model into Spark NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2h9RJrbMl1s",
        "outputId": "e764dff9-f743-4864-8dd2-8deccfacaa15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📦 Loading model from ./spark_nlp_jsl_ncbi_ner...\n",
            "✓ Model loaded successfully into Spark NLP\n",
            "\n",
            "💾 Saving Spark NLP model to ./ncbi_disease_ner_bert_spark_nlp_onnx...\n",
            "✅ Spark NLP model saved to ./ncbi_disease_ner_bert_spark_nlp_onnx\n",
            "\n",
            "📋 Final output locations:\n",
            "  1. ONNX Export: ./spark_nlp_jsl_ncbi_ner/\n",
            "  2. Spark NLP Model: ./ncbi_disease_ner_bert_spark_nlp_onnx/\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n📦 Loading model from {SPARK_NLP_PATH}...\")\n",
        "\n",
        "tokenClassifier = MedicalBertForTokenClassifier\\\n",
        "    .loadSavedModel(SPARK_NLP_PATH, spark)\\\n",
        "    .setInputCols([\"document\", 'token'])\\\n",
        "    .setOutputCol(\"ner\")\\\n",
        "    .setCaseSensitive(False)\\\n",
        "    .setMaxSentenceLength(512)\n",
        "\n",
        "print(\"✓ Model loaded successfully into Spark NLP\")\n",
        "\n",
        "# Save the model in Spark NLP format\n",
        "output_path = f\"./{MODEL_NAME}_spark_nlp_onnx\"\n",
        "print(f\"\\n💾 Saving Spark NLP model to {output_path}...\")\n",
        "\n",
        "tokenClassifier.write().overwrite().save(output_path)\n",
        "\n",
        "print(f\"✅ Spark NLP model saved to {output_path}\")\n",
        "print(f\"\\n📋 Final output locations:\")\n",
        "print(f\"  1. ONNX Export: {SPARK_NLP_PATH}/\")\n",
        "print(f\"  2. Spark NLP Model: {output_path}/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzXL40dO4XQ7"
      },
      "source": [
        "## Test Spark NLP Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Xx9wE6LnMl9W"
      },
      "outputs": [],
      "source": [
        "document_assembler = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer()\\\n",
        "    .setInputCols([\"document\"])\\\n",
        "    .setOutputCol(\"token\")\n",
        "\n",
        "# Load the saved Spark NLP model\n",
        "ner_model = MedicalBertForTokenClassifier.load(output_path)\\\n",
        "    .setInputCols([\"document\", \"token\"])\\\n",
        "    .setOutputCol(\"ner\")\\\n",
        "    .setCaseSensitive(False)\\\n",
        "    .setMaxSentenceLength(512)\n",
        "\n",
        "ner_converter = NerConverterInternal() \\\n",
        "    .setInputCols([\"document\", \"token\", \"ner\"]) \\\n",
        "    .setOutputCol(\"ner_chunk\")\n",
        "\n",
        "pipeline = Pipeline(stages=[\n",
        "    document_assembler,\n",
        "    tokenizer,\n",
        "    ner_model,\n",
        "    ner_converter\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFQd_-OWUJRv",
        "outputId": "6cbad9f4-053a-41f0-8d6b-8f6274345ddf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['B-Disease', 'I-Disease', 'O']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ner_model.getClasses()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj-R09vHD9cW",
        "outputId": "091f732c-c9cc-4f31-9a5d-57f20b4b8d2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🧪 Testing Spark NLP model...\n",
            "\n",
            "📊 Results:\n",
            "+-------------+----------+--------+---------+\n",
            "|token        |char_begin|char_end|label    |\n",
            "+-------------+----------+--------+---------+\n",
            "|Breast       |0         |5       |B-Disease|\n",
            "|cancer       |7         |12      |I-Disease|\n",
            "|is           |14        |15      |O        |\n",
            "|a            |17        |17      |O        |\n",
            "|disease      |19        |25      |O        |\n",
            "|in           |27        |28      |O        |\n",
            "|which        |30        |34      |O        |\n",
            "|cells        |36        |40      |O        |\n",
            "|in           |42        |43      |O        |\n",
            "|the          |45        |47      |O        |\n",
            "|breast       |49        |54      |O        |\n",
            "|grow         |56        |59      |O        |\n",
            "|out          |61        |63      |O        |\n",
            "|of           |65        |66      |O        |\n",
            "|control      |68        |74      |O        |\n",
            "|.            |75        |75      |O        |\n",
            "|Patients     |0         |7       |O        |\n",
            "|with         |9         |12      |O        |\n",
            "|diabetes     |14        |21      |B-Disease|\n",
            "|mellitus     |23        |30      |I-Disease|\n",
            "|require      |32        |38      |O        |\n",
            "|insulin      |40        |46      |O        |\n",
            "|therapy      |48        |54      |O        |\n",
            "|.            |55        |55      |O        |\n",
            "|Alzheimer    |0         |8       |B-Disease|\n",
            "|disease      |10        |16      |I-Disease|\n",
            "|is           |18        |19      |O        |\n",
            "|characterized|21        |33      |O        |\n",
            "|by           |35        |36      |O        |\n",
            "|progressive  |38        |48      |O        |\n",
            "|cognitive    |50        |58      |B-Disease|\n",
            "|deterioration|60        |72      |I-Disease|\n",
            "|.            |73        |73      |O        |\n",
            "+-------------+----------+--------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "print(\"\\n🧪 Testing Spark NLP model...\")\n",
        "\n",
        "# Test data\n",
        "test_texts = [\n",
        "    \"Breast cancer is a disease in which cells in the breast grow out of control.\",\n",
        "    \"Patients with diabetes mellitus require insulin therapy.\",\n",
        "    \"Alzheimer disease is characterized by progressive cognitive deterioration.\"\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "test_df = spark.createDataFrame([[text] for text in test_texts], [\"text\"])\n",
        "\n",
        "# Fit and transform\n",
        "model = pipeline.fit(test_df)\n",
        "result = model.transform(test_df)\n",
        "\n",
        "# Show results\n",
        "print(\"\\n📊 Results:\")\n",
        "\n",
        "# After transformation, inspect token metadata\n",
        "result.select(\n",
        "    F.explode(\n",
        "        F.arrays_zip(\n",
        "            result.token.result,\n",
        "            result.token.begin,\n",
        "            result.token.end,\n",
        "            result.ner.result\n",
        "        )\n",
        "    ).alias(\"cols\")\n",
        ").select(\n",
        "    F.expr(\"cols['0']\").alias(\"token\"),\n",
        "    F.expr(\"cols['1']\").alias(\"char_begin\"),\n",
        "    F.expr(\"cols['2']\").alias(\"char_end\"),\n",
        "    F.expr(\"cols['3']\").alias(\"label\")\n",
        ").show(50, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUaHpS9lg5P4",
        "outputId": "3fe71998-d07c-4928-d2ca-d368d74c0812"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------------+---------+\n",
            "|chunk                  |ner_label|\n",
            "+-----------------------+---------+\n",
            "|Breast cancer          |Disease  |\n",
            "|diabetes mellitus      |Disease  |\n",
            "|Alzheimer disease      |Disease  |\n",
            "|cognitive deterioration|Disease  |\n",
            "+-----------------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "result.select(F.explode(F.arrays_zip(result.ner_chunk.result,\n",
        "                                     result.ner_chunk.metadata)).alias(\"cols\")) \\\n",
        "      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
        "              F.expr(\"cols['1']['entity']\").alias(\"ner_label\")).show(truncate=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
