{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.3.prepare_CoNLL_from_annotations_for_NER.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare CoNLL file from annotations for NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Install java\n",
    "! apt-get update -qq\n",
    "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "! java -version\n",
    "\n",
    "# Install pyspark\n",
    "! pip install --ignore-installed -q pyspark==2.4.4\n",
    "! pip install --ignore-installed -q spark-nlp==2.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>entity</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11319232</td>\n",
       "      <td>CHEMICAL</td>\n",
       "      <td>242</td>\n",
       "      <td>250</td>\n",
       "      <td>acyl-CoAs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11319232</td>\n",
       "      <td>CHEMICAL</td>\n",
       "      <td>1193</td>\n",
       "      <td>1200</td>\n",
       "      <td>triacsin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11319232</td>\n",
       "      <td>CHEMICAL</td>\n",
       "      <td>1441</td>\n",
       "      <td>1447</td>\n",
       "      <td>sucrose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11319232</td>\n",
       "      <td>CHEMICAL</td>\n",
       "      <td>1637</td>\n",
       "      <td>1651</td>\n",
       "      <td>triacylglycerol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11319232</td>\n",
       "      <td>CHEMICAL</td>\n",
       "      <td>1702</td>\n",
       "      <td>1710</td>\n",
       "      <td>acyl-CoAs</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    text_id    entity  begin   end            chunk\n",
       "0  11319232  CHEMICAL    242   250        acyl-CoAs\n",
       "1  11319232  CHEMICAL   1193  1200         triacsin\n",
       "2  11319232  CHEMICAL   1441  1447          sucrose\n",
       "3  11319232  CHEMICAL   1637  1651  triacylglycerol\n",
       "4  11319232  CHEMICAL   1702  1710        acyl-CoAs"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_entities_df = pd.read_csv('data/ChemProt/chemprot_train_entities.csv')\n",
    "train_entities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16357751</td>\n",
       "      <td>Selective costimulation modulators: a novel ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14967461</td>\n",
       "      <td>Emerging role of epidermal growth factor recep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23468099</td>\n",
       "      <td>Effects of chronic social defeat stress on beh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23293962</td>\n",
       "      <td>Hepatocyte growth factor activator inhibitor t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7678677</td>\n",
       "      <td>Alprenolol and bromoacetylalprenololmenthane a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    text_id                                               text\n",
       "0  16357751  Selective costimulation modulators: a novel ap...\n",
       "1  14967461  Emerging role of epidermal growth factor recep...\n",
       "2  23468099  Effects of chronic social defeat stress on beh...\n",
       "3  23293962  Hepatocyte growth factor activator inhibitor t...\n",
       "4   7678677  Alprenolol and bromoacetylalprenololmenthane a..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text_df = pd.read_csv('data/ChemProt/chemprot_train_text.csv')\n",
    "train_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_anc download started this may take some time.\n",
      "Approximate size to download 4.3 MB\n",
      "[OK!]\n",
      "Spark NLP lightpipeline is created\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "from sparknlp.base import *\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "\n",
    "def get_nlp_pipeline ():\n",
    "\n",
    "    document_assembler = DocumentAssembler() \\\n",
    "        .setInputCol(\"text\")\\\n",
    "        .setOutputCol('document')\n",
    "\n",
    "    sentence = SentenceDetector()\\\n",
    "    .setInputCols([\"document\"])\\\n",
    "    .setOutputCol(\"sentence\")\\\n",
    "    .setDetectLists(False) \n",
    "\n",
    "    # modify the tokenizer as you wish depending on your data specs\n",
    "    tokenizer = Tokenizer() \\\n",
    "        .setInputCols([\"sentence\"]) \\\n",
    "        .setOutputCol(\"token\")\n",
    "    \n",
    "    pos = PerceptronModel.pretrained() \\\n",
    "              .setInputCols([\"sentence\", \"token\"]) \\\n",
    "              .setOutputCol(\"pos\")\n",
    "    \n",
    "    pipeline = Pipeline(\n",
    "        stages = [\n",
    "            document_assembler,\n",
    "            sentence,\n",
    "            tokenizer,\n",
    "            pos]\n",
    "    )\n",
    "\n",
    "    empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
    "\n",
    "    pipelineFit = pipeline.fit(empty_data)\n",
    "\n",
    "    lp_pipeline = LightPipeline(pipelineFit)\n",
    "    \n",
    "    print (\"Spark NLP lightpipeline is created\")\n",
    "    \n",
    "    return lp_pipeline\n",
    "\n",
    "\n",
    "lp_pipeline =  get_nlp_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import string\n",
    "\n",
    "def get_conll_per_doc (lp_pipeline, single_entities_df, text, first_doc=True):\n",
    "    \n",
    "    if first_doc:\n",
    "        conll_lines=[\"-DOCSTART- -X- -X- O\\n\\n\"]\n",
    "    else:\n",
    "        conll_lines = []\n",
    "\n",
    "    n = lp_pipeline.fullAnnotate(text)\n",
    "\n",
    "    parsed = [(int(x.metadata['sentence']), x.result, x.begin, x.end, y.result) for x,y in zip(n[0][\"token\"],n[0][\"pos\"])]\n",
    "\n",
    "    ents = []\n",
    "\n",
    "    ann_results = single_entities_df\n",
    "\n",
    "\n",
    "    for i, row in single_entities_df.iterrows(): \n",
    "\n",
    "\n",
    "        temp_text = row['chunk']\n",
    "        start = row['begin']\n",
    "        end = row['end']\n",
    "\n",
    "        if len(temp_text)!=len(temp_text.rstrip()):\n",
    "            end = end-(len(temp_text)-len(temp_text.rstrip()))\n",
    "            temp_text = temp_text.rstrip()\n",
    "\n",
    "        if len(temp_text)!=len(temp_text.lstrip()):\n",
    "            start = start+(len(temp_text)-len(temp_text.lstrip())) \n",
    "            temp_text = temp_text.lstrip()\n",
    "\n",
    "        ents.append((temp_text, row['entity'], start, end))\n",
    "\n",
    "    df = pd.DataFrame(ents, columns=['chunk','label','start','end'])    \n",
    "    \n",
    "    ix_list=[]\n",
    "    token_list=[]\n",
    "    tag_list=[]\n",
    "\n",
    "    for i,row in df.iterrows():\n",
    "\n",
    "        base_ix= row[\"start\"]\n",
    "\n",
    "        w_len = 0\n",
    "\n",
    "        punc_flag = False\n",
    "\n",
    "        try:\n",
    "            if row[\"chunk\"][-1] in string.punctuation:\n",
    "                punc_flag=True\n",
    "                chunk = row[\"chunk\"][:-1]+' '+row[\"chunk\"][-1]\n",
    "            else:\n",
    "                chunk = row[\"chunk\"]\n",
    "        except:\n",
    "            chunk = row[\"chunk\"]\n",
    "\n",
    "        last_ix = len(chunk.split())\n",
    "\n",
    "\n",
    "        for i,t in enumerate(chunk.split()):\n",
    "\n",
    "            if i==0:\n",
    "                ix=base_ix\n",
    "                iob = \"B-\"\n",
    "            else:\n",
    "                ix=ix+w_len+1\n",
    "                iob = \"I-\"\n",
    "\n",
    "            token_list.append(t)\n",
    "            if punc_flag and i == last_ix-1:\n",
    "                ix_list.append(ix-1)\n",
    "            else:\n",
    "                ix_list.append(ix)\n",
    "\n",
    "            tag_list.append(iob+row['label'])\n",
    "\n",
    "            w_len = len(t)\n",
    "\n",
    "    tagged= list(zip(ix_list,token_list,tag_list))\n",
    "\n",
    "    tag_dict = {(ix,token):tag for ix,token,tag in tagged}\n",
    "\n",
    "    s=0\n",
    "\n",
    "    for i, p in enumerate(parsed):\n",
    "\n",
    "        if p[0]!=s:\n",
    "            conll_lines.append(\"\\n\")\n",
    "            s+=1\n",
    "\n",
    "        conll_lines.append(\"{} {} {} {}\\n\".format(p[1], p[4], p[4], tag_dict.get((p[2],p[1]),\"O\")))\n",
    "\n",
    "    conll_lines.append(\"\\n\")\n",
    "\n",
    "    return conll_lines\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_Conll_file (text_df, entities_df, path=None, limit = None):\n",
    "    \n",
    "    if limit is not None:\n",
    "    \n",
    "        text_df = text_df[:limit]\n",
    "    \n",
    "    conll_lines_list = []\n",
    "    \n",
    "    for i, row in tqdm(text_df.iterrows(), total=text_df.shape[0]):\n",
    "        \n",
    "        single_entities_df = entities_df[entities_df.text_id==row['text_id']]\n",
    "        \n",
    "        if i==0:\n",
    "            first_doc = True\n",
    "        else:\n",
    "            first_doc = False\n",
    "            \n",
    "        lines = get_conll_per_doc (lp_pipeline, single_entities_df, row['text'], first_doc)\n",
    "    \n",
    "        conll_lines_list.extend(lines)\n",
    "        \n",
    "    if path is not None:\n",
    "        \n",
    "        \n",
    "        conll_filename = '{}/ner_annotations_{}.conll'.format(path, str(datetime.now().date()))\n",
    "        \n",
    "        with open(conll_filename, 'w') as f:\n",
    "            for i in conll_lines_list:\n",
    "                f.write(i)\n",
    "\n",
    "        print (conll_filename,  'is saved.')\n",
    "\n",
    "    else:\n",
    "        \n",
    "        return conll_lines_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ChemProt/ner_annotations_2020-09-24.conll is saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "conll = get_Conll_file (train_text_df, train_entities_df, path='data/ChemProt', limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:04<00:00,  2.46it/s]\n"
     ]
    }
   ],
   "source": [
    "conll = get_Conll_file (train_text_df, train_entities_df, path=None, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-DOCSTART- -X- -X- O\\n\\n',\n",
       " 'Selective NNP NNP O\\n',\n",
       " 'costimulation NN NN O\\n',\n",
       " 'modulators NNS NNS O\\n',\n",
       " ': : : O\\n',\n",
       " 'a DT DT O\\n',\n",
       " 'novel NN NN O\\n',\n",
       " 'approach NN NN O\\n',\n",
       " 'for IN IN O\\n',\n",
       " 'the DT DT O\\n',\n",
       " 'treatment NN NN O\\n',\n",
       " 'of IN IN O\\n',\n",
       " 'rheumatoid NN NN O\\n',\n",
       " 'arthritis NN NN O\\n',\n",
       " '. . . O\\n',\n",
       " '\\n',\n",
       " 'T NN NN O\\n',\n",
       " 'cells NNS NNS O\\n',\n",
       " 'have VBP VBP O\\n',\n",
       " 'a DT DT O\\n']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
