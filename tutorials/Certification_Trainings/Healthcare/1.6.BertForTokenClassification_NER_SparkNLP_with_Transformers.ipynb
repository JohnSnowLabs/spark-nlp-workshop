{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.6.BertForTokenClassification_NER_SparkNLP_with_Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "63fe598b5f604b70832bd3add966f09f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_49beed4a49954ddfadff9011dc6f3337",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b000b59fd4b44693a5622b634c43da55",
              "IPY_MODEL_3aa5c5b92227442a82ac25579e9c8636",
              "IPY_MODEL_647b9c1668704f69af5e5d712bc12172"
            ]
          }
        },
        "49beed4a49954ddfadff9011dc6f3337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b000b59fd4b44693a5622b634c43da55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3d5dd8b385c448169dc2c7a8c3a95f32",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_921ad329a1344a569f935cd895f139af"
          }
        },
        "3aa5c5b92227442a82ac25579e9c8636": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5fd4a36ac9fb40e7a52e86af8412bb61",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 213450,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 213450,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ac9f1e649ae8431ab214da734b7e0e73"
          }
        },
        "647b9c1668704f69af5e5d712bc12172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b322eafb515b4baebd8107d054d023ed",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 213k/213k [00:00&lt;00:00, 630kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d6130212b5bd412d96b0a19f0c9f382e"
          }
        },
        "3d5dd8b385c448169dc2c7a8c3a95f32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "921ad329a1344a569f935cd895f139af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5fd4a36ac9fb40e7a52e86af8412bb61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ac9f1e649ae8431ab214da734b7e0e73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b322eafb515b4baebd8107d054d023ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d6130212b5bd412d96b0a19f0c9f382e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbt8Q6j3RJxW"
      },
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZOMrR1fRZq3"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/1.6.BertForTokenClassification_NER_SparkNLP_with_Transformers.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhiz8WdthoJm"
      },
      "source": [
        "# BertForTokenClassification NER Model Training with Transformers\n",
        "\n",
        "In this notebook, you will find how to train BertForTokenClassification NER model with transformers and then import into Spark NLP. (There is no Approach() in this notebook, so you can use only transformers for training.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqSpuOmV6dQE"
      },
      "source": [
        "%%capture\n",
        "\n",
        "! pip -q install seqeval\n",
        "! pip install transformers==4.8.1\n",
        "! pip install pyspark==3.1.2\n",
        "! pip install spark-nlp\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertConfig\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import sparknlp\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "from sparknlp.training import CoNLL\n",
        "from google.colab import files\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "import transformers\n",
        "from transformers import BertForTokenClassification, TFBertForTokenClassification, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "\n",
        "torch.cuda.get_device_name(0)\n",
        "\n",
        "spark = sparknlp.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kKaWwaX6Pos"
      },
      "source": [
        "## Download NCBI Disease CoNLL Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBEuXQnw6VPG"
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/data/NER_NCBIconlltrain.txt\n",
        "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Healthcare/data/NER_NCBIconlltest.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO0VnBGa81Wb"
      },
      "source": [
        "PROJECT_NAME = 'ner_disease_main'\n",
        "\n",
        "train_set =  \"NER_NCBIconlltrain.txt\"\n",
        "test_set = \"NER_NCBIconlltest.txt\"\n",
        "\n",
        "test_metrics = True\n",
        "\n",
        "# select any Bert model from >> https://huggingface.co/models?pipeline_tag=token-classification&sort=downloads&search=bert\n",
        "\n",
        "MODEL_TO_TRAIN = 'dmis-lab/biobert-base-cased-v1.2'\n",
        "# emilyalsentzer/Bio_ClinicalBERT\n",
        "\n",
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 128 # 512\n",
        "TRAIN_BATCH_SIZE = 64 # 8\n",
        "VALID_BATCH_SIZE = 64 # 8\n",
        "EPOCHS = 5\n",
        "LEARNING_RATE = 2e-05\n",
        "\n",
        "!mkdir {PROJECT_NAME}\n",
        "!mkdir {PROJECT_NAME}/logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iebvp7r-t4tl"
      },
      "source": [
        "## Run the follwing cells with no change"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "63fe598b5f604b70832bd3add966f09f",
            "49beed4a49954ddfadff9011dc6f3337",
            "b000b59fd4b44693a5622b634c43da55",
            "3aa5c5b92227442a82ac25579e9c8636",
            "647b9c1668704f69af5e5d712bc12172",
            "3d5dd8b385c448169dc2c7a8c3a95f32",
            "921ad329a1344a569f935cd895f139af",
            "5fd4a36ac9fb40e7a52e86af8412bb61",
            "ac9f1e649ae8431ab214da734b7e0e73",
            "b322eafb515b4baebd8107d054d023ed",
            "d6130212b5bd412d96b0a19f0c9f382e",
            "e6aa32115847446b804f54af916ad294",
            "a3d6b0c8af7940fda69cbb5a6d9fc7d9"
          ]
        },
        "id": "XvUBSCIo9ZWR",
        "outputId": "296d18da-23d1-4029-d58f-0efb61694702"
      },
      "source": [
        "def get_conll_df(pth):\n",
        "  data = CoNLL().readDataset(spark, pth)\n",
        "  data = data.withColumn(\"sentence_idx\", F.monotonically_increasing_id())\n",
        "  data = data.withColumn('unique', F.array_distinct(\"label.result\"))\\\n",
        "              .withColumn('c', F.size('unique'))\\\n",
        "              .filter(F.col('c')>1)\n",
        "\n",
        "  df = data.select('sentence_idx', F.explode(F.arrays_zip('token.result','label.result','pos.result')).alias(\"cols\")) \\\n",
        "          .select('sentence_idx',\n",
        "                  F.expr(\"cols['0']\").alias(\"word\"),\n",
        "                  F.expr(\"cols['1']\").alias(\"tag\"),\n",
        "                  F.expr(\"cols['2']\").alias(\"pos\")).toPandas()\n",
        "  \n",
        "  return df\n",
        "\n",
        "\n",
        "train_data_df = get_conll_df(train_set)\n",
        "test_data_df = get_conll_df(test_set)\n",
        "\n",
        "print ('=== TRAINING SET DISTRIBUTION ===')\n",
        "print (train_data_df['tag'].value_counts())\n",
        "\n",
        "print ('=== TEST SET DISTRIBUTION ===')\n",
        "print (test_data_df['tag'].value_counts())\n",
        "\n",
        "if not test_metrics:\n",
        "\n",
        "  train_data_df = pd.concat([train_data_df, test_data_df])\n",
        "\n",
        "\n",
        "## convert conll file to sentences\n",
        "\n",
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, dataset):\n",
        "        self.n_sent = 1\n",
        "        self.dataset = dataset\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w,p, t) for w,p, t in zip(s[\"word\"].values.tolist(),\n",
        "                                                       s['pos'].values.tolist(),\n",
        "                                                        s[\"tag\"].values.tolist())]\n",
        "        self.grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None\n",
        "\n",
        "train_getter = SentenceGetter(train_data_df)\n",
        "\n",
        "if test_metrics:  \n",
        "  test_getter = SentenceGetter(test_data_df)\n",
        "\n",
        "\n",
        "print ('=== Getting sentences and labels ===')\n",
        "\n",
        "# Sentences \n",
        "train_sentences = [[word[0] for word in sentence] for sentence in train_getter.sentences]\n",
        "print(\"Example of train sentence:\")\n",
        "print (train_sentences[5])\n",
        "\n",
        "if test_metrics:\n",
        "  test_sentences = [[word[0] for word in sentence] for sentence in test_getter.sentences]\n",
        "  print(\"Example of test sentence:\")\n",
        "  print (test_sentences[5])\n",
        "\n",
        "# Labels\n",
        "train_labels = [[s[2] for s in sentence] for sentence in train_getter.sentences]\n",
        "print(\"Example of train sentence:\")\n",
        "print(train_labels[5])\n",
        "\n",
        "if test_metrics:\n",
        "  test_labels = [[s[2] for s in sentence] for sentence in test_getter.sentences]\n",
        "  print(\"Example of test sentence:\")\n",
        "  print(test_labels[5])\n",
        "\n",
        "\n",
        "tag_values = list(set(train_data_df[\"tag\"].values))\n",
        "tag_values.append(\"PAD\")\n",
        "tag2idx = {t: i for i, t in enumerate(tag_values)}\n",
        "\n",
        "print(tag_values[:10])\n",
        "print(tag2idx)\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_TO_TRAIN, do_lower_case=False)\n",
        "\n",
        "\n",
        "def tokenize_and_preserve_labels(sentence, text_labels):\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    for word, label in zip(sentence, text_labels):\n",
        "\n",
        "        # Tokenize the word and count # of subwords the word is broken into\n",
        "        tokenized_word = tokenizer.tokenize(word)\n",
        "        n_subwords = len(tokenized_word)\n",
        "\n",
        "        # Add the tokenized word to the final tokenized word list\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "        # Add the same label to the new list of labels `n_subwords` times\n",
        "        labels.extend([label] * n_subwords)\n",
        "\n",
        "    return tokenized_sentence, labels\n",
        "\n",
        "\n",
        "train_tokenized_texts_and_labels = [\n",
        "    tokenize_and_preserve_labels(sent, labs)\n",
        "    for sent, labs in zip(train_sentences, train_labels)\n",
        "]\n",
        "\n",
        "if test_metrics:\n",
        "\n",
        "  test_tokenized_texts_and_labels = [\n",
        "      tokenize_and_preserve_labels(sent, labs)\n",
        "      for sent, labs in zip(test_sentences, test_labels)\n",
        "  ]\n",
        "\n",
        "train_tokenized_texts_tokens = [token_label_pair[0] for token_label_pair in train_tokenized_texts_and_labels]\n",
        "\n",
        "if test_metrics:\n",
        "  test_tokenized_texts_tokens = [token_label_pair[0] for token_label_pair in test_tokenized_texts_and_labels]\n",
        "  print(test_tokenized_texts_tokens[5])\n",
        "\n",
        "train_tokenized_texts_labels = [token_label_pair[1] for token_label_pair in train_tokenized_texts_and_labels]\n",
        "\n",
        "if test_metrics:\n",
        "  test_tokenized_texts_labels = [token_label_pair[1] for token_label_pair in test_tokenized_texts_and_labels]\n",
        "  print(test_tokenized_texts_labels[5])\n",
        "\n",
        "\n",
        "\n",
        "train_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in train_tokenized_texts_tokens],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "\n",
        "if test_metrics:\n",
        "\n",
        "  test_input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in test_tokenized_texts_tokens],\n",
        "                          maxlen=MAX_LEN, dtype=\"long\", value=0.0,\n",
        "                          truncating=\"post\", padding=\"post\")\n",
        "\n",
        "train_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in train_tokenized_texts_labels],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "\n",
        "train_attention_masks = [[float(i != 0.0) for i in ii] for ii in train_input_ids]\n",
        "\n",
        "if test_metrics:\n",
        "\n",
        "  test_tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in test_tokenized_texts_labels],\n",
        "                     maxlen=MAX_LEN, value=tag2idx[\"PAD\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "  test_attention_masks = [[float(i != 0.0) for i in ii] for ii in test_input_ids]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tr_inputs = torch.tensor(train_input_ids)\n",
        "tr_tags = torch.tensor(train_tags)\n",
        "tr_masks = torch.tensor(train_attention_masks)\n",
        "\n",
        "if test_metrics:\n",
        "\n",
        "  val_inputs = torch.tensor(test_input_ids)\n",
        "  val_tags = torch.tensor(test_tags)\n",
        "  val_masks = torch.tensor(test_attention_masks)\n",
        "\n",
        "\n",
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=TRAIN_BATCH_SIZE)\n",
        "\n",
        "if test_metrics:\n",
        "\n",
        "  valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "  valid_sampler = SequentialSampler(valid_data)\n",
        "  valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=TRAIN_BATCH_SIZE)\n",
        "\n",
        "\n",
        "\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    MODEL_TO_TRAIN,\n",
        "    num_labels=len(tag2idx),\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "FULL_FINETUNING = True\n",
        "if FULL_FINETUNING:\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "else:\n",
        "    param_optimizer = list(model.classifier.named_parameters())\n",
        "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters,\n",
        "    lr=3e-5,\n",
        "    eps=1e-8\n",
        ")\n",
        "\n",
        "\n",
        "epochs = EPOCHS\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "\n",
        "## Store the average loss after each epoch so we can plot them.\n",
        "loss_values, validation_loss_values = [], []\n",
        "\n",
        "for EPOCH in trange(epochs, desc=\"Epoch\"):\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Training loop\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        # add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        # Always clear any previously calculated gradients before performing a backward pass.\n",
        "        model.zero_grad()\n",
        "        # forward pass\n",
        "        # This will return the loss (rather than the model output)\n",
        "        # because we have provided the `labels`.\n",
        "        outputs = model(b_input_ids, token_type_ids=None,\n",
        "                        attention_mask=b_input_mask, labels=b_labels)\n",
        "        # get the loss\n",
        "        loss = outputs[0]\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        # track train loss\n",
        "        total_loss += loss.item()\n",
        "        # Clip the norm of the gradient\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    tr_loss = f\"Average train loss: {str(avg_train_loss)}\\n\"\n",
        "\n",
        "    # Saving partial models (this creates the folder too)    \n",
        "    tokenizer.save_pretrained(f'{PROJECT_NAME}/{str(EPOCH)}/tokenizer/')\n",
        "    model.save_pretrained(save_directory=f'{PROJECT_NAME}/{str(EPOCH)}/',\n",
        "                          save_config=True, state_dict=model.state_dict)\n",
        "    # Saving checkpoint in case it crashes, to restore work\n",
        "    torch.save({\n",
        "        'epoch': EPOCH,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': avg_train_loss,\n",
        "        }, f'{PROJECT_NAME}/{str(EPOCH)}/checkpoint.pth')\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "    \n",
        "    if test_metrics:\n",
        "\n",
        "      # Put the model into evaluation mode\n",
        "      model.eval()\n",
        "      # Reset the validation loss for this epoch.\n",
        "      eval_loss, eval_accuracy = 0, 0\n",
        "      nb_eval_steps, nb_eval_examples = 0, 0\n",
        "      predictions , true_labels = [], []\n",
        "      for batch in valid_dataloader:\n",
        "          batch = tuple(t.to(device) for t in batch)\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "          # Telling the model not to compute or store gradients,\n",
        "          # saving memory and speeding up validation\n",
        "          with torch.no_grad():\n",
        "              # Forward pass, calculate logit predictions.\n",
        "              # This will return the logits rather than the loss because we have not provided labels.\n",
        "              outputs = model(b_input_ids, token_type_ids=None,\n",
        "                              attention_mask=b_input_mask, labels=b_labels)\n",
        "          # Move logits and labels to CPU\n",
        "          logits = outputs[1].detach().cpu().numpy()\n",
        "          label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "          # Calculate the accuracy for this batch of test sentences.\n",
        "          eval_loss += outputs[0].mean().item()\n",
        "          predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "          true_labels.extend(label_ids)\n",
        "\n",
        "      eval_loss = eval_loss / len(valid_dataloader)\n",
        "      validation_loss_values.append(eval_loss)\n",
        "\n",
        "      val_loss = f\"Validation loss: {str(eval_loss)}\\n\"\n",
        "      \n",
        "    # Saving losses log\n",
        "    with open(f'{PROJECT_NAME}/logs/epoch_' + str(EPOCH) + '_loss.log', 'a') as f:\n",
        "      f.write(tr_loss)\n",
        "      f.write('')\n",
        "      if test_metrics:\n",
        "          f.write(val_loss)\n",
        "\n",
        "    # Calculating metrics\n",
        "    pred_tags = [tag_values[p_i] for p, l in zip(predictions, true_labels)\n",
        "                                 for p_i, l_i in zip(p, l) if tag_values[l_i] != \"PAD\"]\n",
        "    valid_tags = [tag_values[l_i] for l in true_labels\n",
        "                                  for l_i in l if tag_values[l_i] != \"PAD\"]\n",
        "    \n",
        "    report = classification_report(valid_tags, pred_tags)\n",
        "    \n",
        "    # Saving metrics\n",
        "    with open(f'{PROJECT_NAME}/logs/epoch_' + str(EPOCH) + '_metrics.log', 'a') as f:\n",
        "      f.write(report)\n",
        "\n",
        "    # Printing also to stdout\n",
        "    print(tr_loss)\n",
        "\n",
        "    if test_metrics:\n",
        "      print(val_loss)\n",
        "      print(report)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== TRAINING SET DISTRIBUTION ===\n",
            "O            39427\n",
            "I-Disease     3547\n",
            "B-Disease     3093\n",
            "Name: tag, dtype: int64\n",
            "=== TEST SET DISTRIBUTION ===\n",
            "O            9316\n",
            "I-Disease     789\n",
            "B-Disease     708\n",
            "Name: tag, dtype: int64\n",
            "=== Getting sentences and labels ===\n",
            "Example of train sentence:\n",
            "['A', 'common', 'MSH2', 'mutation', 'in', 'English', 'and', 'North', 'American', 'HNPCC', 'families', ':', 'origin', ',', 'phenotypic', 'expression', ',', 'and', 'sex', 'specific', 'differences', 'in', 'colorectal', 'cancer', '.']\n",
            "Example of test sentence:\n",
            "['Two', 'of', 'seventeen', 'mutated', 'T', '-', 'PLL', 'samples', 'had', 'a', 'previously', 'reported', 'A', '-', 'T', 'allele', '.']\n",
            "Example of train sentence:\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Disease', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Disease', 'I-Disease', 'O']\n",
            "Example of test sentence:\n",
            "['O', 'O', 'O', 'O', 'B-Disease', 'I-Disease', 'I-Disease', 'O', 'O', 'O', 'O', 'O', 'B-Disease', 'I-Disease', 'I-Disease', 'O', 'O']\n",
            "['O', 'I-Disease', 'B-Disease', 'PAD']\n",
            "{'O': 0, 'I-Disease': 1, 'B-Disease': 2, 'PAD': 3}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63fe598b5f604b70832bd3add966f09f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Two', 'of', 'seventeen', 'm', '##uta', '##ted', 'T', '-', 'P', '##LL', 'samples', 'had', 'a', 'previously', 'reported', 'A', '-', 'T', 'all', '##ele', '.']\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'B-Disease', 'I-Disease', 'I-Disease', 'I-Disease', 'O', 'O', 'O', 'O', 'O', 'B-Disease', 'I-Disease', 'I-Disease', 'O', 'O', 'O']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6aa32115847446b804f54af916ad294",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3d6b0c8af7940fda69cbb5a6d9fc7d9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/436M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:  20%|██        | 1/5 [01:17<05:10, 77.53s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train loss: 0.4448511252800624\n",
            "\n",
            "Validation loss: 0.18799577866281783\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   B-Disease       0.78      0.70      0.74      1718\n",
            "   I-Disease       0.74      0.79      0.76      1560\n",
            "           O       0.97      0.98      0.97     11654\n",
            "\n",
            "    accuracy                           0.93     14932\n",
            "   macro avg       0.83      0.82      0.82     14932\n",
            "weighted avg       0.92      0.93      0.92     14932\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  40%|████      | 2/5 [02:35<03:53, 77.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train loss: 0.14443378233247334\n",
            "\n",
            "Validation loss: 0.1239968655364854\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   B-Disease       0.89      0.85      0.87      1718\n",
            "   I-Disease       0.84      0.91      0.87      1560\n",
            "           O       0.98      0.98      0.98     11654\n",
            "\n",
            "    accuracy                           0.95     14932\n",
            "   macro avg       0.90      0.91      0.91     14932\n",
            "weighted avg       0.96      0.95      0.95     14932\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  60%|██████    | 3/5 [03:54<02:36, 78.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train loss: 0.078425330006414\n",
            "\n",
            "Validation loss: 0.12757520909820283\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   B-Disease       0.88      0.88      0.88      1718\n",
            "   I-Disease       0.86      0.90      0.88      1560\n",
            "           O       0.98      0.98      0.98     11654\n",
            "\n",
            "    accuracy                           0.96     14932\n",
            "   macro avg       0.91      0.92      0.91     14932\n",
            "weighted avg       0.96      0.96      0.96     14932\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch:  80%|████████  | 4/5 [05:12<01:18, 78.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train loss: 0.0506643035483581\n",
            "\n",
            "Validation loss: 0.14034557661839894\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   B-Disease       0.88      0.91      0.90      1718\n",
            "   I-Disease       0.85      0.93      0.89      1560\n",
            "           O       0.99      0.97      0.98     11654\n",
            "\n",
            "    accuracy                           0.96     14932\n",
            "   macro avg       0.91      0.94      0.92     14932\n",
            "weighted avg       0.96      0.96      0.96     14932\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 100%|██████████| 5/5 [06:30<00:00, 78.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average train loss: 0.03918191556025435\n",
            "\n",
            "Validation loss: 0.13432587683200836\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   B-Disease       0.89      0.90      0.89      1718\n",
            "   I-Disease       0.86      0.92      0.89      1560\n",
            "           O       0.99      0.98      0.98     11654\n",
            "\n",
            "    accuracy                           0.96     14932\n",
            "   macro avg       0.91      0.93      0.92     14932\n",
            "weighted avg       0.96      0.96      0.96     14932\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCXnULJwMwW-"
      },
      "source": [
        "!rm -rf /content/ner_disease_main/0\n",
        "!rm -rf /content/ner_disease_main/1\n",
        "!rm -rf /content/ner_disease_main/2\n",
        "!rm -rf /content/ner_disease_main/3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJQEdfA0Npmz"
      },
      "source": [
        "## Load the model as TF and save properly\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "er8xdlkKNqZ4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a9b48df-2ced-4e4c-fdd4-3407cc8fb13f"
      },
      "source": [
        "last_successfull_epoch = len(loss_values) - 1\n",
        "if last_successfull_epoch < 0:\n",
        "  last_successfull_epoch = None\n",
        "\n",
        "if last_successfull_epoch is None:\n",
        "  print(\"No epochs finished successfully.\")\n",
        "else:\n",
        "  print(f\"Last successfull epoch: {str(last_successfull_epoch)}\")\n",
        "\n",
        "# first save the model as pytorch model (we'll cast later)\n",
        "MODEL_NAME_PYTORCH = 'model_epoch_'+str(last_successfull_epoch)+'_pytorch'\n",
        "MODEL_NAME_TF = 'model_epoch_'+str(last_successfull_epoch)+'_tf'\n",
        "\n",
        "print(MODEL_NAME_PYTORCH)\n",
        "print(MODEL_NAME_TF)\n",
        "\n",
        "# now load the model as TF and save properly\n",
        "from transformers import TFBertForTokenClassification\n",
        "\n",
        "tokenizer.save_pretrained(f'./{PROJECT_NAME}/{MODEL_NAME_PYTORCH}_tokenizer/')\n",
        "model.save_pretrained(f'./{PROJECT_NAME}/{MODEL_NAME_PYTORCH}', saved_model=True, save_format='tf')\n",
        "loaded_model = TFBertForTokenClassification.from_pretrained(f'./{PROJECT_NAME}/{MODEL_NAME_PYTORCH}', from_pt=True)\n",
        "loaded_model.save_pretrained(f'./{PROJECT_NAME}/{MODEL_NAME_TF}', saved_model=True)\n",
        "\n",
        "labels = sorted(tag2idx, key=tag2idx.get)\n",
        "\n",
        "print (labels)\n",
        "\n",
        "with open(f'./{PROJECT_NAME}/{MODEL_NAME_TF}/saved_model/1/assets/labels.txt', 'w') as f:\n",
        "    f.write('\\n'.join(labels))\n",
        "\n",
        "vocab_pth = f\"./{PROJECT_NAME}/{MODEL_NAME_PYTORCH}_tokenizer/vocab.txt\"\n",
        "saved_model_pth = f'./{PROJECT_NAME}/{MODEL_NAME_TF}/saved_model/1/assets/'\n",
        "\n",
        "! cp $vocab_pth $saved_model_pth"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last successfull epoch: 4\n",
            "model_epoch_4_pytorch\n",
            "model_epoch_4_tf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, add_layer_call_fn while saving (showing 5 of 1045). These functions will not be directly callable after loading.\n",
            "INFO:tensorflow:Assets written to: ./ner_disease_main/model_epoch_4_tf/saved_model/1/assets\n",
            "/usr/local/lib/python3.7/dist-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['O', 'I-Disease', 'B-Disease', 'PAD']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcQNjWOeN83V"
      },
      "source": [
        "## Load the saved model in Spark NLP and save it properly¶\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7lvrYkvOC3L"
      },
      "source": [
        "from sparknlp.annotator import *\n",
        "\n",
        "tokenClassifier = BertForTokenClassification.loadSavedModel(\n",
        "     f'./{PROJECT_NAME}/{MODEL_NAME_TF}/saved_model/1',\n",
        "     spark)\\\n",
        "  .setInputCols([\"sentence\",'token'])\\\n",
        "  .setOutputCol(\"ner\")\\\n",
        "  .setCaseSensitive(True)\\\n",
        "  .setMaxSentenceLength(128) # 512\n",
        "\n",
        "tokenClassifier.write().overwrite().save(f\"./{PROJECT_NAME}/{MODEL_NAME_TF}_spark_nlp\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYDq9D-cOGbo"
      },
      "source": [
        "## Test the imported model in Spark NLP¶\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fugqP283OG6u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de620c28-bd4f-47cb-94e5-c680d0a991d3"
      },
      "source": [
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "documentAssembler = DocumentAssembler()\\\n",
        "  .setInputCol(\"text\")\\\n",
        "  .setOutputCol(\"document\")\n",
        "\n",
        "sentenceDetector = SentenceDetectorDLModel.pretrained()\\\n",
        "  .setInputCols([\"document\"])\\\n",
        "  .setOutputCol(\"sentence\")\n",
        "\n",
        "tokenizer = Tokenizer()\\\n",
        "  .setInputCols(\"sentence\")\\\n",
        "  .setOutputCol(\"token\")\n",
        "\n",
        "tokenClassifier = BertForTokenClassification.load(f\"./{PROJECT_NAME}/{MODEL_NAME_TF}_spark_nlp\")\\\n",
        "  .setInputCols(\"token\", \"sentence\")\\\n",
        "  .setOutputCol(\"label\")\\\n",
        "  .setCaseSensitive(True)\n",
        "\n",
        "ner_converter = NerConverter()\\\n",
        "  .setInputCols([\"sentence\",\"token\",\"label\"])\\\n",
        "  .setOutputCol(\"ner_chunk\")\n",
        "\n",
        "\n",
        "pipeline =  Pipeline(\n",
        "    stages=[\n",
        "  documentAssembler,\n",
        "  sentenceDetector,\n",
        "  tokenizer,\n",
        "  tokenClassifier,\n",
        "  ner_converter\n",
        "    ]\n",
        ")\n",
        "\n",
        "p_model = pipeline.fit(spark.createDataFrame(pd.DataFrame({'text': ['']})))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentence_detector_dl download started this may take some time.\n",
            "Approximate size to download 354.6 KB\n",
            "[OK!]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k808iCsakEfn"
      },
      "source": [
        "text = 'A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced pancreatitis three years prior to presentation , associated with an acute hepatitis , and obesity with a body mass index ( BMI ) of 33.5 kg/m2 , presented with a one-week history of polyuria , polydipsia , poor appetite , and vomiting . Two weeks prior to presentation , she was treated with a five-day course of amoxicillin for a respiratory tract infection . She was on metformin , glipizide , and dapagliflozin for T2DM and atorvastatin and gemfibrozil for HTG . She had been on dapagliflozin for six months at the time of presentation . Physical examination on presentation was significant for dry oral mucosa ; significantly , her abdominal examination was benign with no tenderness , guarding , or rigidity . Pertinent laboratory findings on admission were : serum glucose 111 mg/dl , bicarbonate 18 mmol/l , anion gap 20 , creatinine 0.4 mg/dL , triglycerides 508 mg/dL , total cholesterol 122 mg/dL , glycated hemoglobin ( HbA1c ) 10% , and venous pH 7.27 . Serum lipase was normal at 43 U/L . Serum acetone levels could not be assessed as blood samples kept hemolyzing due to significant lipemia . The patient was initially admitted for starvation ketosis , as she reported poor oral intake for three days prior to admission . However , serum chemistry obtained six hours after presentation revealed her glucose was 186 mg/dL , the anion gap was still elevated at 21 , serum bicarbonate was 16 mmol/L , triglyceride level peaked at 2050 mg/dL , and lipase was 52 U/L . The β-hydroxybutyrate level was obtained and found to be elevated at 5.29 mmol/L - the original sample was centrifuged and the chylomicron layer removed prior to analysis due to interference from turbidity caused by lipemia again . The patient was treated with an insulin drip for euDKA and HTG with a reduction in the anion gap to 13 and triglycerides to 1400 mg/dL , within 24 hours . Her euDKA was thought to be precipitated by her respiratory tract infection in the setting of SGLT2 inhibitor use . The patient was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day . It was determined that all SGLT2 inhibitors should be discontinued indefinitely . She had close follow-up with endocrinology post discharge .'\n",
        "\n",
        "result = p_model.transform(spark.createDataFrame([[text]]).toDF('text'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mx1Qio5_LzKx",
        "outputId": "5228e7f4-2310-4750-f466-8ea6f67ad83d"
      },
      "source": [
        "result.select(F.explode(F.arrays_zip('token.result', 'label.result')).alias(\"cols\")) \\\n",
        "      .select(F.expr(\"cols['0']\").alias(\"token\"),\n",
        "              F.expr(\"cols['1']\").alias(\"label\")).show(50, truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+---------+\n",
            "|token       |label    |\n",
            "+------------+---------+\n",
            "|A           |O        |\n",
            "|28-year-old |O        |\n",
            "|female      |O        |\n",
            "|with        |O        |\n",
            "|a           |O        |\n",
            "|history     |O        |\n",
            "|of          |O        |\n",
            "|gestational |B-Disease|\n",
            "|diabetes    |I-Disease|\n",
            "|mellitus    |I-Disease|\n",
            "|diagnosed   |O        |\n",
            "|eight       |O        |\n",
            "|years       |O        |\n",
            "|prior       |O        |\n",
            "|to          |O        |\n",
            "|presentation|O        |\n",
            "|and         |O        |\n",
            "|subsequent  |O        |\n",
            "|type        |B-Disease|\n",
            "|two         |I-Disease|\n",
            "|diabetes    |I-Disease|\n",
            "|mellitus    |I-Disease|\n",
            "|(           |O        |\n",
            "|T2DM        |B-Disease|\n",
            "|),          |O        |\n",
            "|one         |O        |\n",
            "|prior       |O        |\n",
            "|episode     |O        |\n",
            "|of          |O        |\n",
            "|HTG-induced |B-Disease|\n",
            "|pancreatitis|I-Disease|\n",
            "|three       |O        |\n",
            "|years       |O        |\n",
            "|prior       |O        |\n",
            "|to          |O        |\n",
            "|presentation|O        |\n",
            "|,           |O        |\n",
            "|associated  |O        |\n",
            "|with        |O        |\n",
            "|an          |O        |\n",
            "|acute       |B-Disease|\n",
            "|hepatitis   |B-Disease|\n",
            "|,           |O        |\n",
            "|and         |O        |\n",
            "|obesity     |B-Disease|\n",
            "|with        |O        |\n",
            "|a           |O        |\n",
            "|body        |O        |\n",
            "|mass        |O        |\n",
            "|index       |O        |\n",
            "+------------+---------+\n",
            "only showing top 50 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPtJkXRlkHD7",
        "outputId": "9944ba16-f8fd-4332-d496-a650a32d7f98"
      },
      "source": [
        "result.select(F.explode(F.arrays_zip('ner_chunk.result', 'ner_chunk.metadata')).alias(\"cols\")) \\\n",
        "      .select(F.expr(\"cols['0']\").alias(\"chunk\"),\n",
        "              F.expr(\"cols['1']['entity']\").alias(\"ner_label\")).show(truncate=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------+---------+\n",
            "|chunk                        |ner_label|\n",
            "+-----------------------------+---------+\n",
            "|gestational diabetes mellitus|Disease  |\n",
            "|type two diabetes mellitus   |Disease  |\n",
            "|T2DM                         |Disease  |\n",
            "|HTG-induced pancreatitis     |Disease  |\n",
            "|acute                        |Disease  |\n",
            "|hepatitis                    |Disease  |\n",
            "|obesity                      |Disease  |\n",
            "|polyuria                     |Disease  |\n",
            "|polydipsia                   |Disease  |\n",
            "|poor                         |Disease  |\n",
            "|vomiting                     |Disease  |\n",
            "|respiratory tract infection  |Disease  |\n",
            "|T2DM                         |Disease  |\n",
            "|HTG                          |Disease  |\n",
            "|oral mucosa                  |Disease  |\n",
            "|lipemia                      |Disease  |\n",
            "|starvation ketosis           |Disease  |\n",
            "|lipemia                      |Disease  |\n",
            "|euDKA                        |Disease  |\n",
            "|HTG                          |Disease  |\n",
            "+-----------------------------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}