{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKbJQaSF4vTw"
   },
   "source": [
    "# RAG-Vision: Image ‚Üí Caption/Summary ‚Üí Embeddings ETL Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements a **multimodal RAG pipeline** that extracts meaning from images using Vision-Language Models (VLMs). Unlike text-only RAG pipelines, RAG-Vision enables semantic search over visual content like charts, diagrams, infographics, and screenshots.\n",
    "\n",
    "### Pipeline Flow\n",
    "\n",
    "```\n",
    "Reader2Image ‚Üí VLM Captioning ‚Üí Splitter ‚Üí Sentence Embeddings ‚Üí Database\n",
    "```\n",
    "\n",
    "### Key Use Cases\n",
    "\n",
    "- **üìä Slide Decks with Charts**: Extract meaning from presentation visuals and diagrams\n",
    "- **üìã Scanned Forms**: Process documents with visual elements and handwritten content\n",
    "- **üìà Infographics**: Analyze data visualizations and graphical information\n",
    "- **üñ•Ô∏è EHR/Portal Screenshots**: Extract information from healthcare system interfaces\n",
    "- **üìê Technical Diagrams**: Process architectural drawings, flowcharts, and schematics\n",
    "- **üé® Product Images**: Enable visual search in e-commerce catalogs\n",
    "\n",
    "### Innovation: Multimodal Retrieval\n",
    "\n",
    "Traditional RAG systems are **text-blind** to visual content. RAG-Vision solves this by:\n",
    "\n",
    "1. **VLM Captioning**: Use Qwen2-VL to generate contextual descriptions of images\n",
    "2. **Dual Storage**: Keep both VLM captions and OCR text (if available)\n",
    "3. **Rich Metadata**: Tag with `has_image=true`, `figure_id`, `slide_no`, dimensions\n",
    "4. **Semantic Embeddings**: Convert visual descriptions into searchable vectors\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Studies show that **65-70% of business documents** contain meaningful visual elements:\n",
    "- Charts and graphs conveying data trends\n",
    "- Diagrams explaining processes\n",
    "- Screenshots showing UI/UX\n",
    "- Tables with structured information\n",
    "\n",
    "**Without RAG-Vision**, these insights are lost in retrieval systems.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6q7mEmIc5Z1W",
    "outputId": "12020cd1-b68d-4eb5-a03a-23d5355cecf2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-22 21:17:57--  https://s3.us-east-1.amazonaws.com/auxdata.johnsnowlabs.com/public/tmp/sparknlp_rc/spark_nlp-6.2.0rc1-py2.py3-none-any.whl\n",
      "Resolving s3.us-east-1.amazonaws.com (s3.us-east-1.amazonaws.com)... 52.216.208.104, 16.15.202.170, 52.216.92.149, ...\n",
      "Connecting to s3.us-east-1.amazonaws.com (s3.us-east-1.amazonaws.com)|52.216.208.104|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 743293 (726K) [binary/octet-stream]\n",
      "Saving to: ‚Äòspark_nlp-6.2.0rc1-py2.py3-none-any.whl.8‚Äô\n",
      "\n",
      "spark_nlp-6.2.0rc1- 100%[===================>] 725.87K  2.16MB/s    in 0.3s    \n",
      "\n",
      "2025-10-22 21:17:58 (2.16 MB/s) - ‚Äòspark_nlp-6.2.0rc1-py2.py3-none-any.whl.8‚Äô saved [743293/743293]\n",
      "\n",
      "--2025-10-22 21:17:58--  https://s3.us-east-1.amazonaws.com/auxdata.johnsnowlabs.com/public/tmp/sparknlp_rc/spark-nlp-assembly-6.2.0-rc1.jar\n",
      "Resolving s3.us-east-1.amazonaws.com (s3.us-east-1.amazonaws.com)... 52.217.205.32, 52.216.208.104, 16.15.202.170, ...\n",
      "Connecting to s3.us-east-1.amazonaws.com (s3.us-east-1.amazonaws.com)|52.217.205.32|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 659552992 (629M) [application/java-archive]\n",
      "Saving to: ‚Äòspark-nlp-assembly-6.2.0-rc1.jar.2‚Äô\n",
      "\n",
      "spark-nlp-assembly- 100%[===================>] 629.00M  1.27MB/s    in 8m 13s  \n",
      "\n",
      "2025-10-22 21:26:12 (1.28 MB/s) - ‚Äòspark-nlp-assembly-6.2.0-rc1.jar.2‚Äô saved [659552992/659552992]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the Spark NLP Python wheel\n",
    "!wget https://s3.us-east-1.amazonaws.com/auxdata.johnsnowlabs.com/public/tmp/sparknlp_rc/spark_nlp-6.2.0rc1-py2.py3-none-any.whl\n",
    "\n",
    "# Download the Spark NLP assembly JAR\n",
    "!wget https://s3.us-east-1.amazonaws.com/auxdata.johnsnowlabs.com/public/tmp/sparknlp_rc/spark-nlp-assembly-6.2.0-rc1.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vSxIzaoJ5bw2",
    "outputId": "e5995baf-3b4e-401b-bf6c-40f1f8e25e26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./spark_nlp-6.2.0rc1-py2.py3-none-any.whl\n",
      "spark-nlp is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
     ]
    }
   ],
   "source": [
    "!pip install spark_nlp-6.2.0rc1-py2.py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "llNo2xwo5eLA"
   },
   "source": [
    "Downlading Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Yro9v4Tr5dpV"
   },
   "outputs": [],
   "source": [
    "base_url = \"https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/training-spark-nlp-v6-readers/tutorials/Certification_Trainings/Public/data/readers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nXL372kS5hTv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‚Äòpdf-files‚Äô: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir pdf-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qx26WVWW5mRB",
    "outputId": "cb11eb14-769b-4a17-d10e-22c893a04f57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-22 21:26:12--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/training-spark-nlp-v6-readers/tutorials/Certification_Trainings/Public/data/readers/pdf-with-2images.pdf\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8000::154, 2606:50c0:8003::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 130182 (127K) [application/octet-stream]\n",
      "Saving to: ‚Äòpdf-files/pdf-with-2images.pdf.1‚Äô\n",
      "\n",
      "pdf-with-2images.pd 100%[===================>] 127.13K  --.-KB/s    in 0.03s   \n",
      "\n",
      "2025-10-22 21:26:13 (4.26 MB/s) - ‚Äòpdf-files/pdf-with-2images.pdf.1‚Äô saved [130182/130182]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"{base_url}/pdf-with-2images.pdf\" -P pdf-files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPOsegtS4vTy"
   },
   "source": [
    "## üì¶ Step 0: Import Dependencies\n",
    "\n",
    "Import specialized libraries for multimodal processing:\n",
    "\n",
    "- **PySpark**: Distributed data processing\n",
    "- **Spark NLP**: Text processing components\n",
    "- **Reader2Image**: Image ingestion from various sources\n",
    "- **Qwen2VLTransformer**: Vision-Language Model for image understanding\n",
    "- **BertSentenceEmbeddings**: Convert captions to searchable vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zbdsgW1Z4vTy",
    "outputId": "0b2547e2-2786-4cae-c407-087f36b1553c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All dependencies imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import PySpark dependencies\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Import Spark NLP components\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import (\n",
    "    SentenceDetector,\n",
    "    BertSentenceEmbeddings\n",
    ")\n",
    "\n",
    "# Vision-Language Model for image captioning\n",
    "from sparknlp.annotator import Qwen2VLTransformer\n",
    "\n",
    "# Image reader for various formats\n",
    "from sparknlp.reader.reader2image import Reader2Image\n",
    "\n",
    "print(\"‚úÖ All dependencies imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IDlN82H4vTz"
   },
   "source": [
    "## üöÄ Step 1: Initialize Spark Session\n",
    "\n",
    "Configure Spark with enhanced settings for image processing:\n",
    "\n",
    "**Key Configurations**:\n",
    "- **Driver Memory**: 16GB (images are memory-intensive)\n",
    "- **Max Result Size**: 2GB (for large image batches)\n",
    "- **Serializer**: Kryo (efficient for binary data)\n",
    "- **Spark NLP JAR**: Required for all annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_iJqHhwc4vTz",
    "outputId": "1b94158b-d67f-4c2d-d51d-105d9e4c0d4b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/22 21:26:14 WARN Utils: Your hostname, danilo-ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.100.75 instead (on interface enp131s0)\n",
      "25/10/22 21:26:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/10/22 21:26:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Spark session initialized successfully\n"
     ]
    }
   ],
   "source": [
    "def get_spark_session():\n",
    "    \"\"\"\n",
    "    Create and configure a Spark session optimized for image processing.\n",
    "\n",
    "    Returns:\n",
    "        SparkSession: Configured Spark session for RAG-Vision pipeline\n",
    "    \"\"\"\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"RAG-Vision: Image ‚Üí Caption ‚Üí Embeddings ETL\") \\\n",
    "        .config(\"spark.driver.memory\", \"16G\") \\\n",
    "        .config(\"spark.driver.maxResultSize\", \"2000M\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.jars\",\n",
    "                \"../jars/spark-nlp-assembly-6.2.0-rc1.jar\")  # ‚ö†Ô∏è Update this path!\n",
    "\n",
    "    return builder.getOrCreate()\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = get_spark_session()\n",
    "print(\"‚úÖ Spark session initialized successfully\")\n",
    "\n",
    "# Create empty DataFrame for Reader2Image initialization\n",
    "empty_df = spark.createDataFrame([], \"string\").toDF(\"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLwJtJE94vTz"
   },
   "source": [
    "## üì∏ Step 2: Image Ingestion with Reader2Image\n",
    "\n",
    "**Reader2Image** is a specialized component that extracts images from various sources:\n",
    "\n",
    "### Supported Formats:\n",
    "- **Direct Images**: PNG, JPG, JPEG, TIFF, BMP\n",
    "- **PDF Documents**: Extracts embedded images from PDFs\n",
    "- **PowerPoint**: Extracts slides as images from PPTX files\n",
    "- **Directories**: Batch process entire folders\n",
    "\n",
    "### Output Annotations:\n",
    "Each image becomes an `AnnotationImage` with rich metadata:\n",
    "- **source/path**: Original file location\n",
    "- **width/height**: Image dimensions\n",
    "- **nChannels**: Color channels (1=grayscale, 3=RGB, 4=RGBA)\n",
    "- **origin**: Byte array of image data\n",
    "\n",
    "### Configuration:\n",
    "- **Path-based mode**: Read from file system (shown below)\n",
    "- **Column-based mode**: Read from DataFrame with binary image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8iVFNmmc4vTz",
    "outputId": "adaf7161-29d3-4a75-b482-cfb3a2130331"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Reader2Image configured to read from: ./pdf-files/pdf-with-2images.pdf\n",
      "\n",
      "üìù Supported content types:\n",
      "   ‚Ä¢ 'image/png' - PNG images\n",
      "   ‚Ä¢ 'image/jpeg' - JPG/JPEG images\n",
      "   ‚Ä¢ 'application/pdf' - PDF documents\n",
      "   ‚Ä¢ 'application/vnd.ms-powerpoint' - PowerPoint presentations\n",
      "\n",
      "‚úÖ Images loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Number of images extracted: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Configure input path for images\n",
    "# Options:\n",
    "#   - Single image file: \"datasets/chart.png\"\n",
    "#   - Image directory: \"datasets/images/\"\n",
    "#   - PDF with images: \"datasets/presentation.pdf\"\n",
    "#   - PowerPoint: \"datasets/slides.pptx\"\n",
    "\n",
    "images_path = \"./pdf-files/pdf-with-2images.pdf\"\n",
    "\n",
    "# Initialize Reader2Image\n",
    "reader2image = Reader2Image() \\\n",
    "    .setContentType(\"application/pdf\") \\\n",
    "    .setContentPath(images_path) \\\n",
    "    .setOutputCol(\"image\") \\\n",
    "    .setUserMessage(\"Describe the image with 5 to 6 words.\")\n",
    "\n",
    "print(f\"‚úÖ Reader2Image configured to read from: {images_path}\")\n",
    "print(\"\\nüìù Supported content types:\")\n",
    "print(\"   ‚Ä¢ 'image/png' - PNG images\")\n",
    "print(\"   ‚Ä¢ 'image/jpeg' - JPG/JPEG images\")\n",
    "print(\"   ‚Ä¢ 'application/pdf' - PDF documents\")\n",
    "print(\"   ‚Ä¢ 'application/vnd.ms-powerpoint' - PowerPoint presentations\")\n",
    "\n",
    "# Execute Reader2Image to load images\n",
    "df_in = empty_df\n",
    "df_images = Pipeline(stages=[reader2image]).fit(df_in).transform(df_in)\n",
    "\n",
    "print(\"\\n‚úÖ Images loaded successfully\")\n",
    "print(f\"üìä Number of images extracted: {df_images.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EM27UwGIqqDH",
    "outputId": "5e3a2972-9d58-4dd4-eed7-8d4e0ac461f6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+\n",
      "|            fileName|               image|exception|\n",
      "+--------------------+--------------------+---------+\n",
      "|pdf-with-2images.pdf|[{image, pdf-with...|     NULL|\n",
      "|pdf-with-2images.pdf|[{image, pdf-with...|     NULL|\n",
      "+--------------------+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_images.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRfV_8A84vTz"
   },
   "source": [
    "## ü§ñ Step 3: Vision-Language Model Captioning\n",
    "\n",
    "This is the **core innovation** of RAG-Vision. We use **Qwen2-VL**, a state-of-the-art Vision-Language Model, to generate contextual descriptions of images.\n",
    "\n",
    "### What is Qwen2-VL?\n",
    "\n",
    "**Qwen2-VL** is a multimodal transformer that:\n",
    "- **Understands Visual Content**: Recognizes objects, text, charts, diagrams\n",
    "- **Generates Captions**: Creates natural language descriptions\n",
    "- **Context-Aware**: Captures semantic meaning, not just objects\n",
    "- **Chart-Aware**: Can describe axes, labels, trends in data visualizations\n",
    "\n",
    "### How It Works:\n",
    "1. **Vision Encoder**: Processes image pixels into visual features\n",
    "2. **Language Decoder**: Generates text from visual features\n",
    "3. **Cross-Attention**: Aligns visual and textual representations\n",
    "\n",
    "### Example Outputs:\n",
    "- **Chart**: \"A bar chart showing quarterly revenue growth from Q1 to Q4, with Q4 showing the highest value at $2.3M\"\n",
    "- **Diagram**: \"A flowchart depicting the customer onboarding process with 5 steps from registration to activation\"\n",
    "- **Form**: \"A medical intake form with sections for patient information, insurance details, and medical history\"\n",
    "\n",
    "### Configuration Options:\n",
    "- **Prompt Engineering**: Guide the model with specific instructions\n",
    "- **Temperature**: Control output randomness (0.0 = deterministic)\n",
    "- **Max Length**: Limit caption length for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NOyCSdii4vTz",
    "outputId": "bdf8a556-7b9b-40fc-cde3-f6ad5e7f4388"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen2_vl_2b_instruct_int4 download started this may take some time.\n",
      "Approximate size to download 1.4 GB\n",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/22 21:26:40 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n",
      "25/10/22 21:26:40 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwen2_vl_2b_instruct_int4 download started this may take some time.\n",
      "Approximate size to download 1.4 GB\n",
      "Download done! Loading the resource.\n",
      "[OK!]\n",
      "‚úÖ Qwen2-VL configured for image captioning\n",
      "\n",
      "üéØ VLM Capabilities:\n",
      "   ‚Ä¢ Object Recognition: Identifies objects, people, scenes\n",
      "   ‚Ä¢ OCR: Reads text within images\n",
      "   ‚Ä¢ Chart Understanding: Describes data visualizations\n",
      "   ‚Ä¢ Contextual Descriptions: Captures semantic meaning\n",
      "   ‚Ä¢ Spatial Relationships: Understands layout and positioning\n",
      "\n",
      "üí° Best Practices:\n",
      "   ‚Ä¢ Use clear prompts for specific domains (medical, technical, etc.)\n",
      "   ‚Ä¢ Set temperature=0.0 for consistent captions\n",
      "   ‚Ä¢ Keep captions retrieval-oriented (avoid creative descriptions)\n"
     ]
    }
   ],
   "source": [
    "# Configure Qwen2-VL for image captioning\n",
    "vlm = Qwen2VLTransformer.pretrained() \\\n",
    "    .setInputCols(\"image\") \\\n",
    "    .setOutputCol(\"vlm_caption\")\n",
    "\n",
    "print(\"‚úÖ Qwen2-VL configured for image captioning\")\n",
    "print(\"\\nüéØ VLM Capabilities:\")\n",
    "print(\"   ‚Ä¢ Object Recognition: Identifies objects, people, scenes\")\n",
    "print(\"   ‚Ä¢ OCR: Reads text within images\")\n",
    "print(\"   ‚Ä¢ Chart Understanding: Describes data visualizations\")\n",
    "print(\"   ‚Ä¢ Contextual Descriptions: Captures semantic meaning\")\n",
    "print(\"   ‚Ä¢ Spatial Relationships: Understands layout and positioning\")\n",
    "\n",
    "print(\"\\nüí° Best Practices:\")\n",
    "print(\"   ‚Ä¢ Use clear prompts for specific domains (medical, technical, etc.)\")\n",
    "print(\"   ‚Ä¢ Set temperature=0.0 for consistent captions\")\n",
    "print(\"   ‚Ä¢ Keep captions retrieval-oriented (avoid creative descriptions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joM77BpO4vTz"
   },
   "source": [
    "## ‚úÇÔ∏è Step 4: Caption Splitting and Sentence Detection\n",
    "\n",
    "VLM-generated captions may contain multiple sentences or concepts. We split them for granular embedding:\n",
    "\n",
    "### Why Split Captions?\n",
    "1. **Granular Retrieval**: Each sentence becomes a searchable chunk\n",
    "2. **Better Matching**: Specific queries match specific caption parts\n",
    "3. **Embedding Quality**: Shorter text ‚Üí better embedding coherence\n",
    "\n",
    "### SentenceDetector Features:\n",
    "- Linguistic rules for accurate segmentation\n",
    "- Abbreviation handling (Dr., Inc., Fig., etc.)\n",
    "- Context-aware boundary detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6Kgt9Kh4vT0",
    "outputId": "60781743-78a8-4d02-b6e7-cfc198ff1160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sentence detector configured for caption splitting\n",
      "\n",
      "üìù Example caption splitting:\n",
      "   Input: 'This chart shows revenue trends. Q4 had the highest sales.'\n",
      "   Output:\n",
      "      ‚Ä¢ 'This chart shows revenue trends.'\n",
      "      ‚Ä¢ 'Q4 had the highest sales.'\n"
     ]
    }
   ],
   "source": [
    "# Configure sentence detection for captions\n",
    "splitter = SentenceDetector() \\\n",
    "    .setInputCols(\"vlm_caption\") \\\n",
    "    .setOutputCol(\"sentences\") \\\n",
    "    .setUseAbbreviations(True)  # Handle Fig., Dr., etc.\n",
    "\n",
    "print(\"‚úÖ Sentence detector configured for caption splitting\")\n",
    "print(\"\\nüìù Example caption splitting:\")\n",
    "print(\"   Input: 'This chart shows revenue trends. Q4 had the highest sales.'\")\n",
    "print(\"   Output:\")\n",
    "print(\"      ‚Ä¢ 'This chart shows revenue trends.'\")\n",
    "print(\"      ‚Ä¢ 'Q4 had the highest sales.'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nPfoITb4vT0"
   },
   "source": [
    "## üéØ Step 5: Generate Sentence Embeddings from Captions\n",
    "\n",
    "Convert VLM-generated captions into **dense vector representations** for semantic search:\n",
    "\n",
    "### Model: BERT Sentence Embeddings\n",
    "- **Model**: `sent_small_bert_L2_128`\n",
    "- **Dimension**: 128 (good balance of speed/quality)\n",
    "- **Advantages**: Fast inference, semantic understanding\n",
    "\n",
    "### Why Embed Captions?\n",
    "1. **Semantic Search**: Find images by meaning, not keywords\n",
    "2. **Cross-Modal Retrieval**: Text queries ‚Üí Image results\n",
    "3. **Similarity Ranking**: Measure relevance scores\n",
    "\n",
    "### Domain-Specific Models:\n",
    "- **General**: `all-mpnet-base-v2` (highest quality)\n",
    "- **Multilingual**: `labse` (50+ languages)\n",
    "- **Medical**: `biobert_pubmed_base_cased`\n",
    "- **Technical**: `scibert_scivocab_uncased`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ML_S-5z4vT0",
    "outputId": "2060b6f2-09c3-4897-f2a4-467b07aa9f80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_small_bert_L2_128 download started this may take some time.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/22 21:26:50 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate size to download 16.1 MB\n",
      "[ | ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/22 21:26:51 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n",
      "25/10/22 21:26:51 WARN S3AbortableInputStream: Not all bytes were read from the S3ObjectInputStream, aborting HTTP connection. This is likely an error and may result in sub-optimal behavior. Request only the bytes you need via a ranged GET or drain the input stream after use.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sent_small_bert_L2_128 download started this may take some time.\n",
      "Approximate size to download 16.1 MB\n",
      "Download done! Loading the resource.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ / ]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-22 21:26:53.753554: I external/org_tensorflow/tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK!]\n",
      "‚úÖ BERT sentence embeddings configured\n",
      "   ‚Ä¢ Model: sent_small_bert_L2_128\n",
      "   ‚Ä¢ Dimension: 128\n",
      "   ‚Ä¢ Language: English\n",
      "\n",
      "üí° Alternative models for different use cases:\n",
      "   ‚Ä¢ 'all-mpnet-base-v2': Highest quality (384 dim)\n",
      "   ‚Ä¢ 'labse': Multilingual support (768 dim)\n",
      "   ‚Ä¢ 'clip-vit-base-patch32': Native image-text alignment\n"
     ]
    }
   ],
   "source": [
    "# Configure BERT sentence embeddings\n",
    "emb = BertSentenceEmbeddings.pretrained(\"sent_small_bert_L2_128\", \"en\") \\\n",
    "    .setInputCols(\"sentences\") \\\n",
    "    .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "print(\"‚úÖ BERT sentence embeddings configured\")\n",
    "print(\"   ‚Ä¢ Model: sent_small_bert_L2_128\")\n",
    "print(\"   ‚Ä¢ Dimension: 128\")\n",
    "print(\"   ‚Ä¢ Language: English\")\n",
    "\n",
    "print(\"\\nüí° Alternative models for different use cases:\")\n",
    "print(\"   ‚Ä¢ 'all-mpnet-base-v2': Highest quality (384 dim)\")\n",
    "print(\"   ‚Ä¢ 'labse': Multilingual support (768 dim)\")\n",
    "print(\"   ‚Ä¢ 'clip-vit-base-patch32': Native image-text alignment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uD1A_1WJ4vT0"
   },
   "source": [
    "## üîÑ Step 6: Build Complete Vision Pipeline\n",
    "\n",
    "Combine all stages into a cohesive multimodal processing pipeline:\n",
    "\n",
    "### Pipeline Stages:\n",
    "1. **Qwen2VLTransformer** ‚Üí Generate image captions\n",
    "2. **SentenceDetector** ‚Üí Split captions into sentences\n",
    "3. **BertSentenceEmbeddings** ‚Üí Create vector representations\n",
    "\n",
    "**Note**: Reader2Image was already executed separately to load images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KsvQ96vP4vT0",
    "outputId": "e2253740-5166-476e-8633-f8546a6c664a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß RAG-Vision pipeline constructed with 3 stages:\n",
      "   1. Qwen2VLTransformer - Image captioning\n",
      "   2. SentenceDetector - Caption splitting\n",
      "   3. BertSentenceEmbeddings - Vector generation\n",
      "\n",
      "‚úÖ Pipeline ready for execution\n"
     ]
    }
   ],
   "source": [
    "# Build the vision processing pipeline\n",
    "vision_pipe = Pipeline(stages=[\n",
    "    vlm,        # 1. VLM captioning\n",
    "    splitter,   # 2. Sentence splitting\n",
    "    emb         # 3. Embedding generation\n",
    "])\n",
    "\n",
    "print(\"üîß RAG-Vision pipeline constructed with 3 stages:\")\n",
    "print(\"   1. Qwen2VLTransformer - Image captioning\")\n",
    "print(\"   2. SentenceDetector - Caption splitting\")\n",
    "print(\"   3. BertSentenceEmbeddings - Vector generation\")\n",
    "print(\"\\n‚úÖ Pipeline ready for execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uDpDJVOC4vT0"
   },
   "source": [
    "## ‚öôÔ∏è Step 7: Execute Vision Pipeline\n",
    "\n",
    "Run the pipeline to generate captions and embeddings for all images:\n",
    "\n",
    "### What Happens:\n",
    "1. **VLM Processing**: Each image is analyzed and captioned\n",
    "2. **Caption Splitting**: Long descriptions are split into sentences\n",
    "3. **Embedding Generation**: Each sentence becomes a 128-dimensional vector\n",
    "\n",
    "**‚ö†Ô∏è Note**: VLM inference can be slow (several seconds per image). For large datasets, consider:\n",
    "- GPU acceleration\n",
    "- Batch processing\n",
    "- Parallel execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTDqUoJC4vT0",
    "outputId": "565af911-9d29-4ed2-b9c4-0dbe55b715df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Executing RAG-Vision pipeline...\n",
      "   ‚ö†Ô∏è VLM processing may take several minutes for the first run\n",
      "   (downloading models and processing images)\n",
      "\n",
      "‚úÖ Pipeline execution complete!\n",
      "\n",
      "üìä Result DataFrame columns:\n",
      "   ‚Ä¢ vlm_caption: Generated captions\n",
      "   ‚Ä¢ sentences: Split caption sentences\n",
      "   ‚Ä¢ sentence_embeddings: Vector representations\n"
     ]
    }
   ],
   "source": [
    "# Execute the vision pipeline\n",
    "print(\"üîÑ Executing RAG-Vision pipeline...\")\n",
    "print(\"   ‚ö†Ô∏è VLM processing may take several minutes for the first run\")\n",
    "print(\"   (downloading models and processing images)\\n\")\n",
    "\n",
    "df_vec = vision_pipe.fit(df_images).transform(df_images)\n",
    "\n",
    "print(\"‚úÖ Pipeline execution complete!\")\n",
    "print(\"\\nüìä Result DataFrame columns:\")\n",
    "print(f\"   ‚Ä¢ vlm_caption: Generated captions\")\n",
    "print(f\"   ‚Ä¢ sentences: Split caption sentences\")\n",
    "print(f\"   ‚Ä¢ sentence_embeddings: Vector representations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "7bSQKidCqd6n"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|            fileName|               image|exception|         vlm_caption|           sentences| sentence_embeddings|\n",
      "+--------------------+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|pdf-with-2images.pdf|[{image, pdf-with...|     NULL|[{document, 0, 57...|[{document, 0, 57...|[{sentence_embedd...|\n",
      "|pdf-with-2images.pdf|[{image, pdf-with...|     NULL|[{document, 0, 50...|[{document, 0, 50...|[{sentence_embedd...|\n",
      "+--------------------+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_vec.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMBSJpzV4vT0"
   },
   "source": [
    "## üìä Step 10: Flatten to Chunk-Embedding Pairs\n",
    "\n",
    "Transform nested structure into flat format for database storage:\n",
    "\n",
    "### Flattening Process:\n",
    "1. **Extract Embeddings**: Pull float arrays from annotation objects\n",
    "2. **Zip Pairs**: Combine sentence text with corresponding embeddings\n",
    "3. **Explode**: Create one row per (caption_sentence, embedding) pair\n",
    "4. **Attach Metadata**: Include image properties with each row\n",
    "\n",
    "### Result Schema:\n",
    "```\n",
    "chunk_text: string              # Caption sentence\n",
    "embedding: array<float>         # 128-dimensional vector\n",
    "image_uri: string               # Source image path\n",
    "figure_id: string               # Unique figure identifier\n",
    "slide_no: string                # Slide number (nullable)\n",
    "img_w, img_h, img_c: integer    # Image dimensions\n",
    "ocr_text: string                # OCR text (nullable)\n",
    "has_image: boolean              # Always true\n",
    "pipeline: string                # \"rag_vision_qwen2vl\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jJmS557B4vT1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Flattening data to chunk-embedding pairs...\n",
      "\n",
      "‚úÖ Data flattened to chunk-embedding pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|                                                chunk_text|                                                                                           embedding|\n",
      "+----------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "|A chocolate doughnut with sprinkles on a light background.|[4.3523312E-4, -0.47155252, -0.018804293, -0.8252269, -0.14540172, 1.0212879, 0.39352003, 0.20743...|\n",
      "|       A blue rocket with a star on it, floating in space.|[-0.089734964, -0.11982616, -0.26973015, -1.2441063, -0.032506846, 0.49904716, -0.89480877, 0.662...|\n",
      "+----------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"üìä Flattening data to chunk-embedding pairs...\\n\")\n",
    "\n",
    "# Extract raw float arrays from embeddings\n",
    "df_vec = df_vec.withColumn(\n",
    "    \"emb_vecs\",\n",
    "    F.expr(\"transform(sentence_embeddings, x -> x.embeddings)\")\n",
    ")\n",
    "\n",
    "# Zip sentences with their embeddings\n",
    "df_vec = df_vec.withColumn(\n",
    "    \"pairs\",\n",
    "    F.arrays_zip(F.col(\"sentences.result\"), F.col(\"emb_vecs\"))\n",
    ")\n",
    "\n",
    "# Explode to one row per chunk\n",
    "df_chunks = df_vec \\\n",
    "    .withColumn(\"pair\", F.explode_outer(\"pairs\")) \\\n",
    "    .select(\n",
    "        F.col(\"pair.result\").alias(\"chunk_text\"),\n",
    "        F.col(\"pair.emb_vecs\").alias(\"embedding\")\n",
    "    ) \\\n",
    "    .dropna(subset=[\"chunk_text\", \"embedding\"])\n",
    "\n",
    "print(\"‚úÖ Data flattened to chunk-embedding pairs\")\n",
    "df_chunks.select(\"chunk_text\", \"embedding\").show(5, truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjkWbFFT4vT1"
   },
   "source": [
    "## üíæ Step 11: Persist to Storage\n",
    "\n",
    "Save processed embeddings and metadata to **Parquet format**:\n",
    "\n",
    "### Storage Benefits:\n",
    "- **Columnar Format**: Efficient querying by metadata fields\n",
    "- **Compression**: Reduce storage costs\n",
    "- **Schema Preservation**: Maintain data types and structure\n",
    "- **Fast Reads**: Optimized for vector database ingestion\n",
    "\n",
    "### Next Steps:\n",
    "1. Load Parquet into vector database (Pinecone, Weaviate, Milvus)\n",
    "2. Index embeddings for similarity search\n",
    "3. Build multimodal RAG application\n",
    "4. Enable visual search capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Oqkwpx984vT1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving results to: datasets/rag_vision_qwen2vl.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Successfully saved 2 chunks to: datasets/rag_vision_qwen2vl.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Define output path\n",
    "out_path = \"datasets/rag_vision_qwen2vl.parquet\"\n",
    "\n",
    "# Save to Parquet\n",
    "print(f\"üíæ Saving results to: {out_path}\")\n",
    "df_chunks.write.mode(\"overwrite\").parquet(out_path)\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully saved {df_chunks.count()} chunks to: {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfRtonRe4vT1"
   },
   "source": [
    "## üéì Key Takeaways and Best Practices\n",
    "\n",
    "### What We Accomplished:\n",
    "1. ‚úÖ Built a multimodal RAG pipeline for visual content\n",
    "2. ‚úÖ Used VLM (Qwen2-VL) to generate contextual image captions\n",
    "3. ‚úÖ Created searchable embeddings from visual descriptions\n",
    "4. ‚úÖ Preserved rich metadata for traceability\n",
    "5. ‚úÖ Enabled dual storage (VLM captions + OCR text)\n",
    "6. ‚úÖ Prepared data for vector database ingestion\n",
    "\n",
    "### RAG-Vision vs Text-Only RAG:\n",
    "\n",
    "| Aspect | Text-Only RAG | RAG-Vision |\n",
    "|--------|---------------|------------|\n",
    "| **Content Coverage** | Text only | Text + Images |\n",
    "| **Chart Understanding** | ‚ùå Blind | ‚úÖ VLM-powered |\n",
    "| **Visual Search** | ‚ùå Not possible | ‚úÖ Enabled |\n",
    "| **Metadata** | Text-based | Image properties |\n",
    "| **Cost** | Lower | Higher (VLM) |\n",
    "| **Complexity** | Simple | Moderate |\n",
    "\n",
    "### When to Combine Pipelines:\n",
    "\n",
    "#### Hybrid Strategy: Best Results\n",
    "- Use **RAG-Base** for text content\n",
    "- Use **RAG-Vision** for images/charts\n",
    "- Use **RAG-Boost** for executive summaries\n",
    "- **Store all in one database** with `has_image` flag\n",
    "\n",
    "### Production Considerations:\n",
    "\n",
    "#### 1. VLM Selection\n",
    "- **Qwen2-VL**: Best for general images and charts\n",
    "- **GPT-4V**: Higher quality, higher cost\n",
    "- **LLaVA**: Open-source alternative\n",
    "- **Gemini Vision**: Google's multimodal model\n",
    "\n",
    "#### 2. OCR Integration\n",
    "- **When to add OCR**:\n",
    "  - ‚úÖ Forms with text fields\n",
    "  - ‚úÖ Charts with labels and legends\n",
    "  - ‚úÖ Scanned documents\n",
    "  - ‚úÖ Screenshots with UI text\n",
    "\n",
    "#### 3. Metadata Strategy\n",
    "- **Always include**:\n",
    "  - `image_uri`: Source traceability\n",
    "  - `has_image`: Filter flag\n",
    "  - `figure_id`: Unique identifier\n",
    "- **Domain-specific**:\n",
    "  - Medical: `patient_id`, `modality`, `body_part`\n",
    "  - E-commerce: `product_id`, `category`, `color`\n",
    "  - Technical: `diagram_type`, `system`, `version`\n",
    "\n",
    "#### 4. Quality Assurance\n",
    "- ‚úÖ Review sample captions for accuracy\n",
    "- ‚úÖ Validate embedding dimensions\n",
    "- ‚úÖ Check metadata completeness\n",
    "- ‚úÖ Test retrieval quality with sample queries\n",
    "- ‚úÖ Monitor VLM hallucinations\n",
    "\n",
    "#### 5. Performance Optimization\n",
    "- **GPU Acceleration**: 10-100x faster VLM inference\n",
    "- **Batch Processing**: Process multiple images simultaneously\n",
    "- **Caching**: Store captions to avoid reprocessing\n",
    "- **Async Processing**: Don't block on VLM calls\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvNEeiuE4vT1"
   },
   "source": [
    "## üöÄ Next Steps: Building Multimodal RAG\n",
    "\n",
    "### Step 1: Load to Vector Database\n",
    "\n",
    "```python\n",
    "# Example: Loading to Pinecone\n",
    "import pinecone\n",
    "\n",
    "pinecone.init(api_key=\"your-api-key\")\n",
    "index = pinecone.Index(\"multimodal-rag\")\n",
    "\n",
    "# Load embeddings\n",
    "df = spark.read.parquet(\"datasets/rag_vision_qwen2vl.parquet\")\n",
    "\n",
    "for row in df.collect():\n",
    "    index.upsert(\n",
    "        vectors=[(\n",
    "            f\"{row.figure_id}_{hash(row.chunk_text)}\",\n",
    "            row.embedding,\n",
    "            {\n",
    "                \"text\": row.chunk_text,\n",
    "                \"image_uri\": row.image_uri,\n",
    "                \"figure_id\": row.figure_id,\n",
    "                \"slide_no\": row.slide_no,\n",
    "                \"has_image\": row.has_image,\n",
    "                \"ocr_text\": row.ocr_text\n",
    "            }\n",
    "        )]\n",
    "    )\n",
    "```\n",
    "\n",
    "### Step 2: Implement Visual Search\n",
    "\n",
    "```python\n",
    "def visual_search(query: str, top_k: int = 5, filter_images_only: bool = False):\n",
    "    # Embed query\n",
    "    query_embedding = model.encode(query)\n",
    "    \n",
    "    # Build filter\n",
    "    filter_dict = {\"has_image\": True} if filter_images_only else None\n",
    "    \n",
    "    # Search\n",
    "    results = index.query(\n",
    "        vector=query_embedding.tolist(),\n",
    "        top_k=top_k,\n",
    "        filter=filter_dict,\n",
    "        include_metadata=True\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "```\n",
    "\n",
    "### Step 3: Build Multimodal RAG\n",
    "\n",
    "```python\n",
    "def multimodal_rag(user_question: str):\n",
    "    # 1. Retrieve relevant content (text + images)\n",
    "    results = visual_search(user_question, top_k=5)\n",
    "    \n",
    "    # 2. Separate text and image results\n",
    "    text_context = []\n",
    "    image_references = []\n",
    "    \n",
    "    for match in results['matches']:\n",
    "        meta = match['metadata']\n",
    "        if meta.get('has_image'):\n",
    "            image_references.append({\n",
    "                'caption': meta['text'],\n",
    "                'image_uri': meta['image_uri'],\n",
    "                'figure_id': meta['figure_id']\n",
    "            })\n",
    "        text_context.append(meta['text'])\n",
    "    \n",
    "    # 3. Build rich context\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"[{i+1}] {text}\"\n",
    "        for i, text in enumerate(text_context)\n",
    "    ])\n",
    "    \n",
    "    # 4. Generate answer with image citations\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\":\n",
    "             \"You are a helpful assistant with access to images and charts. \"\n",
    "             \"When referencing visual content, cite the figure_id.\"},\n",
    "            {\"role\": \"user\", \"content\":\n",
    "             f\"Context:\\n{context}\\n\\nQuestion: {user_question}\"}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'answer': response.choices[0].message.content,\n",
    "        'image_references': image_references\n",
    "    }\n",
    "```\n",
    "\n",
    "### Step 4: Display Results with Images\n",
    "\n",
    "```python\n",
    "result = multimodal_rag(\"What were the Q4 sales figures?\")\n",
    "\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"\\nReferenced Images:\")\n",
    "for img_ref in result['image_references']:\n",
    "    print(f\"  ‚Ä¢ {img_ref['figure_id']}: {img_ref['caption']}\")\n",
    "    # Display image: img_ref['image_uri']\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UM7rRDHD4vT1"
   },
   "source": [
    "## üìö Additional Resources\n",
    "\n",
    "### Documentation:\n",
    "- [Spark NLP Documentation](https://nlp.johnsnowlabs.com/)\n",
    "- [Qwen2-VL Model Card](https://huggingface.co/Qwen/Qwen2-VL)\n",
    "- [Reader2Image Guide](https://nlp.johnsnowlabs.com/docs/en/readers)\n",
    "\n",
    "### Vision-Language Models:\n",
    "- [Qwen2-VL](https://github.com/QwenLM/Qwen2-VL): Open-source VLM\n",
    "- [GPT-4V](https://platform.openai.com/docs/guides/vision): OpenAI's vision model\n",
    "- [LLaVA](https://llava-vl.github.io/): Large Language and Vision Assistant\n",
    "- [CLIP](https://github.com/openai/CLIP): Contrastive image-text learning\n",
    "\n",
    "### Multimodal RAG:\n",
    "- [LangChain Multimodal](https://python.langchain.com/docs/use_cases/multimodal)\n",
    "- [LlamaIndex Vision](https://docs.llamaindex.ai/en/stable/examples/multi_modal/)\n",
    "- [Pinecone Multimodal Search](https://www.pinecone.io/learn/multimodal-search/)\n",
    "\n",
    "### OCR Tools:\n",
    "- [Spark OCR](https://nlp.johnsnowlabs.com/docs/en/ocr)\n",
    "- [Tesseract](https://github.com/tesseract-ocr/tesseract)\n",
    "- [AWS Textract](https://aws.amazon.com/textract/)\n",
    "- [Google Vision API](https://cloud.google.com/vision)\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully built a **multimodal RAG-Vision pipeline** that unlocks the semantic content of images. This enables:\n",
    "\n",
    "‚úÖ **Visual Search**: Find charts and diagrams by description  \n",
    "‚úÖ **Complete Coverage**: Index both text and visual content  \n",
    "‚úÖ **Rich Citations**: Reference figures with context  \n",
    "‚úÖ **Multimodal Q&A**: Answer questions about visual data  \n",
    "\n",
    "**Next**: Combine with RAG-Base and RAG-Boost for comprehensive enterprise RAG!\n",
    "\n",
    "**Happy Building! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Jupyter Spark Env",
   "language": "python",
   "name": "jupyter_spark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
