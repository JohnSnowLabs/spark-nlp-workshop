{"cells":[{"cell_type":"markdown","metadata":{"id":"06OYtjljPBI4"},"source":["#ðŸ§  RAG-Boost: LLM-Enriched Summaries ETL Pipeline\n","\n","## Overview\n","\n","This notebook implements a **RAG-Boost pipeline** that enhances document retrieval by enriching documents with LLM-generated summaries before chunking and embedding. This approach significantly improves semantic search quality and retrieval precision.\n","\n","### Pipeline Flow\n","\n","```\n","Reader2Doc â†’ Cleaner (optional) â†’ LLM Summary â†’ Splitter â†’ Embeddings â†’ Database\n","```\n","\n","### Key Benefits\n","\n","- **Executive Summaries**: Generate concise abstracts of lengthy reports and documents\n","- **Faster Retrieval**: Improve search performance over long or verbose documents\n","- **Enhanced Precision**: Better semantic matching through enriched context\n","- **Compliance & Playbooks**: Concise abstraction improves precision in policy documents\n","- **Knowledge Distillation**: Extract key insights from technical documentation\n","- **Context-Aware Search**: Enhanced semantic search with better context awareness\n","\n","### Architecture Highlights\n","\n","1. **Document-Level Enrichment**: Uses LLM to generate abstractive summaries and keywords\n","2. **Dual Context**: Prepends summaries to original text for enhanced semantic context\n","3. **Hallucination Mitigation**: Keeps original chunks in database for grounding\n","4. **Flexible Design**: Can be adapted to chunk-first-then-enrich for finer control\n","\n","---"]},{"cell_type":"markdown","source":["Downlading Files"],"metadata":{"id":"OsZkBL8Xh0iS"}},{"cell_type":"code","source":["base_url = \"https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/training-spark-nlp-v6-readers/tutorials/Certification_Trainings/Public/data/readers\""],"metadata":{"id":"f89A0JKzh2wi","executionInfo":{"status":"ok","timestamp":1761227625997,"user_tz":300,"elapsed":13,"user":{"displayName":"Danilo Burbano","userId":"08593331088765378019"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["!mkdir all-files\n","!mkdir all-files/word"],"metadata":{"id":"qhKq_pAoh37q","executionInfo":{"status":"ok","timestamp":1761227626269,"user_tz":300,"elapsed":263,"user":{"displayName":"Danilo Burbano","userId":"08593331088765378019"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!wget \"{base_url}/Clinical_Notes.docx\" -P all-files/word"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MKhXD98ch6do","executionInfo":{"status":"ok","timestamp":1761227626561,"user_tz":300,"elapsed":293,"user":{"displayName":"Danilo Burbano","userId":"08593331088765378019"}},"outputId":"2bfa335f-bfd5-4fe7-ab4b-6b4112c0fe4c"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["--2025-10-23 13:53:46--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/training-spark-nlp-v6-readers/tutorials/Certification_Trainings/Public/data/readers/Clinical_Notes.docx\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 37332 (36K) [application/octet-stream]\n","Saving to: â€˜all-files/word/Clinical_Notes.docxâ€™\n","\n","\rClinical_Notes.docx   0%[                    ]       0  --.-KB/s               \rClinical_Notes.docx 100%[===================>]  36.46K  --.-KB/s    in 0.004s  \n","\n","2025-10-23 13:53:46 (8.99 MB/s) - â€˜all-files/word/Clinical_Notes.docxâ€™ saved [37332/37332]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"OHFeBh5kPBI6"},"source":["## ðŸ“¦ Step 0: Import Dependencies and Setup\n","\n","We'll import all necessary libraries from **Spark NLP** for building our ETL pipeline. This includes:\n","\n","- **PySpark**: For distributed data processing\n","- **Spark NLP**: For NLP transformations (document reading, normalization, summarization, embeddings)\n","- **Reader2Doc**: For ingesting various document formats (PDF, Word, HTML, email)"]},{"cell_type":"code","source":["! wget -q http://setup.johnsnowlabs.com/colab.sh -O - | bash"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iQ0AeQDMGhM6","executionInfo":{"status":"ok","timestamp":1761227687281,"user_tz":300,"elapsed":60717,"user":{"displayName":"Danilo Burbano","userId":"08593331088765378019"}},"outputId":"0806df3a-c4fd-4bc9-e7cf-19f810701932"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing PySpark 3.4.4 and Spark NLP 6.2.0\n","setup Colab for PySpark 3.4.4 and Spark NLP 6.2.0\n","\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m311.4/311.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m743.3/743.3 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","dataproc-spark-connect 0.8.3 requires pyspark[connect]~=3.5.1, but you have pyspark 3.4.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","execution_count":5,"metadata":{"id":"mBN4C6zpPBI6","executionInfo":{"status":"ok","timestamp":1761227687838,"user_tz":300,"elapsed":549,"user":{"displayName":"Danilo Burbano","userId":"08593331088765378019"}}},"outputs":[],"source":["# Import PySpark dependencies\n","from pyspark.sql import SparkSession, functions as F, types as T\n","from pyspark.ml import Pipeline\n","\n","# Import Spark NLP components\n","from sparknlp.base import DocumentAssembler\n","from sparknlp import Finisher\n","from sparknlp.annotator import (\n","    DocumentNormalizer,\n","    SentenceDetector,\n","    T5Transformer,\n","    BertSentenceEmbeddings\n",")\n","\n","from sparknlp.base import EmbeddingsFinisher\n","# Document reader for various formats\n","from sparknlp.reader.reader2doc import Reader2Doc"]},{"cell_type":"markdown","metadata":{"id":"VGElteGgPBI7"},"source":["## ðŸš€ Step 1: Initialize Spark Session\n","\n","Configure Spark with appropriate memory settings for processing large documents.\n","\n","**Key configurations**:\n","- `spark.driver.memory`: 16GB for handling large documents\n","- `spark.kryoserializer.buffer.max`: 2GB for serialization\n","- Spark NLP JAR path (adjust to your environment)"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zxsVyL-oPBI7","executionInfo":{"status":"ok","timestamp":1761227799020,"user_tz":300,"elapsed":111177,"user":{"displayName":"Danilo Burbano","userId":"08593331088765378019"}},"outputId":"02f3dc0d-1a63-4f5d-d0e4-84e17c3f8bb2"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Spark session initialized successfully\n"]}],"source":["import sparknlp\n","\n","spark = sparknlp.start()\n","print(\"âœ… Spark session initialized successfully\")\n","\n","# Create empty DataFrame for Reader2Image initialization\n","empty_df = spark.createDataFrame([], \"string\").toDF(\"text\")"]},{"cell_type":"markdown","metadata":{"id":"jusIizx6PBI7"},"source":["## ðŸ“„ Step 2: Document Ingestion with Reader2Doc\n","\n","**Reader2Doc** is a powerful component that can ingest various document formats:\n","- PDF files\n","- Microsoft Word documents (.doc, .docx)\n","- HTML pages\n","- Email formats\n","\n","It extracts text content while preserving document structure and metadata.\n","\n","### Configuration Options:\n","- **Path-based**: Read directly from file system (shown below)\n","- **Column-based**: Read from a DataFrame column containing document content"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b9mRsJ-XPBI7","executionInfo":{"status":"ok","timestamp":1761227799416,"user_tz":300,"elapsed":389,"user":{"displayName":"Danilo Burbano","userId":"08593331088765378019"}},"outputId":"93f314b2-c826-4894-d169-267d6eb04c7e"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Reader2Doc configured to read from: all-files/word/Clinical_Notes.docx\n"]}],"source":["# Configure input document path\n","docs_path = \"all-files/word/Clinical_Notes.docx\"  # Can be a single file or directory\n","\n","# Initialize Reader2Doc\n","reader2doc = Reader2Doc() \\\n","    .setContentType(\"application/msword\") \\\n","    .setContentPath(docs_path) \\\n","    .setOutputCol(\"document\")\n","\n","# Alternative: Read from string column in DataFrame\n","# reader2doc = Reader2Doc().setInputCol(\"content\").setOutputCol(\"document\")\n","\n","df_in = empty_df  # Triggers Reader2Doc to read from paths\n","\n","print(f\"âœ… Reader2Doc configured to read from: {docs_path}\")"]},{"cell_type":"markdown","metadata":{"id":"FzJ4hoKTPBI8"},"source":["## ðŸ§¹ Step 3: Text Cleaning and Normalization (Optional)\n","\n","The **DocumentNormalizer** performs minimal preprocessing to clean the text:\n","\n","- Removes bullet points and list markers\n","- Normalizes whitespace\n","\n","**Note**: We keep preprocessing minimal to preserve the semantic content for LLM summarization."]},{"cell_type":"markdown","source":["## ðŸ§¹ New: `autoMode` and `presetPattern` in `DocumentNormalizer`"],"metadata":{"id":"SvKqjbAISv9-"}},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EWcF6OQWPBI8","executionInfo":{"status":"ok","timestamp":1761227799475,"user_tz":300,"elapsed":71,"user":{"displayName":"Danilo Burbano","userId":"08593331088765378019"}},"outputId":"1d75b0e4-d66a-4087-b091-158950a18020"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Document normalizer configured\n"]}],"source":["# Configure text cleaner\n","cleaner = DocumentNormalizer() \\\n","    .setInputCols(\"document\") \\\n","    .setOutputCol(\"normalized_document\") \\\n","    .setAutoMode(\"DOCUMENT_CLEAN\")\n","\n","print(\"âœ… Document normalizer configured\")"]},{"cell_type":"markdown","source":["\n","The `DocumentNormalizer` now supports **automatic cleaning modes** that let you easily remove unwanted characters, bullets, punctuation, and other formatting issues â€” **without writing custom regex rules**.\n","\n","You can now use:\n","\n","- **`presetPattern`** â†’ to apply a *single* specific cleaning rule (for example `\"CLEAN_BULLETS\"` or `\"REMOVE_PUNCTUATION\"`).  \n","- **`autoMode`** â†’ to apply a *group of related cleaning functions* automatically."],"metadata":{"id":"NfQdgPjGSz9m"}},{"cell_type":"markdown","source":["### âš™ï¸ Available Auto Modes\n","\n","| Auto Mode | What it does under the hood |\n","|------------|------------------------------|\n","| **`LIGHT_CLEAN`** | Removes extra whitespace and trailing punctuation. |\n","| **`DOCUMENT_CLEAN`** | Cleans bullets (`â€¢`), ordered bullets (`1.` or `a.`), dashes, and extra whitespace â€” great for document-style text. |\n","| **`SOCIAL_CLEAN`** | Removes punctuation, dashes, and extra whitespace â€” ideal for tweets, posts, or chat text. |\n","| **`HTML_CLEAN`** | Replaces Unicode symbols, removes non-ASCII characters, and decodes HTML entities like `&copy;` â†’ `Â©`. |\n","| **`FULL_AUTO`** | Applies *all* cleaning functions together for maximum normalization. |"],"metadata":{"id":"T--6XyT0S2i-"}},{"cell_type":"markdown","metadata":{"id":"2JrzWVjtPBI8"},"source":["## ðŸ¤– Step 4: LLM-Based Summarization\n","\n","This is the **core innovation** of the RAG-Boost pipeline. We use a **T5 Transformer** to generate:\n","- Abstractive summaries of documents\n","- Key phrases and concepts\n","\n","### Why This Matters:\n","1. **Context Enrichment**: Summaries provide high-level context for each chunk\n","2. **Better Embeddings**: Enriched text creates more meaningful vector representations\n","3. **Improved Retrieval**: Search queries match better against summaries + content\n","\n","### Process:\n","1. Generate summary using T5 (max 256 tokens)\n","2. Finish annotations to extract clean text\n","3. Prepend summary to original document for dual-context embedding"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rqt4t5m0PBI8","executionInfo":{"status":"ok","timestamp":1761227875338,"user_tz":300,"elapsed":75858,"user":{"displayName":"Danilo Burbano","userId":"08593331088765378019"}},"outputId":"de440e82-c0a1-47ec-9716-ccfdb7215c86"},"outputs":[{"output_type":"stream","name":"stdout","text":["t5_small download started this may take some time.\n","Approximate size to download 241.9 MB\n","[OK!]\n","âœ… T5 summarizer configured\n"]}],"source":["# Configure T5 summarizer\n","summarizer = T5Transformer.pretrained(\"t5_small\", \"en\") \\\n","    .setTask(\"summarize:\") \\\n","    .setInputCols(\"normalized_document\") \\\n","    .setOutputCol(\"summary\") \\\n","    .setMaxOutputLength(256)  # Adjust based on your needs\n","\n","print(\"âœ… T5 summarizer configured\")"]},{"cell_type":"markdown","metadata":{"id":"Yu5R55uXPBI8"},"source":["## ðŸ”„ Step 5: Build and Execute Enrichment Pipeline\n","\n","Now we combine all the stages into a single pipeline and execute the transformation:\n","\n","1. **Read** documents\n","2. **Clean** and normalize text\n","3. **Summarize** using LLM\n","4. **Enrich** by prepending summaries to original text\n","\n","The result is an enriched document where each chunk will contain both:\n","- The LLM-generated summary (high-level context)\n","- The original text (grounding and detail)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YlE8vtL8PBI8","executionInfo":{"status":"ok","timestamp":1761228022084,"user_tz":300,"elapsed":146740,"user":{"displayName":"Danilo Burbano","userId":"08593331088765378019"}},"outputId":"d421964e-9756-4105-c633-ff1855a9d9bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ”„ Running enrichment pipeline...\n","âœ… Documents enriched with LLM summaries\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|                                                                                                                                                                                                  result|\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|[Clinical Note Summary is a comprehensive summary of the clinical notes., Overview of the world of information and information., this document summarizes key elements of the patient's recent visit ...|\n","+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","\n"]}],"source":["# Build the enrichment pipeline\n","enrich_pipe = Pipeline(stages=[\n","    reader2doc,\n","    cleaner,\n","    summarizer\n","])\n","\n","# Execute pipeline\n","print(\"ðŸ”„ Running enrichment pipeline...\")\n","enriched_df = enrich_pipe.fit(df_in).transform(df_in)\n","\n","print(\"âœ… Documents enriched with LLM summaries\")\n","enriched_df.select(\"summary.result\").show(1, truncate=200)"]},{"cell_type":"markdown","metadata":{"id":"MRcE6XqrPBI8"},"source":["## âœ‚ï¸ Step 6: Document Chunking\n","\n","Split the enriched documents into **manageable chunks** (sentences) for embedding:\n","\n","- **DocumentAssembler**: Converts enriched text back to document format\n","- **SentenceDetector**: Intelligently splits text into sentences using:\n","  - Grammar rules\n","  - Abbreviation handling\n","  - Context-aware segmentation\n","\n","Each chunk will now contain both summary context and original content."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"a1oV6AgGPBI8","executionInfo":{"status":"ok","timestamp":1761228022132,"user_tz":300,"elapsed":50,"user":{"displayName":"Danilo Burbano","userId":"08593331088765378019"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bffc5798-fc19-4785-d884-b7c45d013b76"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Document chunking configured\n"]}],"source":["# Split into sentences\n","splitter = SentenceDetector() \\\n","    .setInputCols(\"summary\") \\\n","    .setOutputCol(\"sentences\") \\\n","    .setUseAbbreviations(True)  # Handle abbreviations intelligently\n","\n","print(\"âœ… Document chunking configured\")"]},{"cell_type":"markdown","metadata":{"id":"iUp4dF_9PBI9"},"source":["## ðŸŽ¯ Step 7: Generate Sentence Embeddings\n","\n","Convert each sentence chunk into a **dense vector representation** using BERT:\n","\n","- **Model**: `sent_small_bert_L2_128` - Efficient sentence embeddings\n","- **Output**: Vector embeddings that capture semantic meaning\n","- **Benefit**: Enriched chunks (summary + content) create more meaningful embeddings\n","\n","These embeddings will enable semantic search in your RAG system."]},{"cell_type":"code","execution_count":12,"metadata":{"id":"aHBcTgM-PBI9","executionInfo":{"status":"ok","timestamp":1761228179355,"user_tz":300,"elapsed":157220,"user":{"displayName":"Danilo Burbano","userId":"08593331088765378019"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2be780e4-ac48-4812-a84c-87392862d25a"},"outputs":[{"output_type":"stream","name":"stdout","text":["sent_small_bert_L2_128 download started this may take some time.\n","Approximate size to download 16.1 MB\n","[OK!]\n","ðŸ”„ Generating sentence embeddings...\n","+-------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n","|           fileName|            document|exception| normalized_document|             summary|           sentences| sentence_embeddings|\n","+-------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n","|Clinical_Notes.docx|[{document, 0, 20...|     null|[{document, 0, 20...|[{document, 0, 70...|[{document, 0, 70...|[{sentence_embedd...|\n","+-------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+\n","\n"]}],"source":["# Configure BERT sentence embeddings\n","emb = BertSentenceEmbeddings.pretrained(\"sent_small_bert_L2_128\", \"en\") \\\n","    .setInputCols(\"sentences\") \\\n","    .setOutputCol(\"sentence_embeddings\")\n","\n","# Build post-processing pipeline\n","post_pipe = Pipeline(stages=[\n","    splitter,      # Split into sentences\n","    emb            # Generate embeddings\n","])\n","\n","# Execute pipeline\n","print(\"ðŸ”„ Generating sentence embeddings...\")\n","df_vec = post_pipe.fit(enriched_df).transform(enriched_df)\n","\n","df_vec.show()"]},{"cell_type":"code","source":["# Sanity check: verify sentence and embedding counts match\n","df_vec.selectExpr(\n","    \"size(sentences) as n_sent\",\n","    \"size(sentence_embeddings) as n_emb\"\n",").show(5)\n","\n","print(\"âœ… Sentence embeddings generated successfully\")"],"metadata":{"id":"-5zWJe0l1c0n","executionInfo":{"status":"ok","timestamp":1761228318097,"user_tz":300,"elapsed":138745,"user":{"displayName":"Danilo Burbano","userId":"08593331088765378019"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fe17ba4e-9823-4b30-d214-9c804ee6b60f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["+------+-----+\n","|n_sent|n_emb|\n","+------+-----+\n","|    33|   33|\n","+------+-----+\n","\n","âœ… Sentence embeddings generated successfully\n"]}]},{"cell_type":"markdown","metadata":{"id":"qJgMdQCGPBI9"},"source":["## ðŸ“Š Step 8: Flatten to Chunk-Embedding Pairs\n","\n","Transform the nested structure into a flat format suitable for database storage:\n","\n","**Process**:\n","1. Extract raw float arrays from annotations\n","2. Zip sentences with their corresponding embeddings\n","3. Explode to create one row per (chunk_text, embedding) pair\n","4. Clean and prepare for persistence\n","\n","**Result**: A DataFrame with:\n","- `chunk_text`: The sentence/chunk text\n","- `embedding`: Dense vector representation\n","- `pipeline`: Pipeline identifier for tracking"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"5Qg9_ej-PBI9","executionInfo":{"status":"ok","timestamp":1761228460412,"user_tz":300,"elapsed":142314,"user":{"displayName":"Danilo Burbano","userId":"08593331088765378019"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9a80a4b9-8ad1-4057-c8d1-2e7f51afaeed"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… Data flattened to chunk-embedding pairs\n","+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n","|                                                                                          chunk_text|                                                                                           embedding|\n","+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n","|                             Clinical Note Summary is a comprehensive summary of the clinical notes.|[-1.5387037, 0.8033937, 0.1073381, -2.0959268, 0.49129936, -0.09233003, 0.07712458, -0.7835938, -...|\n","|                                               Overview of the world of information and information.|[-0.83442736, 0.8440716, 0.23424295, -2.1142566, 0.33953342, -0.28851476, -0.08780137, 1.2756112,...|\n","|this document summarizes key elements of the patient's recent visit including medications, past m...|[-1.2714072, 0.6117244, -0.51666576, -0.6389227, 0.4509276, -0.43416765, -0.05859183, -0.00921273...|\n","|                              Medications & Course is a prestigious and respected course in the u.s.|[-1.5458109, 1.1822354, 0.15364084, -2.3845265, 0.08211696, -0.61855245, 0.68152267, 0.27139792, ...|\n","|                                                Prescription: Acetaminophen 500 mg for pain relief .|[-1.2963105, 0.22754598, -0.7971092, -1.9427052, 0.34922865, 0.6145828, -0.025807265, -0.36951354...|\n","+----------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+\n","only showing top 5 rows\n","\n"]}],"source":["# Extract raw float arrays from embeddings\n","df_vec = df_vec.withColumn(\n","    \"emb_vecs\",\n","    F.expr(\"transform(sentence_embeddings, x -> x.embeddings)\")\n",")\n","\n","# Zip sentences with their embeddings\n","df_vec = df_vec.withColumn(\n","    \"pairs\",\n","    F.arrays_zip(F.col(\"sentences.result\"), F.col(\"emb_vecs\"))\n",")\n","\n","# Explode to one row per chunk\n","df_chunks = df_vec \\\n","    .withColumn(\"pair\", F.explode_outer(\"pairs\")) \\\n","    .select(\n","        F.col(\"pair.result\").alias(\"chunk_text\"),\n","        F.col(\"pair.emb_vecs\").alias(\"embedding\")\n","    ) \\\n","    .dropna(subset=[\"chunk_text\", \"embedding\"])\n","\n","print(\"âœ… Data flattened to chunk-embedding pairs\")\n","df_chunks.select(\"chunk_text\", \"embedding\").show(5, truncate=100)"]},{"cell_type":"markdown","metadata":{"id":"YhlvtRqdPBI9"},"source":["## ðŸ’¾ Step 9: Persist to Storage\n","\n","Save the processed chunks and embeddings to **Parquet format** for efficient storage and retrieval:\n","\n","**Benefits of Parquet**:\n","- Columnar storage for efficient querying\n","- Native support for complex types (arrays/vectors)\n","- Excellent compression\n","- Fast read performance\n","\n","**Next Steps**: Load this data into your vector database (e.g., Pinecone, Weaviate, Milvus) for semantic search."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"MpoPpKwvPBI9","executionInfo":{"status":"ok","timestamp":1761228748708,"user_tz":300,"elapsed":288295,"user":{"displayName":"Danilo Burbano","userId":"08593331088765378019"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cbf00576-a63e-45a9-d66b-e6198824e140"},"outputs":[{"output_type":"stream","name":"stdout","text":["ðŸ’¾ Saving results to: datasets/rag_boost_llm_only.parquet\n","âœ… Successfully saved 33 chunks to: datasets/rag_boost_llm_only.parquet\n"]}],"source":["# Define output path\n","out_path = \"datasets/rag_boost_llm_only.parquet\"\n","\n","# Save to Parquet\n","print(f\"ðŸ’¾ Saving results to: {out_path}\")\n","df_chunks.write.mode(\"overwrite\").parquet(out_path)\n","\n","print(f\"âœ… Successfully saved {df_chunks.count()} chunks to: {out_path}\")"]},{"cell_type":"markdown","metadata":{"id":"jx142tPRPBI9"},"source":["## ðŸŽ“ Key Takeaways and Best Practices\n","\n","### What We Accomplished:\n","1. âœ… Ingested documents from various formats\n","2. âœ… Generated LLM-powered abstractive summaries\n","3. âœ… Enriched chunks with dual context (summary + original)\n","4. âœ… Created semantic embeddings for vector search\n","5. âœ… Persisted results in efficient format\n","\n","### Pipeline Variations:\n","\n","#### Current Approach: **Document-Level Enrichment**\n","- Summarize entire document first\n","- Then split and embed\n","- Best for: Executive summaries, high-level context\n","\n","#### Alternative: **Chunk-Level Enrichment**\n","- Split document first\n","- Enrich each chunk individually\n","- Best for: Fine-grained control, section-specific summaries\n","\n","### Hallucination Mitigation:\n","- Original text preserved alongside summaries\n","- Database contains both for grounding\n","- Retrieval system can return original chunks for verification\n","\n","### Production Considerations:\n","1. **Model Selection**: Choose T5 size based on quality/speed tradeoff\n","2. **Chunk Size**: Adjust sentence detection for optimal chunk granularity\n","3. **Embedding Model**: Select BERT variant based on domain (clinical, legal, etc.)\n","4. **Vector Database**: Integrate with Pinecone, Weaviate, or Milvus\n","5. **Monitoring**: Track summary quality, embedding coherence, retrieval metrics\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"1dht3-0nPBI9"},"source":["## ðŸš€ Next Steps\n","\n","To use this pipeline in production:\n","\n","1. **Configure your environment**: Update Spark NLP JAR path and memory settings\n","2. **Point to your documents**: Update `docs_path` to your document directory\n","3. **Tune parameters**: Adjust summary length, embedding model, chunk size\n","4. **Run the pipeline**: Execute all cells sequentially\n","5. **Load to vector DB**: Import the generated Parquet file into your vector database\n","6. **Build RAG app**: Use the embeddings for semantic search and retrieval\n","\n","### Integration Example:\n","\n","```python\n","# Load processed chunks\n","chunks_df = spark.read.parquet(\"datasets/rag_boost_llm_only.parquet\")\n","\n","# Upload to vector database (pseudo-code)\n","for row in chunks_df.collect():\n","    vector_db.upsert(\n","        id=generate_id(),\n","        text=row.chunk_text,\n","        embedding=row.embedding,\n","        metadata={\"pipeline\": row.pipeline}\n","    )\n","```\n","\n","### Resources:\n","- [Spark NLP Documentation](https://nlp.johnsnowlabs.com/)\n","- [RAG Best Practices](https://www.pinecone.io/learn/retrieval-augmented-generation/)\n","- [Vector Database Comparison](https://www.datastax.com/blog/vector-database-comparison)\n","\n","---\n","\n","**Happy Building! ðŸŽ‰**"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}