{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9954f8f4-58ab-4c50-a6c9-f8fc5310d7c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# CPU \n",
    "\n",
    "- Install Spark NLP in your Databricks cluster\n",
    "    - In Libraries tab inside your cluster you need to follow these steps:\n",
    "    - Install New -> PyPI -> spark-nlp==4.1.0 -> Install\n",
    "    - Install New -> Maven -> Coordinates -> com.johnsnowlabs.nlp:spark-nlp_2.12:4.1.0 -> Install\n",
    "- Will add `TF_ENABLE_ONEDNN_OPTS=1` to `Cluster->Advacend Options->Spark->Environment variables` to enable oneDNN\n",
    "\n",
    "Databricks:\n",
    "* Runtime: `11.1 ML (includes Apache Spark 3.3.0, Scala 2.12)`\n",
    "* Cluster mode: `Multi Node`\n",
    "* Executors: `m5n.8xlarge 128 GB Memory, 32 Cores`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dcc4c4b2-8219-4365-ae0b-cce704900a45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "imageNetDataset = spark.read\\\n",
    "      .format(\"image\")\\\n",
    "      .option(\"dropInvalid\", value = True)\\\n",
    "      .load(\"dbfs:/maziyar/datasets/imagenet-mini/\")\n",
    "\n",
    "image_assembler = ImageAssembler() \\\n",
    "    .setInputCol(\"image\") \\\n",
    "    .setOutputCol(\"image_assembler\")\n",
    "\n",
    "imageClassifier = ViTForImageClassification \\\n",
    "    .pretrained(\"image_classifier_vit_base_patch16_224\") \\\n",
    "    .setInputCols(\"image_assembler\") \\\n",
    "    .setOutputCol(\"class\") \\\n",
    "    .setBatchSize(16)\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "  image_assembler,\n",
    "  imageClassifier,\n",
    "])\n",
    "\n",
    "model = pipeline.fit(imageNetDataset)\n",
    "pipelineDF = model.transform(imageNetDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6fa3958c-db84-47e0-8794-99f0bf384b3c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2 Workers:  256 GB Memory 64 Cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "579967ea-dfd0-4b1b-8b4f-f0c2d485c2db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "took 549.6623425070002 seconds to finish computing 34742 images with batch size 16\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "took 549.6623425070002 seconds to finish computing 34742 images with batch size 16\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = timer()\n",
    "total_count = pipelineDF.select(\"class.result\").count()\n",
    "end = timer() - start\n",
    "print(f'took {end} seconds to finish computing {total_count} images with batch size {imageClassifier.getBatchSize()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "36565a49-c510-4f90-b3f9-34fd8dc7c451",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4 Workers: 512 GB Memory 128 Cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "99c798dc-30fb-4335-8ed2-2e3bb832de81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "took 288.5531326099999 seconds to finish computing 34744 images with batch size 16\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "took 288.5531326099999 seconds to finish computing 34744 images with batch size 16\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = timer()\n",
    "total_count = pipelineDF.select(\"class.result\").count()\n",
    "end = timer() - start\n",
    "print(f'took {end} seconds to finish computing {total_count} images with batch size {imageClassifier.getBatchSize()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ce9cce6c-9a3e-4f1c-8b0d-9f4ff198dcba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 8 Workers: 1024 GB Memory 256 Cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c47e8c02-c1ab-4253-b0bc-d81dbec1a5e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "took 161.2116333849999 seconds to finish computing 34742 images with batch size 16\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "took 161.2116333849999 seconds to finish computing 34742 images with batch size 16\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = timer()\n",
    "total_count = pipelineDF.select(\"class.result\").count()\n",
    "end = timer() - start\n",
    "print(f'took {end} seconds to finish computing {total_count} images with batch size {imageClassifier.getBatchSize()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a974ef3b-5188-4c0d-aa73-3f217a2c406b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 10 Workers: 1280 GB Memory 320 Cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "de378aba-3ed4-4677-8b48-c2805817645e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "took 111.69446445700032 seconds to finish computing 34742 images with batch size 16\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "took 111.69446445700032 seconds to finish computing 34742 images with batch size 16\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = timer()\n",
    "total_count = pipelineDF.select(\"class.result\").count()\n",
    "end = timer() - start\n",
    "print(f'took {end} seconds to finish computing {total_count} images with batch size {imageClassifier.getBatchSize()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "01c8bc9b-d60a-43de-ae61-2c7b52620056",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## GPU \n",
    "- Install Spark NLP in your Databricks cluster\n",
    "    - In Libraries tab inside your cluster you need to follow these steps:\n",
    "    - Install New -> PyPI -> spark-nlp==4.1.0 -> Install\n",
    "    - Install New -> Maven -> Coordinates -> com.johnsnowlabs.nlp:spark-nlp-gpu_2.12:4.1.0 -> Install\n",
    "\n",
    "Databricks:\n",
    "* Runtime: `11.1 ML (includes Apache Spark 3.3.0, GPU, Scala 2.12)`\n",
    "* Cluster mode: `Multi Node`\n",
    "* Executors: `g4dn.8xlarge 128 GB Memory, 1 GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7f87c9d2-7d74-44fc-91c4-d6cf25f95573",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The only config I have set is `spark.task.resource.gpu.amount` to control the number of tasks per machine (considering 1 GPU and 32 cores per node)\n",
    "https://docs.databricks.com/clusters/gpu.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5ca06336-7b54-4076-8073-64e32bc033d7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "imageNetDataset = spark.read\\\n",
    "      .format(\"image\")\\\n",
    "      .option(\"dropInvalid\", value = True)\\\n",
    "      .load(\"dbfs:/maziyar/datasets/imagenet-mini/\")\n",
    "\n",
    "image_assembler = ImageAssembler() \\\n",
    "    .setInputCol(\"image\") \\\n",
    "    .setOutputCol(\"image_assembler\")\n",
    "\n",
    "imageClassifier = ViTForImageClassification \\\n",
    "    .pretrained(\"image_classifier_vit_base_patch16_224\") \\\n",
    "    .setInputCols(\"image_assembler\") \\\n",
    "    .setOutputCol(\"class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d1e0ffbf-1704-4f5a-8ce6-337b6601c220",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "root\n",
       " |-- image: struct (nullable = true)\n",
       " |    |-- origin: string (nullable = true)\n",
       " |    |-- height: integer (nullable = true)\n",
       " |    |-- width: integer (nullable = true)\n",
       " |    |-- nChannels: integer (nullable = true)\n",
       " |    |-- mode: integer (nullable = true)\n",
       " |    |-- data: binary (nullable = true)\n",
       "\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "root\n |-- image: struct (nullable = true)\n |    |-- origin: string (nullable = true)\n |    |-- height: integer (nullable = true)\n |    |-- width: integer (nullable = true)\n |    |-- nChannels: integer (nullable = true)\n |    |-- mode: integer (nullable = true)\n |    |-- data: binary (nullable = true)\n\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imageNetDataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "defb3ed4-4504-4f55-bab8-baa35ea54832",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2 Workers: 2x NVIDIA T4 GPU 16GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dfe61f52-89f1-4731-8908-5ea4435242ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPU instance type of g4dn.8xlarge\n",
       "0.04 task gpu amount\n",
       "2 executors\n",
       "total partitions 1099\n",
       "total running tasks 50.0 - tasks per machines 25.0\n",
       "Running tasks to partition ratio is 22\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "GPU instance type of g4dn.8xlarge\n0.04 task gpu amount\n2 executors\ntotal partitions 1099\ntotal running tasks 50.0 - tasks per machines 25.0\nRunning tasks to partition ratio is 22\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_per_gpu = float(spark.conf.get(\"spark.task.resource.gpu.amount\"))\n",
    "executors = int(spark.conf.get(\"spark.databricks.clusterUsageTags.clusterTargetWorkers\"))\n",
    "gpu_instance = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterNodeType\")\n",
    "\n",
    "partitions = int(imageNetDataset.rdd.getNumPartitions())\n",
    "total_tasks = (1/task_per_gpu) * executors\n",
    "\n",
    "print(f'GPU instance type of {gpu_instance}')\n",
    "print(f'{task_per_gpu} task gpu amount')\n",
    "print(f'{executors} executors')\n",
    "print(f'total partitions {partitions}')\n",
    "print(f'total running tasks {total_tasks} - tasks per machines {total_tasks/executors}')\n",
    "print(f'Running tasks to partition ratio is {partitions/total_tasks:0.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "de5aa073-0ee5-405d-92cd-734fbbaa674f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "took 319.87963630299964 seconds to finish computing 34742 images with batch size 1\n",
       "took 259.5521752349996 seconds to finish computing 34742 images with batch size 2\n",
       "took 235.64105476499935 seconds to finish computing 34742 images with batch size 4\n",
       "took 231.89560089599945 seconds to finish computing 34742 images with batch size 8\n",
       "took 231.2127806669996 seconds to finish computing 34742 images with batch size 16\n",
       "took 237.9331510810007 seconds to finish computing 34742 images with batch size 32\n",
       "took 238.78765448600097 seconds to finish computing 34742 images with batch size 64\n",
       "took 234.17468454200025 seconds to finish computing 34742 images with batch size 128\n",
       "took 232.502572753001 seconds to finish computing 34742 images with batch size 256\n",
       "took 240.10180371500064 seconds to finish computing 34742 images with batch size 512\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "took 319.87963630299964 seconds to finish computing 34742 images with batch size 1\ntook 259.5521752349996 seconds to finish computing 34742 images with batch size 2\ntook 235.64105476499935 seconds to finish computing 34742 images with batch size 4\ntook 231.89560089599945 seconds to finish computing 34742 images with batch size 8\ntook 231.2127806669996 seconds to finish computing 34742 images with batch size 16\ntook 237.9331510810007 seconds to finish computing 34742 images with batch size 32\ntook 238.78765448600097 seconds to finish computing 34742 images with batch size 64\ntook 234.17468454200025 seconds to finish computing 34742 images with batch size 128\ntook 232.502572753001 seconds to finish computing 34742 images with batch size 256\ntook 240.10180371500064 seconds to finish computing 34742 images with batch size 512\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "for b in [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]:\n",
    "    imageClassifier.setBatchSize(b)\n",
    "\n",
    "    pipeline = Pipeline(stages=[\n",
    "      image_assembler,\n",
    "      imageClassifier,\n",
    "    ])\n",
    "\n",
    "    model = pipeline.fit(imageNetDataset)\n",
    "    pipelineDF = model.transform(imageNetDataset)\n",
    "\n",
    "    start = timer()\n",
    "    total_count = pipelineDF.select(\"class.result\").count()\n",
    "    end = timer() - start\n",
    "    print(f'took {end} seconds to finish computing {total_count} images with batch size {imageClassifier.getBatchSize()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e3bec7a1-283b-4085-ae77-46691215a7f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4 Workers: 4x NVIDIA T4 GPU 16GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b8945961-920d-45e3-b9dd-0ade59febe49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPU instance type of g4dn.8xlarge\n",
       "0.04 task gpu amount\n",
       "4 executors\n",
       "total partitions 1099\n",
       "total tasks 100.0 - tasks per machines 25.0\n",
       "All tasks finish in 11 try\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "GPU instance type of g4dn.8xlarge\n0.04 task gpu amount\n4 executors\ntotal partitions 1099\ntotal tasks 100.0 - tasks per machines 25.0\nAll tasks finish in 11 try\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_per_gpu = float(spark.conf.get(\"spark.task.resource.gpu.amount\"))\n",
    "executors = int(spark.conf.get(\"spark.databricks.clusterUsageTags.clusterTargetWorkers\"))\n",
    "gpu_instance = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterNodeType\")\n",
    "\n",
    "partitions = int(imageNetDataset.rdd.getNumPartitions())\n",
    "total_tasks = (1/task_per_gpu) * executors\n",
    "\n",
    "print(f'GPU instance type of {gpu_instance}')\n",
    "print(f'{task_per_gpu} task gpu amount')\n",
    "print(f'{executors} executors')\n",
    "print(f'total partitions {partitions}')\n",
    "print(f'total tasks {total_tasks} - tasks per machines {total_tasks/executors}')\n",
    "print(f'All tasks finish in {partitions/total_tasks:0.0f} try')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d5e66b58-0997-4c2a-9221-0ceaa6ffddcb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "took 173.41131163000045 seconds to finish computing 34742 images with batch size 1\n",
       "took 129.19329973999993 seconds to finish computing 34742 images with batch size 2\n",
       "took 117.66454362900004 seconds to finish computing 34742 images with batch size 4\n",
       "took 119.80921282099916 seconds to finish computing 34742 images with batch size 8\n",
       "took 122.0702281770009 seconds to finish computing 34742 images with batch size 16\n",
       "took 126.85776031200112 seconds to finish computing 34742 images with batch size 32\n",
       "took 125.29146863700043 seconds to finish computing 34742 images with batch size 64\n",
       "took 125.21238900500066 seconds to finish computing 34742 images with batch size 128\n",
       "took 123.46789567799897 seconds to finish computing 34742 images with batch size 256\n",
       "took 127.55372481199993 seconds to finish computing 34742 images with batch size 512\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "took 173.41131163000045 seconds to finish computing 34742 images with batch size 1\ntook 129.19329973999993 seconds to finish computing 34742 images with batch size 2\ntook 117.66454362900004 seconds to finish computing 34742 images with batch size 4\ntook 119.80921282099916 seconds to finish computing 34742 images with batch size 8\ntook 122.0702281770009 seconds to finish computing 34742 images with batch size 16\ntook 126.85776031200112 seconds to finish computing 34742 images with batch size 32\ntook 125.29146863700043 seconds to finish computing 34742 images with batch size 64\ntook 125.21238900500066 seconds to finish computing 34742 images with batch size 128\ntook 123.46789567799897 seconds to finish computing 34742 images with batch size 256\ntook 127.55372481199993 seconds to finish computing 34742 images with batch size 512\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "for b in [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]:\n",
    "    imageClassifier.setBatchSize(b)\n",
    "\n",
    "    pipeline = Pipeline(stages=[\n",
    "      image_assembler,\n",
    "      imageClassifier,\n",
    "    ])\n",
    "\n",
    "    model = pipeline.fit(imageNetDataset)\n",
    "    pipelineDF = model.transform(imageNetDataset)\n",
    "\n",
    "    start = timer()\n",
    "    total_count = pipelineDF.select(\"class.result\").count()\n",
    "    end = timer() - start\n",
    "    print(f'took {end} seconds to finish computing {total_count} images with batch size {imageClassifier.getBatchSize()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5f129459-0960-4d10-9841-0c97b09cd619",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 8 Workers: 8x NVIDIA T4 GPU 16GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "711ce91e-be7b-45c0-a54e-44016c1484cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPU instance type of g4dn.8xlarge\n",
       "0.04 task gpu amount\n",
       "8 executors\n",
       "total partitions 1099\n",
       "total tasks 200.0 - tasks per machines 25.0\n",
       "All tasks finish in 5 try\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "GPU instance type of g4dn.8xlarge\n0.04 task gpu amount\n8 executors\ntotal partitions 1099\ntotal tasks 200.0 - tasks per machines 25.0\nAll tasks finish in 5 try\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_per_gpu = float(spark.conf.get(\"spark.task.resource.gpu.amount\"))\n",
    "executors = int(spark.conf.get(\"spark.databricks.clusterUsageTags.clusterTargetWorkers\"))\n",
    "gpu_instance = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterNodeType\")\n",
    "\n",
    "partitions = int(imageNetDataset.rdd.getNumPartitions())\n",
    "total_tasks = (1/task_per_gpu) * executors\n",
    "\n",
    "print(f'GPU instance type of {gpu_instance}')\n",
    "print(f'{task_per_gpu} task gpu amount')\n",
    "print(f'{executors} executors')\n",
    "print(f'total partitions {partitions}')\n",
    "print(f'total tasks {total_tasks} - tasks per machines {total_tasks/executors}')\n",
    "print(f'All tasks finish in {partitions/total_tasks:0.0f} try')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a261a11a-ad55-452f-9c9b-d5601417982f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "took 101.19505050400039 seconds to finish computing 34742 images with batch size 1\n",
       "took 66.77397438299977 seconds to finish computing 34742 images with batch size 2\n",
       "took 61.54780982599914 seconds to finish computing 34742 images with batch size 4\n",
       "took 61.45625800400012 seconds to finish computing 34742 images with batch size 8\n",
       "took 62.51468649199978 seconds to finish computing 34742 images with batch size 16\n",
       "took 65.90733181500036 seconds to finish computing 34742 images with batch size 32\n",
       "took 72.0166545129996 seconds to finish computing 34742 images with batch size 64\n",
       "took 64.86596081300013 seconds to finish computing 34742 images with batch size 128\n",
       "took 66.45860373800042 seconds to finish computing 34742 images with batch size 256\n",
       "took 65.32813091099979 seconds to finish computing 34742 images with batch size 512\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "took 101.19505050400039 seconds to finish computing 34742 images with batch size 1\ntook 66.77397438299977 seconds to finish computing 34742 images with batch size 2\ntook 61.54780982599914 seconds to finish computing 34742 images with batch size 4\ntook 61.45625800400012 seconds to finish computing 34742 images with batch size 8\ntook 62.51468649199978 seconds to finish computing 34742 images with batch size 16\ntook 65.90733181500036 seconds to finish computing 34742 images with batch size 32\ntook 72.0166545129996 seconds to finish computing 34742 images with batch size 64\ntook 64.86596081300013 seconds to finish computing 34742 images with batch size 128\ntook 66.45860373800042 seconds to finish computing 34742 images with batch size 256\ntook 65.32813091099979 seconds to finish computing 34742 images with batch size 512\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "for b in [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]:\n",
    "    imageClassifier.setBatchSize(b)\n",
    "\n",
    "    pipeline = Pipeline(stages=[\n",
    "      image_assembler,\n",
    "      imageClassifier,\n",
    "    ])\n",
    "\n",
    "    model = pipeline.fit(imageNetDataset)\n",
    "    pipelineDF = model.transform(imageNetDataset)\n",
    "\n",
    "    start = timer()\n",
    "    total_count = pipelineDF.select(\"class.result\").count()\n",
    "    end = timer() - start\n",
    "    print(f'took {end} seconds to finish computing {total_count} images with batch size {imageClassifier.getBatchSize()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "265dfab3-38b0-4e88-952a-658d4429ad62",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 10 Workers: 10x NVIDIA T4 GPU 16GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1eaa4502-ce9b-4bb1-bd4f-ce888cac443d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPU instance type of g4dn.8xlarge\n",
       "0.04 task gpu amount\n",
       "10 executors\n",
       "total partitions 1099\n",
       "total tasks 250.0 - tasks per machines 25.0\n",
       "All tasks finish in 4 try\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "GPU instance type of g4dn.8xlarge\n0.04 task gpu amount\n10 executors\ntotal partitions 1099\ntotal tasks 250.0 - tasks per machines 25.0\nAll tasks finish in 4 try\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_per_gpu = float(spark.conf.get(\"spark.task.resource.gpu.amount\"))\n",
    "executors = int(spark.conf.get(\"spark.databricks.clusterUsageTags.clusterTargetWorkers\"))\n",
    "gpu_instance = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterNodeType\")\n",
    "\n",
    "partitions = int(imageNetDataset.rdd.getNumPartitions())\n",
    "total_tasks = (1/task_per_gpu) * executors\n",
    "\n",
    "print(f'GPU instance type of {gpu_instance}')\n",
    "print(f'{task_per_gpu} task gpu amount')\n",
    "print(f'{executors} executors')\n",
    "print(f'total partitions {partitions}')\n",
    "print(f'total tasks {total_tasks} - tasks per machines {total_tasks/executors}')\n",
    "print(f'All tasks finish in {partitions/total_tasks:0.0f} try')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "154f8fe3-2d0d-4904-a01c-296d9952ac85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "took 88.49770859 seconds to finish computing 34743 images with batch size 1\n",
       "took 55.147554744000445 seconds to finish computing 34742 images with batch size 2\n",
       "took 50.35473146100048 seconds to finish computing 34742 images with batch size 4\n",
       "took 50.822814176000065 seconds to finish computing 34744 images with batch size 8\n",
       "took 53.320964721000564 seconds to finish computing 34742 images with batch size 16\n",
       "took 55.740571029999955 seconds to finish computing 34742 images with batch size 32\n",
       "took 53.99739717600005 seconds to finish computing 34743 images with batch size 64\n",
       "took 54.02455361400007 seconds to finish computing 34742 images with batch size 128\n",
       "took 53.79816353100068 seconds to finish computing 34742 images with batch size 256\n",
       "took 53.49764558999959 seconds to finish computing 34743 images with batch size 512\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "took 88.49770859 seconds to finish computing 34743 images with batch size 1\ntook 55.147554744000445 seconds to finish computing 34742 images with batch size 2\ntook 50.35473146100048 seconds to finish computing 34742 images with batch size 4\ntook 50.822814176000065 seconds to finish computing 34744 images with batch size 8\ntook 53.320964721000564 seconds to finish computing 34742 images with batch size 16\ntook 55.740571029999955 seconds to finish computing 34742 images with batch size 32\ntook 53.99739717600005 seconds to finish computing 34743 images with batch size 64\ntook 54.02455361400007 seconds to finish computing 34742 images with batch size 128\ntook 53.79816353100068 seconds to finish computing 34742 images with batch size 256\ntook 53.49764558999959 seconds to finish computing 34743 images with batch size 512\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "ansi"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "for b in [1, 2, 4, 8, 16, 32, 64, 128, 256, 512]:\n",
    "    imageClassifier.setBatchSize(b)\n",
    "\n",
    "    pipeline = Pipeline(stages=[\n",
    "      image_assembler,\n",
    "      imageClassifier,\n",
    "    ])\n",
    "\n",
    "    model = pipeline.fit(imageNetDataset)\n",
    "    pipelineDF = model.transform(imageNetDataset)\n",
    "\n",
    "    start = timer()\n",
    "    total_count = pipelineDF.select(\"class.result\").count()\n",
    "    end = timer() - start\n",
    "    print(f'took {end} seconds to finish computing {total_count} images with batch size {imageClassifier.getBatchSize()}')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Databricks - Spark NLP Image Classification at Scale",
   "notebookOrigID": 4496231523440499,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
