{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3inuQHTI-G-P"
   },
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Cd4gz6vxHiQ"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/logging/Comet_SparkNLP_Intergration.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Comet](https://www.comet.ml/site/) is a meta machine learning platform designed to help AI practitioners and teams build reliable machine learning models for real-world applications by streamlining the machine learning model lifecycle. By leveraging Comet, users can track, compare, explain and reproduce their machine learning experiments.\n",
    "\n",
    "Comet can easily integrated into the Spark NLP workflow with the a dedicated logging class CometLogger, to log training and evaluation metrics, pipeline parameters and NER visualization made with sparknlp-display.\n",
    "\n",
    "To log a Spark NLP annotator, it will need an “outputLogPath” parameter, as the CometLogger reads the log file generated during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwNADYvbZZ1o"
   },
   "source": [
    "# Installing SparkNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2ye-bu7yeTJ"
   },
   "outputs": [],
   "source": [
    "# This is only to setup PySpark and Spark NLP on Colab\n",
    "!wget http://setup.johnsnowlabs.com/colab.sh -O - | bash\n",
    "\n",
    "# Install Spark NLP Display for visualization\n",
    "!pip install --ignore-installed spark-nlp-display\n",
    "\n",
    "# Installing Comet\n",
    "!pip install comet_ml --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNcH1eDyZddS"
   },
   "source": [
    "# Importing Dependencies and Starting Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hukcCcW17kIx"
   },
   "outputs": [],
   "source": [
    "# Import Spark NLP\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = sparknlp.start()\n",
    "\n",
    "# Import Comet\n",
    "import comet_ml\n",
    "from sparknlp.logging.comet import CometLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHfmcVlAZfjz"
   },
   "source": [
    "# Initialize Comet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run an online experiment, you will need an API key From Comet. See [Quick Start - Comet.ml](https://www.comet.ml/docs/quick-start/) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6uDqwwTzTNY"
   },
   "outputs": [],
   "source": [
    "comet_ml.init(project_name='sparknlp-test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running An Offline Experiment\n",
    "Alternatively, an offline experiment can be run. The resulting zip archive of the logs can then be submitted to comet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CometLogger' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29703/1884634088.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCometLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomet_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"offline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffline_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOUTPUT_LOG_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcomet_ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"sparknlp_experiment\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffline_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/tmp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CometLogger' is not defined"
     ]
    }
   ],
   "source": [
    "# OUTPUT_LOG_PATH = './run'\n",
    "# logger = CometLogger(comet_mode=\"offline\", offline_directory=OUTPUT_LOG_PATH)\n",
    "# comet_ml.init(project_name=\"sparknlp_experiment\", offline_directory=\"/tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLKpFxErIzM0"
   },
   "source": [
    "# Logging from SparkNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZ3Zgy06scBG"
   },
   "source": [
    "## Logging Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ogPNh7zLR4B"
   },
   "outputs": [],
   "source": [
    "OUTPUT_LOG_PATH = './run'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylr9czEMrA_f"
   },
   "source": [
    "### Create a Comet Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0dpw910rDkJ"
   },
   "outputs": [],
   "source": [
    "logger = CometLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kl0UxLzAstRr"
   },
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYr7ho0LRefi"
   },
   "outputs": [],
   "source": [
    "!curl -O 'https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/toxic_comments/toxic_train.snappy.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkgWO1nEsrIX"
   },
   "source": [
    "### Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SuAnuxj5Rmee"
   },
   "outputs": [],
   "source": [
    "trainDataset = spark.read.parquet(\"toxic_train.snappy.parquet\").repartition(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "And0LEcJsnw1"
   },
   "source": [
    "### Define SparkNLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37B1olK_Iy_C"
   },
   "outputs": [],
   "source": [
    "# Let's use shrink to remove new lines in the comments\n",
    "document = (\n",
    "    DocumentAssembler()\n",
    "    .setInputCol(\"text\")\n",
    "    .setOutputCol(\"document\")\n",
    "    .setCleanupMode(\"shrink\")\n",
    ")\n",
    "\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "\n",
    "# Here we use the state-of-the-art Universal Sentence Encoder model from TF Hub\n",
    "use = (\n",
    "    UniversalSentenceEncoder.pretrained()\n",
    "    .setInputCols([\"document\"])\n",
    "    .setOutputCol(\"sentence_embeddings\")\n",
    ")\n",
    "\n",
    "# We will use MultiClassifierDL built by using Bidirectional GRU and CNNs inside TensorFlow that supports up to 100 classes\n",
    "# We will use only 5 Epochs but feel free to increase it on your own dataset\n",
    "multiClassifier = (\n",
    "    MultiClassifierDLApproach()\n",
    "    .setInputCols(\"sentence_embeddings\")\n",
    "    .setOutputCol(\"category\")\n",
    "    .setLabelColumn(\"labels\")\n",
    "    .setBatchSize(128)\n",
    "    .setMaxEpochs(10)\n",
    "    .setLr(1e-3)\n",
    "    .setThreshold(0.5)\n",
    "    .setShufflePerEpoch(False)\n",
    "    .setEnableOutputLogs(True)\n",
    "    .setOutputLogsPath(OUTPUT_LOG_PATH)\n",
    "    .setValidationSplit(0.1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izYqvCWvswV9"
   },
   "source": [
    "### Monitor the Model log file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JW9fOEux-G74"
   },
   "source": [
    "SparkNLP will write the training metrics for this run to a log file. We're going to monitor this file for updates and log the entries to Comet as metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EncrR31RHmtH"
   },
   "outputs": [],
   "source": [
    "logger.monitor(OUTPUT_LOG_PATH, multiClassifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3L1NAyN9j61"
   },
   "source": [
    "Before starting to fit our model, lets display Comet's Experiment View in the cell below so that we can view the metrics as they are reported. \n",
    "\n",
    "**Note:** It may take a few minutes before you see metrics displayed in the UI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thz1LBc0ES0a"
   },
   "outputs": [],
   "source": [
    "logger.experiment.display(tab='charts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFoyiY8fs1pH"
   },
   "source": [
    "### Run the training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFujeSMH44y5"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[document, use, multiClassifier])\n",
    "model = pipeline.fit(trainDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FKAGSwdMLAD"
   },
   "source": [
    "## Logging Completed Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CMSCJpAMcEc"
   },
   "source": [
    "We can also log runs to Comet after training has finished. Let's take a look at the created log file from the earlier training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FecUrluUJxLT"
   },
   "outputs": [],
   "source": [
    "!ls ./run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-oDkX9RtMo1f"
   },
   "outputs": [],
   "source": [
    "logger = CometLogger()\n",
    "logger.log_completed_run('./run/MultiClassifierDLApproach_736adb7a5ea5.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2s7O9e_IMvcg"
   },
   "outputs": [],
   "source": [
    "logger.experiment.display(tab='charts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tm0SYS5JjjBE"
   },
   "source": [
    "## Logging Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxG8ZVNz-jDJ"
   },
   "source": [
    "SparkNLP model predictions are easily convertible to Pandas Dataframes. We can then evaluate these predictions using libraries like `scikit-learn`. \n",
    "\n",
    "Here we demonstrate how to log metrics from a classification report to Comet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egi4u_jTOp_I"
   },
   "outputs": [],
   "source": [
    "logger = CometLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nbayBf41Nd55"
   },
   "outputs": [],
   "source": [
    "!curl -O 'https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/resources/en/classifier-dl/toxic_comments/toxic_test.snappy.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBXOhlgUN0ju"
   },
   "outputs": [],
   "source": [
    "testDataset = spark.read.parquet(\"/content/toxic_test.snappy.parquet\").repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIf9YDSt-QT9"
   },
   "outputs": [],
   "source": [
    "prediction = model.transform(testDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4FerI_GeJT-Y"
   },
   "outputs": [],
   "source": [
    "preds_df = prediction.select('labels', 'category.result').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQ-o2FDwE9ba"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "y_true = mlb.fit_transform(preds_df['labels'])\n",
    "y_pred = mlb.fit_transform(preds_df['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2y5ZvKJvMTJz"
   },
   "outputs": [],
   "source": [
    "report = classification_report(y_true, y_pred, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbIGJVJ4N3QL"
   },
   "outputs": [],
   "source": [
    "for key, value in report.items():\n",
    "  logger.log_metrics(value, prefix=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2IJEqUR4Ova-"
   },
   "outputs": [],
   "source": [
    "logger.experiment.display(tab='metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkkZ9tj83ZZd"
   },
   "source": [
    "## Logging Pipeline Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2M7OmN-m_imR"
   },
   "source": [
    "You can also use the CometLogger to log SparkNLP Pipeline Parameters to Comet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MwP3EkXwsd5_"
   },
   "outputs": [],
   "source": [
    "logger = CometLogger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ShrQ4NKrlU5"
   },
   "source": [
    "#### Define a Pipeline with a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0bWqEYrJrkEB"
   },
   "outputs": [],
   "source": [
    "# If you change the model, re-run all the cells below\n",
    "# Other applicable models: ner_dl, ner_dl_bert\n",
    "MODEL_NAME = \"onto_100\"\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "# ner_dl and onto_100 model are trained with glove_100d, so the embeddings in\n",
    "# the pipeline should match\n",
    "if (MODEL_NAME == \"ner_dl\") or (MODEL_NAME == \"onto_100\"):\n",
    "    embeddings = WordEmbeddingsModel.pretrained('glove_100d') \\\n",
    "        .setInputCols([\"document\", 'token']) \\\n",
    "        .setOutputCol(\"embeddings\")\n",
    "\n",
    "# Bert model uses Bert embeddings\n",
    "elif MODEL_NAME == \"ner_dl_bert\":\n",
    "    embeddings = BertEmbeddings.pretrained(name='bert_base_cased', lang='en') \\\n",
    "        .setInputCols(['document', 'token']) \\\n",
    "        .setOutputCol('embeddings')\n",
    "\n",
    "ner_model = NerDLModel.pretrained(MODEL_NAME, 'en') \\\n",
    "    .setInputCols(['document', 'token', 'embeddings']) \\\n",
    "    .setOutputCol('ner')\n",
    "\n",
    "ner_converter = NerConverter() \\\n",
    "    .setInputCols(['document', 'token', 'ner']) \\\n",
    "    .setOutputCol('ner_chunk')\n",
    "\n",
    "nlp_pipeline = Pipeline(stages=[\n",
    "    documentAssembler, \n",
    "    tokenizer,\n",
    "    embeddings,\n",
    "    ner_model,\n",
    "    ner_converter\n",
    "])\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF('text')\n",
    "pipeline_model = nlp_pipeline.fit(empty_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psAACN7KO8fu"
   },
   "source": [
    "### Log PipelineModel Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjvdsaSy3gTC"
   },
   "outputs": [],
   "source": [
    "logger.log_pipeline_parameters(pipeline_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eaOQeZQ_0gj"
   },
   "source": [
    "### Logging Individual Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtF-0mec_54S"
   },
   "outputs": [],
   "source": [
    "logger.log_parameters({\"run-type\": \"training\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P40CX_nlAVcF"
   },
   "source": [
    "Let's take a look at the logged Parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4QbtImGrzxw"
   },
   "outputs": [],
   "source": [
    "logger.experiment.display(tab='parameters')\n",
    "logger.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0LDwOSCp_I5"
   },
   "source": [
    "## Logging Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQNfPwVLAcvM"
   },
   "source": [
    "SparkNLP comes with a rich suite of visualization tools. Comet supports logging these visualizations so that they are readily available for analysis. \n",
    "\n",
    "In this section we will cover how to log SparkNLP visualzations to Comet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJZo_mwU-U6e"
   },
   "source": [
    "#### Define a Pipeline with a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jEARtSE-XJY"
   },
   "outputs": [],
   "source": [
    "# If you change the model, re-run all the cells below\n",
    "# Other applicable models: ner_dl, ner_dl_bert\n",
    "MODEL_NAME = \"onto_100\"\n",
    "\n",
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols(['document']) \\\n",
    "    .setOutputCol('token')\n",
    "\n",
    "# ner_dl and onto_100 model are trained with glove_100d, so the embeddings in\n",
    "# the pipeline should match\n",
    "if (MODEL_NAME == \"ner_dl\") or (MODEL_NAME == \"onto_100\"):\n",
    "    embeddings = WordEmbeddingsModel.pretrained('glove_100d') \\\n",
    "        .setInputCols([\"document\", 'token']) \\\n",
    "        .setOutputCol(\"embeddings\")\n",
    "\n",
    "# Bert model uses Bert embeddings\n",
    "elif MODEL_NAME == \"ner_dl_bert\":\n",
    "    embeddings = BertEmbeddings.pretrained(name='bert_base_cased', lang='en') \\\n",
    "        .setInputCols(['document', 'token']) \\\n",
    "        .setOutputCol('embeddings')\n",
    "\n",
    "ner_model = NerDLModel.pretrained(MODEL_NAME, 'en') \\\n",
    "    .setInputCols(['document', 'token', 'embeddings']) \\\n",
    "    .setOutputCol('ner')\n",
    "\n",
    "ner_converter = NerConverter() \\\n",
    "    .setInputCols(['document', 'token', 'ner']) \\\n",
    "    .setOutputCol('ner_chunk')\n",
    "\n",
    "nlp_pipeline = Pipeline(stages=[\n",
    "    documentAssembler, \n",
    "    tokenizer,\n",
    "    embeddings,\n",
    "    ner_model,\n",
    "    ner_converter\n",
    "])\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF('text')\n",
    "pipeline_model = nlp_pipeline.fit(empty_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDJs1HJ03Iff"
   },
   "source": [
    "### Run Inference with the NER Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bRq2rgCWCPpb"
   },
   "outputs": [],
   "source": [
    "empty_df = spark.createDataFrame([['']]).toDF('text')\n",
    "pipeline_model = nlp_pipeline.fit(empty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0jEqnP6JBlPK"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "text_list = [\n",
    "    \"\"\"William Henry Gates III (born October 28, 1955) is an American business magnate, software developer, investor, and philanthropist. He is best known as the co-founder of Microsoft Corporation. During his career at Microsoft, Gates held the positions of chairman, chief executive officer (CEO), president and chief software architect, while also being the largest individual shareholder until May 2014. He is one of the best-known entrepreneurs and pioneers of the microcomputer revolution of the 1970s and 1980s. Born and raised in Seattle, Washington, Gates co-founded Microsoft with childhood friend Paul Allen in 1975, in Albuquerque, New Mexico; it went on to become the world's largest personal computer software company. Gates led the company as chairman and CEO until stepping down as CEO in January 2000, but he remained chairman and became chief software architect. During the late 1990s, Gates had been criticized for his business tactics, which have been considered anti-competitive. This opinion has been upheld by numerous court rulings. In June 2006, Gates announced that he would be transitioning to a part-time role at Microsoft and full-time work at the Bill & Melinda Gates Foundation, the private charitable foundation that he and his wife, Melinda Gates, established in 2000.[9] He gradually transferred his duties to Ray Ozzie and Craig Mundie. He stepped down as chairman of Microsoft in February 2014 and assumed a new post as technology adviser to support the newly appointed CEO Satya Nadella.\"\"\",\n",
    "    \"\"\"The Mona Lisa is a 16th century oil painting created by Leonardo. It's held at the Louvre in Paris.\"\"\"\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(pd.DataFrame({'text': text_list}))\n",
    "results = pipeline_model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqPYNro5Iua0"
   },
   "source": [
    "### Create and Log Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBTfftcN3uIs"
   },
   "outputs": [],
   "source": [
    "from sparknlp_display import NerVisualizer\n",
    "\n",
    "logger = CometLogger()\n",
    "\n",
    "for idx, result in enumerate(results.collect()):\n",
    "  viz = NerVisualizer().display(\n",
    "    result=result,\n",
    "    label_col='ner_chunk',\n",
    "    document_col='document',\n",
    "    return_html=True\n",
    "  )\n",
    "  logger.log_visualization(viz, name=f'viz-{idx}.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SONCPiIBBAkr"
   },
   "source": [
    "The HTML files from these files can be found in the \"Assets & Artifacts\" tab in the Experiment View.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9saI2h6Cy5I"
   },
   "outputs": [],
   "source": [
    "logger.experiment.display(tab='assets')"
   ]
  }
 ],
"metadata": {
    "colab": {
      "name": "Comet SparkNLP Intergration.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
 "nbformat": 4,
 "nbformat_minor": 4
}
