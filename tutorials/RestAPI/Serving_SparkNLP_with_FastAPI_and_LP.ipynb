{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "c168511e-1278-4fe1-9e10-f55f7cb69213"
        },
        "id": "qZyBOOIzgMFf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Serving Spark NLP with API: Fast API with LightPipelines"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "64263998-73cc-4de7-914b-63f09d5f6ce2"
        },
        "id": "P41lOks_gMFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Fast API and LightPipeline\n",
        "You can serve SparkNLP + FastAPI on Docker. To do that, we will create a project with the following files:\n",
        "- `Dockerfile`: Image for creating a SparkNLP + FastAPI Docker image\n",
        "- `requirements.txt`: PIP Requirements \n",
        "- `entrypoint.sh`: Dockerfile entrypoint\n",
        "- `content/`: folder containing FastAPI webapp and SparkNLP keys\n",
        "- `content/main.py`: FastAPI webapp, entrypoint\n",
        "- `content/sparknlp_keys.json`: SparkNLP keys (for Healthcare or OCR)\n"
      ],
      "metadata": {
        "id": "LXWB2qBefoUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dockerfile\n",
        "`Dockerfile`: Image for creating a SparkNLP + FastAPI Docker image"
      ],
      "metadata": {
        "id": "vVEWnuZzhFc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "FROM ubuntu:18.04\n",
        "RUN apt-get update && apt-get -y update\n",
        "\n",
        "RUN apt-get -y update \\\n",
        "    && apt-get install -y wget \\\n",
        "    && apt-get install -y jq \\\n",
        "    && apt-get install -y lsb-release \\\n",
        "    && apt-get install -y openjdk-8-jdk-headless \\\n",
        "    && apt-get install -y build-essential python3-pip \\\n",
        "    && pip3 -q install pip --upgrade \\\n",
        "    && apt-get clean && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* \\\n",
        "         /usr/share/man /usr/share/doc /usr/share/doc-base\n",
        "\n",
        "ENV PYSPARK_DRIVER_PYTHON=python3\n",
        "ENV PYSPARK_PYTHON=python3\n",
        "\n",
        "ENV LC_ALL=C.UTF-8\n",
        "ENV LANG=C.UTF-8\n",
        "\n",
        "EXPOSE 8515\n",
        "\n",
        "COPY requirements.txt /\n",
        "RUN pip install -r /requirements.txt\n",
        "\n",
        "COPY entrypoint.sh /\n",
        "RUN chmod +x /entrypoint.sh\n",
        "\n",
        "COPY ./content/ /content/\n",
        "WORKDIR content/\n",
        "\n",
        "ENTRYPOINT [\"/entrypoint.sh\"]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "wHddmQ1Xfn0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other files of the project"
      ],
      "metadata": {
        "id": "XxHVXamhlvTF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `requirements.txt`: PIP Requirements "
      ],
      "metadata": {
        "id": "-Q9N_OevhCxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "pyspark==3.1.2\n",
        "fastapi==0.70.1\n",
        "uvicorn==0.16\n",
        "wget==3.2\n",
        "pandas\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "oG0J0wQeg-4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `entrypoint.sh`: Dockerfile entrypoint"
      ],
      "metadata": {
        "id": "U9D0oDrQhH5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "#!/bin/bash\n",
        "\n",
        "export_json () {\n",
        "    for s in $(echo $values | jq -r 'to_entries|map(\"\\(.key)=\\(.value|tostring)\")|.[]' $1 ); do\n",
        "        export $s\n",
        "    done\n",
        "}\n",
        "\n",
        "export_json \"/content/sparknlp_keys.json\"\n",
        "\n",
        "pip install --upgrade spark-nlp-jsl==$JSL_VERSION --user --extra-index-url https://pypi.johnsnowlabs.com/$SECRET\n",
        "\n",
        "if [ $? != 0 ];\n",
        "then\n",
        "    exit 1\n",
        "fi\n",
        "\n",
        "python3 /content/main.py\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "VZ-wZ3lOhIP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example to serve 2 pipelines\n",
        "We are going to download and store in memory two pipelines: `ner_profiling_clinical` and `clinical_deidentification`. This will reduce the latency of loading the models every time."
      ],
      "metadata": {
        "id": "l0lyotF4lyt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `content/main.py`: FastAPI webapp, entrypoint"
      ],
      "metadata": {
        "id": "G5u8IP82hgPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "import uvicorn\n",
        "import json\n",
        "import os\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp_jsl.annotator import *\n",
        "from sparknlp.base import *\n",
        "import sparknlp_jsl\n",
        "import sparknlp\n",
        "from datetime import datetime\n",
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "\n",
        "app = FastAPI()\n",
        "event_list = dict()\n",
        "pipelines = {}\n",
        "\n",
        "@app.get(\"/benchmark/pipeline\")\n",
        "async def get_one_sequential_pipeline_result(modelname, text=''):\n",
        "    # print(list(pipelines.keys()))\n",
        "    if modelname is None or modelname not in pipelines.keys():\n",
        "        return json.dumps({'error': f\"{modelname} not in loaded list: {list(pipelines.keys())}\"})\n",
        "\n",
        "    return pipelines[modelname].annotate(text)\n",
        "\n",
        "@app.on_event(\"startup\")\n",
        "async def startup_event():\n",
        "    if 'pipeline_loaded' in event_list:\n",
        "        return\n",
        "    event_list['start_up']=datetime.now()\n",
        "    print(f'startup has been started at {datetime.now()}...', )\n",
        "    print(list(pipelines.keys()))\n",
        "\n",
        "    print(f'****** spark nlp healthcare version fired up {datetime.now()} ******')\n",
        "    event_list['sparknlp_fired']=datetime.now()\n",
        "\n",
        "    print(\"- App started.\")\n",
        "    with open('/content/sparknlp_keys.json', 'r') as f:\n",
        "        license_keys = json.load(f)\n",
        "\n",
        "    params = {'spark.driver.memory': '4G'}\n",
        "    spark = sparknlp_jsl.start(secret=license_keys['SECRET'], params=params)\n",
        "\n",
        "    # Defining license key-value pairs as local variables\n",
        "    locals().update(license_keys)\n",
        "\n",
        "    # Adding license key-value pairs to environment variables\n",
        "    os.environ.update(license_keys)\n",
        "\n",
        "    print(\"-- Spark NLP Version :\", sparknlp.version())\n",
        "    print(\"-- Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
        "\n",
        "    print(f'****** Loading pretrained pipelines fired up {datetime.now()} ****** ')\n",
        "    pipelines['ner_profiling_clinical'] = PretrainedPipeline('ner_profiling_clinical', 'en', 'clinical/models')\n",
        "    pipelines['clinical_deidentification'] = PretrainedPipeline(\"clinical_deidentification\", \"en\", \"clinical/models\")\n",
        "    event_list['pipeline_loaded'] = datetime.now()\n",
        "\n",
        "    print(event_list)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run('main:app', host='0.0.0.0', port=8515)"
      ],
      "metadata": {
        "id": "Dw4q8teuhinv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keys file"
      ],
      "metadata": {
        "id": "x06d8oc9l7TT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "...and last, but not least, add your sparknlp_keys.json to `content/sparknlp_keys.json`!\n",
        "Don't forget to fulfill with your license values."
      ],
      "metadata": {
        "id": "B_nQCoPShmvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "{\n",
        "  \"AWS_ACCESS_KEY_ID\": \"\",\n",
        "  \"AWS_SECRET_ACCESS_KEY\": \"\",\n",
        "  \"SECRET\": \"\",\n",
        "  \"SPARK_NLP_LICENSE\": \"\",\n",
        "  \"JSL_VERSION\": \"\",\n",
        "  \"PUBLIC_VERSION\": \"\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "CdIBuHdrhkYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building and running Docker\n",
        "Spin up a Docker container using the SparkNLP+FastAPI Docker image we created before"
      ],
      "metadata": {
        "id": "7bDRq4VkiHle"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "docker build -t johnsnowlabs/sparknlp:sparknlp_api .\n",
        "docker run -v jsl_keys.json:/content/sparknlp_keys.json -p 8515:8515 -it johnsnowlabs/sparknlp:sparknlp_api\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "CNjbwfPpiIkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consuming the API from a Python Script\n",
        "\n",
        "Use this code to query the API either sequentially (1 call at a time) or sending N concurrent calls using ThreadPoolExecutor"
      ],
      "metadata": {
        "id": "aDCAI5KriNTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "ner_text = \"\"\"\n",
        "A 28-year-old female with a history of gestational diabetes mellitus diagnosed eight years prior to presentation and subsequent type two diabetes mellitus ( T2DM ), one prior episode of HTG-induced pancreatitis three years prior to presentation , associated with an acute hepatitis , and obesity with a body mass index ( BMI ) of 33.5 kg/m2 , presented with a one-week history of polyuria , polydipsia , poor appetite , and vomiting. The patient was prescribed 1 capsule of Advil 10 mg for 5 days and magnesium hydroxide 100mg/1ml suspension PO. \n",
        "He was seen by the endocrinology service and she was discharged on 40 units of insulin glargine at night , 12 units of insulin lispro with meals , and metformin 1000 mg two times a day.\n",
        "\"\"\"\n",
        "\n",
        "modelname = 'clinical_deidentification'\n",
        "# modelname = 'ner_profiling_clinical'\n",
        "\n",
        "def get_url(args):\n",
        "    res = requests.get(args[0])\n",
        "    return res    "
      ],
      "metadata": {
        "id": "Kpr1_XjniO4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 call\n",
        "# ==================\n",
        "query = f\"?modelname={modelname}&text={ner_text}\"\n",
        "url = f\"http://localhost:8515/benchmark/pipeline{query}\"\n",
        "\n",
        "print(get_url([url]))"
      ],
      "metadata": {
        "id": "Jzv1yn86546d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N calls in parallel\n",
        "# ==================\n",
        "list_of_urls = []\n",
        "\n",
        "N_CALLS = 10    \n",
        "for i in range(0, N_CALLS):\n",
        "  list_of_urls.append((url, i))\n",
        "\n",
        "with ThreadPoolExecutor() as pool:\n",
        "  response_list = list(pool.map(get_url, list_of_urls))\n",
        "  print(response_list)"
      ],
      "metadata": {
        "id": "tWFAI9kU56Lt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "synapse2",
      "language": "python",
      "name": "synapse2"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.8.12",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Synapse example",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 4082361247410207
    },
    "colab": {
      "name": "Serving SparkNLP with FastAPI and LP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}