{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8W51t04BN6B"
   },
   "source": [
    "![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "A5g1XTOJif3u"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings_JSL/Legal/06.2.Relation_Extraction_Training.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nT0QsH4lulSx"
   },
   "source": [
    "# Relation Extraction Model Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZX9WeJ9QuIyW"
   },
   "source": [
    "This is our BertSpan-based Relation Extraction model, based on [this paper](https://arxiv.org/abs/1907.10529), an implemented by John Snow Labs on Tensorflow 1.x\n",
    "\n",
    "Unfortunately, from Nov 2022 Google Colab discontinued the support of TF 1.x. \n",
    "\n",
    "**We are working on the TF 2.x version of it.**\n",
    "\n",
    "In the meantime, please use non-colab environments with jupyter and TF 1.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kVDAlIrvaHc"
   },
   "source": [
    "If you use GPU machine, you can save your training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oE5JUoLwiNuR",
    "outputId": "e70093da-12a8-411f-b24f-fa8e8206dd51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan  6 21:16:04 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   30C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eU6ABamPiNuS",
    "outputId": "05e3e05c-bccc-46b4-acf9-98c1f0929085"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\r\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\r\n",
      "Built on Wed_Oct_23_19:24:38_PDT_2019\r\n",
      "Cuda compilation tools, release 10.2, V10.2.89\r\n"
     ]
    }
   ],
   "source": [
    "! nvcc --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bV_uPKLEkBOY"
   },
   "source": [
    "# 1.1. Installing Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgui51vikAke"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from os.path import exists\n",
    "\n",
    "with open('your_license_path', 'r') as f:\n",
    "    license_keys = json.load(f)\n",
    "\n",
    "# Defining license key-value pairs as local variables\n",
    "locals().update(license_keys)\n",
    "\n",
    "# Adding license key-value pairs to environment variables\n",
    "os.environ.update(license_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zN6kazlLkP7v"
   },
   "source": [
    "# 1.2. Installing Spark NLP (licensed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fcM-1RQkPhW"
   },
   "outputs": [],
   "source": [
    "# Installing pyspark and spark-nlp\n",
    "! pip install --upgrade pyspark==3.1.2 spark-nlp==$PUBLIC_VERSION\n",
    "\n",
    "# Installing Spark NLP Healthcare\n",
    "! pip install --upgrade spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET\n",
    "\n",
    "# Installing Spark NLP Display Library for visualization\n",
    "! pip install spark-nlp-display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7a-_DjLmkYIu"
   },
   "source": [
    "# 1.3. Starting Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "Y8u2LfDqkYcO",
    "outputId": "6bd07e49-4395-4075-97d7-ab2a744b45eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP Version : 4.2.4\n",
      "Spark NLP_JSL Version : 4.2.4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-18-232.us-east-2.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark NLP Licensed</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9d78406cc0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from zipfile import ZipFile\n",
    "from io import BytesIO\n",
    "import os\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl\n",
    "import sparknlp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Spark NLP Version :\", sparknlp.version())\n",
    "print(\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n",
    "\n",
    "spark = sparknlp_jsl.start(license_keys['SECRET'])\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3R2CM28aC-wB"
   },
   "source": [
    "# Relation Extraction training using TensorFlow 1.x and BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9a7QwiuC-wC"
   },
   "source": [
    "# 2. Download BERT code implementation and BERT weights\n",
    "In this section we will download official BERT code and the Bert pretrained weights we will use to finetune and create our RE model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASRUv_ZAouZM"
   },
   "source": [
    "## 2.1. Downloading BERT code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lnnr5eENC-wD",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/google-research/bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32eMMkztowrk"
   },
   "source": [
    "## 2.2. Downloading pretrained BERT weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F_92PRTXPboJ"
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/bert_models/2018_10_18/cased_L-12_H-768_A-12.zip\n",
    "#!wget https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJq67k3EukMH"
   },
   "outputs": [],
   "source": [
    "!rm -Rf models trained || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcLXdbt2Qyxv"
   },
   "outputs": [],
   "source": [
    "!mkdir models || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8T7HVt4acHKA"
   },
   "outputs": [],
   "source": [
    "!mkdir trained || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_K3_f2DGQwCH"
   },
   "outputs": [],
   "source": [
    "!mv cased_L-12_H-768_A-12.zip ./models\n",
    "#!mv uncased_L-12_H-768_A-12.zip ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mf3iaDXePd35"
   },
   "outputs": [],
   "source": [
    "!cd models && unzip -n cased_L-12_H-768_A-12.zip\n",
    "#!cd models && unzip -n uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ICMfjrjPI3DS"
   },
   "outputs": [],
   "source": [
    "!mv models/cased_L-12_H-768_A-12/* models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "plRnHzH7C-wE"
   },
   "source": [
    "## 2.3.Add BERT to System Path\n",
    "Bert code will look for several modules in the system path, so we need to set that they can be also find in `bert` folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sMzq11giC-wF"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "BERT_CODE = os.getcwd() + \"/bert\"\n",
    "\n",
    "if not BERT_CODE in sys.path:\n",
    "    sys.path += [BERT_CODE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hV32u1bXC-wF"
   },
   "source": [
    "## 3. Library installation\n",
    "Required:\n",
    "```\n",
    "- Java 8\n",
    "- The Data Science classical kit (pandas+numpy+scipy)\n",
    "- Tensorflow 1.x\n",
    "- PySpark+SparkNLP\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvijWW7uC-wG"
   },
   "source": [
    "#### Java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YD3_ZCerC-wG"
   },
   "outputs": [],
   "source": [
    "# Make sure java 8 is installed.\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Zj7BSFxDKFv",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# If not, run:\n",
    "!sudo apt-get update\n",
    "!sudo apt-get purge -y openjdk-11* -qq > /dev/null && sudo apt-get autoremove -y -qq > /dev/null\n",
    "!sudo apt-get install -y openjdk-8-jdk-headless -qq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndZfkT4TDPap"
   },
   "outputs": [],
   "source": [
    "# Make sure java 8 is installed.\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YsC41EZ7C-wH"
   },
   "source": [
    "#### Data Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IuziH1twC-wI",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandas numpy==1.19.5 scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hwbGACJLD6q2",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4v68yr0oC-wJ"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7tCDK5vC-wJ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhTLs1GEDuWc"
   },
   "source": [
    "#### Make sure TF 1.x is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3NBkVhy-rooZ",
    "outputId": "efd0a0cb-b275-475c-f1b7-78584be0151e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae0959zLC-wJ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import modeling\n",
    "import optimization\n",
    "import tokenization\n",
    "import run_classifier\n",
    "import shutil\n",
    "import os\n",
    "import pprint\n",
    "from IPython.display import clear_output\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "from functools import reduce\n",
    "import scipy.stats as stats\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNhPnkptjOSk",
    "outputId": "ef93fe66-6c80-4393-b6b9-5a8619bb33b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device_count {\n",
       "  key: \"GPU\"\n",
       "  value: 1\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = tf.ConfigProto(device_count = {'GPU': 1})\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqdZ1zP6GANF"
   },
   "source": [
    "## Hyperparam configuration\n",
    "There are 2 available: generic BERT and specific BioBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJb7DF_mGhD1"
   },
   "source": [
    "### Base BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OByOhtRZGqBK"
   },
   "outputs": [],
   "source": [
    "class BaseBertI2B2Config:\n",
    "    #maximum sequence length, can be up to 512 for standard Bertmodels\n",
    "    #larger values require more GPU memory\n",
    "    #A GTX1080 Ti with 11 GB of memory can do no more than batch size 16 with max_seq_len 128\n",
    "    MAX_SEQ_LENGTH = 256\n",
    "\n",
    "    #location of pretrained Bert model\n",
    "    BERT_MODEL_PATH = \"./models\"\n",
    "    #location of Bert chekpoint used for initializing the model\n",
    "    BERT_MODEL_CHECKPOINT_PATH = \"{}/bert_model.ckpt\".format(BERT_MODEL_PATH)\n",
    "    #location of Bert configuration file\n",
    "    BERT_MODEL_CONFIG_PATH = \"{}/bert_config.json\".format(BERT_MODEL_PATH)\n",
    "    #location of Bert vocabulary file\n",
    "    BERT_VOCAB_PATH = \"{}/vocab.txt\".format(BERT_MODEL_PATH)\n",
    "\n",
    "    #Location for storing trained models  (in checkpoing format)\n",
    "    CHKPOINT_PATH = \"./trained\"\n",
    "   \n",
    "    #Location to export trained models to (in saved_model format)\n",
    "    EXPORT_PATH = \"./models/basebert_re\"\n",
    "\n",
    "    #Initial LR, real LR depends on warm-up and training progress\n",
    "    LEARNING_RATE = 2e-5\n",
    "    #Number of training epochs (how many time to iterate through the training set)\n",
    "    NUM_TRAIN_EPOCHS = 3\n",
    "    #Proportion of training steps(i.e. number of batches) used for warming up (adaptive LR in the begging)\n",
    "    WARMUP_PROPORTION = 0\n",
    "    #Training batch size\n",
    "    BATCH_SIZE = 16\n",
    "    #Batch size during testing/valdiation\n",
    "    V_BATCH_SIZE = 100\n",
    "\n",
    "    #Sentence column name\n",
    "    SENTENCE_COLUMN = \"text\"\n",
    "    #Relation label column name\n",
    "    REL_LABEL_COLUMN = \"rel\"\n",
    "    #Relation argument binding colum name - used if (some of the) relations are not symmetric\n",
    "    #0 - symmetric relation, argument order doesn't matter\n",
    "    #1 - rel(ARG1, ARG2), where ARGS1 is the entity which first appears in the text\n",
    "    #2 - rel(ARG2, ARG1)\n",
    "    #if None, then ignore argument order(i.e. treat all relations as symmetric)\n",
    "    REL_ARG_BINDING_COLUMN = 'direction'\n",
    "\n",
    "    #Entities positions in the dataset\n",
    "    ENTITY1_BEGIN_COLUMN = \"firstCharEnt1\"\n",
    "    ENTITY1_END_COLUMN = \"lastCharEnt1\"\n",
    "    ENTITY2_BEGIN_COLUMN = \"firstCharEnt2\"\n",
    "    ENTITY2_END_COLUMN = \"lastCharEnt2\"\n",
    "\n",
    "\n",
    "    ENTITY1_START_TAG = \"e1b\"\n",
    "    ENTITY1_END_TAG = \"e1e\"\n",
    "    ENTITY2_START_TAG = \"e2b\"\n",
    "    ENTITY2_END_TAG = \"e2e\"\n",
    "\n",
    "    ENTITY1_START_TAG_ID = 10\n",
    "    ENTITY1_END_TAG_ID = 11\n",
    "    ENTITY2_START_TAG_ID = 12\n",
    "    ENTITY2_END_TAG_ID = 13\n",
    "\n",
    "    \n",
    "    NUM_HIDDEN_UNITS = 0\n",
    "    \n",
    "    DROPOUT_RATE = 0\n",
    "\n",
    "    #stadard padding id value for Bert models\n",
    "    PAD_ID = 0\n",
    "\n",
    "    #proportion of training examples\n",
    "    TRAIN_SET_PROB = 0.8\n",
    "    \n",
    "    #Not used at the moment\n",
    "    REPLACE_ARG_PROB = 0    \n",
    "    \n",
    "    USE_ENTITY_POSITIONS = True\n",
    "    USE_CLS_POSITION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6HGO7u-_-Ei"
   },
   "outputs": [],
   "source": [
    "# By default, we will use BaseBert (see step 1 in Main to change it)\n",
    "BertREConfig = BaseBertI2B2Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEt6gSySHOO0"
   },
   "source": [
    "## Data collection\n",
    "Set of functions to get input data from pandas or Spark dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cda7eoHxHkfG"
   },
   "source": [
    "### Reading RE data from a pandas dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWEMFeYJHh4Z"
   },
   "outputs": [],
   "source": [
    "def collect_data_from_pandas_dataset(dataset):\n",
    "    \n",
    "    rel_labels = sorted(dataset[BertREConfig.REL_LABEL_COLUMN].unique())\n",
    "    \n",
    "    def process_row(row):\n",
    "\n",
    "        row[\"sentence\"] = annotate_sentence(\n",
    "            row[BertREConfig.SENTENCE_COLUMN], \n",
    "            row[BertREConfig.ENTITY1_BEGIN_COLUMN],\n",
    "            row[BertREConfig.ENTITY1_END_COLUMN],\n",
    "            row[BertREConfig.ENTITY2_BEGIN_COLUMN],\n",
    "            row[BertREConfig.ENTITY2_END_COLUMN]\n",
    "        )\n",
    "        row[\"rel_label_id\"] = rel_labels.index(row[BertREConfig.REL_LABEL_COLUMN])\n",
    "        row[\"rel_arg_binding\"] = row[BertREConfig.REL_ARG_BINDING_COLUMN] if BertREConfig.REL_ARG_BINDING_COLUMN else 0\n",
    "\n",
    "        return row\n",
    "    \n",
    "    \n",
    "    dataset = dataset.apply(process_row, axis=1)\n",
    "    \n",
    "    return dataset.sentence, dataset.rel_label_id, dataset.rel_arg_binding, rel_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oa6ZlI3RHoXp"
   },
   "source": [
    "### Reading RE data from a spark dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yC9xTGgjHn3r"
   },
   "outputs": [],
   "source": [
    "def collect_data_from_spark_dataset(dataset):\n",
    "    \n",
    "    rel_labels = sorted([row[0] for row in dataset.select(BertREConfig.REL_LABEL_COLUMN).distinct().collect()])\n",
    "    \n",
    "    def process_row(row):\n",
    "        sentence = annotate_sentence(\n",
    "            row[BertREConfig.SENTENCE_COLUMN], \n",
    "            int(row[BertREConfig.ENTITY1_BEGIN_COLUMN]),\n",
    "            int(row[BertREConfig.ENTITY1_END_COLUMN]),\n",
    "            int(row[BertREConfig.ENTITY2_BEGIN_COLUMN]),\n",
    "            int(row[BertREConfig.ENTITY2_END_COLUMN])\n",
    "        )       \n",
    "        rel_label_id = rel_labels.index(row[BertREConfig.REL_LABEL_COLUMN])\n",
    "        \n",
    "        rel_arg_binding = (\n",
    "            row[BertREConfig.REL_ARG_BINDING_COLUMN] if BertREConfig.REL_ARG_BINDING_COLUMN else 0)\n",
    "        \n",
    "        return (sentence, rel_label_id, rel_arg_binding)\n",
    "    \n",
    "    sentences, rel_label_ids, rel_arg_bindings = tuple(\n",
    "        map(list, zip(*dataset.rdd.map(process_row).collect())))\n",
    "    \n",
    "    return sentences, rel_label_ids, rel_arg_bindings, rel_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hO0lZQ6Mvw_"
   },
   "source": [
    "## Data annotation\n",
    "Set of functions to properly annnotate the sentences using Bert reserved tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "TTvWYiKHC-wL"
   },
   "outputs": [],
   "source": [
    "#Add entity markers to Bert vocabulary\n",
    "def update_vocab():\n",
    "    vocab = []\n",
    "\n",
    "    with open(BertREConfig.BERT_VOCAB_PATH, 'r') as F:\n",
    "        vocab = F.readlines()\n",
    "        vocab[BertREConfig.ENTITY1_START_TAG_ID] = BertREConfig.ENTITY1_START_TAG + \"\\n\"\n",
    "        vocab[BertREConfig.ENTITY1_END_TAG_ID] = BertREConfig.ENTITY1_END_TAG + \"\\n\"\n",
    "        vocab[BertREConfig.ENTITY2_START_TAG_ID] = BertREConfig.ENTITY2_START_TAG + \"\\n\"\n",
    "        vocab[BertREConfig.ENTITY2_END_TAG_ID] = BertREConfig.ENTITY2_END_TAG + \"\\n\"\n",
    "\n",
    "    with open(BertREConfig.BERT_VOCAB_PATH, \"w\") as F:\n",
    "        F.writelines(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQ-m2tWNOcZ8"
   },
   "outputs": [],
   "source": [
    "#Tokenize sentence using Bert tokenizer, adding entity markers\n",
    "def tokenize_sentence(sentence, tokenizer, seq_length=BertREConfig.MAX_SEQ_LENGTH, is_test=False):\n",
    "    \n",
    "    tokens = [\"[CLS]\"]\n",
    "\n",
    "    entity_starts = []\n",
    "    entity_ends = []\n",
    "    \n",
    "    for token in tokenizer.tokenize(sentence)[:seq_length - 2]:\n",
    "        if token in [BertREConfig.ENTITY1_START_TAG, BertREConfig.ENTITY2_START_TAG]:\n",
    "            entity_starts.append(len(tokens))\n",
    "            \n",
    "        elif token in [BertREConfig.ENTITY1_END_TAG, BertREConfig.ENTITY2_END_TAG]:\n",
    "            entity_ends.append(len(tokens))\n",
    "            \n",
    "        tokens.append(token)\n",
    "    \n",
    "    tokens.append(\"[SEP]\")    \n",
    "        \n",
    "    if (len(entity_starts) != 2) or (len(entity_ends) != 2):\n",
    "        return False\n",
    "\n",
    "    if not is_test:\n",
    "        if np.random.rand() < BertREConfig.REPLACE_ARG_PROB:\n",
    "            e1_length_diff = (entity_ends[0] - entity_starts[0]) - 2\n",
    "\n",
    "            tokens = tokens[:entity_starts[0] + 1] + [\"[MASK]\"] + tokens[entity_ends[0]:]\n",
    "\n",
    "            entity_ends[0] = entity_starts[0] + 2        \n",
    "\n",
    "            entity_starts[1] = entity_starts[1] - e1_length_diff\n",
    "            entity_ends[1] = entity_ends[1] - e1_length_diff\n",
    "\n",
    "        if np.random.rand() < BertREConfig.REPLACE_ARG_PROB:\n",
    "            tokens = tokens[:entity_starts[1] + 1] + [\"[MASK]\"] + tokens[entity_ends[1]:]\n",
    "            entity_ends[1] = entity_starts[1] + 2        \n",
    "    \n",
    "    assert(tokens[entity_starts[0]] == BertREConfig.ENTITY1_START_TAG)\n",
    "    assert(tokens[entity_starts[1]] == BertREConfig.ENTITY2_START_TAG)\n",
    "    assert(tokens[entity_ends[0]] == BertREConfig.ENTITY1_END_TAG)\n",
    "    assert(tokens[entity_ends[1]] == BertREConfig.ENTITY2_END_TAG)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "    return (input_ids, entity_starts[0], entity_starts[1], tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     22,
     45
    ],
    "id": "yJ47h7yXC-wL"
   },
   "outputs": [],
   "source": [
    "def annotate_sentence(sentence, e1_begin, e1_end, e2_begin, e2_end):\n",
    "    \n",
    "    a1_start = min(e1_begin - 1, e2_begin)    \n",
    "    a1_end = min(e1_end + 1, e2_end + 1)\n",
    "\n",
    "    a2_start = max(e1_begin - 1, e2_begin)\n",
    "    a2_end = max(e1_end + 1, e2_end + 1)\n",
    "    \n",
    "    new_sentence = \" \".join([\n",
    "        sentence[:a1_start], \n",
    "        BertREConfig.ENTITY1_START_TAG, \n",
    "        sentence[a1_start:a1_end],\n",
    "        BertREConfig.ENTITY1_END_TAG, \n",
    "        sentence[a1_end:a2_start],\n",
    "        BertREConfig.ENTITY2_START_TAG, \n",
    "        sentence[a2_start:a2_end],\n",
    "        BertREConfig.ENTITY2_END_TAG, \n",
    "        sentence[a2_end:]\n",
    "    ])\n",
    "\n",
    "    return new_sentence      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZuklXrSOGjr"
   },
   "source": [
    "## Feature Engineering\n",
    "RE Feature Engineering consists of token ids (input_ids), entities POS and label ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "id": "7UgoCaSEC-wM"
   },
   "outputs": [],
   "source": [
    "#Representation of RE featurues\n",
    "class REFeatures(object):\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"{} ({})\".format(\n",
    "            \", \".join(\n",
    "                map(lambda x: str(x), self.input_ids)), \n",
    "            self.sentence)\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_ids,\n",
    "                 entity1_pos,\n",
    "                 entity2_pos,\n",
    "                 rel_label_id,\n",
    "                 rel_arg_binding,\n",
    "                 sentence=\"\"):\n",
    "\n",
    "        self.input_ids = input_ids\n",
    "        self.entity1_pos = entity1_pos\n",
    "        self.entity2_pos = entity2_pos\n",
    "        self.rel_label_id = rel_label_id\n",
    "        self.rel_arg_binding = rel_arg_binding\n",
    "        self.sentence = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKDfz-YtS1Uy"
   },
   "outputs": [],
   "source": [
    "#Create RE features from a list of sentences and targets\n",
    "def make_features(sentences, targets, tokenizer, is_test=False):    \n",
    "    features = []\n",
    "    for i in range(len(sentences)):\n",
    "        ts = tokenize_sentence(sentences[i], tokenizer, is_test=is_test)\n",
    "        if ts:\n",
    "            features.append(\n",
    "                REFeatures(\n",
    "                    input_ids=ts[0],\n",
    "                    entity1_pos=ts[1],\n",
    "                    entity2_pos=ts[2],\n",
    "                    rel_label_id=targets[i][0],\n",
    "                    rel_arg_binding=targets[i][1],\n",
    "                    sentence=\" \".join(ts[3])\n",
    "                ))\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4jtoNOaRTlq-"
   },
   "source": [
    "## Batches creation\n",
    "For feeding the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVhHAaF3Thd5"
   },
   "outputs": [],
   "source": [
    "#Make a batch of training /testing examples. \n",
    "#If max_seq_len is None, then use the sequence max length in the batch\n",
    "\n",
    "def make_batch(features, max_seq_len = None):\n",
    "    batch_size = len(features)\n",
    "    use_rel_args = BertREConfig.REL_ARG_BINDING_COLUMN is not None\n",
    "    if max_seq_len is None:\n",
    "        max_seq_len = max([len(f.input_ids) for f in features])\n",
    "    \n",
    "    input_ids = np.ones([batch_size, max_seq_len], dtype=np.int32) * BertREConfig.PAD_ID\n",
    "    input_mask = np.zeros([batch_size, max_seq_len], dtype=np.int32)\n",
    "    segment_ids = np.zeros([batch_size, max_seq_len], dtype=np.int32)\n",
    "    entity1_pos = np.zeros([batch_size], dtype=np.int32)\n",
    "    entity2_pos = np.zeros([batch_size], dtype=np.int32)\n",
    "    rel_label_ids = np.zeros([batch_size], dtype=np.int32)\n",
    "    rel_arg_bindings = np.zeros([batch_size], dtype=np.int32)\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    for f in features:\n",
    "        \n",
    "        input_ids[i, :len(f.input_ids)] = np.array(f.input_ids)\n",
    "        input_mask[i, :len(f.input_ids)] = 1\n",
    "        rel_label_ids[i] = f.rel_label_id\n",
    "        rel_arg_bindings[i] = f.rel_arg_binding\n",
    "        entity1_pos[i] = f.entity1_pos\n",
    "        entity2_pos[i] = f.entity2_pos\n",
    "        i += 1\n",
    "    \n",
    "    batch = {\n",
    "        \"input_ids:0\": input_ids,\n",
    "        \"input_mask:0\": input_mask,\n",
    "        \"segment_ids:0\": segment_ids,\n",
    "        \"rel_label_ids:0\": rel_label_ids,\n",
    "        \"entity1_pos:0\": entity1_pos,\n",
    "        \"entity2_pos:0\": entity2_pos,\n",
    "    }\n",
    "    \n",
    "    if use_rel_args:\n",
    "        batch[\"rel_arg_bindings:0\"] = rel_arg_bindings\n",
    "        \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9sho0T5S8EH"
   },
   "source": [
    "## Optimizer creation\n",
    "To carry out gradient descent and weight update with specific warm up, learning rate, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "id": "3OiHJCQTC-wM"
   },
   "outputs": [],
   "source": [
    "#Create Bert RE optimizer graph\n",
    "def create_optimizer(loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu):\n",
    "    \n",
    "    global_step = tf.train.get_or_create_global_step()    \n",
    "    \n",
    "    # Implements linear decay of the learning rate.\n",
    "    learning_rate = tf.train.polynomial_decay(\n",
    "      learning_rate,\n",
    "      global_step,\n",
    "      num_train_steps,\n",
    "      end_learning_rate=0.0,\n",
    "      power=1.0,\n",
    "      cycle=False)\n",
    "\n",
    "    tf.identity(learning_rate, name=\"c_lr\")\n",
    "    \n",
    "    # Implements linear warmup. I.e., if global_step < num_warmup_steps, the\n",
    "    # learning rate will be `global_step/num_warmup_steps * init_lr`.\n",
    "    \n",
    "    global_steps_int = tf.cast(global_step, tf.int32)\n",
    "    warmup_steps_int = num_warmup_steps\n",
    "\n",
    "    global_steps_float = tf.cast(global_steps_int, tf.float32)\n",
    "    warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n",
    "\n",
    "    warmup_percent_done = global_steps_float / warmup_steps_float\n",
    "    warmup_learning_rate = learning_rate * warmup_percent_done\n",
    "\n",
    "    is_warmup = tf.cast(global_steps_int < warmup_steps_int, tf.float32)\n",
    "    learning_rate = (\n",
    "        (1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n",
    "\n",
    "    \n",
    "    tf.identity(learning_rate, name=\"c_lr2\")\n",
    "    \n",
    "    # It is recommended that you use this optimizer for fine tuning, since this\n",
    "    # is how the model was trained (note that the Adam m/v variables are NOT\n",
    "    # loaded from init_checkpoint.)\n",
    "    optimizer = optimization.AdamWeightDecayOptimizer(\n",
    "      learning_rate=learning_rate,\n",
    "      weight_decay_rate=0.01,\n",
    "      beta_1=0.9,\n",
    "      beta_2=0.999,\n",
    "      epsilon=1e-6,\n",
    "      exclude_from_weight_decay=[\"LayerNorm\", \"layer_norm\", \"bias\"])\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads = tf.gradients(loss, tvars)\n",
    "\n",
    "    # This is how the model was pre-trained.\n",
    "    (grads, _) = tf.clip_by_global_norm(grads, clip_norm=1.0)\n",
    "\n",
    "    train_op = optimizer.apply_gradients(\n",
    "      zip(grads, tvars), global_step=global_step)\n",
    "\n",
    "    # Normally the global step update is done inside of `apply_gradients`.\n",
    "    # However, `AdamWeightDecayOptimizer` doesn't do this. But if you use\n",
    "    # a different optimizer, you should probably take this line out.\n",
    "    new_global_step = global_step + 1\n",
    "    train_op = tf.group(train_op, [global_step.assign(new_global_step)], name=\"optimizer\")\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mo-ebbmCT1fR"
   },
   "source": [
    "## BERT model creation\n",
    "Creationg of the model using BERT architecture on TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a14QFWt9TFsR"
   },
   "outputs": [],
   "source": [
    "#Create Bert RE model graph, for training (is_trainable = True) and for inference (is_trainable = False)\n",
    "def create_model(\n",
    "    num_relations, \n",
    "    num_arg_bindings=3, \n",
    "    num_hidden_units=BertREConfig.NUM_HIDDEN_UNITS, \n",
    "    chkpoint_path=None, \n",
    "    is_trainable=True,\n",
    "    config=config):\n",
    "    \n",
    "    with tf.Session(config=config) as session:    \n",
    "\n",
    "        num_train_steps = tf.compat.v1.placeholder_with_default(\n",
    "            input=tf.constant(1000, dtype=tf.float32), shape=(), name=\"num_train_steps\")\n",
    "        \n",
    "        num_warm_up_steps = tf.compat.v1.placeholder_with_default(\n",
    "            input=tf.cast(tf.round(0.1 * num_train_steps), tf.int32), shape=(), name=\"num_warm_up_steps\")\n",
    "\n",
    "        input_ids = tf.compat.v1.placeholder(\n",
    "            dtype=tf.compat.v1.int32, shape=(None, None), name=\"input_ids\")\n",
    "        \n",
    "        batch_size = tf.shape(input_ids)[0]\n",
    "        seq_len = tf.shape(input_ids)[1]\n",
    "        \n",
    "        input_mask = tf.compat.v1.placeholder(\n",
    "            dtype=tf.compat.v1.int32, shape=(None, None), name=\"input_mask\")\n",
    "        \n",
    "        segment_ids = tf.compat.v1.placeholder(\n",
    "            dtype=tf.compat.v1.int32, shape=(None, None), name=\"segment_ids\")\n",
    "        \n",
    "        rel_label_ids = tf.compat.v1.placeholder(\n",
    "            dtype=tf.compat.v1.int32, shape=(None), name=\"rel_label_ids\")\n",
    "        \n",
    "        rel_arg_bindings = tf.compat.v1.placeholder_with_default(\n",
    "            input=tf.zeros(shape=(batch_size),dtype=tf.compat.v1.int32), shape=(None), name=\"rel_arg_bindings\")\n",
    "        \n",
    "        entity1_pos = tf.compat.v1.placeholder(\n",
    "            dtype=tf.compat.v1.int32, shape=(None), name=\"entity1_pos\")\n",
    "        \n",
    "        entity2_pos = tf.compat.v1.placeholder(\n",
    "            dtype=tf.compat.v1.int32, shape=(None), name=\"entity2_pos\")\n",
    "        \n",
    "        dropout_rate = tf.compat.v1.placeholder_with_default(\n",
    "            input=tf.constant(BertREConfig.DROPOUT_RATE, dtype=tf.float32), shape=(), name=\"dropout_rate\")\n",
    "        \n",
    "        learning_rate = tf.compat.v1.placeholder_with_default(\n",
    "            input=tf.constant(2e-5, dtype=tf.float32), shape=(), name=\"learning_rate\")\n",
    "        \n",
    "        config = modeling.BertConfig.from_json_file(BertREConfig.BERT_MODEL_CONFIG_PATH)\n",
    "\n",
    "        bert_model = modeling.BertModel(\n",
    "            config=config,\n",
    "            is_training=is_trainable,\n",
    "            input_ids=input_ids,\n",
    "            input_mask=input_mask,\n",
    "            token_type_ids=segment_ids)\n",
    "\n",
    "        if chkpoint_path:\n",
    "            tvars = tf.trainable_variables()\n",
    "            (assignment_map, initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(\n",
    "                tvars, chkpoint_path)\n",
    "            tf.train.init_from_checkpoint(chkpoint_path, assignment_map)    \n",
    "\n",
    "        output_layer = bert_model.get_sequence_output()\n",
    "\n",
    "        if BertREConfig.USE_ENTITY_POSITIONS:\n",
    "            #get entity start marker embeddings\n",
    "\n",
    "            #E1 mask\n",
    "            entity1_mask =  tf.repeat(\n",
    "                tf.one_hot(entity1_pos, seq_len), \n",
    "                config.hidden_size, \n",
    "                axis=1)\n",
    "\n",
    "            #E2 mask\n",
    "            entity2_mask =  tf.repeat(\n",
    "                tf.one_hot(entity2_pos, seq_len), \n",
    "                config.hidden_size, \n",
    "                axis=1)\n",
    "\n",
    "\n",
    "            #Hidden layer representation for E1\n",
    "            entity1_embd = tf.reduce_sum(\n",
    "                tf.reshape(\n",
    "                    (tf.reshape(output_layer, shape=(batch_size, -1)) * entity1_mask), \n",
    "                    shape=[batch_size, seq_len, config.hidden_size]), \n",
    "                axis=1)\n",
    "\n",
    "            #Hidden layer representation for E2\n",
    "            entity2_embd = tf.reduce_sum(\n",
    "                tf.reshape(\n",
    "                    (tf.reshape(output_layer, shape=(batch_size, -1)) * entity2_mask), \n",
    "                    shape=[batch_size, seq_len, config.hidden_size]), \n",
    "                axis=1)\n",
    "\n",
    "            #Concat representions\n",
    "            if BertREConfig.USE_CLS_POSITION:                \n",
    "                classification_layer = tf.concat([entity1_embd, entity2_embd, output_layer[:,0,:]], axis=1)\n",
    "            else:\n",
    "                classification_layer = tf.concat([entity1_embd, entity2_embd], axis=1)\n",
    "        else:\n",
    "            if not BertREConfig.USE_CLS_POSITION:\n",
    "                raise(\"Either USE_ENTITY_POSITIONS or USE_CLS_POSITION should be set to True.\")\n",
    "            else:\n",
    "                classification_layer = output_layer[:,0,:]\n",
    "                \n",
    "        '''Add full connection layer and dropout layer'''\n",
    "        \n",
    "        if num_hidden_units > 0:\n",
    "            fc = tf.layers.dense(\n",
    "                classification_layer, \n",
    "                num_hidden_units, \n",
    "                name='fc1')\n",
    "            fc = tf.nn.relu(fc)\n",
    "        else:\n",
    "            fc = tf.identity(classification_layer, name='fc1')\n",
    "\n",
    "        fc = tf.nn.dropout(fc, rate=dropout_rate)\n",
    "\n",
    "        '''logits'''\n",
    "        rel_label_logits = tf.layers.dense(fc, num_relations, name='rel_label_logits')\n",
    "        rel_label_log_probs = tf.nn.softmax(rel_label_logits, name=\"rel_label_probs\")\n",
    "        rel_label_predictions = tf.argmax(\n",
    "            rel_label_log_probs, axis=-1, output_type=tf.int32, name=\"rel_label_predictions\")\n",
    "\n",
    "        if num_arg_bindings > 1:    \n",
    "            rel_arg_binding_logits = tf.layers.dense(fc, num_arg_bindings, name='rel_arg_binding_logits')\n",
    "            rel_arg_binding_probs = tf.nn.softmax(rel_arg_binding_logits, name=\"rel_arg_binding_probs\")\n",
    "            rel_arg_binding_predictions = tf.argmax(\n",
    "                rel_arg_binding_probs, axis=-1, output_type=tf.int32, name=\"rel_arg_binding_predictions\")\n",
    "        else:\n",
    "            rel_arg_binding_probs = tf.ones_like(rel_arg_bindings, name=\"rel_arg_binding_probs\")\n",
    "            rel_arg_binding_predictions = tf.zeros_like(\n",
    "                rel_arg_bindings, name=\"rel_arg_binding_predictions\")\n",
    "\n",
    "        '''Calculate loss. Convert predicted labels into one hot form. '''            \n",
    "        rel_label_targets = tf.one_hot(rel_label_ids, depth=num_relations)\n",
    "        rel_label_loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                labels=rel_label_targets,\n",
    "                logits=rel_label_logits)\n",
    "\n",
    "        if num_arg_bindings > 1:                \n",
    "            rel_arg_binding_targets = tf.one_hot(rel_arg_bindings, depth=num_arg_bindings)\n",
    "            rel_arg_binding_loss = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                    labels=rel_arg_binding_targets,\n",
    "                    logits=rel_arg_binding_logits)\n",
    "        else:\n",
    "            rel_arg_binding_loss = 0                    \n",
    "        \n",
    "        rel_label_example_accuracy = tf.cast(\n",
    "            tf.equal(rel_label_predictions, rel_label_ids), tf.float32, name=\"rel_label_acc\")\n",
    "        \n",
    "        rel_label_accuracy = tf.reduce_mean(rel_label_example_accuracy, name=\"rel_label_mean_acc\")        \n",
    "        \n",
    "        rel_arg_binding_example_accuracy = tf.cast(\n",
    "            tf.equal(rel_arg_binding_predictions, rel_arg_bindings), tf.float32, name=\"rel_arg_binding_acc\")\n",
    "        \n",
    "        rel_arg_binding_accuracy = tf.reduce_mean(\n",
    "            rel_arg_binding_example_accuracy, name=\"rel_arg_binding_mean_acc\")\n",
    "        \n",
    "        total_example_accuracy = tf.identity(\n",
    "            rel_label_example_accuracy * rel_arg_binding_example_accuracy, name=\"total_acc\")\n",
    "        \n",
    "        total_accuracy = tf.identity(total_example_accuracy, name=\"total_mean_acc\")\n",
    "        \n",
    "        \n",
    "        loss = tf.reduce_mean(rel_label_loss + rel_arg_binding_loss, name=\"loss\")\n",
    "\n",
    "        if is_trainable:\n",
    "            train_op = create_optimizer(\n",
    "                loss, \n",
    "                learning_rate, \n",
    "                num_train_steps, \n",
    "                num_warm_up_steps,\n",
    "                use_tpu=False)        \n",
    "        else:\n",
    "            train_op = tf.no_op()\n",
    "            \n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        return (\n",
    "                train_op,\n",
    "                loss,       \n",
    "                total_accuracy,\n",
    "                (rel_label_predictions, rel_arg_binding_predictions), \n",
    "                (rel_label_log_probs, rel_arg_binding_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XWjsPywUsNd"
   },
   "source": [
    "## Model saving\n",
    "Function to export the trained BERT model to disk in TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "iK6lj4PiC-wN"
   },
   "outputs": [],
   "source": [
    "def export_model(model_id, is_trainable = True, num_arg_bindings = 3, config=config):\n",
    "    \n",
    "    with tf.Session(config=config) as session:\n",
    "    \n",
    "        model = create_model(\n",
    "            len(rel_labels), \n",
    "            is_trainable=is_trainable,\n",
    "            num_arg_bindings=num_arg_bindings)\n",
    "    \n",
    "        input_tensors = {}\n",
    "        output_tensors = {}\n",
    "\n",
    "        input_tensors_names = {\n",
    "            \"input_ids:0\",\n",
    "            \"input_mask:0\",\n",
    "            \"segment_ids:0\",\n",
    "            \"entity1_pos:0\",\n",
    "            \"entity2_pos:0\",\n",
    "        }\n",
    "\n",
    "        output_tensors_names = [\n",
    "            \"loss:0\",\n",
    "            \"rel_label_acc:0\",\n",
    "            \"rel_arg_binding_acc:0\",\n",
    "            \"total_acc:0\",\n",
    "            \"rel_label_probs:0\",\n",
    "            \"rel_label_predictions:0\",\n",
    "            \"rel_arg_binding_probs:0\",\n",
    "            \"rel_arg_binding_predictions:0\"        \n",
    "        ]\n",
    "\n",
    "\n",
    "        for k in input_tensors_names:\n",
    "            t = session.graph.get_tensor_by_name(k)\n",
    "            input_tensors[t.name] = t\n",
    "\n",
    "        for k in output_tensors_names:\n",
    "            t = session.graph.get_tensor_by_name(k)\n",
    "            output_tensors[k] = t\n",
    "        \n",
    "        print(\"{} trainable variables: \".format(len(tf.trainable_variables())))\n",
    "        size_f = lambda v: reduce(lambda x, y: x*y, v.get_shape().as_list())\n",
    "        n = sum(size_f(v) for v in tf.trainable_variables())\n",
    "        print(\"{} trainbale parameters.\".format(n))    \n",
    "        \n",
    "        tf.train.Saver().restore(session, f\"{BertREConfig.CHKPOINT_PATH}/{model_id}/model\")\n",
    "\n",
    "        shutil.rmtree(BertREConfig.EXPORT_PATH, ignore_errors=True)\n",
    "\n",
    "        #save model\n",
    "        tf.saved_model.simple_save(\n",
    "            session,\n",
    "            BertREConfig.EXPORT_PATH,\n",
    "            inputs=input_tensors,\n",
    "            outputs=output_tensors\n",
    "        )\n",
    "\n",
    "        #copy assets to the destiation folder\n",
    "        shutil.copytree(\n",
    "            f\"{BertREConfig.CHKPOINT_PATH}/{model_id}/assets\", \n",
    "            f\"{BertREConfig.EXPORT_PATH}/assets\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcIgaAyfYH_V"
   },
   "source": [
    "## Training\n",
    "Function to train the model for RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kZfx5USVgxQP",
    "outputId": "2cd504ea-89fb-47c3-8f07-0e5a56aa3069"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan  6 21:17:08 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 470.161.03   Driver Version: 470.161.03   CUDA Version: 11.4     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   30C    P0    23W / 300W |      0MiB / 16160MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ],
    "id": "PaKoD-QDC-wO"
   },
   "outputs": [],
   "source": [
    "#Train a Bert RE model and save it in the checkpoints folder\n",
    "def train_model(model_id, train_features, test_features, rel_labels, num_arg_bindings = 3, config=config):\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    use_rel_args = BertREConfig.REL_ARG_BINDING_COLUMN is not None\n",
    "    \n",
    "    with tf.Session(config=config) as session:\n",
    "\n",
    "        model = create_model(\n",
    "            len(rel_labels), \n",
    "            chkpoint_path=BertREConfig.BERT_MODEL_CHECKPOINT_PATH, \n",
    "            num_arg_bindings=num_arg_bindings, \n",
    "            is_trainable=True,\n",
    "            config=config\n",
    "        )\n",
    "\n",
    "        session.run(\"init\")\n",
    "\n",
    "        num_train_steps = (BertREConfig.NUM_TRAIN_EPOCHS * len(train_features)) // BertREConfig.BATCH_SIZE    \n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        print(\"{:^11}{:^11}{:>10}{:>10}{:>10}{:>10}{:>10}{:>10}{:>10}\".format(\n",
    "            \"Epoch\", \"Batch\", \n",
    "            \"Loss\", \n",
    "            \"L_ACC\", \"Arg_ACC\", \"ACC\",\n",
    "            \"vL_ACC\", \"vArg_ACC\", \"vACC\"))\n",
    "\n",
    "        for e in range(BertREConfig.NUM_TRAIN_EPOCHS):\n",
    "\n",
    "            np.random.shuffle(train_features)            \n",
    "\n",
    "            b_loss = []\n",
    "            b_rel_label_acc = []\n",
    "            b_rel_arg_binding_acc = []\n",
    "            b_total_acc = []\n",
    "            for b in range(0, len(train_features) // BertREConfig.BATCH_SIZE):\n",
    "\n",
    "                batch = make_batch(\n",
    "                    train_features[b * BertREConfig.BATCH_SIZE: (b + 1) * BertREConfig.BATCH_SIZE]\n",
    "                )#, max_seq_len=MAX_SEQ_LENGTH)\n",
    "\n",
    "                data = batch\n",
    "\n",
    "                if b == 0:\n",
    "                    data[\"num_train_steps:0\"] = num_train_steps\n",
    "                    data[\"learning_rate:0\"] = BertREConfig.LEARNING_RATE\n",
    "\n",
    "                eval_tensors = [\n",
    "                    \"optimizer\", \n",
    "                    \"loss:0\", \n",
    "                    \"rel_label_mean_acc:0\",\n",
    "                    \"rel_arg_binding_mean_acc:0\",\n",
    "                    \"total_mean_acc:0\"\n",
    "                ]\n",
    "                _, loss, rel_label_acc, rel_arg_bind_acc, total_acc = session.run(\n",
    "                    eval_tensors, feed_dict=data)\n",
    "                b_loss.append(loss)\n",
    "                b_rel_label_acc.append(rel_label_acc)\n",
    "                b_rel_arg_binding_acc.append(rel_arg_bind_acc)\n",
    "                b_total_acc.append(total_acc)\n",
    "\n",
    "                print(\"\\r{:>5}/{:<5}{:>5}/{:<5}{:>10.4f}{:>10.3f}{:>10.3f}{:>10.3f}\".format(\n",
    "                        e+1,\n",
    "                        BertREConfig.NUM_TRAIN_EPOCHS,\n",
    "                        b + 1, \n",
    "                        len(train_features) // BertREConfig.BATCH_SIZE,\n",
    "                        np.mean(b_loss), \n",
    "                        np.mean(b_rel_label_acc), \n",
    "                        np.mean(b_rel_arg_binding_acc), \n",
    "                        np.mean(b_total_acc)  \n",
    "                    ), end=\"\")\n",
    "\n",
    "\n",
    "            v_rel_label_acc = []\n",
    "            v_rel_arg_binding_acc = []\n",
    "            v_total_acc = []\n",
    "\n",
    "            for v_b in range(0, len(test_features) // BertREConfig.V_BATCH_SIZE):\n",
    "                batch = make_batch(\n",
    "                    test_features[v_b * BertREConfig.V_BATCH_SIZE: (v_b + 1) * BertREConfig.V_BATCH_SIZE])\n",
    "\n",
    "                data = batch\n",
    "\n",
    "                eval_tensors = [\n",
    "                    \"rel_label_mean_acc:0\",\n",
    "                    \"rel_arg_binding_mean_acc:0\",\n",
    "                    \"total_mean_acc:0\",\n",
    "                ]\n",
    "\n",
    "                rel_label_acc, rel_arg_bind_acc, total_acc = session.run(eval_tensors, feed_dict=data)\n",
    "                v_rel_label_acc.append(rel_label_acc)\n",
    "                v_rel_arg_binding_acc.append(rel_arg_bind_acc)\n",
    "                v_total_acc.append(total_acc)\n",
    "\n",
    "            print(\"{:>10.3f}{:>10.3f}{:>10.3f}\".format(\n",
    "                np.mean(v_rel_label_acc), \n",
    "                np.mean(v_rel_arg_binding_acc), \n",
    "                np.mean(v_total_acc)))                \n",
    "\n",
    "\n",
    "        \n",
    "        shutil.rmtree(f\"{BertREConfig.CHKPOINT_PATH}/{model_id}\", ignore_errors=True)\n",
    "        os.mkdir(f\"{BertREConfig.CHKPOINT_PATH}/{model_id}\")\n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(session, f\"{BertREConfig.CHKPOINT_PATH}/{model_id}/model\")\n",
    "        \n",
    "        os.mkdir(f\"{BertREConfig.CHKPOINT_PATH}/{model_id}/assets/\")\n",
    "        \n",
    "        shutil.copy(\n",
    "            BertREConfig.BERT_VOCAB_PATH, \n",
    "            f\"{BertREConfig.CHKPOINT_PATH}/{model_id}/assets/vocab.txt\")\n",
    "        \n",
    "        with open(f\"{BertREConfig.CHKPOINT_PATH}/{model_id}/assets/categories.txt\", \"wt\") as F:\n",
    "            F.writelines(\"\\n\".join(rel_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sfy0E-sdYJmV"
   },
   "source": [
    "## Evaluation\n",
    "Functions to get the metrics on the RE model and print them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     62
    ],
    "id": "wxZCnuUJC-wO"
   },
   "outputs": [],
   "source": [
    "def eval_metrics(model_id, features, rel_labels, num_arg_bindings = 3, exclude_rels=[], config=config):\n",
    "    with tf.Session(config=config) as session:\n",
    "\n",
    "        model = create_model(\n",
    "            len(rel_labels), \n",
    "            num_arg_bindings=num_arg_bindings,\n",
    "            is_trainable=False)\n",
    "        \n",
    "        tf.train.Saver().restore(session, f\"{BertREConfig.CHKPOINT_PATH}/{model_id}/model\")\n",
    "\n",
    "        metrics_data = {}\n",
    "        for rel in rel_labels:\n",
    "            metrics_data[rel] = ([], [], [])\n",
    "            \n",
    "        for v_b in range(0, len(features) // BertREConfig.V_BATCH_SIZE):\n",
    "            batch = make_batch(\n",
    "                features[v_b * BertREConfig.V_BATCH_SIZE: (v_b + 1) * BertREConfig.V_BATCH_SIZE])\n",
    "\n",
    "            data = batch\n",
    "\n",
    "            eval_tensors = [\"total_acc:0\", \"rel_label_ids:0\", \"rel_label_predictions:0\"]\n",
    "\n",
    "            total_acc, rel_label_ids, rel_label_preds = session.run(eval_tensors, feed_dict=data)\n",
    "            \n",
    "            for i in range(len(rel_label_ids)):\n",
    "                acc = total_acc[i]\n",
    "                pred = rel_label_preds[i]\n",
    "                target = rel_label_ids[i]\n",
    "                rel_target = rel_labels[target]\n",
    "                rel_pred = rel_labels[pred]\n",
    "                \n",
    "                metrics_data[rel_target][2].append(1)\n",
    "                \n",
    "                if acc:\n",
    "                    metrics_data[rel_target][0].append(1)\n",
    "                    metrics_data[rel_pred][1].append(1)\n",
    "                else:\n",
    "                    metrics_data[rel_target][0].append(0)\n",
    "                    metrics_data[rel_pred][1].append(0)                \n",
    "\n",
    "        results = {}        \n",
    "        \n",
    "        for rel in [rel for rel in rel_labels if rel not in exclude_rels]:\n",
    "            if len(metrics_data[rel][0]):\n",
    "                recall = np.mean(metrics_data[rel][0])\n",
    "            else:\n",
    "                recall = 0\n",
    "            if len(metrics_data[rel][1]):\n",
    "                precision = np.mean(metrics_data[rel][1])\n",
    "            else:\n",
    "                precision = 0\n",
    "            if (recall + precision):\n",
    "                f1 = 2 * (recall * precision) / (recall + precision)\n",
    "            else:\n",
    "                f1 = np.NaN\n",
    "               \n",
    "            support = np.sum(metrics_data[rel][2])\n",
    "            \n",
    "            results[rel] = (recall, precision, f1, support)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLmYyut-Yait"
   },
   "outputs": [],
   "source": [
    "def print_metrics(results):\n",
    "    print(\"\\n\")\n",
    "    print(\"{:<15}{:>10}{:>10}{:>10}{:>10}\\n\".format(\"Relation\", \"Recall\", \"Precision\", \"F1\", \"Support\"))\n",
    "\n",
    "    for rel in results:\n",
    "\n",
    "        print(f\"{rel:<15}{results[rel][0]:>10.3f}{results[rel][1]:>10.3f}{results[rel][2]:>10.3f}{results[rel][3]:>10}\")\n",
    "\n",
    "    mean_recall = np.mean([results[rel][0] for rel in results])\n",
    "    mean_precision = np.mean([results[rel][1] for rel in results])\n",
    "    mean_f1 = np.mean([results[rel][2] for rel in results])\n",
    "\n",
    "    support_sum = np.sum([results[rel][3] for rel in results])\n",
    "\n",
    "    w_mean_recall = np.sum([results[rel][0] * results[rel][3] for rel in results]) / support_sum\n",
    "    w_mean_precision = np.sum([results[rel][1] * results[rel][3] for rel in results]) / support_sum\n",
    "    w_mean_f1 = np.sum([results[rel][2] * results[rel][3] for rel in results]) / support_sum\n",
    "\n",
    "\n",
    "    metrics_name = \"Avg.\"\n",
    "\n",
    "    print(f\"\\n{metrics_name:<15}{mean_recall:>10.3f}{mean_precision:>10.3f}{mean_f1:>10.3f}\")\n",
    "\n",
    "    metrics_name = \"Weighted Avg.\"\n",
    "\n",
    "    print(f\"\\n{metrics_name:<15}{w_mean_recall:>10.3f}{w_mean_precision:>10.3f}{w_mean_f1:>10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5nW1WF2UYhZ6"
   },
   "source": [
    "## MAIN: STEP-BY-STEP RE MODEL TRAINING EXECUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "De4-ElQ5Y16S"
   },
   "source": [
    "### 1. Using BioBERT hyperparams instead of BertBase ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_SBohPwY0xV"
   },
   "outputs": [],
   "source": [
    "# You can change me to BaseBertI2B2Config\n",
    "BertREConfig = BaseBertI2B2Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6l_Z2MvCZHTs"
   },
   "source": [
    "### 2. Update Bert vocabulary with special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45ho-NbBC-wO"
   },
   "outputs": [],
   "source": [
    "#Update Bert vocabylary\n",
    "update_vocab()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dM26YYEtaJao"
   },
   "source": [
    "### 3. Creating a Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oHNtrmFzaILL"
   },
   "outputs": [],
   "source": [
    "#create tokenizer\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file=BertREConfig.BERT_VOCAB_PATH, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wWtxzECjbSIb"
   },
   "source": [
    "### 5. Read the input data.\n",
    "It should look like as follows (see output) and have the following columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guu863lUfmT7"
   },
   "source": [
    "| Column      |          Explanation                     |\n",
    "|:-----------:|:----------------------------------------:|\n",
    "|dataset      | train/test                               |\n",
    "|source       | data provider                            |\n",
    "|txt_file     | .txt file                                |\n",
    "|sentence     | tokenized text sentence                  |\n",
    "|sent_id      | sentence id                              |\n",
    "|chunk1       | first entity                             |\n",
    "|begin1       | first token number of the first entity   |\n",
    "|end1         | last token number of the first entity    |\n",
    "|rel          | relation (O for no-relation)             |\n",
    "|chunk2       | second entity                            |\n",
    "|begin2       | first token number of the second entity  |\n",
    "|end2         | last token number of the second entity   |\n",
    "|label1       | label of the first entity                |\n",
    "|label2       | label of the second entity               |\n",
    "|lastCharEnt1 | last char number of the first entity     |\n",
    "|firstCharEnt1| first char number of the first entity    |\n",
    "|lastCharEnt2 | last char number of the second entity    |\n",
    "|firstCharEnt2| first char number of the second entity   |\n",
    "|words_in_ent1| number of words in first entity          |\n",
    "|words_in_ent2| number of words in second entity         |\n",
    "|words_between| word between entities                    |\n",
    "|is_train     | is it used for training?                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAMHT9Cki-G9"
   },
   "source": [
    "Yes, We are ready to train REDL model. Now we will train a REDL model to get relations between **DOC**, **PARTY**, **ALIAS** and **EFFDATE** entitties. \n",
    "\n",
    "Let's look at our dataset. \n",
    "\n",
    "Your dataset have to be like this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 852
    },
    "id": "w9fykiyRSYeH",
    "outputId": "44a7a94f-6a70-4823-8002-d43dd8eb1b9e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>firstCharEnt1</th>\n",
       "      <th>firstCharEnt2</th>\n",
       "      <th>lastCharEnt1</th>\n",
       "      <th>lastCharEnt2</th>\n",
       "      <th>chunk1</th>\n",
       "      <th>chunk2</th>\n",
       "      <th>label1</th>\n",
       "      <th>label2</th>\n",
       "      <th>rel</th>\n",
       "      <th>direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EXHIBIT 10.43 Dated 29/3/18\\n\\nDistributorship...</td>\n",
       "      <td>29</td>\n",
       "      <td>65</td>\n",
       "      <td>54</td>\n",
       "      <td>95</td>\n",
       "      <td>Distributorship agreement</td>\n",
       "      <td>Signature Orthopaedics Pty Ltd</td>\n",
       "      <td>DOC</td>\n",
       "      <td>PARTY</td>\n",
       "      <td>signed_by</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EXHIBIT 10.43 Dated 29/3/18\\n\\nDistributorship...</td>\n",
       "      <td>29</td>\n",
       "      <td>102</td>\n",
       "      <td>54</td>\n",
       "      <td>129</td>\n",
       "      <td>Distributorship agreement</td>\n",
       "      <td>CPM Medical Consultants LLC</td>\n",
       "      <td>DOC</td>\n",
       "      <td>PARTY</td>\n",
       "      <td>signed_by</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXHIBIT 10.43 Dated 29/3/18\\n\\nDistributorship...</td>\n",
       "      <td>20</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>54</td>\n",
       "      <td>29/3/18</td>\n",
       "      <td>Distributorship agreement</td>\n",
       "      <td>EFFDATE</td>\n",
       "      <td>DOC</td>\n",
       "      <td>dated_as</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sections 200.80(b)(4) and Rule 406 of the Secu...</td>\n",
       "      <td>173</td>\n",
       "      <td>236</td>\n",
       "      <td>196</td>\n",
       "      <td>247</td>\n",
       "      <td>Collaboration Agreement</td>\n",
       "      <td>Xencor, Inc</td>\n",
       "      <td>DOC</td>\n",
       "      <td>PARTY</td>\n",
       "      <td>signed_by</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monrovia, CA 91016 USA\\n\\n(hereinafter called ...</td>\n",
       "      <td>63</td>\n",
       "      <td>170</td>\n",
       "      <td>97</td>\n",
       "      <td>173</td>\n",
       "      <td>Boehringer Ingelheim International</td>\n",
       "      <td>BII</td>\n",
       "      <td>PARTY</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>has_alias</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3496</th>\n",
       "      <td>By execution of this Supplier/Subcontractor Co...</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>Supplier</td>\n",
       "      <td>Supplier</td>\n",
       "      <td>PARTY</td>\n",
       "      <td>ROLE</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3497</th>\n",
       "      <td>/s Liu Gang Name: LIU GANG   Title: Authoriz...</td>\n",
       "      <td>20</td>\n",
       "      <td>38</td>\n",
       "      <td>28</td>\n",
       "      <td>58</td>\n",
       "      <td>LIU GANG</td>\n",
       "      <td>Authorized Signatory</td>\n",
       "      <td>SIGNING_PERSON</td>\n",
       "      <td>SIGNING_TITLE</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3498</th>\n",
       "      <td>HOFV: HOF VILLAGE, LLC By: /s / Brian Parisi N...</td>\n",
       "      <td>193</td>\n",
       "      <td>212</td>\n",
       "      <td>204</td>\n",
       "      <td>227</td>\n",
       "      <td>David Baker</td>\n",
       "      <td>President &amp; CEO</td>\n",
       "      <td>SIGNING_PERSON</td>\n",
       "      <td>SIGNING_TITLE</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>By: /s/ Robert Mattacchione Name: Robert Matta...</td>\n",
       "      <td>34</td>\n",
       "      <td>61</td>\n",
       "      <td>53</td>\n",
       "      <td>64</td>\n",
       "      <td>Robert Mattacchione</td>\n",
       "      <td>CEO</td>\n",
       "      <td>SIGNING_PERSON</td>\n",
       "      <td>SIGNING_TITLE</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3500</th>\n",
       "      <td>EXHIBIT 10.2 EXECUTION VERSION NON-COMPETITION...</td>\n",
       "      <td>591</td>\n",
       "      <td>609</td>\n",
       "      <td>604</td>\n",
       "      <td>627</td>\n",
       "      <td>Gulf Houghton</td>\n",
       "      <td>Gulf International</td>\n",
       "      <td>ORG</td>\n",
       "      <td>ORG</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3501 rows  11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  firstCharEnt1  \\\n",
       "0     EXHIBIT 10.43 Dated 29/3/18\\n\\nDistributorship...             29   \n",
       "1     EXHIBIT 10.43 Dated 29/3/18\\n\\nDistributorship...             29   \n",
       "2     EXHIBIT 10.43 Dated 29/3/18\\n\\nDistributorship...             20   \n",
       "3     Sections 200.80(b)(4) and Rule 406 of the Secu...            173   \n",
       "4     Monrovia, CA 91016 USA\\n\\n(hereinafter called ...             63   \n",
       "...                                                 ...            ...   \n",
       "3496  By execution of this Supplier/Subcontractor Co...             85   \n",
       "3497    /s Liu Gang Name: LIU GANG   Title: Authoriz...             20   \n",
       "3498  HOFV: HOF VILLAGE, LLC By: /s / Brian Parisi N...            193   \n",
       "3499  By: /s/ Robert Mattacchione Name: Robert Matta...             34   \n",
       "3500  EXHIBIT 10.2 EXECUTION VERSION NON-COMPETITION...            591   \n",
       "\n",
       "      firstCharEnt2  lastCharEnt1  lastCharEnt2  \\\n",
       "0                65            54            95   \n",
       "1               102            54           129   \n",
       "2                29            27            54   \n",
       "3               236           196           247   \n",
       "4               170            97           173   \n",
       "...             ...           ...           ...   \n",
       "3496             85            93            93   \n",
       "3497             38            28            58   \n",
       "3498            212           204           227   \n",
       "3499             61            53            64   \n",
       "3500            609           604           627   \n",
       "\n",
       "                                  chunk1                          chunk2  \\\n",
       "0              Distributorship agreement  Signature Orthopaedics Pty Ltd   \n",
       "1              Distributorship agreement     CPM Medical Consultants LLC   \n",
       "2                                29/3/18       Distributorship agreement   \n",
       "3                Collaboration Agreement                     Xencor, Inc   \n",
       "4     Boehringer Ingelheim International                             BII   \n",
       "...                                  ...                             ...   \n",
       "3496                            Supplier                        Supplier   \n",
       "3497                            LIU GANG            Authorized Signatory   \n",
       "3498                         David Baker                 President & CEO   \n",
       "3499                 Robert Mattacchione                             CEO   \n",
       "3500                       Gulf Houghton              Gulf International   \n",
       "\n",
       "              label1         label2        rel  direction  \n",
       "0                DOC          PARTY  signed_by          1  \n",
       "1                DOC          PARTY  signed_by          1  \n",
       "2            EFFDATE            DOC   dated_as          2  \n",
       "3                DOC          PARTY  signed_by          1  \n",
       "4              PARTY          ALIAS  has_alias          1  \n",
       "...              ...            ...        ...        ...  \n",
       "3496           PARTY           ROLE      other          0  \n",
       "3497  SIGNING_PERSON  SIGNING_TITLE      other          0  \n",
       "3498  SIGNING_PERSON  SIGNING_TITLE      other          0  \n",
       "3499  SIGNING_PERSON  SIGNING_TITLE      other          0  \n",
       "3500             ORG            ORG      other          0  \n",
       "\n",
       "[3501 rows x 11 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('relations.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGRfDow9iNuc",
    "outputId": "4c255389-05c5-45f0-f155-874cf402513c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rel\n",
       "other                   1639\n",
       "signed_by                865\n",
       "has_alias                471\n",
       "dated_as                 435\n",
       "has_collective_alias      91\n",
       "dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.value_counts('rel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ApZ33Yx5iNuc",
    "outputId": "7f3badc9-91a8-4a6b-8875-9d34900bfb06"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label1\n",
       "PARTY                         1284\n",
       "DOC                           1267\n",
       "SIGNING_PERSON                 737\n",
       "ORG                             72\n",
       "ALIAS                           57\n",
       "ROLE                            41\n",
       "EFFDATE                         18\n",
       "SIGNING_TITLE                   11\n",
       "TITLE                            7\n",
       "NAME                             6\n",
       "PERMISSION_INDIRECT_OBJECT       1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.value_counts('label1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19e-OdVYiNuc",
    "outputId": "1a590d32-a079-4e1a-fccd-6278d983aa61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label2\n",
       "PARTY                882\n",
       "SIGNING_TITLE        724\n",
       "SIGNING_PERSON       498\n",
       "ALIAS                483\n",
       "ROLE                 389\n",
       "EFFDATE              376\n",
       "ORG                   63\n",
       "AGRDATE               52\n",
       "DOC                   19\n",
       "NAME                   7\n",
       "TITLE                  6\n",
       "PERMISSION             1\n",
       "FORMER_PARTY_NAME      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.value_counts('label2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WKj-K1dkgukV",
    "outputId": "ed200f61-797a-4267-88ec-45fc769a9349"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['signed_by', 'dated_as', 'has_alias', 'has_collective_alias',\n",
       "       'other'], dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get a list of valid relation names (less than 10 occurrences are probably wrong labels, or at least with a very low representation)\n",
    "valid_rel_labels = data['rel'].unique()\n",
    "valid_rel_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3aEQUffhJdH"
   },
   "source": [
    "### 6. Create the training and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfKsVk08TJQz"
   },
   "outputs": [],
   "source": [
    "df_train = data.sample(frac=0.9, random_state=1)\n",
    "df_test = data.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iAhEOmLkhIql"
   },
   "outputs": [],
   "source": [
    "#collect data\n",
    "train_sentences, train_rel_label_ids, train_rel_arg_bindings, rel_labels = (\n",
    "    collect_data_from_pandas_dataset(df_train))\n",
    "\n",
    "test_sentences, test_rel_label_ids, test_rel_arg_bindings, _ = (\n",
    "    collect_data_from_pandas_dataset(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otn-PNtrVFNt"
   },
   "outputs": [],
   "source": [
    "train_sentences = train_sentences.values\n",
    "train_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OQkdDhf2VG4i"
   },
   "outputs": [],
   "source": [
    "train_rel_label_ids = train_rel_label_ids.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADXviawkVIFq"
   },
   "outputs": [],
   "source": [
    "train_rel_arg_bindings = train_rel_arg_bindings.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GGaJETXyVMd7",
    "outputId": "71d0f8c2-b3fe-4290-edec-76d3beadc6ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dated_as', 'has_alias', 'has_collective_alias', 'other', 'signed_by']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1sWoYdqhWUDD"
   },
   "outputs": [],
   "source": [
    "test_sentences = test_sentences.values\n",
    "test_rel_label_ids = test_rel_label_ids.values\n",
    "test_rel_arg_bindings = test_rel_arg_bindings.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xa_cg2pxhSzL"
   },
   "source": [
    "### 7. Create the features from the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7O2Vr7hhVLY"
   },
   "outputs": [],
   "source": [
    "#create features\n",
    "train_features = make_features(\n",
    "    train_sentences, \n",
    "    list(zip(train_rel_label_ids, train_rel_arg_bindings)),\n",
    "    tokenizer, \n",
    "    is_test=False)\n",
    "\n",
    "if BertREConfig.REPLACE_ARG_PROB and BertREConfig.REPLICATE_DATASET:\n",
    "    train_features += make_features(\n",
    "        train_sentences, \n",
    "        list(zip(train_rel_label_ids, train_rel_arg_bindings)),\n",
    "        tokenizer, \n",
    "        is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "du3lL9UfWbmS"
   },
   "outputs": [],
   "source": [
    "test_features = make_features(\n",
    "    test_sentences, \n",
    "    list(zip(test_rel_label_ids, test_rel_arg_bindings)),\n",
    "    tokenizer, \n",
    "    is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q10cqRFyC-wP",
    "outputId": "41b7045c-c72c-4259-b709-10810e4c9ff2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2995 training examples\n",
      "333 test examples\n"
     ]
    }
   ],
   "source": [
    "print(\"{} training examples\".format(len(train_features)))\n",
    "print(\"{} test examples\".format(len(test_features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dceO1UwiO8t"
   },
   "source": [
    "### 8. Training the model\n",
    "Here is where all the fun happens! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-K7pr_5tin04"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3EscngcHC-wP",
    "outputId": "6221d419-2300-4e29-b815-c0a3348729e2",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch      Batch         Loss     L_ACC   Arg_ACC       ACC    vL_ACC  vArg_ACC      vACC\n",
      "    1/3      187/187      0.5342     0.893     0.918     0.865     0.963     0.980     0.957\n",
      "    2/3      187/187      0.1363     0.986     0.985     0.980     0.977     0.980     0.970\n",
      "    3/3      187/187      0.0643     0.994     0.991     0.988     0.977     0.987     0.973\n"
     ]
    }
   ],
   "source": [
    "train_model(\n",
    "    \"CONTRACT_DOC_PARTIES\", \n",
    "    train_features=train_features, \n",
    "    test_features=test_features, \n",
    "    rel_labels=rel_labels,\n",
    "    num_arg_bindings=3,\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KVfT6p-oVG8"
   },
   "source": [
    "### 9. Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DmZ0xIJIC-wP",
    "outputId": "85e6e4be-8278-4fed-bf91-02d451298fdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./trained/CONTRACT_DOC_PARTIES/model\n",
      "\n",
      "\n",
      "Relation           Recall Precision        F1   Support\n",
      "\n",
      "dated_as            1.000     0.957     0.978        44\n",
      "has_alias           0.950     0.974     0.962        40\n",
      "has_collective_alias     0.667     1.000     0.800         3\n",
      "other               0.992     0.968     0.980       121\n",
      "signed_by           0.957     0.989     0.972        92\n",
      "\n",
      "Avg.                0.913     0.977     0.938\n",
      "\n",
      "Weighted Avg.       0.973     0.974     0.973\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()        \n",
    "metrics = eval_metrics(\n",
    "    \"CONTRACT_DOC_PARTIES\", test_features, rel_labels, num_arg_bindings=3, exclude_rels=[])        \n",
    "print_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mnx9sUAO7OFr"
   },
   "source": [
    "# Train with all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_3XfLpm77RzQ",
    "outputId": "3206ae92-db2c-4ef1-81aa-a955e5619c12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch      Batch         Loss     L_ACC   Arg_ACC       ACC    vL_ACC  vArg_ACC      vACC\n",
      "    1/3      208/208      0.5453     0.887     0.918     0.868       nan       nan       nan\n",
      "    2/3      208/208      0.1223     0.986     0.986     0.980       nan       nan       nan\n",
      "    3/3      208/208      0.0614     0.994     0.991     0.988       nan       nan       nan\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()    \n",
    "\n",
    "train_model(\n",
    "    \"CONTRACT_DOC_PARTIES\", \n",
    "    train_features=train_features+test_features,\n",
    "    test_features=[], \n",
    "    rel_labels=rel_labels,\n",
    "    num_arg_bindings=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s7ef7-50obgT"
   },
   "source": [
    "### 10. Finally saving it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "id": "Yvart7zxC-wQ",
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "export_model(\"CONTRACT_DOC_PARTIES\", is_trainable=False, num_arg_bindings=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOBOjF1giNue"
   },
   "outputs": [],
   "source": [
    "!sudo apt-get -y install zip unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JteBYaHueUMF"
   },
   "outputs": [],
   "source": [
    "!zip -r redl.zip ./models/basebert_re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oYSUxcZtiNue"
   },
   "source": [
    "# We test in SPARK NLP\n",
    "\n",
    "Let's test our model with SparkNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yVTR9LaRiNue"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl\n",
    "import sparknlp\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl\n",
    "from sparknlp_jsl.annotator import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sparknlp.training import CoNLL\n",
    "\n",
    "import re\n",
    "import copy\n",
    "\n",
    "import random\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "import glob\n",
    "import pickle\n",
    "from termcolor import colored\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cokHOoyViNue"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_relations_df (results, col='relations'):\n",
    "  rel_pairs=[]\n",
    "  for rel in results[0][col]:\n",
    "      rel_pairs.append((\n",
    "          rel.result, \n",
    "          rel.metadata['entity1'], \n",
    "          rel.metadata['entity1_begin'],\n",
    "          rel.metadata['entity1_end'],\n",
    "          rel.metadata['chunk1'], \n",
    "          rel.metadata['entity2'],\n",
    "          rel.metadata['entity2_begin'],\n",
    "          rel.metadata['entity2_end'],\n",
    "          rel.metadata['chunk2'], \n",
    "          rel.metadata['confidence']\n",
    "      ))\n",
    "\n",
    "  rel_df = pd.DataFrame(rel_pairs, columns=['relation','entity1','entity1_begin','entity1_end','chunk1','entity2','entity2_begin','entity2_end','chunk2', 'confidence'])\n",
    "\n",
    "  return rel_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_W-OfQw3jWtJ"
   },
   "source": [
    "Here, we import our model to SparkNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wdt5RR3OiNue"
   },
   "outputs": [],
   "source": [
    "re = RelationExtractionDLModel().loadSavedModel('models/basebert_re', spark)\n",
    "re.write().overwrite().save('legre_contract_doc_parties_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ah9Rn47jfhl"
   },
   "source": [
    "Now before getting relations, we have to extract entities from the given text. For this, we will use `legner_contract_doc_parties` NER model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qSj5PJOxiNue"
   },
   "outputs": [],
   "source": [
    "document_assembler = DocumentAssembler()\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "    .setInputCols(\"document\")\\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "embeddings = RoBertaEmbeddings.pretrained(\"roberta_embeddings_legal_roberta_base\", \"en\") \\\n",
    "    .setInputCols(\"document\", \"token\") \\\n",
    "    .setOutputCol(\"embeddings\")\\\n",
    "    .setMaxSentenceLength(512)\n",
    "\n",
    "ner_model = legal.LegalNerModel.pretrained('legner_contract_doc_parties', 'en', 'legal/models')\\\n",
    "    .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
    "    .setOutputCol(\"ner\")\n",
    "\n",
    "ner_converter = NerConverter()\\\n",
    "    .setInputCols([\"document\",\"token\",\"ner\"])\\\n",
    "    .setOutputCol(\"ner_chunk\")\n",
    "\n",
    "# We use the load function to run our trained model.\n",
    "reDL = RelationExtractionDLModel().load('legre_contract_doc_parties_md')\\\n",
    "    .setPredictionThreshold(0.5)\\\n",
    "    .setInputCols([\"ner_chunk\", \"document\"])\\\n",
    "    .setOutputCol(\"relations\")\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    embeddings,\n",
    "    ner_model,\n",
    "    ner_converter,\n",
    "    reDL\n",
    "    ])\n",
    "\n",
    "data = spark.createDataFrame([[text]]).toDF(\"text\")\n",
    "\n",
    "model = nlpPipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tDZ2tLeviNue"
   },
   "outputs": [],
   "source": [
    "light_model = LightPipeline(model)\n",
    "\n",
    "text='''\n",
    "This INTELLECTUAL PROPERTY AGREEMENT (this \"Agreement\"), dated as of December 31, 2018 (the \"Effective Date\") is entered into by and between Armstrong Flooring, Inc., a Delaware corporation (\"Seller\") and AFI Licensing LLC, a Delaware limited liability company (\"Licensing\" and together with Seller, \"Arizona\") and AHF Holding, Inc. (formerly known as Tarzan HoldCo, Inc.), a Delaware corporation (\"Buyer\") and Armstrong Hardwood Flooring Company, a Tennessee corporation (the \"Company\" and together with Buyer the \"Buyer Entities\") (each of Arizona on the one hand and the Buyer Entities on the other hand, a \"Party\" and collectively, the \"Parties\").\n",
    "'''\n",
    "\n",
    "results = light_model.fullAnnotate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RCkP3vFgiNue",
    "outputId": "396590ef-48d2-4d85-f0a5-00a45859f7ba"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relation</th>\n",
       "      <th>entity1</th>\n",
       "      <th>entity1_begin</th>\n",
       "      <th>entity1_end</th>\n",
       "      <th>chunk1</th>\n",
       "      <th>entity2</th>\n",
       "      <th>entity2_begin</th>\n",
       "      <th>entity2_end</th>\n",
       "      <th>chunk2</th>\n",
       "      <th>confidence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dated_as</td>\n",
       "      <td>DOC</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>INTELLECTUAL PROPERTY AGREEMENT</td>\n",
       "      <td>EFFDATE</td>\n",
       "      <td>70</td>\n",
       "      <td>86</td>\n",
       "      <td>December 31, 2018</td>\n",
       "      <td>0.99994016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>signed_by</td>\n",
       "      <td>DOC</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>INTELLECTUAL PROPERTY AGREEMENT</td>\n",
       "      <td>PARTY</td>\n",
       "      <td>142</td>\n",
       "      <td>164</td>\n",
       "      <td>Armstrong Flooring, Inc</td>\n",
       "      <td>0.9995191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>signed_by</td>\n",
       "      <td>DOC</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>INTELLECTUAL PROPERTY AGREEMENT</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>193</td>\n",
       "      <td>198</td>\n",
       "      <td>Seller</td>\n",
       "      <td>0.9823355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>signed_by</td>\n",
       "      <td>DOC</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>INTELLECTUAL PROPERTY AGREEMENT</td>\n",
       "      <td>PARTY</td>\n",
       "      <td>206</td>\n",
       "      <td>222</td>\n",
       "      <td>AFI Licensing LLC</td>\n",
       "      <td>0.9989542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>signed_by</td>\n",
       "      <td>DOC</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>INTELLECTUAL PROPERTY AGREEMENT</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>264</td>\n",
       "      <td>272</td>\n",
       "      <td>Licensing</td>\n",
       "      <td>0.92109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>signed_by</td>\n",
       "      <td>DOC</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>INTELLECTUAL PROPERTY AGREEMENT</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>293</td>\n",
       "      <td>298</td>\n",
       "      <td>Seller</td>\n",
       "      <td>0.9938019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>signed_by</td>\n",
       "      <td>DOC</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>INTELLECTUAL PROPERTY AGREEMENT</td>\n",
       "      <td>PARTY</td>\n",
       "      <td>316</td>\n",
       "      <td>331</td>\n",
       "      <td>AHF Holding, Inc</td>\n",
       "      <td>0.9989403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>signed_by</td>\n",
       "      <td>DOC</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>INTELLECTUAL PROPERTY AGREEMENT</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>400</td>\n",
       "      <td>404</td>\n",
       "      <td>Buyer</td>\n",
       "      <td>0.89959186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>signed_by</td>\n",
       "      <td>DOC</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>INTELLECTUAL PROPERTY AGREEMENT</td>\n",
       "      <td>PARTY</td>\n",
       "      <td>412</td>\n",
       "      <td>446</td>\n",
       "      <td>Armstrong Hardwood Flooring Company</td>\n",
       "      <td>0.9974464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>signed_by</td>\n",
       "      <td>DOC</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>INTELLECTUAL PROPERTY AGREEMENT</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>479</td>\n",
       "      <td>485</td>\n",
       "      <td>Company</td>\n",
       "      <td>0.95839113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>signed_by</td>\n",
       "      <td>DOC</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>INTELLECTUAL PROPERTY AGREEMENT</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>506</td>\n",
       "      <td>510</td>\n",
       "      <td>Buyer</td>\n",
       "      <td>0.95839113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>signed_by</td>\n",
       "      <td>DOC</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>INTELLECTUAL PROPERTY AGREEMENT</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>517</td>\n",
       "      <td>530</td>\n",
       "      <td>Buyer Entities</td>\n",
       "      <td>0.95839113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>signed_by</td>\n",
       "      <td>DOC</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>INTELLECTUAL PROPERTY AGREEMENT</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>612</td>\n",
       "      <td>616</td>\n",
       "      <td>Party</td>\n",
       "      <td>0.95839113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>signed_by</td>\n",
       "      <td>DOC</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>INTELLECTUAL PROPERTY AGREEMENT</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>642</td>\n",
       "      <td>648</td>\n",
       "      <td>Parties</td>\n",
       "      <td>0.95839113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>dated_as</td>\n",
       "      <td>EFFDATE</td>\n",
       "      <td>70</td>\n",
       "      <td>86</td>\n",
       "      <td>December 31, 2018</td>\n",
       "      <td>PARTY</td>\n",
       "      <td>142</td>\n",
       "      <td>164</td>\n",
       "      <td>Armstrong Flooring, Inc</td>\n",
       "      <td>0.69556713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dated_as</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>193</td>\n",
       "      <td>198</td>\n",
       "      <td>Seller</td>\n",
       "      <td>EFFDATE</td>\n",
       "      <td>70</td>\n",
       "      <td>86</td>\n",
       "      <td>December 31, 2018</td>\n",
       "      <td>0.7183331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dated_as</td>\n",
       "      <td>EFFDATE</td>\n",
       "      <td>70</td>\n",
       "      <td>86</td>\n",
       "      <td>December 31, 2018</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>264</td>\n",
       "      <td>272</td>\n",
       "      <td>Licensing</td>\n",
       "      <td>0.7500907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>dated_as</td>\n",
       "      <td>EFFDATE</td>\n",
       "      <td>70</td>\n",
       "      <td>86</td>\n",
       "      <td>December 31, 2018</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>293</td>\n",
       "      <td>298</td>\n",
       "      <td>Seller</td>\n",
       "      <td>0.6601122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>dated_as</td>\n",
       "      <td>EFFDATE</td>\n",
       "      <td>70</td>\n",
       "      <td>86</td>\n",
       "      <td>December 31, 2018</td>\n",
       "      <td>PARTY</td>\n",
       "      <td>316</td>\n",
       "      <td>331</td>\n",
       "      <td>AHF Holding, Inc</td>\n",
       "      <td>0.52062315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>dated_as</td>\n",
       "      <td>EFFDATE</td>\n",
       "      <td>70</td>\n",
       "      <td>86</td>\n",
       "      <td>December 31, 2018</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>400</td>\n",
       "      <td>404</td>\n",
       "      <td>Buyer</td>\n",
       "      <td>0.7104727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>dated_as</td>\n",
       "      <td>EFFDATE</td>\n",
       "      <td>70</td>\n",
       "      <td>86</td>\n",
       "      <td>December 31, 2018</td>\n",
       "      <td>PARTY</td>\n",
       "      <td>412</td>\n",
       "      <td>446</td>\n",
       "      <td>Armstrong Hardwood Flooring Company</td>\n",
       "      <td>0.70473474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>dated_as</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>479</td>\n",
       "      <td>485</td>\n",
       "      <td>Company</td>\n",
       "      <td>EFFDATE</td>\n",
       "      <td>70</td>\n",
       "      <td>86</td>\n",
       "      <td>December 31, 2018</td>\n",
       "      <td>0.98484945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>dated_as</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>506</td>\n",
       "      <td>510</td>\n",
       "      <td>Buyer</td>\n",
       "      <td>EFFDATE</td>\n",
       "      <td>70</td>\n",
       "      <td>86</td>\n",
       "      <td>December 31, 2018</td>\n",
       "      <td>0.98484945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>dated_as</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>517</td>\n",
       "      <td>530</td>\n",
       "      <td>Buyer Entities</td>\n",
       "      <td>EFFDATE</td>\n",
       "      <td>70</td>\n",
       "      <td>86</td>\n",
       "      <td>December 31, 2018</td>\n",
       "      <td>0.98484945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>dated_as</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>612</td>\n",
       "      <td>616</td>\n",
       "      <td>Party</td>\n",
       "      <td>EFFDATE</td>\n",
       "      <td>70</td>\n",
       "      <td>86</td>\n",
       "      <td>December 31, 2018</td>\n",
       "      <td>0.98484945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>dated_as</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>642</td>\n",
       "      <td>648</td>\n",
       "      <td>Parties</td>\n",
       "      <td>EFFDATE</td>\n",
       "      <td>70</td>\n",
       "      <td>86</td>\n",
       "      <td>December 31, 2018</td>\n",
       "      <td>0.98484945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>has_alias</td>\n",
       "      <td>PARTY</td>\n",
       "      <td>142</td>\n",
       "      <td>164</td>\n",
       "      <td>Armstrong Flooring, Inc</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>264</td>\n",
       "      <td>272</td>\n",
       "      <td>Licensing</td>\n",
       "      <td>0.686296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>has_alias</td>\n",
       "      <td>PARTY</td>\n",
       "      <td>206</td>\n",
       "      <td>222</td>\n",
       "      <td>AFI Licensing LLC</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>264</td>\n",
       "      <td>272</td>\n",
       "      <td>Licensing</td>\n",
       "      <td>0.8194909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>has_collective_alias</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>264</td>\n",
       "      <td>272</td>\n",
       "      <td>Licensing</td>\n",
       "      <td>PARTY</td>\n",
       "      <td>316</td>\n",
       "      <td>331</td>\n",
       "      <td>AHF Holding, Inc</td>\n",
       "      <td>0.5534526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>has_alias</td>\n",
       "      <td>PARTY</td>\n",
       "      <td>316</td>\n",
       "      <td>331</td>\n",
       "      <td>AHF Holding, Inc</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>479</td>\n",
       "      <td>485</td>\n",
       "      <td>Company</td>\n",
       "      <td>0.52909577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>has_alias</td>\n",
       "      <td>PARTY</td>\n",
       "      <td>316</td>\n",
       "      <td>331</td>\n",
       "      <td>AHF Holding, Inc</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>506</td>\n",
       "      <td>510</td>\n",
       "      <td>Buyer</td>\n",
       "      <td>0.52909607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>has_alias</td>\n",
       "      <td>PARTY</td>\n",
       "      <td>316</td>\n",
       "      <td>331</td>\n",
       "      <td>AHF Holding, Inc</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>517</td>\n",
       "      <td>530</td>\n",
       "      <td>Buyer Entities</td>\n",
       "      <td>0.52909607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>has_alias</td>\n",
       "      <td>PARTY</td>\n",
       "      <td>316</td>\n",
       "      <td>331</td>\n",
       "      <td>AHF Holding, Inc</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>612</td>\n",
       "      <td>616</td>\n",
       "      <td>Party</td>\n",
       "      <td>0.52909607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>has_alias</td>\n",
       "      <td>PARTY</td>\n",
       "      <td>316</td>\n",
       "      <td>331</td>\n",
       "      <td>AHF Holding, Inc</td>\n",
       "      <td>ALIAS</td>\n",
       "      <td>642</td>\n",
       "      <td>648</td>\n",
       "      <td>Parties</td>\n",
       "      <td>0.52909607</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                relation  entity1 entity1_begin entity1_end  \\\n",
       "0               dated_as      DOC             6          36   \n",
       "1              signed_by      DOC             6          36   \n",
       "2              signed_by      DOC             6          36   \n",
       "3              signed_by      DOC             6          36   \n",
       "4              signed_by      DOC             6          36   \n",
       "5              signed_by      DOC             6          36   \n",
       "6              signed_by      DOC             6          36   \n",
       "7              signed_by      DOC             6          36   \n",
       "8              signed_by      DOC             6          36   \n",
       "9              signed_by      DOC             6          36   \n",
       "10             signed_by      DOC             6          36   \n",
       "11             signed_by      DOC             6          36   \n",
       "12             signed_by      DOC             6          36   \n",
       "13             signed_by      DOC             6          36   \n",
       "14              dated_as  EFFDATE            70          86   \n",
       "15              dated_as    ALIAS           193         198   \n",
       "16              dated_as  EFFDATE            70          86   \n",
       "17              dated_as  EFFDATE            70          86   \n",
       "18              dated_as  EFFDATE            70          86   \n",
       "19              dated_as  EFFDATE            70          86   \n",
       "20              dated_as  EFFDATE            70          86   \n",
       "21              dated_as    ALIAS           479         485   \n",
       "22              dated_as    ALIAS           506         510   \n",
       "23              dated_as    ALIAS           517         530   \n",
       "24              dated_as    ALIAS           612         616   \n",
       "25              dated_as    ALIAS           642         648   \n",
       "28             has_alias    PARTY           142         164   \n",
       "42             has_alias    PARTY           206         222   \n",
       "53  has_collective_alias    ALIAS           264         272   \n",
       "65             has_alias    PARTY           316         331   \n",
       "66             has_alias    PARTY           316         331   \n",
       "67             has_alias    PARTY           316         331   \n",
       "68             has_alias    PARTY           316         331   \n",
       "69             has_alias    PARTY           316         331   \n",
       "\n",
       "                             chunk1  entity2 entity2_begin entity2_end  \\\n",
       "0   INTELLECTUAL PROPERTY AGREEMENT  EFFDATE            70          86   \n",
       "1   INTELLECTUAL PROPERTY AGREEMENT    PARTY           142         164   \n",
       "2   INTELLECTUAL PROPERTY AGREEMENT    ALIAS           193         198   \n",
       "3   INTELLECTUAL PROPERTY AGREEMENT    PARTY           206         222   \n",
       "4   INTELLECTUAL PROPERTY AGREEMENT    ALIAS           264         272   \n",
       "5   INTELLECTUAL PROPERTY AGREEMENT    ALIAS           293         298   \n",
       "6   INTELLECTUAL PROPERTY AGREEMENT    PARTY           316         331   \n",
       "7   INTELLECTUAL PROPERTY AGREEMENT    ALIAS           400         404   \n",
       "8   INTELLECTUAL PROPERTY AGREEMENT    PARTY           412         446   \n",
       "9   INTELLECTUAL PROPERTY AGREEMENT    ALIAS           479         485   \n",
       "10  INTELLECTUAL PROPERTY AGREEMENT    ALIAS           506         510   \n",
       "11  INTELLECTUAL PROPERTY AGREEMENT    ALIAS           517         530   \n",
       "12  INTELLECTUAL PROPERTY AGREEMENT    ALIAS           612         616   \n",
       "13  INTELLECTUAL PROPERTY AGREEMENT    ALIAS           642         648   \n",
       "14                December 31, 2018    PARTY           142         164   \n",
       "15                           Seller  EFFDATE            70          86   \n",
       "16                December 31, 2018    ALIAS           264         272   \n",
       "17                December 31, 2018    ALIAS           293         298   \n",
       "18                December 31, 2018    PARTY           316         331   \n",
       "19                December 31, 2018    ALIAS           400         404   \n",
       "20                December 31, 2018    PARTY           412         446   \n",
       "21                          Company  EFFDATE            70          86   \n",
       "22                            Buyer  EFFDATE            70          86   \n",
       "23                   Buyer Entities  EFFDATE            70          86   \n",
       "24                            Party  EFFDATE            70          86   \n",
       "25                          Parties  EFFDATE            70          86   \n",
       "28          Armstrong Flooring, Inc    ALIAS           264         272   \n",
       "42                AFI Licensing LLC    ALIAS           264         272   \n",
       "53                        Licensing    PARTY           316         331   \n",
       "65                 AHF Holding, Inc    ALIAS           479         485   \n",
       "66                 AHF Holding, Inc    ALIAS           506         510   \n",
       "67                 AHF Holding, Inc    ALIAS           517         530   \n",
       "68                 AHF Holding, Inc    ALIAS           612         616   \n",
       "69                 AHF Holding, Inc    ALIAS           642         648   \n",
       "\n",
       "                                 chunk2  confidence  \n",
       "0                     December 31, 2018  0.99994016  \n",
       "1               Armstrong Flooring, Inc   0.9995191  \n",
       "2                                Seller   0.9823355  \n",
       "3                     AFI Licensing LLC   0.9989542  \n",
       "4                             Licensing     0.92109  \n",
       "5                                Seller   0.9938019  \n",
       "6                      AHF Holding, Inc   0.9989403  \n",
       "7                                 Buyer  0.89959186  \n",
       "8   Armstrong Hardwood Flooring Company   0.9974464  \n",
       "9                               Company  0.95839113  \n",
       "10                                Buyer  0.95839113  \n",
       "11                       Buyer Entities  0.95839113  \n",
       "12                                Party  0.95839113  \n",
       "13                              Parties  0.95839113  \n",
       "14              Armstrong Flooring, Inc  0.69556713  \n",
       "15                    December 31, 2018   0.7183331  \n",
       "16                            Licensing   0.7500907  \n",
       "17                               Seller   0.6601122  \n",
       "18                     AHF Holding, Inc  0.52062315  \n",
       "19                                Buyer   0.7104727  \n",
       "20  Armstrong Hardwood Flooring Company  0.70473474  \n",
       "21                    December 31, 2018  0.98484945  \n",
       "22                    December 31, 2018  0.98484945  \n",
       "23                    December 31, 2018  0.98484945  \n",
       "24                    December 31, 2018  0.98484945  \n",
       "25                    December 31, 2018  0.98484945  \n",
       "28                            Licensing    0.686296  \n",
       "42                            Licensing   0.8194909  \n",
       "53                     AHF Holding, Inc   0.5534526  \n",
       "65                              Company  0.52909577  \n",
       "66                                Buyer  0.52909607  \n",
       "67                       Buyer Entities  0.52909607  \n",
       "68                                Party  0.52909607  \n",
       "69                              Parties  0.52909607  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rel_df = get_relations_df(results)\n",
    "rel_df = rel_df[rel_df['relation']!='other']\n",
    "rel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTTsG1S8iNue"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
