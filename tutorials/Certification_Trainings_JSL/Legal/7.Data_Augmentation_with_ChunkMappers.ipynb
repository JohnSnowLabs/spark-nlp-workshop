{"cells":[{"cell_type":"markdown","id":"db5f4f9a-7776-42b3-8758-85624d4c15ea","metadata":{"id":"db5f4f9a-7776-42b3-8758-85624d4c15ea"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"]},{"cell_type":"markdown","id":"21e9eafb","metadata":{"id":"21e9eafb"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings_JSL/Legal/7.Data_Augmentation_with_ChunkMappers.ipynb)"]},{"cell_type":"markdown","id":"cfbbcfc0-e0b7-4c25-8bd7-c64d90f836d1","metadata":{"id":"cfbbcfc0-e0b7-4c25-8bd7-c64d90f836d1"},"source":["# Legal Data Augmentation with Chunk Mappers"]},{"cell_type":"markdown","id":"d818acfe-3b90-4e05-93c9-74e67fc55a13","metadata":{"id":"d818acfe-3b90-4e05-93c9-74e67fc55a13"},"source":["# Colab Setup"]},{"cell_type":"code","execution_count":null,"id":"ELqzaf32MT6E","metadata":{"id":"ELqzaf32MT6E"},"outputs":[],"source":["# Install the johnsnowlabs library to access Spark-OCR and Spark-NLP for Healthcare, Finance, and Legal.\n","! pip install johnsnowlabs "]},{"cell_type":"code","execution_count":null,"id":"RO2dIA414yL_","metadata":{"id":"RO2dIA414yL_"},"outputs":[],"source":["from google.colab import files\n","print('Please Upload your John Snow Labs License using the button below')\n","license_keys = files.upload()"]},{"cell_type":"code","execution_count":null,"id":"dmcB5zVBHZO8","metadata":{"id":"dmcB5zVBHZO8"},"outputs":[],"source":["from johnsnowlabs import * \n","# After uploading your license run this to install all licensed Python Wheels and pre-download Jars the Spark Session JVM\n","# Make sure to restart your notebook afterwards for changes to take effect\n","nlp.install()"]},{"cell_type":"markdown","id":"ec74cbb6-b4f8-4eb8-bc55-44a3791bc338","metadata":{"id":"ec74cbb6-b4f8-4eb8-bc55-44a3791bc338"},"source":["# Start Spark Session"]},{"cell_type":"code","execution_count":null,"id":"lQ8-BI-_5QjG","metadata":{"id":"lQ8-BI-_5QjG"},"outputs":[],"source":["from johnsnowlabs import * \n","# Automatically load license data and start a session with all jars user has access to\n","spark = nlp.start()"]},{"cell_type":"code","execution_count":null,"id":"0NHLTd2zb5ch","metadata":{"id":"0NHLTd2zb5ch"},"outputs":[],"source":["import pandas as pd\n","import warnings\n","warnings.filterwarnings('ignore')\n","# if you want to start the session with custom params as in start function above\n","def start(SECRET):\n","    builder = SparkSession.builder \\\n","        .appName(\"Spark NLP Licensed\") \\\n","        .master(\"local[*]\") \\\n","        .config(\"spark.driver.memory\", \"16G\") \\\n","        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n","        .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n","        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:\"+PUBLIC_VERSION) \\\n","        .config(\"spark.jars\", \"https://pypi.johnsnowlabs.com/\"+SECRET+\"/spark-nlp-jsl-\"+JSL_VERSION+\".jar\")\n","      \n","    return builder.getOrCreate()\n","\n","#spark = start(SECRET)\n"]},{"cell_type":"markdown","id":"d2cd4221-fbca-4ca1-86a9-65e6264c4ad1","metadata":{"id":"d2cd4221-fbca-4ca1-86a9-65e6264c4ad1"},"source":["# About Data Augmentation"]},{"cell_type":"markdown","id":"bf9835fd-9def-44e4-b022-e8db0f045fec","metadata":{"id":"bf9835fd-9def-44e4-b022-e8db0f045fec"},"source":["__Data Augmentation__ is the process of increase an extracted datapoint with external sources. \n","\n","For example, let's suppose I work with a document which mentions the company _Amazon_. We could be talking about stock prices, or some legal litigations, or just a commercial agreement with a provider, among others.\n","\n","In the document, we can extract `Amazon` using NER as an Organization, but that's all the information available about `Amazon` in that document.\n","\n","Well, with __Data Augmentation__, we can use external sources, as _SEC Edgar, Crunchbase, Nasdaq_ or even _Wikipedia_, to enrich `Amazon` with much more information, allowing us to take better decisions.\n","\n","Let's see how to do it."]},{"cell_type":"markdown","id":"eef8c0e5-6793-4db5-ab39-f6381c9e500d","metadata":{"id":"eef8c0e5-6793-4db5-ab39-f6381c9e500d"},"source":["# Step 1: Name Entity Recognition"]},{"cell_type":"markdown","id":"612f9fe0-4d7c-4d6e-afc1-47a59b99f529","metadata":{"id":"612f9fe0-4d7c-4d6e-afc1-47a59b99f529"},"source":["Let's suppose we get this news from scrapping the Internet, or from Twitter."]},{"cell_type":"code","execution_count":null,"id":"27f44267-72be-45de-afaa-a2c911195d89","metadata":{"id":"27f44267-72be-45de-afaa-a2c911195d89"},"outputs":[],"source":["text = \"We have entered into a definitive merger agreement with Amazon.\""]},{"cell_type":"markdown","id":"71a5e806-9659-4b41-8ab6-38f81b26797f","metadata":{"id":"71a5e806-9659-4b41-8ab6-38f81b26797f"},"source":["We use NER to extract the companies name, in this case, Amazon."]},{"cell_type":"code","execution_count":null,"id":"cb765952-24c2-48b6-8d86-5413b13bd9fa","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":81305,"status":"ok","timestamp":1669742001149,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":300},"id":"cb765952-24c2-48b6-8d86-5413b13bd9fa","outputId":"d6627842-60e0-457e-f7c6-76b3afe843d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["sentence_detector_dl download started this may take some time.\n","Approximate size to download 514.9 KB\n","[OK!]\n","bert_embeddings_sec_bert_base download started this may take some time.\n","Approximate size to download 390.4 MB\n","[OK!]\n","legner_orgs_prods_alias download started this may take some time.\n","[OK!]\n"]}],"source":["documentAssembler = nlp.DocumentAssembler()\\\n","        .setInputCol(\"text\")\\\n","        .setOutputCol(\"document\")\n","        \n","sentenceDetector = nlp.SentenceDetectorDLModel.pretrained(\"sentence_detector_dl\",\"xx\")\\\n","        .setInputCols([\"document\"])\\\n","        .setOutputCol(\"sentence\")\n","\n","tokenizer = nlp.Tokenizer()\\\n","        .setInputCols([\"sentence\"])\\\n","        .setOutputCol(\"token\")\n","\n","embeddings = nlp.BertEmbeddings.pretrained(\"bert_embeddings_sec_bert_base\",\"en\") \\\n","    .setInputCols([\"sentence\", \"token\"]) \\\n","    .setOutputCol(\"embeddings\")\n","\n","ner_model = legal.NerModel.pretrained(\"legner_orgs_prods_alias\", \"en\", \"legal/models\")\\\n","        .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n","        .setOutputCol(\"ner\")\n","        \n","ner_converter = nlp.NerConverter()\\\n","        .setInputCols([\"sentence\",\"token\",\"ner\"])\\\n","        .setOutputCol(\"ner_chunk\")\n","\n","nlpPipeline = nlp.Pipeline(stages=[\n","        documentAssembler,\n","        sentenceDetector,\n","        tokenizer,\n","        embeddings,\n","        ner_model,\n","        ner_converter,\n","])\n","\n","empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n","\n","model = nlpPipeline.fit(empty_data)"]},{"cell_type":"markdown","id":"37eae9a4-52e1-400e-a1dd-effc6ed1da35","metadata":{"id":"37eae9a4-52e1-400e-a1dd-effc6ed1da35"},"source":["## We use LightPipelines to get the result"]},{"cell_type":"code","execution_count":null,"id":"76a05a20-b9b1-4868-a198-0e950c05a786","metadata":{"id":"76a05a20-b9b1-4868-a198-0e950c05a786"},"outputs":[],"source":["lp_ner = nlp.LightPipeline(model)"]},{"cell_type":"code","execution_count":null,"id":"19545c7d-1738-47be-afdd-5f44236a4a2a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2235,"status":"ok","timestamp":1669742003373,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":300},"id":"19545c7d-1738-47be-afdd-5f44236a4a2a","outputId":"c6549358-dbbe-41ae-ceee-3836ff783ce4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'document': ['We have entered into a definitive merger agreement with Amazon.'],\n"," 'ner_chunk': ['Amazon'],\n"," 'token': ['We',\n","  'have',\n","  'entered',\n","  'into',\n","  'a',\n","  'definitive',\n","  'merger',\n","  'agreement',\n","  'with',\n","  'Amazon',\n","  '.'],\n"," 'ner': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O'],\n"," 'embeddings': ['We',\n","  'have',\n","  'entered',\n","  'into',\n","  'a',\n","  'definitive',\n","  'merger',\n","  'agreement',\n","  'with',\n","  'Amazon',\n","  '.'],\n"," 'sentence': ['We have entered into a definitive merger agreement with Amazon.']}"]},"metadata":{},"execution_count":9}],"source":["ner_result = lp_ner.annotate(text)\n","ner_result"]},{"cell_type":"markdown","id":"9fe41161-c8fd-467e-9fff-5d4fe1cb5160","metadata":{"id":"9fe41161-c8fd-467e-9fff-5d4fe1cb5160"},"source":["Alright! Amazon has been detected as an organization. \n","\n","Now, let's augment `Amazon` with more information about the company, given that there are no more details in the tweet I can use.\n","\n","But before __augmenting__, there is a very important step we need to carry out: `Company Name Normalization`"]},{"cell_type":"markdown","id":"eb3e2808-3550-46d9-835b-f747cac4123c","metadata":{"id":"eb3e2808-3550-46d9-835b-f747cac4123c"},"source":["# Step 2: Company Names Normalization"]},{"cell_type":"markdown","id":"4e55a84c-2806-4a0e-b30f-90a43dc497ca","metadata":{"id":"4e55a84c-2806-4a0e-b30f-90a43dc497ca"},"source":["Let's suppose we want to manually get information about Amazon.\n","\n","Since it's a public US company, we can go to [SEC Edgar's database](https://www.sec.gov/edgar/searchedgar/companysearch) and look for it."]},{"cell_type":"markdown","id":"13f414e7-d727-4b8a-ba8a-8b0c644bb7da","metadata":{"id":"13f414e7-d727-4b8a-ba8a-8b0c644bb7da"},"source":["Unfortunately, `Amazon` is not the official name of the company, which means no entry for `Amazon` is available. That's were __Company Names Normalization__ comes in handy."]},{"cell_type":"markdown","id":"6eb8eb7f-1bda-454c-8318-bb4df34f0b6a","metadata":{"id":"6eb8eb7f-1bda-454c-8318-bb4df34f0b6a"},"source":["`Company Name Normalization` is the process of obtaining the name of the company used by data providers, usually the \"official\" name of the company.\n","\n","Sometimes, some data providers may have different versions of the name with different punctuation. For example, for Meta:\n","- Meta Platforms, Inc.\n","- Meta Platforms Inc.\n","- Meta Platforms, Inc\n","- etc\n","\n","So, it's mandatory we do `Company Normalization` taking into account the database / datasource provider we want to extract data from. The data providers we have are:\n","- SEC Edgar\n","- Crunchbase until 2015\n","- Wikidata (in progress)\n","\n","Let's normalize `Amazon` to the official name in _SEC Edgar_."]},{"cell_type":"code","execution_count":null,"id":"2564fd32-99ec-451c-ae34-2792cf3036ef","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":155773,"status":"ok","timestamp":1669742159124,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":300},"id":"2564fd32-99ec-451c-ae34-2792cf3036ef","outputId":"910de538-6e0f-40ca-94d1-605e800690b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["tfhub_use download started this may take some time.\n","Approximate size to download 923.7 MB\n","[OK!]\n","legel_edgar_company_name download started this may take some time.\n","[OK!]\n"]}],"source":["embeddings = nlp.UniversalSentenceEncoder.pretrained(\"tfhub_use\", \"en\") \\\n","      .setInputCols(\"document\") \\\n","      .setOutputCol(\"sentence_embeddings\")\n","    \n","resolver = legal.SentenceEntityResolverModel.pretrained(\"legel_edgar_company_name\", \"en\", \"legal/models\")\\\n","      .setInputCols([\"text\", \"sentence_embeddings\"]) \\\n","      .setOutputCol(\"resolution\")\\\n","      .setDistanceFunction(\"EUCLIDEAN\")\n","\n","pipelineModel = nlp.PipelineModel(\n","      stages = [\n","          documentAssembler,\n","          embeddings,\n","          resolver])\n","\n","lp_res = nlp.LightPipeline(pipelineModel)"]},{"cell_type":"code","execution_count":null,"id":"36a6d5f7-6477-4219-acf7-53a95d1ebea3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1669742159125,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":300},"id":"36a6d5f7-6477-4219-acf7-53a95d1ebea3","outputId":"2c857ea9-7947-455b-cbe1-10f5debdc9bd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Amazon']"]},"metadata":{},"execution_count":11}],"source":["ner_result['ner_chunk']"]},{"cell_type":"code","execution_count":null,"id":"c319f7b8-fe7e-4408-9960-15e7675a36c1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":646,"status":"ok","timestamp":1669742159764,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":300},"id":"c319f7b8-fe7e-4408-9960-15e7675a36c1","outputId":"08e36204-6900-4330-cffc-98183516ee9b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[{'document': ['Amazon'],\n","  'sentence_embeddings': ['Amazon'],\n","  'resolution': ['AMAZON COM INC']}]"]},"metadata":{},"execution_count":12}],"source":["el_res = lp_res.annotate(ner_result['ner_chunk'])\n","el_res"]},{"cell_type":"markdown","id":"109efb72-bfae-413b-b1cb-ef1c57b9b66d","metadata":{"id":"109efb72-bfae-413b-b1cb-ef1c57b9b66d"},"source":["Here is our normalized name for Amazon: `AMAZON COM INC`.\n","\n","Now, let's see which information is available in Edgar database for `AMAZON COM INC`"]},{"cell_type":"markdown","id":"520b8b1d-2754-4338-acc4-d74aeab8a673","metadata":{"id":"520b8b1d-2754-4338-acc4-d74aeab8a673"},"source":["# Steps 1 and 2 in the same pipeline"]},{"cell_type":"code","execution_count":null,"id":"46ed95a6-aa6e-47b5-897e-6430249f9532","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17142,"status":"ok","timestamp":1669742176901,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":300},"id":"46ed95a6-aa6e-47b5-897e-6430249f9532","outputId":"784615ce-b04c-4531-8b74-1f4fcc8bbba0"},"outputs":[{"output_type":"stream","name":"stdout","text":["sentence_detector_dl download started this may take some time.\n","Approximate size to download 514.9 KB\n","[OK!]\n","bert_embeddings_sec_bert_base download started this may take some time.\n","Approximate size to download 390.4 MB\n","[OK!]\n","legner_orgs_prods_alias download started this may take some time.\n","[OK!]\n","tfhub_use download started this may take some time.\n","Approximate size to download 923.7 MB\n","[OK!]\n","legel_edgar_company_name download started this may take some time.\n","[OK!]\n"]}],"source":["documentAssembler = nlp.DocumentAssembler()\\\n","        .setInputCol(\"text\")\\\n","        .setOutputCol(\"document\")\n","        \n","sentenceDetector = nlp.SentenceDetectorDLModel.pretrained(\"sentence_detector_dl\",\"xx\")\\\n","        .setInputCols([\"document\"])\\\n","        .setOutputCol(\"sentence\")\n","\n","tokenizer = nlp.Tokenizer()\\\n","        .setInputCols([\"sentence\"])\\\n","        .setOutputCol(\"token\")\n","\n","embeddings = nlp.BertEmbeddings.pretrained(\"bert_embeddings_sec_bert_base\",\"en\") \\\n","    .setInputCols([\"sentence\", \"token\"]) \\\n","    .setOutputCol(\"embeddings\")\n","\n","ner_model = legal.NerModel.pretrained(\"legner_orgs_prods_alias\", \"en\", \"legal/models\")\\\n","        .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n","        .setOutputCol(\"ner\")\n","        \n","ner_converter = nlp.NerConverter()\\\n","        .setInputCols([\"sentence\",\"token\",\"ner\"])\\\n","        .setOutputCol(\"ner_chunk\")\n","\n","chunk2doc = nlp.Chunk2Doc()\\\n","        .setInputCols(\"ner_chunk\")\\\n","        .setOutputCol(\"ner_chunk_doc\")\n","\n","sentence_embeddings = nlp.UniversalSentenceEncoder.pretrained(\"tfhub_use\", \"en\") \\\n","      .setInputCols(\"ner_chunk_doc\") \\\n","      .setOutputCol(\"sentence_embeddings\")\n","    \n","resolver = legal.SentenceEntityResolverModel.pretrained(\"legel_edgar_company_name\", \"en\", \"legal/models\")\\\n","      .setInputCols([\"text\", \"sentence_embeddings\"]) \\\n","      .setOutputCol(\"resolution\")\\\n","      .setDistanceFunction(\"EUCLIDEAN\")\n","\n","nlpPipeline = nlp.Pipeline(stages=[\n","        documentAssembler,\n","        sentenceDetector,\n","        tokenizer,\n","        embeddings,\n","        ner_model,\n","        ner_converter,\n","        chunk2doc,\n","        sentence_embeddings,\n","        resolver\n","])\n","\n","empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n","\n","model = nlpPipeline.fit(empty_data)"]},{"cell_type":"code","execution_count":null,"id":"9647e98c-6a53-4892-93ec-c1187110b024","metadata":{"id":"9647e98c-6a53-4892-93ec-c1187110b024"},"outputs":[],"source":["lp_model = nlp.LightPipeline(model)"]},{"cell_type":"code","execution_count":null,"id":"b29c1146-c38c-4def-8c38-30f494461d3b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1508,"status":"ok","timestamp":1669742178743,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":300},"id":"b29c1146-c38c-4def-8c38-30f494461d3b","outputId":"4c22ce8d-c417-49fb-ac44-54e11ecc2c04"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'document': ['We have entered into a definitive merger agreement with Amazon.'],\n"," 'ner_chunk': ['Amazon'],\n"," 'sentence_embeddings': ['Amazon'],\n"," 'resolution': ['AMAZON COM INC'],\n"," 'token': ['We',\n","  'have',\n","  'entered',\n","  'into',\n","  'a',\n","  'definitive',\n","  'merger',\n","  'agreement',\n","  'with',\n","  'Amazon',\n","  '.'],\n"," 'ner': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O'],\n"," 'embeddings': ['We',\n","  'have',\n","  'entered',\n","  'into',\n","  'a',\n","  'definitive',\n","  'merger',\n","  'agreement',\n","  'with',\n","  'Amazon',\n","  '.'],\n"," 'ner_chunk_doc': ['Amazon'],\n"," 'sentence': ['We have entered into a definitive merger agreement with Amazon.']}"]},"metadata":{},"execution_count":15}],"source":["el_res = lp_model.annotate(text)\n","el_res"]},{"cell_type":"markdown","id":"85e51e16-56a2-4dad-a27c-1047a36ecea3","metadata":{"id":"85e51e16-56a2-4dad-a27c-1047a36ecea3"},"source":["# Step 3: Data Augmentation with Chunk Mappers"]},{"cell_type":"markdown","id":"39dff3ed-08f0-4961-ba54-bbffb2606a81","metadata":{"id":"39dff3ed-08f0-4961-ba54-bbffb2606a81"},"source":["The component which carries out __Data Augmentation__ is called `ChunkMapper`.\n","\n","It's name comes from the way it works: it uses a _Ner Chunk_ to map it to an external data source.\n","\n","As a result, you will get a JSON with a dictionary of additional fields and their values. \n","\n","Let's take a look at how it works."]},{"cell_type":"code","execution_count":null,"id":"b246357e-0ab7-489b-9dc0-6d74d3eb97ef","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60075,"status":"ok","timestamp":1669742238802,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":300},"id":"b246357e-0ab7-489b-9dc0-6d74d3eb97ef","outputId":"2c48da99-f199-4759-f795-6c2327aed38e"},"outputs":[{"output_type":"stream","name":"stdout","text":["legmapper_edgar_companyname download started this may take some time.\n","[OK!]\n"]}],"source":["chunkAssembler = nlp.Doc2Chunk() \\\n","    .setInputCols(\"document\") \\\n","    .setOutputCol(\"chunk\") \\\n","    .setIsArray(False)\n","\n","CM =legal.ChunkMapperModel().pretrained(\"legmapper_edgar_companyname\", \"en\", \"legal/models\")\\\n","      .setInputCols([\"chunk\"])\\\n","      .setOutputCol(\"mappings\")\n","\n","cm_pipeline = nlp.Pipeline(stages=[documentAssembler, chunkAssembler, CM])\n","fit_cm_pipeline = cm_pipeline.fit(empty_data)"]},{"cell_type":"code","execution_count":null,"id":"340f193f-dabd-4e41-96e0-0afe0a22ed8b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1986,"status":"ok","timestamp":1669742240777,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":300},"id":"340f193f-dabd-4e41-96e0-0afe0a22ed8b","outputId":"fc9d74fc-327d-4be5-e9d5-8f64164a6f2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------+\n","|          text|\n","+--------------+\n","|AMAZON COM INC|\n","+--------------+\n","\n"]}],"source":["# LightPipelines don't support Doc2Chunk, so we will use here usual transform\n","\n","df = spark.createDataFrame([el_res['resolution']]).toDF(\"text\")\n","df.show()"]},{"cell_type":"code","execution_count":null,"id":"1b6729d6-32a9-4cea-b88f-0b6b7bf04d83","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1865,"status":"ok","timestamp":1669742242634,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":300},"id":"1b6729d6-32a9-4cea-b88f-0b6b7bf04d83","outputId":"d233d866-41a2-44b1-c6f1-adec703b14e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["+--------------+--------------------+--------------------+--------------------+\n","|          text|            document|               chunk|            mappings|\n","+--------------+--------------------+--------------------+--------------------+\n","|AMAZON COM INC|[{document, 0, 13...|[{chunk, 0, 13, A...|[{labeled_depende...|\n","+--------------+--------------------+--------------------+--------------------+\n","\n"]}],"source":["res = fit_cm_pipeline.transform(df)\n","res.show()"]},{"cell_type":"code","execution_count":null,"id":"c2d4a37d-b0a1-413a-9e36-5489396a042d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":323,"status":"ok","timestamp":1669742242949,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":300},"id":"c2d4a37d-b0a1-413a-9e36-5489396a042d","outputId":"5b38fbbc-c384-4aac-8ca8-a88f374ccf42"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(text='AMAZON COM INC', document=[Row(annotatorType='document', begin=0, end=13, result='AMAZON COM INC', metadata={'sentence': '0'}, embeddings=[])], chunk=[Row(annotatorType='chunk', begin=0, end=13, result='AMAZON COM INC', metadata={'sentence': '0', 'chunk': '0'}, embeddings=[])], mappings=[Row(annotatorType='labeled_dependency', begin=0, end=13, result='AMAZON COM INC', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'name', 'all_relations': ''}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='RETAIL-CATALOG & MAIL-ORDER HOUSES [5961]', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'sic', 'all_relations': '[5961'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='5961', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'sic_code', 'all_relations': '0'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='911646860', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'irs_number', 'all_relations': '0:::261631624'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='1231', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'fiscal_year_end', 'all_relations': '0'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='WA', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'state_location', 'all_relations': ''}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='DE', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'state_incorporation', 'all_relations': ''}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='410 TERRY AVENUE NORTH', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'business_street', 'all_relations': '1200 12TH AVENUE S SUITE 1200'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='SEATTLE', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'business_city', 'all_relations': ''}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='WA', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'business_state', 'all_relations': ''}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='98109', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'business_zip', 'all_relations': '98144'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='2062661000', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'business_phone', 'all_relations': ''}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='ABX Holdings, Inc.', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'former_name', 'all_relations': 'ABX AIR INC'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='20080102', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'former_name_date', 'all_relations': '19950728'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='2017-02-10', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'date', 'all_relations': '2016-01-29:::2016-02-10:::2016-09-08:::2016-10-28:::2019-02-01:::2019-10-25:::2018-02-02:::2018-07-27:::2015-01-30:::2015-07-24:::2015-10-23:::2015-12-04:::2014-01-31:::2014-01-30:::2014-02-11:::2014-02-18:::2014-02-19:::2014-02-13:::2014-02-20:::2014-03-06:::2014-04-09:::2014-04-04:::2013-01-30:::2012-02-01:::2011-01-28:::2011-07-27:::2011-10-26:::2022-02-04:::2022-07-29:::2021-02-03:::2021-04-08:::2020-01-31:::2020-05-01:::2020-10-30:::2010-01-29:::2010-07-23:::2010-10-22:::2009-01-30:::2009-07-24:::2009-10-23:::2008-02-11:::2008-07-25:::2007-02-16:::2007-04-26:::2007-07-26:::2007-10-25:::2006-02-17:::2006-07-27:::2006-10-26:::2005-03-11:::2005-07-28:::2004-02-25:::2004-04-23:::2003-02-19:::2003-07-24:::2003-09-24:::2002-01-24:::2003-10-24:::2002-07-26:::2002-10-25:::2000-08-02:::2000-10-30'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='1018724', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'company_id', 'all_relations': ''}, embeddings=[])])]"]},"metadata":{},"execution_count":19}],"source":["r = res.collect()\n","r"]},{"cell_type":"code","execution_count":null,"id":"bd37fe7a-6823-4ab3-a41e-e1e711fdbbb8","metadata":{"id":"bd37fe7a-6823-4ab3-a41e-e1e711fdbbb8"},"outputs":[],"source":["json_dict = dict()\n","for n in r[0]['mappings']:\n","    json_dict[n.metadata['relation']] = str(n.result)"]},{"cell_type":"code","execution_count":null,"id":"35e22829-8c0d-4853-a896-cc9502a567b1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1669742242952,"user":{"displayName":"Vildan Sarıkaya","userId":"07789644790967768983"},"user_tz":300},"id":"35e22829-8c0d-4853-a896-cc9502a567b1","outputId":"c6b852fe-76e4-423c-91dc-94bec54302b3"},"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","    \"business_city\": \"SEATTLE\",\n","    \"business_phone\": \"2062661000\",\n","    \"business_state\": \"WA\",\n","    \"business_street\": \"410 TERRY AVENUE NORTH\",\n","    \"business_zip\": \"98109\",\n","    \"company_id\": \"1018724\",\n","    \"date\": \"2017-02-10\",\n","    \"fiscal_year_end\": \"1231\",\n","    \"former_name\": \"ABX Holdings, Inc.\",\n","    \"former_name_date\": \"20080102\",\n","    \"irs_number\": \"911646860\",\n","    \"name\": \"AMAZON COM INC\",\n","    \"sic\": \"RETAIL-CATALOG & MAIL-ORDER HOUSES [5961]\",\n","    \"sic_code\": \"5961\",\n","    \"state_incorporation\": \"DE\",\n","    \"state_location\": \"WA\"\n","}\n"]}],"source":["import json\n","print(json.dumps(json_dict, indent=4, sort_keys=True))"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"vscode":{"interpreter":{"hash":"ca1c4b8877e01dec1d65bc94ac0771fb7b4e7d433b24c0ced0afdc05f796f65d"}}},"nbformat":4,"nbformat_minor":5}