{"cells":[{"cell_type":"markdown","id":"cfbbcfc0-e0b7-4c25-8bd7-c64d90f836d1","metadata":{"id":"cfbbcfc0-e0b7-4c25-8bd7-c64d90f836d1"},"source":["# Legal Data Augmentation with Chunk Mappers"]},{"cell_type":"markdown","id":"db5f4f9a-7776-42b3-8758-85624d4c15ea","metadata":{"id":"db5f4f9a-7776-42b3-8758-85624d4c15ea"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"]},{"cell_type":"markdown","id":"21e9eafb","metadata":{"id":"21e9eafb"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Legal/7.Data_Augmentation_with_ChunkMappers.ipynb)"]},{"cell_type":"markdown","id":"d818acfe-3b90-4e05-93c9-74e67fc55a13","metadata":{"id":"d818acfe-3b90-4e05-93c9-74e67fc55a13"},"source":["# Colab Setup"]},{"cell_type":"markdown","id":"ec74cbb6-b4f8-4eb8-bc55-44a3791bc338","metadata":{"id":"ec74cbb6-b4f8-4eb8-bc55-44a3791bc338"},"source":["# Start Spark Session"]},{"cell_type":"code","execution_count":null,"id":"ELqzaf32MT6E","metadata":{"id":"ELqzaf32MT6E"},"outputs":[],"source":["# Install the johnsnowlabs library to access Spark-OCR and Spark-NLP for Healthcare, Finance, and Legal.\n","! pip install johnsnowlabs "]},{"cell_type":"code","execution_count":null,"id":"RO2dIA414yL_","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"executionInfo":{"elapsed":7514,"status":"ok","timestamp":1664476566561,"user":{"displayName":"Vildan Sarƒ±kaya","userId":"07789644790967768983"},"user_tz":240},"id":"RO2dIA414yL_","outputId":"a3bad330-8ed1-4fb5-9158-65c3417b71dd"},"outputs":[],"source":["from google.colab import files\n","print('Please Upload your John Snow Labs License using the button below')\n","license_keys = files.upload()"]},{"cell_type":"code","execution_count":3,"id":"dmcB5zVBHZO8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":145013,"status":"ok","timestamp":1664476733490,"user":{"displayName":"Vildan Sarƒ±kaya","userId":"07789644790967768983"},"user_tz":240},"id":"dmcB5zVBHZO8","outputId":"d7a8e84c-c3ab-4b3c-b394-3685831c3cf6"},"outputs":[{"name":"stdout","output_type":"stream","text":["üëå Detected license file /content/4.1.0.spark_nlp_for_healthcare.json\n","üö® Outdated Medical Secrets in license file. Version=4.1.0 but should be Version=0.1.14\n","üìã Stored John Snow Labs License in /root/.johnsnowlabs/licenses/license_number_0_for_Spark-Healthcare_Spark-OCR.json\n","üë∑ Setting up if John Snow Labs home exists in /root/.johnsnowlabs this might take a few minutes.\n","Downloading üêç+üöÄ Python Library spark_nlp-4.1.0-py2.py3-none-any.whl\n","Downloading üêç+üíä Python Library internal_with_finleg-0.1.14-py3-none-any.whl\n","Downloading üêç+üï∂ Python Library spark_ocr-4.1.0-py3-none-any.whl\n","Downloading ü´ò+üöÄ Java Library spark-nlp-assembly-4.1.0.jar\n","Downloading ü´ò+üíä Java Library spark-nlp-jsl-assembly-4.1.0.jar\n","Downloading ü´ò+üï∂ Java Library spark-ocr-assembly-4.1.0.jar\n","üôÜ JSL Home setup in /root/.johnsnowlabs\n","üëå Detected license file /content/4.1.0.spark_nlp_for_healthcare.json\n","Installing /root/.johnsnowlabs/py_installs/internal_with_finleg-0.1.14-py3-none-any.whl to /usr/bin/python3\n","Running: /usr/bin/python3 -m pip install /root/.johnsnowlabs/py_installs/internal_with_finleg-0.1.14-py3-none-any.whl\n","üëå Detected license file /content/4.1.0.spark_nlp_for_healthcare.json\n","Installing /root/.johnsnowlabs/py_installs/spark_ocr-4.1.0-py3-none-any.whl to /usr/bin/python3\n","Running: /usr/bin/python3 -m pip install /root/.johnsnowlabs/py_installs/spark_ocr-4.1.0-py3-none-any.whl\n","Installing pyspark to /usr/bin/python3\n","Running: /usr/bin/python3 -m pip install pyspark\n","Installed 3 products:\n","üíä Spark-Healthcare==False installed! ‚úÖ Heal the planet with NLP! \n","üï∂ Spark-OCR==4.1.0 installed! ‚úÖ Empower your NLP with a set of eyes \n","üöÄ Spark-NLP==4.1.0 installed! ‚úÖ State of the art NLP at scale \n","üîÅ\u001b[91m If you are on Google Colab, please restart your Notebook for changes to take effect \u001b[39müîÅ\n"]}],"source":["from johnsnowlabs import * \n","# After uploading your license run this to install all licensed Python Wheels and pre-download Jars the Spark Session JVM\n","# Make sure to restart your notebook afterwards for changes to take effect\n","jsl.install()"]},{"cell_type":"code","execution_count":1,"id":"lQ8-BI-_5QjG","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24420,"status":"ok","timestamp":1664476770149,"user":{"displayName":"Vildan Sarƒ±kaya","userId":"07789644790967768983"},"user_tz":240},"id":"lQ8-BI-_5QjG","outputId":"b9f9d4e7-c03b-45b6-9431-3711ed33c559"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[91müö® Your Spark-OCR is outdated, installed==4.0.0a1 but latest version==4.1.0\n","You can run \u001b[92m jsl.install() \u001b[39mto update Spark-OCR\n","üëå Detected license file /content/4.1.0.spark_nlp_for_healthcare.json\n","üö® Outdated Medical Secrets in license file. Version=4.1.0 but should be Version=0.1.14\n","üëå Launched \u001b[92mcpu-Optimized JVM\u001b[39m with SparkSession with Jars for: üöÄSpark-NLP==4.1.0, üíäSpark-Healthcare==4.0.0a1, üï∂Spark-OCR==4.1.0, running on ‚ö° PySpark==3.1.2\n"]}],"source":["from johnsnowlabs import * \n","# Automatically load license data and start a session with all jars user has access to\n","spark = jsl.start()"]},{"cell_type":"code","execution_count":2,"id":"0NHLTd2zb5ch","metadata":{"executionInfo":{"elapsed":39,"status":"ok","timestamp":1664476770151,"user":{"displayName":"Vildan Sarƒ±kaya","userId":"07789644790967768983"},"user_tz":240},"id":"0NHLTd2zb5ch"},"outputs":[],"source":["import pandas as pd\n","import warnings\n","warnings.filterwarnings('ignore')\n","# if you want to start the session with custom params as in start function above\n","def start(SECRET):\n","    builder = SparkSession.builder \\\n","        .appName(\"Spark NLP Licensed\") \\\n","        .master(\"local[*]\") \\\n","        .config(\"spark.driver.memory\", \"16G\") \\\n","        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n","        .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n","        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:\"+PUBLIC_VERSION) \\\n","        .config(\"spark.jars\", \"https://pypi.johnsnowlabs.com/\"+SECRET+\"/spark-nlp-jsl-\"+JSL_VERSION+\".jar\")\n","      \n","    return builder.getOrCreate()\n","\n","#spark = start(SECRET)\n"]},{"cell_type":"markdown","id":"d2cd4221-fbca-4ca1-86a9-65e6264c4ad1","metadata":{"id":"d2cd4221-fbca-4ca1-86a9-65e6264c4ad1"},"source":["# About Data Augmentation"]},{"cell_type":"markdown","id":"bf9835fd-9def-44e4-b022-e8db0f045fec","metadata":{"id":"bf9835fd-9def-44e4-b022-e8db0f045fec"},"source":["__Data Augmentation__ is the process of increase an extracted datapoint with external sources. \n","\n","For example, let's suppose I work with a document which mentions the company _Amazon_. We could be talking about stock prices, or some legal litigations, or just a commercial agreement with a provider, among others.\n","\n","In the document, we can extract `Amazon` using NER as an Organization, but that's all the information available about `Amazon` in that document.\n","\n","Well, with __Data Augmentation__, we can use external sources, as _SEC Edgar, Crunchbase, Nasdaq_ or even _Wikipedia_, to enrich `Amazon` with much more information, allowing us to take better decisions.\n","\n","Let's see how to do it."]},{"cell_type":"markdown","id":"eef8c0e5-6793-4db5-ab39-f6381c9e500d","metadata":{"id":"eef8c0e5-6793-4db5-ab39-f6381c9e500d"},"source":["# Step 1: Name Entity Recognition"]},{"cell_type":"markdown","id":"612f9fe0-4d7c-4d6e-afc1-47a59b99f529","metadata":{"id":"612f9fe0-4d7c-4d6e-afc1-47a59b99f529"},"source":["Let's suppose we get this news from scrapping the Internet, or from Twitter."]},{"cell_type":"code","execution_count":3,"id":"27f44267-72be-45de-afaa-a2c911195d89","metadata":{"executionInfo":{"elapsed":38,"status":"ok","timestamp":1664476770152,"user":{"displayName":"Vildan Sarƒ±kaya","userId":"07789644790967768983"},"user_tz":240},"id":"27f44267-72be-45de-afaa-a2c911195d89"},"outputs":[],"source":["text = \"We have entered into a definitive merger agreement with Amazon.\""]},{"cell_type":"markdown","id":"71a5e806-9659-4b41-8ab6-38f81b26797f","metadata":{"id":"71a5e806-9659-4b41-8ab6-38f81b26797f"},"source":["We use NER to extract the companies name, in this case, Amazon."]},{"cell_type":"code","execution_count":4,"id":"cb765952-24c2-48b6-8d86-5413b13bd9fa","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":69951,"status":"ok","timestamp":1664476840067,"user":{"displayName":"Vildan Sarƒ±kaya","userId":"07789644790967768983"},"user_tz":240},"id":"cb765952-24c2-48b6-8d86-5413b13bd9fa","outputId":"87e2ff1d-3da9-4c08-d0eb-58e933670556"},"outputs":[{"name":"stdout","output_type":"stream","text":["sentence_detector_dl download started this may take some time.\n","Approximate size to download 514.9 KB\n","[OK!]\n","bert_embeddings_sec_bert_base download started this may take some time.\n","Approximate size to download 390.4 MB\n","[OK!]\n","legner_orgs_prods_alias download started this may take some time.\n","[OK!]\n"]}],"source":["documentAssembler = nlp.DocumentAssembler()\\\n","        .setInputCol(\"text\")\\\n","        .setOutputCol(\"document\")\n","        \n","sentenceDetector = nlp.SentenceDetectorDLModel.pretrained(\"sentence_detector_dl\",\"xx\")\\\n","        .setInputCols([\"document\"])\\\n","        .setOutputCol(\"sentence\")\n","\n","tokenizer = nlp.Tokenizer()\\\n","        .setInputCols([\"sentence\"])\\\n","        .setOutputCol(\"token\")\n","\n","embeddings = nlp.BertEmbeddings.pretrained(\"bert_embeddings_sec_bert_base\",\"en\") \\\n","    .setInputCols([\"sentence\", \"token\"]) \\\n","    .setOutputCol(\"embeddings\")\n","\n","ner_model = legal.NerModel.pretrained(\"legner_orgs_prods_alias\", \"en\", \"legal/models\")\\\n","        .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n","        .setOutputCol(\"ner\")\n","        \n","ner_converter = nlp.NerConverter()\\\n","        .setInputCols([\"sentence\",\"token\",\"ner\"])\\\n","        .setOutputCol(\"ner_chunk\")\n","\n","nlpPipeline = Pipeline(stages=[\n","        documentAssembler,\n","        sentenceDetector,\n","        tokenizer,\n","        embeddings,\n","        ner_model,\n","        ner_converter,\n","])\n","\n","empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n","\n","model = nlpPipeline.fit(empty_data)"]},{"cell_type":"markdown","id":"37eae9a4-52e1-400e-a1dd-effc6ed1da35","metadata":{"id":"37eae9a4-52e1-400e-a1dd-effc6ed1da35"},"source":["## We use LightPipelines to get the result"]},{"cell_type":"code","execution_count":5,"id":"76a05a20-b9b1-4868-a198-0e950c05a786","metadata":{"executionInfo":{"elapsed":469,"status":"ok","timestamp":1664476840504,"user":{"displayName":"Vildan Sarƒ±kaya","userId":"07789644790967768983"},"user_tz":240},"id":"76a05a20-b9b1-4868-a198-0e950c05a786"},"outputs":[],"source":["lp_ner = LightPipeline(model)"]},{"cell_type":"code","execution_count":6,"id":"19545c7d-1738-47be-afdd-5f44236a4a2a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2982,"status":"ok","timestamp":1664476843480,"user":{"displayName":"Vildan Sarƒ±kaya","userId":"07789644790967768983"},"user_tz":240},"id":"19545c7d-1738-47be-afdd-5f44236a4a2a","outputId":"a08d5cd9-afa9-449b-a834-6fd7ea07c7ba"},"outputs":[{"data":{"text/plain":["{'document': ['We have entered into a definitive merger agreement with Amazon.'],\n"," 'ner_chunk': ['Amazon'],\n"," 'token': ['We',\n","  'have',\n","  'entered',\n","  'into',\n","  'a',\n","  'definitive',\n","  'merger',\n","  'agreement',\n","  'with',\n","  'Amazon',\n","  '.'],\n"," 'ner': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O'],\n"," 'embeddings': ['We',\n","  'have',\n","  'entered',\n","  'into',\n","  'a',\n","  'definitive',\n","  'merger',\n","  'agreement',\n","  'with',\n","  'Amazon',\n","  '.'],\n"," 'sentence': ['We have entered into a definitive merger agreement with Amazon.']}"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["ner_result = lp_ner.annotate(text)\n","ner_result"]},{"cell_type":"markdown","id":"9fe41161-c8fd-467e-9fff-5d4fe1cb5160","metadata":{"id":"9fe41161-c8fd-467e-9fff-5d4fe1cb5160"},"source":["Alright! Amazon has been detected as an organization. \n","\n","Now, let's augment `Amazon` with more information about the company, given that there are no more details in the tweet I can use.\n","\n","But before __augmenting__, there is a very important step we need to carry out: `Company Name Normalization`"]},{"cell_type":"markdown","id":"eb3e2808-3550-46d9-835b-f747cac4123c","metadata":{"id":"eb3e2808-3550-46d9-835b-f747cac4123c"},"source":["# Step 2: Company Names Normalization"]},{"cell_type":"markdown","id":"4e55a84c-2806-4a0e-b30f-90a43dc497ca","metadata":{"id":"4e55a84c-2806-4a0e-b30f-90a43dc497ca"},"source":["Let's suppose we want to manually get information about Amazon.\n","\n","Since it's a public US company, we can go to [SEC Edgar's database](https://www.sec.gov/edgar/searchedgar/companysearch) and look for it."]},{"cell_type":"markdown","id":"13f414e7-d727-4b8a-ba8a-8b0c644bb7da","metadata":{"id":"13f414e7-d727-4b8a-ba8a-8b0c644bb7da"},"source":["Unfortunately, `Amazon` is not the official name of the company, which means no entry for `Amazon` is available. That's were __Company Names Normalization__ comes in handy."]},{"cell_type":"markdown","id":"6eb8eb7f-1bda-454c-8318-bb4df34f0b6a","metadata":{"id":"6eb8eb7f-1bda-454c-8318-bb4df34f0b6a"},"source":["`Company Name Normalization` is the process of obtaining the name of the company used by data providers, usually the \"official\" name of the company.\n","\n","Sometimes, some data providers may have different versions of the name with different punctuation. For example, for Meta:\n","- Meta Platforms, Inc.\n","- Meta Platforms Inc.\n","- Meta Platforms, Inc\n","- etc\n","\n","So, it's mandatory we do `Company Normalization` taking into account the database / datasource provider we want to extract data from. The data providers we have are:\n","- SEC Edgar\n","- Crunchbase until 2015\n","- Wikidata (in progress)\n","\n","Let's normalize `Amazon` to the official name in _SEC Edgar_."]},{"cell_type":"code","execution_count":7,"id":"2564fd32-99ec-451c-ae34-2792cf3036ef","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":124802,"status":"ok","timestamp":1664476968273,"user":{"displayName":"Vildan Sarƒ±kaya","userId":"07789644790967768983"},"user_tz":240},"id":"2564fd32-99ec-451c-ae34-2792cf3036ef","outputId":"bbbd250f-543b-4c01-e536-10b31629a144"},"outputs":[{"name":"stdout","output_type":"stream","text":["tfhub_use download started this may take some time.\n","Approximate size to download 923.7 MB\n","[OK!]\n","legel_edgar_company_name download started this may take some time.\n","[OK!]\n"]}],"source":["embeddings = nlp.UniversalSentenceEncoder.pretrained(\"tfhub_use\", \"en\") \\\n","      .setInputCols(\"document\") \\\n","      .setOutputCol(\"sentence_embeddings\")\n","    \n","resolver = legal.SentenceEntityResolverModel.pretrained(\"legel_edgar_company_name\", \"en\", \"legal/models\")\\\n","      .setInputCols([\"text\", \"sentence_embeddings\"]) \\\n","      .setOutputCol(\"resolution\")\\\n","      .setDistanceFunction(\"EUCLIDEAN\")\n","\n","pipelineModel = PipelineModel(\n","      stages = [\n","          documentAssembler,\n","          embeddings,\n","          resolver])\n","\n","lp_res = LightPipeline(pipelineModel)"]},{"cell_type":"code","execution_count":8,"id":"36a6d5f7-6477-4219-acf7-53a95d1ebea3","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1664476968275,"user":{"displayName":"Vildan Sarƒ±kaya","userId":"07789644790967768983"},"user_tz":240},"id":"36a6d5f7-6477-4219-acf7-53a95d1ebea3","outputId":"0e4c513e-8f92-4db1-e71d-b5cae77e011b"},"outputs":[{"data":{"text/plain":["['Amazon']"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["ner_result['ner_chunk']"]},{"cell_type":"code","execution_count":9,"id":"c319f7b8-fe7e-4408-9960-15e7675a36c1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1069,"status":"ok","timestamp":1664476969310,"user":{"displayName":"Vildan Sarƒ±kaya","userId":"07789644790967768983"},"user_tz":240},"id":"c319f7b8-fe7e-4408-9960-15e7675a36c1","outputId":"ecefa915-8ba5-484f-f98d-83bef4f16320"},"outputs":[{"data":{"text/plain":["[{'document': ['Amazon'],\n","  'sentence_embeddings': ['Amazon'],\n","  'resolution': ['AMAZON COM INC']}]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["el_res = lp_res.annotate(ner_result['ner_chunk'])\n","el_res"]},{"cell_type":"markdown","id":"109efb72-bfae-413b-b1cb-ef1c57b9b66d","metadata":{"id":"109efb72-bfae-413b-b1cb-ef1c57b9b66d"},"source":["Here is our normalized name for Amazon: `AMAZON COM INC`.\n","\n","Now, let's see which information is available in Edgar database for `AMAZON COM INC`"]},{"cell_type":"markdown","id":"520b8b1d-2754-4338-acc4-d74aeab8a673","metadata":{"id":"520b8b1d-2754-4338-acc4-d74aeab8a673"},"source":["# Steps 1 and 2 in the same pipeline"]},{"cell_type":"code","execution_count":10,"id":"46ed95a6-aa6e-47b5-897e-6430249f9532","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15235,"status":"ok","timestamp":1664476984539,"user":{"displayName":"Vildan Sarƒ±kaya","userId":"07789644790967768983"},"user_tz":240},"id":"46ed95a6-aa6e-47b5-897e-6430249f9532","outputId":"7bc60f8e-f889-4321-b4cb-8f240b8047fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["sentence_detector_dl download started this may take some time.\n","Approximate size to download 514.9 KB\n","[OK!]\n","bert_embeddings_sec_bert_base download started this may take some time.\n","Approximate size to download 390.4 MB\n","[OK!]\n","legner_orgs_prods_alias download started this may take some time.\n","[OK!]\n","tfhub_use download started this may take some time.\n","Approximate size to download 923.7 MB\n","[OK!]\n","legel_edgar_company_name download started this may take some time.\n","[OK!]\n"]}],"source":["documentAssembler = nlp.DocumentAssembler()\\\n","        .setInputCol(\"text\")\\\n","        .setOutputCol(\"document\")\n","        \n","sentenceDetector = nlp.SentenceDetectorDLModel.pretrained(\"sentence_detector_dl\",\"xx\")\\\n","        .setInputCols([\"document\"])\\\n","        .setOutputCol(\"sentence\")\n","\n","tokenizer = nlp.Tokenizer()\\\n","        .setInputCols([\"sentence\"])\\\n","        .setOutputCol(\"token\")\n","\n","embeddings = nlp.BertEmbeddings.pretrained(\"bert_embeddings_sec_bert_base\",\"en\") \\\n","    .setInputCols([\"sentence\", \"token\"]) \\\n","    .setOutputCol(\"embeddings\")\n","\n","ner_model = legal.NerModel.pretrained(\"legner_orgs_prods_alias\", \"en\", \"legal/models\")\\\n","        .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n","        .setOutputCol(\"ner\")\n","        \n","ner_converter = nlp.NerConverter()\\\n","        .setInputCols([\"sentence\",\"token\",\"ner\"])\\\n","        .setOutputCol(\"ner_chunk\")\n","\n","chunk2doc = nlp.Chunk2Doc()\\\n","        .setInputCols(\"ner_chunk\")\\\n","        .setOutputCol(\"ner_chunk_doc\")\n","\n","sentence_embeddings = nlp.UniversalSentenceEncoder.pretrained(\"tfhub_use\", \"en\") \\\n","      .setInputCols(\"ner_chunk_doc\") \\\n","      .setOutputCol(\"sentence_embeddings\")\n","    \n","resolver = legal.SentenceEntityResolverModel.pretrained(\"legel_edgar_company_name\", \"en\", \"legal/models\")\\\n","      .setInputCols([\"text\", \"sentence_embeddings\"]) \\\n","      .setOutputCol(\"resolution\")\\\n","      .setDistanceFunction(\"EUCLIDEAN\")\n","\n","nlpPipeline = Pipeline(stages=[\n","        documentAssembler,\n","        sentenceDetector,\n","        tokenizer,\n","        embeddings,\n","        ner_model,\n","        ner_converter,\n","        chunk2doc,\n","        sentence_embeddings,\n","        resolver\n","])\n","\n","empty_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n","\n","model = nlpPipeline.fit(empty_data)"]},{"cell_type":"code","execution_count":11,"id":"9647e98c-6a53-4892-93ec-c1187110b024","metadata":{"executionInfo":{"elapsed":41,"status":"ok","timestamp":1664476984540,"user":{"displayName":"Vildan Sarƒ±kaya","userId":"07789644790967768983"},"user_tz":240},"id":"9647e98c-6a53-4892-93ec-c1187110b024"},"outputs":[],"source":["lp_model = LightPipeline(model)"]},{"cell_type":"code","execution_count":12,"id":"b29c1146-c38c-4def-8c38-30f494461d3b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2110,"status":"ok","timestamp":1664476986612,"user":{"displayName":"Vildan Sarƒ±kaya","userId":"07789644790967768983"},"user_tz":240},"id":"b29c1146-c38c-4def-8c38-30f494461d3b","outputId":"ef58ad9f-ec06-4b8e-f47b-6d46c91e4e33"},"outputs":[{"data":{"text/plain":["{'document': ['We have entered into a definitive merger agreement with Amazon.'],\n"," 'ner_chunk': ['Amazon'],\n"," 'sentence_embeddings': ['Amazon'],\n"," 'resolution': ['AMAZON COM INC'],\n"," 'token': ['We',\n","  'have',\n","  'entered',\n","  'into',\n","  'a',\n","  'definitive',\n","  'merger',\n","  'agreement',\n","  'with',\n","  'Amazon',\n","  '.'],\n"," 'ner': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O'],\n"," 'embeddings': ['We',\n","  'have',\n","  'entered',\n","  'into',\n","  'a',\n","  'definitive',\n","  'merger',\n","  'agreement',\n","  'with',\n","  'Amazon',\n","  '.'],\n"," 'ner_chunk_doc': ['Amazon'],\n"," 'sentence': ['We have entered into a definitive merger agreement with Amazon.']}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["el_res = lp_model.annotate(text)\n","el_res"]},{"cell_type":"markdown","id":"85e51e16-56a2-4dad-a27c-1047a36ecea3","metadata":{"id":"85e51e16-56a2-4dad-a27c-1047a36ecea3"},"source":["# Step 3: Data Augmentation with Chunk Mappers"]},{"cell_type":"markdown","id":"39dff3ed-08f0-4961-ba54-bbffb2606a81","metadata":{"id":"39dff3ed-08f0-4961-ba54-bbffb2606a81"},"source":["The component which carries out __Data Augmentation__ is called `ChunkMapper`.\n","\n","It's name comes from the way it works: it uses a _Ner Chunk_ to map it to an external data source.\n","\n","As a result, you will get a JSON with a dictionary of additional fields and their values. \n","\n","Let's take a look at how it works."]},{"cell_type":"code","execution_count":13,"id":"b246357e-0ab7-489b-9dc0-6d74d3eb97ef","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":110429,"status":"ok","timestamp":1664477097032,"user":{"displayName":"Vildan Sarƒ±kaya","userId":"07789644790967768983"},"user_tz":240},"id":"b246357e-0ab7-489b-9dc0-6d74d3eb97ef","outputId":"c6bf3d89-5dbe-4988-a716-d3175e22db40"},"outputs":[{"name":"stdout","output_type":"stream","text":["legmapper_edgar_companyname download started this may take some time.\n","[OK!]\n"]}],"source":["chunkAssembler = nlp.Doc2Chunk() \\\n","    .setInputCols(\"document\") \\\n","    .setOutputCol(\"chunk\") \\\n","    .setIsArray(False)\n","\n","CM =legal.ChunkMapperModel().pretrained(\"legmapper_edgar_companyname\", \"en\", \"legal/models\")\\\n","      .setInputCols([\"chunk\"])\\\n","      .setOutputCol(\"mappings\")\n","\n","cm_pipeline = Pipeline(stages=[documentAssembler, chunkAssembler, CM])\n","fit_cm_pipeline = cm_pipeline.fit(empty_data)"]},{"cell_type":"code","execution_count":14,"id":"340f193f-dabd-4e41-96e0-0afe0a22ed8b","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10466,"status":"ok","timestamp":1664477107464,"user":{"displayName":"Vildan Sarƒ±kaya","userId":"07789644790967768983"},"user_tz":240},"id":"340f193f-dabd-4e41-96e0-0afe0a22ed8b","outputId":"016eccf7-e328-4f80-f389-a0ee4e61ab61"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------+\n","|          text|\n","+--------------+\n","|AMAZON COM INC|\n","+--------------+\n","\n"]}],"source":["# LightPipelines don't support Doc2Chunk, so we will use here usual transform\n","\n","df = spark.createDataFrame([el_res['resolution']]).toDF(\"text\")\n","df.show()"]},{"cell_type":"code","execution_count":15,"id":"1b6729d6-32a9-4cea-b88f-0b6b7bf04d83","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3572,"status":"ok","timestamp":1664477111006,"user":{"displayName":"Vildan Sarƒ±kaya","userId":"07789644790967768983"},"user_tz":240},"id":"1b6729d6-32a9-4cea-b88f-0b6b7bf04d83","outputId":"56d153f7-7878-4323-b736-a1b399675226"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------+--------------------+--------------------+--------------------+\n","|          text|            document|               chunk|            mappings|\n","+--------------+--------------------+--------------------+--------------------+\n","|AMAZON COM INC|[{document, 0, 13...|[{chunk, 0, 13, A...|[{labeled_depende...|\n","+--------------+--------------------+--------------------+--------------------+\n","\n"]}],"source":["res = fit_cm_pipeline.transform(df)\n","res.show()"]},{"cell_type":"code","execution_count":16,"id":"c2d4a37d-b0a1-413a-9e36-5489396a042d","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":290,"status":"ok","timestamp":1664477111268,"user":{"displayName":"Vildan Sarƒ±kaya","userId":"07789644790967768983"},"user_tz":240},"id":"c2d4a37d-b0a1-413a-9e36-5489396a042d","outputId":"11ed65aa-0fec-4990-f37b-24386d0b6ad8"},"outputs":[{"data":{"text/plain":["[Row(text='AMAZON COM INC', document=[Row(annotatorType='document', begin=0, end=13, result='AMAZON COM INC', metadata={'sentence': '0'}, embeddings=[])], chunk=[Row(annotatorType='chunk', begin=0, end=13, result='AMAZON COM INC', metadata={'sentence': '0', 'chunk': '0'}, embeddings=[])], mappings=[Row(annotatorType='labeled_dependency', begin=0, end=13, result='AMAZON COM INC', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'name', 'all_relations': ''}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='RETAIL-CATALOG & MAIL-ORDER HOUSES [5961]', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'sic', 'all_relations': '[5961'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='5961', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'sic_code', 'all_relations': '0'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='911646860', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'irs_number', 'all_relations': '0:::261631624'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='1231', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'fiscal_year_end', 'all_relations': '0'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='WA', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'state_location', 'all_relations': ''}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='DE', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'state_incorporation', 'all_relations': ''}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='410 TERRY AVENUE NORTH', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'business_street', 'all_relations': '1200 12TH AVENUE S SUITE 1200'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='SEATTLE', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'business_city', 'all_relations': ''}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='WA', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'business_state', 'all_relations': ''}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='98109', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'business_zip', 'all_relations': '98144'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='2062661000', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'business_phone', 'all_relations': ''}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='ABX Holdings, Inc.', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'former_name', 'all_relations': 'ABX AIR INC'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='20080102', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'former_name_date', 'all_relations': '19950728'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='2017-02-10', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'date', 'all_relations': '2016-01-29:::2016-02-10:::2016-09-08:::2016-10-28:::2019-02-01:::2019-10-25:::2018-02-02:::2018-07-27:::2015-01-30:::2015-07-24:::2015-10-23:::2015-12-04:::2014-01-31:::2014-01-30:::2014-02-11:::2014-02-18:::2014-02-19:::2014-02-13:::2014-02-20:::2014-03-06:::2014-04-09:::2014-04-04:::2013-01-30:::2012-02-01:::2011-01-28:::2011-07-27:::2011-10-26:::2022-02-04:::2022-07-29:::2021-02-03:::2021-04-08:::2020-01-31:::2020-05-01:::2020-10-30:::2010-01-29:::2010-07-23:::2010-10-22:::2009-01-30:::2009-07-24:::2009-10-23:::2008-02-11:::2008-07-25:::2007-02-16:::2007-04-26:::2007-07-26:::2007-10-25:::2006-02-17:::2006-07-27:::2006-10-26:::2005-03-11:::2005-07-28:::2004-02-25:::2004-04-23:::2003-02-19:::2003-07-24:::2003-09-24:::2002-01-24:::2003-10-24:::2002-07-26:::2002-10-25:::2000-08-02:::2000-10-30'}, embeddings=[]), Row(annotatorType='labeled_dependency', begin=0, end=13, result='1018724', metadata={'sentence': '0', 'chunk': '0', 'entity': 'AMAZON COM INC', 'relation': 'company_id', 'all_relations': ''}, embeddings=[])])]"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["r = res.collect()\n","r"]},{"cell_type":"code","execution_count":17,"id":"bd37fe7a-6823-4ab3-a41e-e1e711fdbbb8","metadata":{"executionInfo":{"elapsed":22,"status":"ok","timestamp":1664477111270,"user":{"displayName":"Vildan Sarƒ±kaya","userId":"07789644790967768983"},"user_tz":240},"id":"bd37fe7a-6823-4ab3-a41e-e1e711fdbbb8"},"outputs":[],"source":["json_dict = dict()\n","for n in r[0]['mappings']:\n","    json_dict[n.metadata['relation']] = str(n.result)"]},{"cell_type":"code","execution_count":18,"id":"35e22829-8c0d-4853-a896-cc9502a567b1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1664477111271,"user":{"displayName":"Vildan Sarƒ±kaya","userId":"07789644790967768983"},"user_tz":240},"id":"35e22829-8c0d-4853-a896-cc9502a567b1","outputId":"23787898-4507-4ca7-9525-e1c744dd8ee4"},"outputs":[{"name":"stdout","output_type":"stream","text":["{\n","    \"business_city\": \"SEATTLE\",\n","    \"business_phone\": \"2062661000\",\n","    \"business_state\": \"WA\",\n","    \"business_street\": \"410 TERRY AVENUE NORTH\",\n","    \"business_zip\": \"98109\",\n","    \"company_id\": \"1018724\",\n","    \"date\": \"2017-02-10\",\n","    \"fiscal_year_end\": \"1231\",\n","    \"former_name\": \"ABX Holdings, Inc.\",\n","    \"former_name_date\": \"20080102\",\n","    \"irs_number\": \"911646860\",\n","    \"name\": \"AMAZON COM INC\",\n","    \"sic\": \"RETAIL-CATALOG & MAIL-ORDER HOUSES [5961]\",\n","    \"sic_code\": \"5961\",\n","    \"state_incorporation\": \"DE\",\n","    \"state_location\": \"WA\"\n","}\n"]}],"source":["import json\n","print(json.dumps(json_dict, indent=4, sort_keys=True))"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.9.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"vscode":{"interpreter":{"hash":"ca1c4b8877e01dec1d65bc94ac0771fb7b4e7d433b24c0ced0afdc05f796f65d"}}},"nbformat":4,"nbformat_minor":5}
