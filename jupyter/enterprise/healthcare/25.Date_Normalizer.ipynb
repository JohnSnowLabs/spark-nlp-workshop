{"cells":[{"cell_type":"markdown","metadata":{"id":"I08sFJYCxR0Z"},"source":["![JohnSnowLabs](https://nlp.johnsnowlabs.com/assets/images/logo.png)"]},{"cell_type":"markdown","metadata":{"id":"LKI5K1wQrSe9"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Healthcare/25.Date_Normalizer.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"YiRjfE_SHmQI"},"source":["# 25. Date Normalizer"]},{"cell_type":"markdown","metadata":{"id":"okhT7AcXxben"},"source":["## Colab Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I8Ytt2LLp2rj"},"outputs":[],"source":["import sys\n","import json\n","import os\n","with open('license.json') as f:\n","    license_keys = json.load(f)\n","    \n","import os\n","locals().update(license_keys)\n","os.environ.update(license_keys)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9-PzVG9KYfxa"},"outputs":[],"source":["# Installing pyspark and spark-nlp\n","! pip install --upgrade -q pyspark==3.1.2 spark-nlp==$PUBLIC_VERSION\n","\n","# Installing Spark NLP Healthcare\n","! pip install --upgrade -q spark-nlp-jsl==$JSL_VERSION  --extra-index-url https://pypi.johnsnowlabs.com/$SECRET"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":314},"executionInfo":{"elapsed":69424,"status":"ok","timestamp":1649503790101,"user":{"displayName":"Damla","userId":"03285166568766987047"},"user_tz":-120},"id":"VtLIEJtPf88T","outputId":"9a1a7826-9c8a-4ac5-fda8-24c39d2d2277"},"outputs":[{"name":"stdout","output_type":"stream","text":["Spark NLP Version : 3.4.2\n","Spark NLP_JSL Version : 3.5.0\n"]},{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://dcf19cb16d72:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.1.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Spark NLP Licensed</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f6d032b4190>"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["import json\n","import os\n","import pandas as pd\n","\n","from pyspark.ml import Pipeline, PipelineModel\n","from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F\n","\n","import sparknlp_jsl\n","import sparknlp\n","\n","from sparknlp.annotator import *\n","from sparknlp_jsl.annotator import *\n","from sparknlp.base import *\n","from sparknlp.util import *\n","from sparknlp.pretrained import ResourceDownloader\n","\n","\n","spark = sparknlp_jsl.start(license_keys['SECRET'])\n","\n","print (\"Spark NLP Version :\", sparknlp.version())\n","print (\"Spark NLP_JSL Version :\", sparknlp_jsl.version())\n","\n","spark"]},{"cell_type":"markdown","metadata":{"id":"kqlcNe7FJr31"},"source":["## **Date Normalizer**\n","\n","New Annotator that transforms chunks Dates to a normalized Date with format YYYY/MM/DD. This annotator identifies dates in chunk annotations and transforms those dates to the format YYYY/MM/DD. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"YPvA603jL3TV"},"source":["We going to create a chunks dates with different formats:"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":419,"status":"ok","timestamp":1649503802448,"user":{"displayName":"Damla","userId":"03285166568766987047"},"user_tz":-120},"id":"Cnf_s2yFLpXC"},"outputs":[],"source":["dates = [\n","'08/02/2018',\n","'11/2018',\n","'11/01/2018',\n","'12Mar2021',\n","'Jan 30, 2018',\n","'13.04.1999', \n","'3April 2020',\n","]\n","\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3127,"status":"ok","timestamp":1649503808236,"user":{"displayName":"Damla","userId":"03285166568766987047"},"user_tz":-120},"id":"SW-ND2BjONF_"},"outputs":[],"source":["from pyspark.sql.types import StringType\n","df_dates = spark.createDataFrame(dates,StringType()).toDF('ner_chunk')"]},{"cell_type":"markdown","metadata":{"id":"JiIqDdHDPkQV"},"source":["We going to transform that text to documents in spark-nlp."]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":1049,"status":"ok","timestamp":1649503812666,"user":{"displayName":"Damla","userId":"03285166568766987047"},"user_tz":-120},"id":"13wTh48FPcG-"},"outputs":[],"source":["document_assembler = DocumentAssembler().setInputCol('ner_chunk').setOutputCol('document')\n","documents_DF = document_assembler.transform(df_dates)"]},{"cell_type":"markdown","metadata":{"id":"VelhLulcPxbx"},"source":["After that we going to transform that documents to chunks."]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":414,"status":"ok","timestamp":1649503817654,"user":{"displayName":"Damla","userId":"03285166568766987047"},"user_tz":-120},"id":"lR6bSlXTPvqK"},"outputs":[],"source":["from sparknlp.functions import map_annotations_col\n","\n","chunks_df = map_annotations_col(documents_DF.select(\"document\",\"ner_chunk\"),\n","                    lambda x: [Annotation('chunk', a.begin, a.end, a.result, a.metadata, a.embeddings) for a in x], \"document\",\n","                    \"chunk_date\", \"chunk\")"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4965,"status":"ok","timestamp":1649503825790,"user":{"displayName":"Damla","userId":"03285166568766987047"},"user_tz":-120},"id":"3NqyhuNCQWC2","outputId":"ad694c26-af7c-4195-cc87-164843316359"},"outputs":[{"name":"stdout","output_type":"stream","text":["+---------------------------------------------------+\n","|chunk_date                                         |\n","+---------------------------------------------------+\n","|[{chunk, 0, 9, 08/02/2018, {sentence -> 0}, []}]   |\n","|[{chunk, 0, 6, 11/2018, {sentence -> 0}, []}]      |\n","|[{chunk, 0, 9, 11/01/2018, {sentence -> 0}, []}]   |\n","|[{chunk, 0, 8, 12Mar2021, {sentence -> 0}, []}]    |\n","|[{chunk, 0, 11, Jan 30, 2018, {sentence -> 0}, []}]|\n","|[{chunk, 0, 9, 13.04.1999, {sentence -> 0}, []}]   |\n","|[{chunk, 0, 10, 3April 2020, {sentence -> 0}, []}] |\n","+---------------------------------------------------+\n","\n"]}],"source":["chunks_df.select('chunk_date').show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"iE1wA-Y1QyeI"},"source":["Now we going to normalize that chunks using the DateNormalizer."]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":613,"status":"ok","timestamp":1649503836080,"user":{"displayName":"Damla","userId":"03285166568766987047"},"user_tz":-120},"id":"k9E0T8mjQo0Y"},"outputs":[],"source":["date_normalizer = DateNormalizer().setInputCols('chunk_date').setOutputCol('date')"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":511,"status":"ok","timestamp":1649503839561,"user":{"displayName":"Damla","userId":"03285166568766987047"},"user_tz":-120},"id":"JVX_QlglR9yR"},"outputs":[],"source":["date_normalized_df = date_normalizer.transform(chunks_df)"]},{"cell_type":"markdown","metadata":{"id":"L8IxAZxXRihE"},"source":["We going to show how the date is normalized."]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1569,"status":"ok","timestamp":1649503842733,"user":{"displayName":"Damla","userId":"03285166568766987047"},"user_tz":-120},"id":"LB2yoHDqRVy-","outputId":"383818d3-c43d-4e53-e0e7-5de894399b12"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------------+----------+--------+\n","|ner_chunk   |dateresult|metadata|\n","+------------+----------+--------+\n","|08/02/2018  |2018/08/02|true    |\n","|11/2018     |2018/11/DD|true    |\n","|11/01/2018  |2018/11/01|true    |\n","|12Mar2021   |2021/03/12|true    |\n","|Jan 30, 2018|2018/01/30|true    |\n","|13.04.1999  |1999/04/13|true    |\n","|3April 2020 |2020/04/03|true    |\n","+------------+----------+--------+\n","\n"]}],"source":["dateNormalizedClean = date_normalized_df.selectExpr(\"ner_chunk\",\"date.result as dateresult\",\"date.metadata as metadata\")\n","\n","dateNormalizedClean.withColumn(\"dateresult\", dateNormalizedClean[\"dateresult\"]\n","                               .getItem(0)).withColumn(\"metadata\", dateNormalizedClean[\"metadata\"]\n","                                                       .getItem(0)['normalized']).show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"dwgt4QtgSf6g"},"source":["## Relative Date\n","\n","We can configure the `anchorDateYear`,`anchorDateMonth` and `anchorDateDay` for the relatives dates."]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":456,"status":"ok","timestamp":1649503860908,"user":{"displayName":"Damla","userId":"03285166568766987047"},"user_tz":-120},"id":"gxHErp-K7ssl"},"outputs":[],"source":["rel_dates = [\n","'next monday',\n","'today',\n","'next week'\n","]\n","\n","rel_dates_df = spark.createDataFrame(rel_dates,StringType()).toDF('ner_chunk')"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":545,"status":"ok","timestamp":1649503863552,"user":{"displayName":"Damla","userId":"03285166568766987047"},"user_tz":-120},"id":"JN4QVnrD7ssu"},"outputs":[],"source":["rel_documents_DF = document_assembler.transform(rel_dates_df)\n","\n","rel_chunks_df = map_annotations_col(rel_documents_DF.select(\"document\",\"ner_chunk\"),\n","                    lambda x: [Annotation('chunk', a.begin, a.end, a.result, a.metadata, a.embeddings) for a in x], \"document\",\n","                    \"chunk_date\", \"chunk\")"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1052,"status":"ok","timestamp":1649503867235,"user":{"displayName":"Damla","userId":"03285166568766987047"},"user_tz":-120},"id":"QYnWQMSj7ssw","outputId":"b3b1a223-552f-4722-e05c-66028fe0ca0d"},"outputs":[{"name":"stdout","output_type":"stream","text":["+--------------------------------------------------+\n","|chunk_date                                        |\n","+--------------------------------------------------+\n","|[{chunk, 0, 10, next monday, {sentence -> 0}, []}]|\n","|[{chunk, 0, 4, today, {sentence -> 0}, []}]       |\n","|[{chunk, 0, 8, next week, {sentence -> 0}, []}]   |\n","+--------------------------------------------------+\n","\n"]}],"source":["rel_chunks_df.select('chunk_date').show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"-WrTv3SLTIe7"},"source":["In the following example we will use as a relative date 2021/02/15, to make that possible we need to set up the `anchorDateYear` to 2021, the `anchorDateMonth` to 2 and the `anchorDateDay` to 16. We will show you the configuration with the following example."]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":474,"status":"ok","timestamp":1649503881932,"user":{"displayName":"Damla","userId":"03285166568766987047"},"user_tz":-120},"id":"zttFZDgJSdZi"},"outputs":[],"source":["rel_date_normalizer = DateNormalizer().setInputCols('chunk_date').setOutputCol('date')\\\n","    .setAnchorDateDay(16)\\\n","    .setAnchorDateMonth(2)\\\n","    .setAnchorDateYear(2021)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1276,"status":"ok","timestamp":1649503893141,"user":{"displayName":"Damla","userId":"03285166568766987047"},"user_tz":-120},"id":"HvBFcvVnUge3","outputId":"e89b0a6e-9603-41ee-b4db-8cbc3b809361"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------+----------+--------+\n","|ner_chunk  |dateresult|metadata|\n","+-----------+----------+--------+\n","|next monday|2021/02/22|true    |\n","|today      |2021/02/16|true    |\n","|next week  |2021/02/23|true    |\n","+-----------+----------+--------+\n","\n"]}],"source":["rel_date_normalized_df = rel_date_normalizer.transform(rel_chunks_df)\n","relDateNormalizedClean = rel_date_normalized_df.selectExpr(\"ner_chunk\",\"date.result as dateresult\",\"date.metadata as metadata\")\n","relDateNormalizedClean.withColumn(\"dateresult\", relDateNormalizedClean[\"dateresult\"].getItem(0))\\\n","                      .withColumn(\"metadata\", relDateNormalizedClean[\"metadata\"].getItem(0)['normalized']).show(truncate=False)"]},{"cell_type":"markdown","metadata":{"id":"HiQke1C2VGuH"},"source":["As you see the relatives dates like `next monday` , `today` and `next week` takes the `2021/02/16` as reference date.\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"25.Date_Normalizer.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"nbformat":4,"nbformat_minor":0}
