{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nlp.johnsnowlabs.com/assets/images/logo.png\" width=\"180\" height=\"50\" style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global DEMO version 2.3.4\n",
    "\n",
    "## Example for Named Entity Recognition with Assertion Pipeline\n",
    "\n",
    "A common NLP problem in biomedical aplications is to identify the presence of clinical entities in a given text. This clinical entities could be diseases, symptoms, drugs, results of clinical investigations or others.\n",
    "\n",
    "But just identifying the presence of a clinical entity in an unestructured content is not enough for most of real world applications. As clinical care is full of uncertainty, in practice many of the entities refered in a medical record will not be really present in the patient but are mentioned just as working hypothesis, or identify a condition that want to be ruled out by means of a complementary test, or a condition being prevented by an intervention (for instance \"patient was vaccinated against hepatitis B\" does not imply that patient suffering from hepatitis B). In other cases a disease is mentioned associated with a relative of the patient (as in \"Father with Alzheimer disease\") as those family history is a risk factor in diseases with a genetic component.\n",
    "\n",
    "In order to extract this information from the content the Spark-NLP enterprise version includes an Assertion annotator that based in a Machine Learning pretrained model will assign, for every entity identified, a tag that informs about the nature of that entity in terms of certainty: \"present\", \"absent\", \"hypothesis\", \"conditional\", \"associated_with_other_person\", etc.\n",
    "\n",
    "In this example we will use Spark-NLP to identify some entities present in a a list of sentences adding an assertion about their certainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Prepare the environment\n",
    "\n",
    "#### Install OpenSource spark-nlp and pyspark pip packages\n",
    "As a first step we import the required python dependences including some sparknlp components.\n",
    "\n",
    "Be sure that you have the required python libraries (pyspark 2.4.4, spark-nlp 2.3.4) by running <code>pip list</code>. Check that the versions are correct.\n",
    "\n",
    "If some of them is missing you can run:\n",
    "\n",
    "<code>pip install --ignore-installed pyspark==2.4.4</code><br>\n",
    "<code>pip install --ignore-installed spark-nlp==2.3.4</code><br>\n",
    "\n",
    "The --ignore-installed parameter is to overwrite your previous pip package version if already installed.\n",
    "\n",
    "<i>*If this cell fails means you have not propertly setup the required environment. Please check the pre-requisites guideline at http://www.johnsnowlabs.com</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, time, os\n",
    "\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.util import *\n",
    "from sparknlp.embeddings import *\n",
    "\n",
    "from sparknlp.embeddings import EmbeddingsHelper\n",
    "from sparknlp.pretrained import ResourceDownloader\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Licensed Sparl-NLP package\n",
    "\n",
    "We will use also some Spark-NLP enterprise functionalities contained in the spark-nlp-jsl package.\n",
    "\n",
    "You can check that spark-nlp-jsl is installed by running <code>pip install</code>. Check that version installed is 2.3.4.\n",
    "\n",
    "If it is not then you need to install it by using:\n",
    "\n",
    "<code>pip install spark-nlp-jsl==2.3.4 --extra-index-url #### --ignore-installed</code>\n",
    "\n",
    "The #### is a secret code, if you have not received it please contact us at info@johnsnowlabs.com.\n",
    "\n",
    "<i>*If the next cell fails means your licensed enterprise version is not propertly installed so please check the pre-requisites guideline at http://www.johnsnowlabs.com/</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this fails, means pip module for enterprise has not been properly set up\n",
    "\n",
    "from sparknlp_jsl.annotator import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup credentials to private JohnSnowLabs models repository with AWS-CLI\n",
    "\n",
    "Now is time to configure Spark-NLP in order to access private JohnSnowLabs models repository. This access is done via Amazon aws command line interface (AWSCLI).\n",
    "\n",
    "Instructions about how to install awscli are available at: \n",
    "\n",
    "https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html\n",
    "\n",
    "Make sure you configure your credentials with <code>aws configure</code> following the instructions at:\n",
    "\n",
    "https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html\n",
    "\n",
    "Please substitute the ACCESS_KEY and SECRET_KEY with the credentials you have recived. If you need your credentials contact us at info@johnsnowlabs.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Spark session\n",
    "\n",
    "The following will initialize the spark session in case you have run the jupyter notebook directly. If you have started the notebook using pyspark this cell is just ignored.\n",
    "\n",
    "Initializing the spark session takes some seconds (usually less than 1 minute) as the jar from the server needs to be loaded.\n",
    "\n",
    "We will be using version 2.3.4 of Spark NLP Open Source and 2.3.4 of Spark NLP Enterprise Edition.\n",
    "\n",
    "The #### in <code>.config(\"spark.jars\", \"####\")</code> is a secret code, if you have not received it please contact us at info@johnsnowlabs.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will be ignored if jupyter started using pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Global DEMO - Spark NLP Enterprise 2.3.4\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",\"4G\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
    "    .config(\"spark.jars.packages\", \"JohnSnowLabs:spark-nlp:2.3.4\") \\\n",
    "    .config(\"spark.jars\", \"#####/spark-nlp-jsl-2.3.4.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Clinical NER Pipeline creation\n",
    "\n",
    "In Spark-NLP annotating NLP happens through pipelines. Pipelines are made out of various Annotator steps. In our case the architecture of the Clinical Named Entity Recognition pipeline with Assertion will be:\n",
    "\n",
    "* DocumentAssembler (text -> document)\n",
    "* SentenceDetector (document -> sentence)\n",
    "* Tokenizer (sentence -> token)\n",
    "* WordEmbeddingsModel ([sentence, token] -> embeddings)\n",
    "* NerDLModel ([sentence, token, embeddings] -> ner)\n",
    "* NerConverter([sentence, token, ner] -> ner_chunk)\n",
    "* AssertionLogRegModel ([sentence, ner_chunk, embeddings] -> assertion)\n",
    "\n",
    "So from a text we end having a list of Named Entities (Patient problems, Treatments and Tests) along with their certainty assertion tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1 Initialize all the annotators required by the pipeline\n",
    "\n",
    "The first 3 annotators of the pipeline are \"DocumentAssembler\", \"SentenceDectector\" and \"Tokenizer\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotator that transforms a text column from dataframe into an Annotation ready for NLP\n",
    "\n",
    "documentAssembler = DocumentAssembler()\\\n",
    "  .setInputCol(\"text\")\\\n",
    "  .setOutputCol(\"document\")\n",
    "\n",
    "# Sentence Detector annotator, processes various sentences per line\n",
    "\n",
    "sentenceDetector = SentenceDetector()\\\n",
    "  .setInputCols([\"document\"])\\\n",
    "  .setOutputCol(\"sentence\")\n",
    "\n",
    "# Tokenizer splits words in a relevant format for NLP\n",
    "\n",
    "tokenizer = Tokenizer()\\\n",
    "  .setInputCols([\"sentence\"])\\\n",
    "  .setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fourth annotator in the pipeline is \"WordEmbeddingsModel\". We will download a pretrained model available from \"clinical/models\" named \"embeddings_clinical\".\n",
    "\n",
    "When running this cell your are advised to be patient. \n",
    "\n",
    "First time you call this pretrained model it needs to be downloaded in your local.\n",
    "\n",
    "The model size is about will download the embeddings_clinical corpus it takes a while.\n",
    "\n",
    "The size is about 1.7Gb and will be saved typically in your home folder as\n",
    "\n",
    "    ~HOMEFOLDER/cached_models/embeddings_clinical_en_2.0.2_2.4_1558454742956.zip\n",
    "\n",
    "Next times you call it the model is loaded from your cached copy but even in that case it needs to be indexed each time so expect waiting up to 5 minutes (depending on your machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings_clinical download started this may take some time.\n",
      "Approximate size to download 1.6 GB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = WordEmbeddingsModel.pretrained(\"embeddings_clinical\", \"en\", \"clinical/models\")\\\n",
    "  .setInputCols([\"sentence\", \"token\"])\\\n",
    "  .setOutputCol(\"embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next annotator in our pipeline is the pretrained \"ner_clinical\" NerDLModel avaliable from \"clinical/models\". It requires as input the \"sentence\", \"token\" and \"embeddings\" (clinical embeddings pretrained model) and will classify each token in four categories:\n",
    "<ol>\n",
    "    <li>PROBLEM: for patient problems</li>\n",
    "    <li>TEST: for tests, labs, etc.</li>\n",
    "    <li>TREATMENT: for treatments, medicines, etc.</li>\n",
    "    <li>OTHER: for the rest of tokens.</li>\n",
    "</ol>\n",
    "\n",
    "In order to split those identified NER that are consecutive, the B prefix (as B-PROBLEM) will be used at the first token of each NER. The I prefix (as I-PROBLEM) will be used for the rest of tokens inside the NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_clinical_noncontrib download started this may take some time.\n",
      "Approximate size to download 13.9 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition for clinical concepts. Includes #Problems #Diagnostics\n",
    "\n",
    "#switch to ner_clinical instead of _noncontrib for better performance, if you are in Linux or MAC\n",
    "clinical_ner = NerDLModel.pretrained(\"ner_clinical_noncontrib\", \"en\", \"clinical/models\") \\\n",
    "  .setInputCols([\"sentence\", \"token\", \"embeddings\"]) \\\n",
    "  .setOutputCol(\"ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Assertion annotator requires as an input the NER entities in a chunked format so we need the NerConverter annotator to generate that \"ner_chunk\" column in the Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition concepts parser, transforms entities into CHUNKS (required for next step: assertion status)\n",
    "\n",
    "ner_converter = NerConverter() \\\n",
    "  .setInputCols([\"sentence\", \"token\", \"ner\"]) \\\n",
    "  .setOutputCol(\"ner_chunk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the pretrained AssertionLogRegModel named \"assertion_ml\" is included. It will classify each named entity into its assertion type: \"present\", \"absent\", \"hypothetical\", \"conditional\", \"associated_with_other_person\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assertion_ml download started this may take some time.\n",
      "Approximate size to download 128.5 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Assertion Status, verifies whether a particular subject wears a condition or not, and labels the condition by status\n",
    "\n",
    "assertion_ml = AssertionLogRegModel.pretrained(\"assertion_ml\", \"en\", \"clinical/models\") \\\n",
    "  .setInputCols([\"sentence\", \"ner_chunk\", \"embeddings\"]) \\\n",
    "  .setOutputCol(\"assertion\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2 Define the NER pipeline\n",
    "\n",
    "Now we will define the actual pipeline that puts together the annotators we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up the pipeline\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "    documentAssembler,\n",
    "    sentenceDetector,\n",
    "    tokenizer,\n",
    "    word_embeddings,\n",
    "    clinical_ner,\n",
    "    ner_converter,\n",
    "    assertion_ml\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Create a SparkDataFrame with the content\n",
    "\n",
    "Now we will create a sample Spark dataframe with some sentences. In production environments a table with several of those sentences could be distributed in a cluster and be run in large scale systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+\n",
      "|text                                                            |\n",
      "+----------------------------------------------------------------+\n",
      "|Patient with severe feber and sore throat                       |\n",
      "|Patient shows no stomach pain                                   |\n",
      "|She was maintained on an epidural and PCA for pain control.     |\n",
      "|He also became short of breath with climbing a flight of stairs.|\n",
      "|Lung tumour located at the right lower lobe                     |\n",
      "|Father with Alzheimer.                                          |\n",
      "+----------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We want to know more about this simple dataframe\n",
    "\n",
    "data = spark.createDataFrame([\n",
    "  [\"Patient with severe feber and sore throat\"],\n",
    "  [\"Patient shows no stomach pain\"],\n",
    "  [\"She was maintained on an epidural and PCA for pain control.\"],\n",
    "  [\"He also became short of breath with climbing a flight of stairs.\"],\n",
    "  [\"Lung tumour located at the right lower lobe\"],\n",
    "  [\"Father with Alzheimer.\"]\n",
    "]).toDF(\"text\")\n",
    "\n",
    "data.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Create a model fiting the NER pipeline with the clinical note.\n",
    "\n",
    "Now we can use the pipeline and the sentences to generate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We convert the pipeline into a model, train any annotator if required (not the case)\n",
    "\n",
    "model = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 Transform/annotate the sentences using the model.\n",
    "\n",
    "In order to process the data with the new created model we apply a transformation.\n",
    "\n",
    "This will save in a Spakr DataFrame (output) the resuls of running the model over the clinical note. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets print a column with the Named Entities chunked and a column with the assertion classification assigned by the model.\n",
    "\n",
    "We see for example that in the sentence \"Patient shows no stomach pain\", the sympton \"stomach pain\" has been identified but correctly asserted as \"absent\".\n",
    "\n",
    "In the case of \"She was maintained on an epidural and PCA for pain control.\" the entity \"pain control\" has been identified and asserted as \"hypothetical\". In this case the fact that the PCA effectively controled pain is not completely certain, therefore the entity is marked as an hypothesis. However the presence of an epidural procedure and a PCA are considered as certain and asserted as \"present\".\n",
    "\n",
    "In the case of \"Father with Alzheimer\" the Assertion annotator is able to identify that this condition is associated not with the patient, but with a relative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------------------------------+\n",
      "|result                         |result                        |\n",
      "+-------------------------------+------------------------------+\n",
      "|[severe feber, sore throat]    |[present, present]            |\n",
      "|[short of breath]              |[conditional]                 |\n",
      "|[an epidural, pain control]    |[present, hypothetical]       |\n",
      "|[stomach pain]                 |[absent]                      |\n",
      "|[Alzheimer]                    |[associated_with_someone_else]|\n",
      "|[Lung tumour, right lower lobe]|[present, present]            |\n",
      "+-------------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.select(\"ner_chunk.result\", \"assertion.result\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
