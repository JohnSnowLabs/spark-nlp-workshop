{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://nlp.johnsnowlabs.com/assets/images/logo.png\" width=\"180\" height=\"50\" style=\"float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Deep Learning NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we walk-through an evaluation for a Deep Learning NER. We will check metrics such as accuracy and training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Call necessary imports and set the resource path to read local data files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.eval import *\n",
    "\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Gather the required files**\n",
    "\n",
    "We will require a file with labeled NER entities. This file is used to evaluate the model. So, itâ€™s used for prediction and the labels as ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"./eng.testb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We can also use a directory with several test files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Evaluating Deep Learning NER Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just load a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dl = NerDLModel.pretrained()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just make use of *NerDLEvaluation* component to compute the accuracy. Make sure you are running MLflow UI to check the results locally on the browser. Check how to setup MLflow UI [here](https://github.com/JohnSnowLabs/spark-nlp#eval-module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerDlEvaluation = NerDLEvaluation(spark, test_file)\n",
    "nerDlEvaluation.computeAccuracyModel(ner_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running Mlflow UI you will see an output like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Norvig Eval Model](images/NERPretrainedEval.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Evaluating Deep Learning NER Annotator**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we need to include the files for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = \"./eng.train\"\n",
    "embeddings_file = \"./embeddings.100d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we just define the required annotators and its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = WordEmbeddings() \\\n",
    "            .setInputCols([\"document\", \"token\"]) \\\n",
    "            .setOutputCol(\"embeddings\") \\\n",
    "            .setEmbeddingsSource(embeddings_file, 100, \"TEXT\")\n",
    "            \n",
    "ner_approach = NerDLApproach() \\\n",
    "      .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\n",
    "      .setLabelColumn(\"label\") \\\n",
    "      .setOutputCol(\"ner\") \\\n",
    "      .setMaxEpochs(10) \\\n",
    "      .setRandomSeed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And invoke the required method from eval module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nerDlEvaluation = NerDLEvaluation(spark, test_file)\n",
    "nerDlEvaluation.computeAccuracyAnnotator(train_file, ner_approach, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running Mlflow UI you will see an output like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image of Norvig Eval Annotator](images/NERAnnotatorEval.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
