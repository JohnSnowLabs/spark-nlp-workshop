{"cells":[{"cell_type":"markdown","metadata":{"id":"2vXYNX2lQROB"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/jupyter/transformers/HuggingFace%20in%20Spark%20NLP%20-%20BERT.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"Zva6MvJyLeWi"},"source":["## Import BERT models from HuggingFace ðŸ¤—  into Spark NLP ðŸš€ \n","\n","Let's keep in mind a few things before we start ðŸ˜Š \n","\n","- This feature is only in `Spark NLP 3.1.x` and after. So please make sure you have upgraded to the latest Spark NLP release\n","- You can import models for BERT from HuggingFace but they have to be compatible with `TensorFlow` and they have to be in `Fill Mask` category. Meaning, you cannot use BERT models trained/fine-tuned on a specific task such as token/sequence classification."]},{"cell_type":"markdown","metadata":{"id":"MzxB-Nq6cxOA"},"source":["## Export and Save HuggingFace model"]},{"cell_type":"markdown","metadata":{"id":"yNQkhyMHMgkE"},"source":["- Let's install `HuggingFace` and `TensorFlow`. You don't need `TensorFlow` to be installed for Spark NLP, however, we need it to load and save models from HuggingFace.\n","- We lock TensorFlow on `2.4.1` version and Transformers on `4.6.1`. This doesn't mean it won't work with the future releases, but we wanted you to know which versions have been tested successfully."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":50264,"status":"ok","timestamp":1622377416683,"user":{"displayName":"Maziyar Panahi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64","userId":"06037986691777662786"},"user_tz":-120},"id":"hHXgqiWpMfCY","outputId":"20c8fbd9-caf2-4810-aa7b-b233bea166fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 394.3MB 39kB/s \n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.8MB 37.5MB/s \n","\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 471kB 33.2MB/s \n","\u001b[?25h"]}],"source":["!pip install -q transformers==4.6.1 tensorflow==2.4.1"]},{"cell_type":"markdown","metadata":{"id":"Y3AM6bj4P3NS"},"source":["- HuggingFace comes with a native `saved_model` feature inside `save_pretrained` function for TensorFlow based models. We will use that to save it as TF `SavedModel`.\n","- We'll use [bert-base-cased](https://huggingface.co/bert-base-cased) model from HuggingFace as an example\n","- In addition to `TFBertModel` we also need to save the `BertTokenizer`. This is the same for every model, these are assets needed for tokenization inside Spark NLP."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36261,"status":"ok","timestamp":1622378724253,"user":{"displayName":"Maziyar Panahi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64","userId":"06037986691777662786"},"user_tz":-120},"id":"ZaiirlSKNhVD","outputId":"39a20147-83a6-4877-a6f2-0ccab49518b6"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some layers from the model checkpoint at bert-base-cased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n","- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the layers of TFBertModel were initialized from the model checkpoint at bert-base-cased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"]},{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n","WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 1055). These functions will not be directly callable after loading.\n","WARNING:absl:Found untraced functions such as embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, pooler_layer_call_fn while saving (showing 5 of 1055). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: ./bert-base-cased/saved_model/1/assets\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Assets written to: ./bert-base-cased/saved_model/1/assets\n"]}],"source":["from transformers import TFBertModel, BertTokenizer \n","\n","MODEL_NAME = 'bert-base-cased'\n","\n","tokenizer = BertTokenizer.from_pretrained(MODEL_NAME).save_pretrained('./{}_tokenizer/'.format(MODEL_NAME))\n","\n","# just in case if there is no TF/Keras file provided in the model\n","# we can just use `from_pt` and convert PyTorch to TensorFlow\n","try:\n","  print('try downloading TF weights')\n","  model = TFBertModel.from_pretrained(MODEL_NAME)\n","except:\n","  print('try downloading PyTorch weights')\n","  model = TFBertModel.from_pretrained(MODEL_NAME, from_pt=True)\n","\n","model.save_pretrained(\"./{}\".format(MODEL_NAME), saved_model=True)"]},{"cell_type":"markdown","metadata":{"id":"nlgyZuJfS5IB"},"source":["Let's have a look inside these two directories and see what we are dealing with:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":207,"status":"ok","timestamp":1622378877133,"user":{"displayName":"Maziyar Panahi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64","userId":"06037986691777662786"},"user_tz":-120},"id":"p2XCole7TTef","outputId":"8ea9c871-7096-40f2-df38-3bcc7fe7cd07"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 423352\n","-rw-r--r-- 1 root root       597 May 30 12:45 config.json\n","drwxr-xr-x 3 root root      4096 May 30 12:45 saved_model\n","-rw-r--r-- 1 root root 433503400 May 30 12:45 tf_model.h5\n"]}],"source":["!ls -l {MODEL_NAME}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":218,"status":"ok","timestamp":1622378887608,"user":{"displayName":"Maziyar Panahi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64","userId":"06037986691777662786"},"user_tz":-120},"id":"r0DOGz8VUR-r","outputId":"1b6e7e04-3655-49f9-ed2d-25a6928f19bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 7768\n","drwxr-xr-x 2 root root    4096 May 30 12:45 assets\n","-rw-r--r-- 1 root root 7944426 May 30 12:45 saved_model.pb\n","drwxr-xr-x 2 root root    4096 May 30 12:45 variables\n"]}],"source":["!ls -l {MODEL_NAME}/saved_model/1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":185,"status":"ok","timestamp":1622378898534,"user":{"displayName":"Maziyar Panahi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64","userId":"06037986691777662786"},"user_tz":-120},"id":"Mcm2UpNxUUQN","outputId":"0c1f5eef-c75a-4f7d-ce44-0cc9b99a1095"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 220\n","-rw-r--r-- 1 root root    112 May 30 12:44 special_tokens_map.json\n","-rw-r--r-- 1 root root    521 May 30 12:44 tokenizer_config.json\n","-rw-r--r-- 1 root root 213450 May 30 12:44 vocab.txt\n"]}],"source":["!ls -l {MODEL_NAME}_tokenizer"]},{"cell_type":"markdown","metadata":{"id":"gZegMvuGTmHt"},"source":["- as you can see, we need the SavedModel from `saved_model/1/` path\n","- we also be needing `vocab.txt` from the tokenizer\n","- all we need is to just copy the `vocab.txt` to `saved_model/1/assets` which Spark NLP will look for"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ez6MT-RTT7ss"},"outputs":[],"source":["!cp {MODEL_NAME}_tokenizer/vocab.txt {MODEL_NAME}/saved_model/1/assets"]},{"cell_type":"markdown","metadata":{"id":"NlJKd2tIU0PD"},"source":["## Import and Save BERT in Spark NLP\n"]},{"cell_type":"markdown","metadata":{"id":"A0FXoxHJc5CU"},"source":["- Let's install and setup Spark NLP in Google Colab\n","- This part is pretty easy via our simple script"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8tpW5nkMc53m"},"outputs":[],"source":["! wget http://setup.johnsnowlabs.com/colab.sh -O - | bash"]},{"cell_type":"markdown","metadata":{"id":"m_NAgx4hdCGP"},"source":["Let's start Spark with Spark NLP included via our simple `start()` function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cbNneAVCLU1y"},"outputs":[],"source":["import sparknlp\n","# let's start Spark with Spark NLP\n","spark = sparknlp.start()"]},{"cell_type":"markdown","metadata":{"id":"ABTu9MrdVafM"},"source":["- Let's use `loadSavedModel` functon in `BertEmbeddings` which allows us to load TensorFlow model in SavedModel format\n","- Most params can be set later when you are loading this model in `BertEmbeddings` in runtime, so don't worry what you are setting them now\n","- `loadSavedModel` accepts two params, first is the path to the TF SavedModel. The second is the SparkSession that is `spark` variable we previously started via `sparknlp.start()`\n","- `setStorageRef` is very important. When you are training a task like NER or any Text Classification, we use this reference to bound the trained model to this specific embeddings so you won't load a different embeddings by mistake and see terrible results ðŸ˜Š\n","- It's up to you what you put in `setStorageRef` but it cannot be changed later on. We usually use the name of the model to be clear, but you can get creative if you want! \n","- The `dimension` param is is purely cosmetic and won't change anything. It's mostly for you to know later via `.getDimension` what is the dimension of your model. So set this accordingly.\n","- NOTE: `loadSavedModel` accepts local paths in addition to distributed file systems such as `HDFS`, `S3`, `DBFS`, etc. This feature was introduced in Spark NLP 4.2.2 release. Keep in mind the best and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively.st and recommended way to move/share/reuse Spark NLP models is to use `write.save` so you can use `.load()` from any file systems natively..\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8W_almibVRTj"},"outputs":[],"source":["from sparknlp.annotator import *\n","\n","bert = BertEmbeddings.loadSavedModel(\n","     '{}/saved_model/1'.format(MODEL_NAME),\n","     spark\n"," )\\\n"," .setInputCols([\"sentence\",'token'])\\\n"," .setOutputCol(\"bert\")\\\n"," .setCaseSensitive(True)\\\n"," .setDimension(768)\\\n"," .setStorageRef('bert_base_cased') "]},{"cell_type":"markdown","metadata":{"id":"PjGiq4KnXWuy"},"source":["- Let's save it on disk so it is easier to be moved around and also be used later via `.load` function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iWu5HfbnXAlM"},"outputs":[],"source":["bert.write().overwrite().save(\"./{}_spark_nlp\".format(MODEL_NAME))"]},{"cell_type":"markdown","metadata":{"id":"QCrjxPhzDplN"},"source":["Let's clean up stuff we don't need anymore"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZgkVIJshDtLx"},"outputs":[],"source":["!rm -rf {MODEL_NAME}_tokenizer {MODEL_NAME}"]},{"cell_type":"markdown","metadata":{"id":"-TSeTRZpXqWO"},"source":["Awesome ðŸ˜Ž  !\n","\n","This is your BERT model from HuggingFace ðŸ¤—  loaded and saved by Spark NLP ðŸš€ "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":515,"status":"ok","timestamp":1622379928489,"user":{"displayName":"Maziyar Panahi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64","userId":"06037986691777662786"},"user_tz":-120},"id":"ogpxSWxOXj3W","outputId":"27c4c504-dee4-4acd-b1a7-c0bd64623130"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 393292\n","-rw-r--r-- 1 root root 402718696 May 30 13:04 bert_tensorflow\n","drwxr-xr-x 4 root root      4096 May 30 13:02 fields\n","drwxr-xr-x 2 root root      4096 May 30 13:02 metadata\n"]}],"source":["! ls -l {MODEL_NAME}_spark_nlp"]},{"cell_type":"markdown","metadata":{"id":"Fbehje7fYTDj"},"source":["Now let's see how we can use it on other machines, clusters, or any place you wish to use your new and shiny BERT model ðŸ˜Š "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mm3CvkwYRgs"},"outputs":[],"source":["bert_loaded = BertEmbeddings.load(\"./{}_spark_nlp\".format(MODEL_NAME))\\\n","  .setInputCols([\"sentence\",'token'])\\\n","  .setOutputCol(\"bert\")\\\n","  .setCaseSensitive(True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1622380021828,"user":{"displayName":"Maziyar Panahi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhTmm4Srbdy-IOOALumHToD8y9PvjupF566HEz1zA=s64","userId":"06037986691777662786"},"user_tz":-120},"id":"pGRTNISyYlnO","outputId":"d1b7b9a5-d194-4fda-ba7c-173a163021ad"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'bert_base_cased'"]},"execution_count":23,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["bert_loaded.getStorageRef()"]},{"cell_type":"markdown","metadata":{"id":"_he2LDtBYo1h"},"source":["That's it! You can now go wild and use hundreds of BERT models from HuggingFace ðŸ¤— in Spark NLP ðŸš€ \n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"HuggingFace in Spark NLP - BERT.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"nbformat":4,"nbformat_minor":0}