{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PA-GQ-icbc4l"
   },
   "source": [
    "# Description\n",
    "## This notebok provides set of commands to install Spark NLP for offline usage. It contains 4 sections:\n",
    "\n",
    "0) Initial setup\n",
    "\n",
    "1) Download all dependencies for Spark NLP\n",
    "\n",
    "2) Download all dependencies for Spark NLP (enterprise/licensed)\n",
    "\n",
    "3) Download all dependencies for Spark NLP OCR\n",
    "\n",
    "4) Download all models/embeddings for offline usage\n",
    "\n",
    "5) Example of NER\n",
    "\n",
    "`p.s: This notebook runned succesfully in:\n",
    "                Distributor ID: Ubuntu\n",
    "                Description:    Ubuntu 22.04.3 LTS\n",
    "                Release:        22.04`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQfDxcj_cHfB"
   },
   "source": [
    "## 0) Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "gksUrPmN6uk7",
    "outputId": "05e6b829-2faa-4a1e-f666-00b3279f0052"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['SPARK_NLP_LICENSE', 'SECRET', 'JSL_VERSION', 'SPARK_OCR_LICENSE', 'SPARK_OCR_SECRET', 'OCR_VERSION', 'PUBLIC_VERSION', 'AWS_ACCESS_KEY_ID', 'AWS_SECRET_ACCESS_KEY'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load and point the licence_keys as 'spark_jsl.json' file\n",
    "\n",
    "import json, os\n",
    "\n",
    "with open('spark_jsl.json') as f:\n",
    "    license_keys = json.load(f)\n",
    "\n",
    "\n",
    "locals().update(license_keys)\n",
    "os.environ.update(license_keys)\n",
    "license_keys.keys()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo apt-get update -qq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning processes...                                                           \n",
      "Scanning candidates...                                                          \n",
      "Scanning linux images...                                                        \n",
      "\n",
      "Running kernel seems to be up-to-date.\n",
      "\n",
      "Restarting services...\n"
     ]
    }
   ],
   "source": [
    "! sudo apt-get install -y openjdk-8-jdk-headless -qq > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "VKzKoFqYeuXV",
    "outputId": "e43e3d88-a357-4b0d-86ca-ecd14e4de0a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"1.8.0_392\"\n",
      "OpenJDK Runtime Environment (build 1.8.0_392-8u392-ga-1~22.04-b08)\n",
      "OpenJDK 64-Bit Server VM (build 25.392-b08, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/jvm/java-8-openjdk-amd64\n"
     ]
    }
   ],
   "source": [
    "print(os.environ[\"JAVA_HOME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "wFLhAlrrekgY",
    "outputId": "3645e311-51d5-4175-8562-0d80e1c33b72"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade -q pyspark==3.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "JcCaD_opjW2j",
    "outputId": "40ab71c2-7d13-4550-dc1a-1bc1de9aff3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark                3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\"\n",
    "! unzip awscliv2.zip  # sudo apt install unzip if not installed\n",
    "! sudo ./aws/install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Download all dependencies for Spark NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "c1c6LBDRi94D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-30 18:14:16--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-5.2.0.jar\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 16.182.38.160, 52.216.9.101, 52.216.93.221, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|16.182.38.160|:443... connected.\n",
      "HTTP request sent, awaiting response... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\n",
      "Length: 722128385 (689M) [application/java-archive]\n",
      "Saving to: ‘/usr/lib/spark/jars/spark-nlp-assembly-5.2.0.jar’\n",
      "\n",
      "spark-nlp-assembly- 100%[===================>] 688.67M  35.5MB/s    in 19s     \n",
      "\n",
      "2023-12-30 18:14:36 (35.5 MB/s) - ‘/usr/lib/spark/jars/spark-nlp-assembly-5.2.0.jar’ saved [722128385/722128385]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark-nlp jar\n",
    "!sudo wget  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/jars/spark-nlp-assembly-$PUBLIC_VERSION.jar -P /usr/lib/spark/jars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting spark-nlp==5.2.0\n",
      "  Downloading spark_nlp-5.2.0-py2.py3-none-any.whl (548 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.5/548.5 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: spark-nlp\n",
      "Successfully installed spark-nlp-5.2.0\n"
     ]
    }
   ],
   "source": [
    "# spark-nlp wheel\n",
    "! pip install spark-nlp==$PUBLIC_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark                3.4.1\n",
      "spark-nlp              5.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1lC_kgv0QU8"
   },
   "source": [
    "## 2) Download all dependencies for Spark NLP (enterprise/licensed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "MVP0_TeTIVB9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-30 18:38:58--  https://s3.eu-west-1.amazonaws.com/pypi.johnsnowlabs.com/5.2.0-b05bf6cf9656ef4a0fe918a037481a8ef7245fcb/spark-nlp-jsl-5.2.0.jar\n",
      "Resolving s3.eu-west-1.amazonaws.com (s3.eu-west-1.amazonaws.com)... 52.92.19.224, 52.92.4.16, 52.218.56.11, ...\n",
      "Connecting to s3.eu-west-1.amazonaws.com (s3.eu-west-1.amazonaws.com)|52.92.19.224|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 40061499 (38M) [application/octet-stream]\n",
      "Saving to: ‘/usr/lib/spark/jars/spark-nlp-jsl-5.2.0.jar’\n",
      "\n",
      "spark-nlp-jsl-5.2.0 100%[===================>]  38.21M  19.2MB/s    in 2.0s    \n",
      "\n",
      "2023-12-30 18:39:00 (19.2 MB/s) - ‘/usr/lib/spark/jars/spark-nlp-jsl-5.2.0.jar’ saved [40061499/40061499]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark nlp JSL JAR\n",
    "!sudo wget https://s3.eu-west-1.amazonaws.com/pypi.johnsnowlabs.com/$SECRET/spark-nlp-jsl-$JSL_VERSION.jar -P /usr/lib/spark/jars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Re8Nz55gGINz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-30 18:41:09--  https://s3.eu-west-1.amazonaws.com/pypi.johnsnowlabs.com/5.2.0-b05bf6cf9656ef4a0fe918a037481a8ef7245fcb/spark-nlp-jsl/spark_nlp_jsl-5.2.0-py3-none-any.whl\n",
      "Resolving s3.eu-west-1.amazonaws.com (s3.eu-west-1.amazonaws.com)... 52.218.90.59, 52.218.36.218, 52.92.17.144, ...\n",
      "Connecting to s3.eu-west-1.amazonaws.com (s3.eu-west-1.amazonaws.com)|52.218.90.59|:443... "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 473226 (462K) [application/x-gzip]\n",
      "Saving to: ‘/usr/lib/spark/jars/spark_nlp_jsl-5.2.0-py3-none-any.whl’\n",
      "\n",
      "spark_nlp_jsl-5.2.0 100%[===================>] 462.13K  1.66MB/s    in 0.3s    \n",
      "\n",
      "2023-12-30 18:41:10 (1.66 MB/s) - ‘/usr/lib/spark/jars/spark_nlp_jsl-5.2.0-py3-none-any.whl’ saved [473226/473226]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# park nlp JSL wheel\n",
    "! sudo wget https://s3.eu-west-1.amazonaws.com/pypi.johnsnowlabs.com/$SECRET/spark-nlp-jsl/spark_nlp_jsl-$JSL_VERSION-py3-none-any.whl -P /usr/lib/spark/jars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "MZ9ZXoNZGZfv"
   },
   "outputs": [],
   "source": [
    "!pip install -q /usr/lib/spark/jars/spark_nlp_jsl-$JSL_VERSION-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "eMz1WnR-GbdS",
    "outputId": "db7697f5-9742-4da8-8c39-981701fd3810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark                3.4.1\n",
      "spark-nlp              5.2.0\n",
      "spark-nlp-jsl          5.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O40K3dk0HTTJ"
   },
   "source": [
    "## 3) Download all dependencies for Spark NLP OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "U2wt9j78CM2j"
   },
   "outputs": [],
   "source": [
    "# spark ocr JAR\n",
    "!sudo wget -q https://s3.eu-west-1.amazonaws.com/pypi.johnsnowlabs.com/$SPARK_OCR_SECRET/jars/spark-ocr-assembly-$OCR_VERSION.jar -P /usr/lib/spark/jars/\n",
    "\n",
    "#spark ocr wheel\n",
    "!sudo wget -q https://s3.eu-west-1.amazonaws.com/pypi.johnsnowlabs.com/$SPARK_OCR_SECRET/spark-ocr/spark_ocr-$OCR_VERSION-py3-none-any.whl -P /usr/lib/spark/jars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1231488\n",
      "-rw-r--r-- 1 root root 722128385 Dec  8 21:21 spark-nlp-assembly-5.2.0.jar\n",
      "-rw-r--r-- 1 root root  40061499 Dec 23 21:13 spark-nlp-jsl-5.2.0.jar\n",
      "-rw-r--r-- 1 root root 457626988 Nov 17 16:50 spark-ocr-assembly-5.1.0.jar\n",
      "-rw-r--r-- 1 root root    473226 Dec 23 21:13 spark_nlp_jsl-5.2.0-py3-none-any.whl\n",
      "-rw-r--r-- 1 root root  40736607 Nov 17 16:50 spark_ocr-5.1.0-py3-none-any.whl\n"
     ]
    }
   ],
   "source": [
    "!ls -l /usr/lib/spark/jars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "lvRdkgezH3ZG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spark-nlp-jsl 5.2.0 requires spark-nlp==5.2.0, but you have spark-nlp 5.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q /usr/lib/spark/jars/spark_ocr-$OCR_VERSION-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to fix version incompatibility run below again\n",
    "\n",
    "# pip install spark-nlp==$PUBLIC_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "_jay9Oo4H4vm",
    "outputId": "506068a1-6675-4107-a589-153d99cf0ac8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyspark                3.4.0\n",
      "spark-nlp              5.2.0\n",
      "spark-nlp-jsl          5.2.0\n",
      "spark-ocr              5.1.0\n"
     ]
    }
   ],
   "source": [
    "#sanity check\n",
    "!pip list | grep spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-8LX-epIq2N"
   },
   "source": [
    "## Installation completed. Let's download models using AWS keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlSvYKY4I2jk"
   },
   "source": [
    "## 4) Download all models/embeddings for offline usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example purposes let's download only subset for NER and glove\n",
    "!sudo aws s3 cp --region us-east-2 s3://auxdata.johnsnowlabs.com/public/models/ public_models/ --recursive --exclude \"*\" --include \"ner_dl*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "uwljAlhVKTPL",
    "outputId": "6c843e35-6f26-4caf-a87f-f4ff06bfad2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://auxdata.johnsnowlabs.com/public/models/glove_6B_100_xx_2.4.0_2.4_1579690037117.zip to public_models/glove_6B_100_xx_2.4.0_2.4_1579690037117.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/glove_100d_en_2.0.0_2.4_1553028251278.zip to public_models/glove_100d_en_2.0.0_2.4_1553028251278.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/glove_100d_en_2.0.2_2.4_1556534397055.zip to public_models/glove_100d_en_2.0.2_2.4_1556534397055.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/glove_100d_en_2.4.0_2.4_1579690104032.zip to public_models/glove_100d_en_2.4.0_2.4_1579690104032.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/glove_6B_300_xx_2.4.0_2.4_1579698630432.zip to public_models/glove_6B_300_xx_2.4.0_2.4_1579698630432.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/glove_6B_300_xx_2.0.2_2.4_1559059806004.zip to public_models/glove_6B_300_xx_2.0.2_2.4_1559059806004.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/glove_6B_300_xx_2.1.0_2.4_1564760779318.zip to public_models/glove_6B_300_xx_2.1.0_2.4_1564760779318.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/glove_840B_300_xx_2.0.2_2.4_1558645003344.zip to public_models/glove_840B_300_xx_2.0.2_2.4_1558645003344.zip\n",
      "download: s3://auxdata.johnsnowlabs.com/public/models/glove_840B_300_xx_2.4.0_2.4_1579698926752.zip to public_models/glove_840B_300_xx_2.4.0_2.4_1579698926752.zip\n"
     ]
    }
   ],
   "source": [
    "!sudo aws s3 cp --region us-east-2 s3://auxdata.johnsnowlabs.com/public/models/ public_models/ --recursive --exclude \"*\" --include \"glove*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-30 19:42:15--  https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/glove_100d_en_2.4.0_2.4_1579690104032.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.168.0, 54.231.226.88, 52.217.229.184, ...\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.168.0|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 152394105 (145M) [application/zip]\n",
      "Saving to: ‘models/glove_100d_en_2.4.0_2.4_1579690104032.zip’\n",
      "\n",
      "glove_100d_en_2.4.0 100%[===================>] 145.33M   104MB/s    in 1.4s    \n",
      "\n",
      "2023-12-30 19:42:16 (104 MB/s) - ‘models/glove_100d_en_2.4.0_2.4_1579690104032.zip’ saved [152394105/152394105]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!sudo wget  -P models/ https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/ner_dl_en_2.4.3_2.4_1584624950746.zip \n",
    "!sudo wget  -P models/ https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/glove_100d_en_2.4.0_2.4_1579690104032.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QixCIfYFKqXk"
   },
   "outputs": [],
   "source": [
    "# !sudo aws s3 cp --region us-east-2 s3://auxdata.johnsnowlabs.com/clinical/models/ clinical_models/ --recursive --exclude \"*\" --include \"embeddings_clinical*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "raLv3rlIyLUW"
   },
   "source": [
    "## 5) Example on NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SGI4DSrjLfhJ"
   },
   "outputs": [],
   "source": [
    "!unzip -q models/ner_dl_en_2.4.3_2.4_1584624950746.zip -d ner_dl_glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-uv0nAgB323j"
   },
   "outputs": [],
   "source": [
    "!unzip -q models/glove_100d_en_2.4.0_2.4_1579690104032.zip -d glove_embeddings/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "dSy25ADGyXGS"
   },
   "outputs": [],
   "source": [
    "ner_local_path = 'ner_dl_glove'\n",
    "embeddings_local_path = 'glove_embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OPBk1kOizK9u"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp_jsl.annotator import *\n",
    "from sparknlp.base import *\n",
    "import sparknlp_jsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1231488\n",
      "-rw-r--r-- 1 root root 722128385 Dec  8 21:21 spark-nlp-assembly-5.2.0.jar\n",
      "-rw-r--r-- 1 root root  40061499 Dec 23 21:13 spark-nlp-jsl-5.2.0.jar\n",
      "-rw-r--r-- 1 root root 457626988 Nov 17 16:50 spark-ocr-assembly-5.1.0.jar\n",
      "-rw-r--r-- 1 root root    473226 Dec 23 21:13 spark_nlp_jsl-5.2.0-py3-none-any.whl\n",
      "-rw-r--r-- 1 root root  40736607 Nov 17 16:50 spark_ocr-5.1.0-py3-none-any.whl\n"
     ]
    }
   ],
   "source": [
    "!ls -l /usr/lib/spark/jars/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FVzd5SJFzOAA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/30 19:40:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "def start():\n",
    "    builder = SparkSession.builder \\\n",
    "        .appName(\"Spark NLP Licensed\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"10G\") \\\n",
    "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "        .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "        .config(\"spark.jars\", \"/usr/lib/spark/jars/spark-nlp-assembly-5.2.0.jar,/usr/lib/spark/jars/spark-nlp-jsl-5.2.0.jar\")\n",
    "    return builder.getOrCreate()\n",
    "\n",
    "spark = start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRs4AIfry7dQ"
   },
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler()\\\n",
    "    .setInputCol(\"text\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\")\n",
    "\n",
    "# ner_dl model is trained with glove_100d. So we use the same embeddings in the pipeline\n",
    "glove_embeddings = WordEmbeddingsModel.load(embeddings_local_path).\\\n",
    "  setInputCols([\"document\", 'token']).\\\n",
    "  setOutputCol(\"embeddings\")\n",
    "\n",
    "# NER model trained on i2b2 (sampled from MIMIC) dataset\n",
    "public_ner = NerDLModel.load(ner_local_path) \\\n",
    "  .setInputCols([\"document\", \"token\", \"embeddings\"]) \\\n",
    "  .setOutputCol(\"ner\")\n",
    "\n",
    "nlpPipeline = Pipeline(stages=[\n",
    " documentAssembler, \n",
    " tokenizer,\n",
    " glove_embeddings,\n",
    " public_ner\n",
    " ])\n",
    "\n",
    "empty_df = spark.createDataFrame([['']]).toDF(\"text\")\n",
    "\n",
    "pipelineModel = nlpPipeline.fit(empty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "-F9F8e7CMgYM",
    "outputId": "09c1e8d6-b77f-4ebf-8aa7-b6eb2155660e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+-------------------------------------+\n",
      "|result                                  |result                               |\n",
      "+----------------------------------------+-------------------------------------+\n",
      "|[Peter, Parker, lives, in, New, York, .]|[B-PER, I-PER, O, O, B-LOC, I-LOC, O]|\n",
      "+----------------------------------------+-------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([['Peter Parker lives in New York.']]).toDF(\"text\")\n",
    "\n",
    "result = pipelineModel.transform(df)\n",
    "\n",
    "result.select('token.result','ner.result').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "xoboVMnO4KaD",
    "outputId": "a586aeb4-5431-42c6-b543-bcbba503e2cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Peter', 'B-PER'),\n",
       " ('Parker', 'I-PER'),\n",
       " ('lives', 'O'),\n",
       " ('in', 'O'),\n",
       " ('New', 'B-LOC'),\n",
       " ('York', 'I-LOC'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "light_model = LightPipeline(pipelineModel)\n",
    "\n",
    "text = 'Peter Parker lives in New York.'\n",
    "\n",
    "light_result = light_model.annotate(text)\n",
    "\n",
    "list(zip(light_result['token'], light_result['ner']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Echfpd-4jhc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "SparkNLP_offline_installation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
